filename,full_text
0019.pdf,"Decentralized Anomaly Detection in Cooperative Multi-Agent Reinforcement
Learning
Kiarash Kazari ,Ezzeldin Shereen ,Gy¨orgy D ´an
Division of Network and Systems Engineering, School of Electrical Engineering and Computer Science
KTH Royal Institute of Technology, Stockholm, Sweden
{kkazari, eshereen, gyuri}@kth.se
Abstract
We consider the problem of detecting adversarial
attacks against cooperative multi-agent reinforce-
ment learning. We propose a decentralized scheme
that allows agents to detect the abnormal behavior
of one compromised agent. Our approach is based
on a recurrent neural network (RNN) trained dur-
ing cooperative learning to predict the action distri-
bution of other agents based on local observations.
The predicted distribution is used for computing a
normality score for the agents, which allows the
detection of the misbehavior of other agents. To
explore the robustness of the proposed detection
scheme, we formulate the worst-case attack against
our scheme as a constrained reinforcement learning
problem. We propose to compute an attack policy
via optimizing the corresponding dual function us-
ing reinforcement learning. Extensive simulations
on various multi-agent benchmarks show the effec-
tiveness of the proposed detection scheme in de-
tecting state of the art attacks and in limiting the
impact of undetectable attacks.
1 Introduction
Multi-agent reinforcement learning (MARL) is emerging as
an important tool for solving various sequential decision
making problems in application areas like 5G networks, un-
manned aerial vehicle (UA V) swarms, autonomous driving,
power grid control, and Internet of things [Liet al., 2022;
Canese et al., 2021 ]. Inspired by how human beings learn
from trial and error, MARL utilizes the concept of rewards in
order to teach a team of agents to perform a certain sequential
task. The individual agents receive local observations and re-
wards from the environment, and use those to learn a policy
over their possible actions.
MARL problems can be cooperative, competitive, or a
mix between the two. In cooperative MARL, the agents
need to cooperate to achieve a common goal, thus typically
receiving a common reward, while in competitive MARL,
the agents are competing against each other. MARL has
been shown to perform better on multi-agent decision mak-
ing problems compared to centralized approaches, especially
when the problem is relatively complex [Canese et al., 2021 ].Despite the potential of MARL for solving complex de-
cision making problems, a prerequisite for its adoption is
that it should withstand faults and adversarial manipulations.
For instance, an adversary that compromises a MARL agent
could manipulate the agent to take sub-optimal actions, possi-
bly affecting other agents and the team reward. Alternatively,
an adversary that compromises the communication link be-
tween the environment and the agent could modify the agent’s
observations, causing the agent to take sub-optimal actions
that reduce the team reward [Linet al., 2020 ]. Adversarial
training, which relies on a known attack model for training
robustified ML models [Abusnaina et al., 2021 ], can mitigate
the impact of such adversarial manipulations to some extent,
but it does not provide situational awareness.
Thus, even if an MARL algorithm is robustified against
attacks, attack detection and identification are essential for
timely response, e.g., to be able to promptly evict victim
agents. Attack detection can also be used for improving
the robustness of MARL, e.g., by ensuring that non-victim
agents adjust their policy in a timely manner. Attack de-
tection schemes have thus far been designed for single-agent
RL, focusing on anomaly detection in the observations of an
agent [Sedlmeier et al., 2020; Zhang et al., 2021 ]. Anomaly
detection in the multi-agent setting has so far been addressed
in the control literature (e.g., [Shames et al., 2011 ]and [Ye
et al., 2019 ]), but these works assume a known dynamical
model of the environment and of the control policies of the
agents. Instead, in MARL, the policies are approximated
by neural networks and the environment is unknown, which
makes model-based detection of anomalous behavior infeasi-
ble. At the same time, it is unclear how to design model-free
approaches for anomaly detection that would scale to large
systems and enable decentralized operation.
In this paper, we propose a model-free approach for de-
tecting adversarial attacks against cooperative MARL. The
contributions of our paper are as follows:
1. We propose a decentralized detection scheme based on
training recurrent neural networks (RNNs) to predict the
distribution of actions of other agents conditional on lo-
cal observations, and use the trained predictors for com-
puting a normality score that quantifies the extent to
which agents behave as expected.
2. We propose a dynamic adversary that represents a worst-
Proceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)
162
case attack against our detection scheme and use it to
evaluate the robustness of our detector.
3. We carry out extensive simulations of our proposed
scheme as well as the worst-case attack utilizing dif-
ferent multi-agent benchmarks, and show that our de-
tection scheme can accurately detect adversarial attacks
and limit their impact.
2 Related Work
Most works concerning adversarial attacks against single-
agent RL revolve around perturbation of states (or observa-
tions). In these works, adversarial example generation algo-
rithms such as FGSM [Goodfellow et al., 2014 ]and JSMA
[Papernot et al., 2016 ]are used to generate fake observations
for the agent. Consequently, the agent will take suboptimal
actions as a result of the compromised observation of the en-
vironment´ s state. Among these papers, [Huang et al., 2017 ]
used FGSM to minimize the probability of taking the best ac-
tion by the victim. [Pattanaik et al., 2017 ]utilized the same
algorithm but to encourage selecting the worst action by the
agent. [Behzadan and Munir, 2017 ]exploits the transferabil-
ity of adversarial examples to implement an attack during the
training phase. [Russo and Proutiere, 2021 ]optimized the
attack policy with respect to the perturbation budget.
In the context of MARL, [Linet al., 2020 ]applied the idea
of perturbation of observations to attack one of the agents in
c-MARL. As the first step of the attack, they trained an adver-
sarial policy to minimize the long-term team reward. Then, in
the second step, a JSMA-based algorithm was used to make
the compromised agent follow that adversarial policy. At-
tacking the actions of an agent is another scenario, which is
particularly important in the multi-agent setting. [Gleave et
al., 2019 ]showed that in the competitive MARL, an adver-
sary can fool an agent by controlling another agent’s actions.
[Guo et al., 2022 ]is another work in this area, which inves-
tigated the robustness of state-of-the-art c-MARL algorithms
against attacks on both observations and actions.
Anomaly Detection for sequential data is the focus of
works such as [Oh and Iyengar, 2019; Malhotra et al., 2016;
Wang et al., 2021 ]. However, as argued in [M¨uller et
al., 2022 ], anomaly detection in RL has gained less atten-
tion compared to domains like video, audio, and text. In
this context, [Zhang et al., 2021 ]proposed a framework
to detect anomalous state observations based on a Gaus-
sian approximation of the state representation space. Au-
thors in [Sedlmeier et al., 2020 ]proposed an entropy-based
anomaly detector for the detection of out-of-distribution ob-
servations, though not in an adversarial setting. None of these
works addressed anomaly detection in a multi-agent setting,
and applying these single-agent schemes to MARL would ei-
ther require centralized tracking of all agents or implement-
ing anomaly detection locally (each agent for itself). The first
case might not be feasible in decentralized multi-agent sys-
tems, as collecting data from all agents is very resource in-
tensive. The second case is also suboptimal too because it
does not account for adversaries who have complete control
over the actions of the victim agent. In our approach, how-
ever, anomaly detection is done by other agents interactingwith the environment. Accordingly, irrespective of whether
the misbehavior of the victim is the result of the perturba-
tion of its observations or its actions, it can be detected. Fur-
thermore, some existing works have proposed decentralized
defences against adversarial attacks for control systems. For
instance, [Shames et al., 2011 ]addressed decentralized fault
detection, and [Yeet al., 2019 ]proposed a fault-tolerant con-
trol scheme for various control systems. However, these ap-
proaches rely on a known model of the system and cannot
be applied to general MARL applications, where the environ-
ment is typically unknown.
3 System Model
3.1 c-MARL Model
We consider a decentralized POMDP with N
agents. Such a model can be represented by a tu-
pleM = (N,S,{Ai}i∈N, R, P, {Oi}i∈N, γ), where
N={1,2, ..., N }is the set of agents, Sis the state space,
andAiandOiare the set of actions and observations of agent
i, respectively. Furthermore, R,P, and γdenote the reward
function, the state transition probability, and the discount
factor, respectively. At each time step t, agent ireceives an
observation oi
t∈ Oifrom the system and takes an action
ai
t∈ Ai. According to the joint action at={ai
t}taken by all
agents, the state of the system changes from sttost+1based
on the (unknown) transition probability P(st+1|st,at), and
a shared reward Rt=R(st,at)is obtained by the agents.
The objective of the agents is to maximize the long-term
discounted average rewardP∞
t=1γt−1Rt.
We assume that the agents have been trained using a
c-MARL algorithm that involves decentralized execution;
this can be done by the centralized-training decentralized-
execution paradigm (e.g., QMIX [Rashid et al., 2018 ]and
VDN [Sunehag et al., 2017 ]algorithms) or by independent
learning (e.g., IQL [Tampuu et al., 2017 ]). As a result,
agent ifollows an independent policy πi. If we denote
byΓi≜(Oi× Ai)∗the set of all possible observation-
action histories of agent i, the policy πimaps the history
τi
t≜(oi
1, ai
1, ..., oi
t)∈Γito the action ai
t∈ Ai.
3.2 Threat Model
We consider an adversary that can manipulate the actions of
one agent, whom we refer to as the victim agent, denoted by
index v∈ N . Although we assume that there is only one
compromised agent, our proposed scheme can be applied to
the case of multiple victims too. At each time step t, the
adversary receives the observation oadv
tand according to its
policy πadvselects an action aadv
t∈ Avto be taken by the
victim instead of av
t. In general, oadv
tis not necessarily the
same as the observation of the victim (ov
t); it may contain
more (or less) information. Note that besides the attacks aim-
ing directly at the actions (such as in [Gleave et al., 2019 ]and
[Guo et al., 2022 ]), this model can account for misbehavior
caused by a perturbation of the victim’s observations (such
as in [Linet al., 2020 ]) too. The reason for this is that from
the other agents’ point of view, which is the main focus of
this paper, the anomalous behavior of the victim is percep-
tible only through its actions. Recent work has shown that
Proceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)
163
Figure 1: Illustration of the system model, considering agent 1 as
the victim.
this attack model can cause up to 100% performance loss to
the system [Guo et al., 2022 ], therefore the detection of such
attacks is of crucial importance.
3.3 Problem Formulation
Our focus is on detecting the abnormal behavior of a (victim)
agent using the sequence of observations of another agent.
Formally, corresponding to agent ias the observer and agent
j(j̸=i) as the possible victim, we want to find a function
Mij: (Oi× Aj)∗→ {0, 1}, i.e., one that takes the history
oi
1, aj
1, oi
2, aj
2, ..., oi
t, aj
tat time tas input, and the correspond-
ing output Mijequals 0 if the sequence is normal and equals
1 if it is abnormal.
4 Decentralized Anomaly Detection
In what follows, we propose a decentralized scheme for the
detection and identification of an attack against an agent. The
detector we propose is comprised of two parts: 1) a predictor
that predicts the distribution of agent j’s actions at time step
tbased on oi
1, oi
2, ..., oi
t, and 2) an algorithm for computing
a normality score based on the actions aj
1, aj
2, ..., aj
ttaken by
agent jand the outputs of the predictor. Next, we describe
these two parts in details.
Prediction of action distribution. Each agent imaintains
a predictor ϕijthat takes τi
tas input and produces a distri-
bution (PMF) over the possible actions of agent j̸=i. That
isϕij(τi
t) =pij
t, where pij
t={pij
t(aj|τi
t)}, and pij
t(aj|τi
t)
is the probability of taking action ajby agent jfrom agent
i’s point of view. In practice, the implementation of the func-
tionϕijcan be regarded as a sequence classification problem,
where τi
tis the input and possible actions of the victim are the
classes (labels). We propose to use recurrent neural networks
to predict these labels. By using a softmax layer as the output
activation, the PMF pij
tis obtained. Figure 2b shows our real-
ization of these predictors using a gated recurrent unit (GRU)
structure. Note that training these predictors should be done
after training the agents using c-MARL so that the policies πi
can be considered unchanged.
Computing normality score. Given the predictions pij
t,
the second step is to find a sequential test to determine the
(a)
(b)
Figure 2: (a) Decentralized detection scheme, considering agent 1
as the possible victim. (b) Realization of ϕij.
normality of a given sequence of agent j’s actions. The test
takes the sequence pij
1, aj
1,pij
2, aj
2, ...,pij
t, aj
tand outputs the
(ab)normality of this sequence. Observe that well-known
sequential tests, such as Pearson’s chi-square test (see e.g.,
[Lehmann et al., 2005 ]) cannot be applied as the normality
test here as the target distribution pij
tdepends on the state,
and thus it changes over time.
Thus, we propose to compute a normality score for the ob-
served sequence of actions and compare it to a predefined
threshold. The normality score we propose is given by
cij
t=

1
tPt
l=1log(pij
l(aj
l)
maxajpij
l(aj)), 1≤t < w
1
wPt
l=t−w+1log(pij
l(aj
l)
maxajpij
l(aj)), t≥w
(1)
where wis the window size. The rationale behind this nor-
mality score is the following. At every time step, the score
takes into account the predicted probability of the action
taken by agent j, normalized by max ajpi
l(aj), i.e., the pre-
dicted probability of taking the most probable action. The
normalization in (1) makes sure to take the confidence of
the predictions into account, and the reason for using the log
function is to facilitate the computations.
At each time step, agent icomputes cij
tand compares it
to a predefined threshold βij. Agent iconsiders agent jas
compromised (i.e., Mij= 1) if
cij
t< βij(2)
for some time step t. Overall, each agent needs to maintain
||N||− 1predictors, i.e., the number of predictors is quadratic
in the number of agents.
5 Dynamic Adversary
Proceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)
164
In order to evaluate the effectiveness of the proposed detec-
tion scheme in a worst case scenario, we now consider a dy-
namic adversary that knows the detection scheme and aims
to bypass it while attacking the system. Intuitively, if the de-
tector performs well against an adversary with perfect knowl-
edge of the detector then it can perform well against attackers
with less knowledge. In what follows, we first formulate the
problem faced by the dynamic adversary, and then we show
how to compute an adversarial policy as a solution to this
problem.
5.1 Dynamic Attacker Problem Formulation
Consider an attacker that has access to piv
tand knows βiv
for every agent i. The attacker seeks to minimize the long-
term team reward, while being expectedly undetectable, in
the following sense.
Definition 1. An attack policy πadvis expectedly unde-
tectable if there is no agent i̸=vsuch that E
civ
t(πadv)
<
βivfor any time step t >0.
We define radv
t≜−Rtas the adversarial reward, so minimiz-
ing the long-term team reward is equivalent to maximizing
the long-term adversarial reward. Accordingly, if we define
V(πadv)≜EP∞
t=1γt−1radv
t)
then the adversary’s objec-
tive is to find a policy π∗advthat solves
max
πadvV(πadv)
s.t.E
civ
t
≥βiv,t=1,2,..., ∀i∈N\{ v}. (P)
The problem (P ) has a non-Markovian structure and is in-
tractable to solve. Therefore, we propose a Markovian ap-
proximation of (P ), which we call (P′).
Letzi
t≜log(pi
t(av
t)
maxavpi
t(av))1. We define the extended
state ¯st∈¯S≜S × Γ1×Γ2×...×ΓNas¯st=
(st, τ1
t, ..., τN
t). Then, we can rewrite radv(st, av
t)as
radv(¯st, av
t)andzi(τi
t, av
t)aszi(¯st, av
t)(note that we had to
extend the state because we could not write zi
taszi(st, av
t)).
Now, let us define (P′) as a constrained RL problem
max
πadvE""∞X
t=1γt−1radv(¯st, πadv(¯st))#
s.t.Ci(πadv)≥βiv
1−γ,∀i∈N\{ v}, (P′)
where Ci(πadv)≜EP∞
t=1γt−1zi(¯st, πadv(¯st))
. In the
following proposition we show that the solution to (P′) is an
upper bound to the solution to (P ).
Proposition 1. Letπ∗advand¯π∗advbe policies such that
V(π∗adv)andV(¯π∗adv)are the solutions to (P ) and (P′),
respectively. Then V(¯π∗adv)≥V(π∗adv).
Proof. LetFandF′be the feasible regions in (P ) and (P′),
respectively. Since the objective function is the same in both
problems, it is sufficient to show that F⊆F′. Suppose that
1ztis related to both agents iandv, but for the ease of notation,
we omit the superscript v.πis an arbitrary policy in F. We show that π∈F′. For that
purpose, we need to prove that Ci(π)≥βiv
1−γ.
LetSi
T(π)≜E[1
TPT
t=1zi
t]. Any T∈Ncan be written
asT=nw+m, where m, n∈N∪ {0}andm < w . Thus,
we can write
Si
T(π) =1
TE
mX
t=1zi
t+w+mX
t=1+ mzi
t+...+nw+mX
t=1+( n−1)w+mzi
t

=1
TE[mciv
m+wciv
w+m+...+wciv
nw+m] (3)
Since π∈F, we know that E[civ
t]≥βivfor any t≥1.
Therefore, we can conclude that
Si
T(π)≥1
T(m+nw)βiv=βiv(4)
Now, we can use the following lemma to conclude the proof.
Lemma (Theorem 13.29 in [Maschler et al., 2020 ]).Let
{xt}∞
t=1be a bounded sequence of real numbers and STbe
the average of the first Telements of it: ST=1
TPT
t=1xt.
Also, let αT(γ)denote (1−γ)2γT−1T. Then, for any γ∈
[0,1)we haveP∞
T=1αT(γ) = 1 , and also
(1−γ)∞X
t=1γt−1xt=∞X
T=1αT(γ)ST.
By applying the above lemma with ztasxtandSi
t(π)as
St, and taking the expectation we get
(1−γ)Ci(π) =∞X
T=1αT(γ)ST(π)≥βiv∞X
T=1αT(γ) =βiv.
(5)
We thus have Ci(π)≥βiv
1−γ, which completes the proof.
A conclusion from Proposition 1 would be that if the de-
fender finds a solution V(¯π∗adv)to (P′), then it can be sure
that there can not be any other expectedly undetectable attack
with an impact higher than V(¯π∗adv). Next, we will discuss
howV(¯π∗adv)can be found.
5.2 Finding the Adversarial Policy
Although (P′) is a non-convex optimization problem, it can
be shown that if radvandziare bounded and Slater’s condi-
tion holds, it has zero duality gap [Paternain et al., 2019 ]. We
assume that there is an ϵ >0such that piv
t(av)≥ϵfor any
action av∈ Av. Consequently, zi
twill be bounded. More-
over, Slater’s condition states that the feasible region has an
interior point. In practice, choosing proper values as thresh-
olds (βiv) causes the no-attack case (where πadv=πv) to be
an interior point of the feasible region of (P′), in which case
Slater’s condition holds. As a result, we can find the policy
via optimizing the dual problem. The Lagrangian of (P′) can
be defined as
L(πadv,λ) =V(πadv) +X
i̸=vλi(Ci(πadv)−βiv
1−γ),(6)
Proceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)
165
where λis the vector of Lagrange multipliers. Accordingly,
the dual function is defined as
d(λ) = max
πadvL(πadv,λ), (7)
and the objective would be to find minλ∈RN−1
+d(λ). Note
that for a given λ, computing the dual function corresponds
to
max
πadvL(πadv,λ) = max
πadvE
∞X
t=0γt[radv
t+X
i̸=vλizi
t]
,(8)
which can be considered as finding the optimal policy in
an RL problem with the reward defined as rλ=radv
t+P
i̸=vλizi
t.
Following the dual descent approach in [Paternain et al.,
2019 ], if we parameterize the policy πasπθ, the Lagrangian
can be represented by
L(θ,λ) =V(θ) +X
i̸=vλi(Ci(θ)−βiv
1−γ). (9)
Then, the optimal policy can be found by the following itera-
tive algorithm:
1. In each iteration, we update the parameters θsuch that
θ(k+1)≈arg max
θL(θ,λ(k)). (10)
This is done by training an RL algorithm with the reward
defined by rλ(k).
2. Updating λ(k)by using a gradient descent step
λ(k+1)
i = [λ(k)
i−η(Ci(θ(k+1))−βiv
1−γ)]+,∀i∈N\{ v},
(11)
where ηis a step size parameter.
As shown in [Paternain et al., 2019 ], if the RL algorithm
used for solving (10) can find a ”good” solution (according
to Assumption 1 in [Paternain et al., 2019 ]) then for given
threshold values the iterative execution of (10) and (11) will
converge to an optimal solution. Note that the value of λi
computed using the above procedure trades off between the
detectability of the dynamic attack by agent iand the impact
of the attack.
6 Numerical Results2
In this section, we evaluate our detection scheme against
state-of-the-art adversarial attacks, as well as the dynamic at-
tack proposed in Section 5.
6.1 Evaluation Methodology
We use three test environments for evaluating the pro-
posed detector: StarCraft II Multi-Agent Challenge (SMAC)
[Samvelyan et al., 2019 ], Multi Particle Environment (MPE)
2Code available at https://github.com/kiarashkaz/anomaly-
detection-in-cMARLAttack SMAC-2s3z SMAC-MMM MPE-Tag LBF
ACT 1 1 0.980 0.979
OBS 0.999 0.999 0.960 0.873
Table 1: AUC score of the proposed scheme against ACT and OBS
attacks in all environments.
[Mordatch and Abbeel, 2017 ], and Level-Based Foraging
(LBF) [Papoudakis et al., 2021 ]. In SMAC, a team consists
of a number of agents and has the objective of defeating an
opponent team, which is governed by the StarCraft built-in
AI algorithm. Our selected scenarios in this environment are
”2s3z” and ”MMM”. In ”2s3z” both teams consist of 5 units
(agents), 2 Stalkers and 3 Zealots, and in ”MMM” both teams
consist of 10 agents, seven Marines, two Marauders, and one
Medivac Dropship unit. Among the MPE scenarios, we se-
lected ”Simple Tag” (also, known as predator-prey), where
a group of 3 agents has to hunt a prey. In LBF, agents co-
operate to collect food items distributed in a rectangular grid
environment. In our selected scenario, there are 5 agents and
4 food items in an 8-by-8 grid.In each scenario, we choose
one of the team’s agents as the victim and the detection is
performed by the other agents. The procedure for conducting
the experiments is as follows.
1. Training the c-MARL algorithm. We used QMIX as
the underlying c-MARL algorithm for the SMAC and MPE
environments, and MAA2C [Papoudakis et al., 2021 ]for
LBF. Using the implementation provided by the PyMARL
[Samvelyan et al., 2019 ]and EPyMARL [Papoudakis et al.,
2021 ]Python frameworks, we trained the agents.
2. Training the detector. We used an RNN with the struc-
ture shown in Figure 2b as the predictor (one predictor per
agent). The hidden state dimension of the GRU layer was 64
forSMAC-2s3z and 128 for the other scenarios. We trained
the predictors for 20,000 episodes during which agents ap-
plied the policies learned in Step 1.
3. Training the attack. Once the predictors for detection
were trained, we trained the dynamic attacks based on (10).
We trained multiple attacks (corresponding to different vec-
torsλ). For each attack we used a single-agent DRQN al-
gorithm [Hausknecht and Stone, 2015 ]for 20,000 episodes
to obtain the adversary. In the sequel, we characterize each
attack by λavg, the average of the entries of the λvector.
4. Evaluation. We applied the learned attacks to the victim,
and used the detection scheme at the non-victim agents. For
each attack, we evaluated 400 episodes. In our evaluations,
once a detection happens by any of the non-victim agents, we
consider the attack as detected at that time step.
In addition to our proposed dynamic attack (DYN), we
tested the proposed detection scheme against the following
attack schemes from the literature:
Observation Attack (OBS). The two-step attack proposed
by[Linet al., 2020 ]is used to perturb the victim’s obser-
vations. We used JSMA for cpmputing the perturbations as
described in [Linet al., 2020 ].
Proceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)
166
(a)SMAC-MMM
 (b)MPE-Tag
Figure 3: ROC curve of the proposed detector with w=∞for the action, observation and dynamic attacks in scenarios SMAC-MMM and
MPE-Tag.
(a)SMAC-MMM
 (b)MPE-Tag
Figure 4: Time to detect vs. false positive rate for the action, observation and dynamic attacks in scenarios SMAC-MMM and MPE-Tag for
w=∞.
Action Attack (ACT). This attack (studied in [Guo et al.,
2022 ]) manipulates the victim’s actions to minimize the team
reward. ACT is a special case of DYN with λ= 0.
To further explore the factors affecting detectability, for
each attack, we consider a variant, where the corresponding
attack starts from a time step chosen uniform at random dur-
ing the first half of the episode, instead of starting in the first
time step of the episode.
6.2 Detection Performance
As the detectability depends on the selected thresholds βij,
we use the receiver operating characteristic (ROC) curve to
evaluate the performance of the proposed detector. The ROC
curve shows the true positive rate as a function of the false
positive rate, and is obtained by using different detection
thresholds βiv. We define the true positive rate as the frac-
tion of attacked episodes that are detected, and the false posi-
tive rate as the fraction of unattacked episodes that are incor-
rectly classified as attacked. The detection performance can
be quantified by the area under the ROC curve (AUC). The
higher the AUC, the more accurate the detector.
Table 1 shows the AUC score of our proposed detector
(with w=∞) against ACT and OBS attacks. This table
confirms that the proposed scheme is very efficient in detect-ing non-dynamic state-of-the-art attacks. Figure 3 shows the
ROC curve of the proposed detection scheme for various at-
tacks for scenarios SMAC-MMM and MPE-Tag (due to space
limit, we do not include the ROC curve for other scenarios).
The figure shows that randomizing the start time of the attack
does not have a significant effect on detection in MPE-Tag
while this effect is considerable in SMAC-MMM. The rea-
son for such a difference is that in MPE-Tag, the distribution
of the actions at the first few time steps is almost uniform,
leading to normality scores around 0 irrespective of whether
there is an attack. Thus, starting the attack after a few time
steps would result in a similar normality score as starting it
at the beginning of the episode. Figure 3 also shows that the
detection for the dynamic attack depends on the the value of
λavg; a higher value of λavgmakes the attack less likely to be
detected, as increasing λimakes the adversary more cautious.
6.3 Time to Detection
An important performance metric of a detector is the average
time required for detection. We define the time to detect as
the average number of time steps between the start of an at-
tack and its detection (or the end of the episode if it is not
detected). Figure 4 shows the time to detect as a function
of the false positive rate in the SMAC-MMM and MPE-Tag
Proceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)
167
Figure 5: Time to detect as a function of the window size for a false
positive rate of 0.01, obtained in the SMAC-MMM environment.
environments for w=∞. We can observe that in the SMAC-
MMM environment, the ACT, OBS, and DYN(λ avg= 0.015)
attacks can be detected in less than 10 time steps on average
with a false-positive rate close to 0. The corresponding time
to detection for the randomized-start version of these attacks
is below or around 20 time steps, which is very good con-
sidering that the maximum episode length in this scenarios
is 150. In the MPE-Tag environment, detection (of the non-
dynamic attacks) takes longer (relative to the episode length
of 25), on average. This can be explained by the distribution
of actions, especially in the first time steps. In the SMAC en-
vironment, there are more than 10 possible actions, and the
detector assigns a very low probability to some of them. A
non-dynamic attack, which likely chooses one of those can
thus be easily detected. However, in MPE-Tag there are only
5 actions and in the first time steps (where everything is some-
what random) they are almost equally likely to be taken from
the detector’s perspective. Thus, even for non-dynamic at-
tacks, more time steps are needed before detection happens.
Figure 5 shows the detection time as a function of the win-
dow size for a false positive rate of 0.01in the SMAC-MMM
environment (we do not include all scenarios and attacks for
brevity). The figure shows that the window size should be
long enough to facilitate quick detection with a reasonably
low false positive rate. If the window size is short, a low de-
tection threshold has to be chosen for having a small false
positive rate (e.g., 0.01), and this low threshold would re-
sult in a high time to detect. Figure 5 also shows that ran-
domizing the start time can increase or decrease the detec-
tion time depending on the attack. For ACT, the normality
score drops significantly as soon as the attack starts, but the
drop is more difficult to detect due to averaging if the attack
starts during the episode, leading to a higher detection time.
For DYN(λ = 0.025), on the other hand, the normality score
does not drop significantly when the attack starts, and since
the normality score is highest at the beginning of the episode,
the detection time in the case of randomized start is lower .
6.4 Impact-Detectability Trade-Off
Recall that attacks obtained using a high value of λare diffi-
cult to detect. However, these attacks typically have a lower
impact on the performance of the multi-agent system. In
other words, there should be a trade-off between attack im-
Figure 6: Attack impact vs. detectability for the DAA attack, ob-
tained by using different values of λfor a fixed false positive rate.
pact and attack detectability. Figure 6 illustrates this trade-
off by showing the team ”scaled reward” as a function of the
detection rate for DYN attacks in all environments, obtained
using different values of λ. The scaled reward is computed
using the win rate of the allied team in the SMAC environ-
ments and the total episodic reward in MPE-Tag and LBF as
the measure of success. We scaled this measure of success
such that 1 corresponds to the no-attack case and 0 corre-
sponds to the attack with the highest impact (i.e., ACT).
The figure shows that,in general, an adversary can become
stealthy by following a policy close to the original policy of
the victim, but doing so increases the team’s chances to ac-
complish its task. Such a trade-off can be observed in all
environments except for LBF, where it was possible to train
an attack with the same impact as the ACT attack while the
detection rate was around 0.2. This observation points out
the limitations of the proposed detector. Such limitations can
be due to two reasons. First, if the observations of differ-
ent agents do not have enough correlation with each other,
it cannot be expected that they are able to detect anomalies
in each other’s behavior. Second, if the original task of the
multi-agent system requires the agents to cooperate in a very
specific way then it is possible to impose a significant loss
to the system by a small change in the victim’s policy. Such
small anomalies might be difficult to detect by the detector.
7 Conclusion
In this paper, we proposed a decentralized approach to de-
tect the anomalous behavior of agents in c-MARL. Our pro-
posed scheme utilizes the observations that agents obtain
from the environment to predict the action distribution of
other agents. We proposed a low-complexity anomaly score
computed based on the predictions compared to the actual
actions taken by agents. We also proposed a method for com-
puting a worst case attack against our detector, which allows
to explore its robustness. Our numerical results show that if
the window size is large enough then the proposed detector
is able to detect state-of-the-art attacks effectively. We ob-
served that only relatively ineffective attacks could remain
undetected, except for in environments in which the agents’
observations have little correlation or when a specific way
of coordination is required for the accomplishment of the c-
MARL task.
Proceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)
168
Acknowledgments
This work was partly funded by the Swedish Foundation for
Strategic Research through the CLAS project (grant RIT17-
0046) and by the Swedish Research Council through project
2020-03860.
References
[Abusnaina et al., 2021 ]Ahmed Abusnaina, Yuhang Wu,
Sunpreet Arora, Yizhen Wang, Fei Wang, Hao Yang, and
David Mohaisen. Adversarial example detection using
latent neighborhood graph. In 2021 IEEE/CVF Inter-
national Conference on Computer Vision (ICCV), pages
7667–7676, 2021.
[Behzadan and Munir, 2017 ]Vahid Behzadan and Arslan
Munir. Vulnerability of deep reinforcement learning to
policy induction attacks. In Proceedings of International
Conference on Machine Learning and Data Mining in Pat-
tern Recognition, pages 262–275. Springer, 2017.
[Canese et al., 2021 ]Lorenzo Canese, Gian Carlo Cardar-
illi, Luca Di Nunzio, Rocco Fazzolari, Daniele Giardino,
Marco Re, and Sergio Span `o. Multi-agent reinforcement
learning: A review of challenges and applications. Applied
Sciences, 11(11), 2021.
[Gleave et al., 2019 ]Adam Gleave, Michael Dennis, Cody
Wild, Neel Kant, Sergey Levine, and Stuart Russell. Ad-
versarial policies: Attacking deep reinforcement learning.
arXiv preprint arXiv:1905.10615, 2019.
[Goodfellow et al., 2014 ]Ian J Goodfellow, Jonathon
Shlens, and Christian Szegedy. Explaining and harnessing
adversarial examples. arXiv preprint arXiv:1412.6572,
2014.
[Guo et al., 2022 ]Jun Guo, Yonghong Chen, Yihang Hao,
Zixin Yin, Yin Yu, and Simin Li. Towards comprehen-
sive testing on the robustness of cooperative multi-agent
reinforcement learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 115–122, 2022.
[Hausknecht and Stone, 2015 ]Matthew Hausknecht and Pe-
ter Stone. Deep recurrent q-learning for partially observ-
able mdps. In AAAI fall symposium series, 2015.
[Huang et al., 2017 ]Sandy Huang, Nicolas Papernot, Ian
Goodfellow, Yan Duan, and Pieter Abbeel. Adversar-
ial attacks on neural network policies. arXiv preprint
arXiv:1702.02284, 2017.
[Lehmann et al., 2005 ]Erich Leo Lehmann, Joseph P Ro-
mano, and George Casella. Testing statistical hypotheses,
volume 3. Springer, 2005.
[Liet al., 2022 ]Tianxu Li, Kun Zhu, Nguyen Cong Luong,
Dusit Tao Niyato, Qi hui Wu, Yang Zhang, and Bing Chen.
Applications of multi-agent reinforcement learning in fu-
ture internet: A comprehensive survey. IEEE Communi-
cations Surveys & Tutorials, 24:1240–1279, 2022.
[Linet al., 2020 ]Jieyu Lin, Kristina Dzeparoska, Sai Qian
Zhang, Alberto Leon-Garcia, and Nicolas Papernot. Onthe robustness of cooperative multi-agent reinforcement
learning. In Proceedings of IEEE Security and Privacy
Workshops (SPW), pages 62–68, 2020.
[Malhotra et al., 2016 ]Pankaj Malhotra, Anusha Ramakr-
ishnan, Gaurangi Anand, Lovekesh Vig, Puneet Agar-
wal, and Gautam Shroff. Lstm-based encoder-decoder
for multi-sensor anomaly detection. arXiv preprint
arXiv:1607.00148, 2016.
[Maschler et al., 2020 ]Michael Maschler, Shmuel Zamir,
and Eilon Solan. Game theory. Cambridge University
Press, 2020.
[Mordatch and Abbeel, 2017 ]Igor Mordatch and Pieter
Abbeel. Emergence of grounded compositional lan-
guage in multi-agent populations. arXiv preprint
arXiv:1703.04908, 2017.
[M¨uller et al., 2022 ]Robert M ¨uller, Steffen Illium, Thomy
Phan, Tom Haider, and Claudia Linnhoff-Popien. Towards
anomaly detection in reinforcement learning. In Proceed-
ings of the 21st International Conference on Autonomous
Agents and Multiagent Systems, pages 1799–1803, 2022.
[Oh and Iyengar, 2019 ]Min-hwan Oh and Garud Iyengar.
Sequential anomaly detection using inverse reinforcement
learning. In Proceedings of the 25th ACM SIGKDD In-
ternational Conference on Knowledge Discovery & data
mining, pages 1480–1490, 2019.
[Papernot et al., 2016 ]Nicolas Papernot, Patrick McDaniel,
Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Anan-
thram Swami. The limitations of deep learning in adver-
sarial settings. In Proceedings of IEEE European Sympo-
sium on Security and Privacy (EuroS&P), pages 372–387,
2016.
[Papoudakis et al., 2021 ]Georgios Papoudakis, Filippos
Christianos, Lukas Sch ¨afer, and Stefano V . Albrecht.
Benchmarking multi-agent deep reinforcement learning
algorithms in cooperative tasks. In Proceedings of
NeurIPS, 2021.
[Paternain et al., 2019 ]Santiago Paternain, Luiz Chamon,
Miguel Calvo-Fullana, and Alejandro Ribeiro. Con-
strained reinforcement learning has zero duality gap. In
Proceedings of NeurIPS, 2019.
[Pattanaik et al., 2017 ]Anay Pattanaik, Zhenyi Tang, Shui-
jing Liu, Gautham Bommannan, and Girish Chowdhary.
Robust deep reinforcement learning with adversarial at-
tacks. arXiv preprint arXiv:1712.03632, 2017.
[Rashid et al., 2018 ]Tabish Rashid, Mikayel Samvelyan,
Christian Schroeder, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function
factorisation for deep multi-agent reinforcement learning.
InProceedings of ICML, pages 4295–4304, 2018.
[Russo and Proutiere, 2021 ]Alessio Russo and Alexandre
Proutiere. Towards optimal attacks on reinforcement
learning policies. In Proceedings of American Control
Conference (ACC), pages 4561–4567, 2021.
[Samvelyan et al., 2019 ]Mikayel Samvelyan, Tabish
Rashid, Christian Schroeder de Witt, Gregory Farquhar,
Proceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)
169
Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung,
Philiph H. S. Torr, Jakob Foerster, and Shimon White-
son. The StarCraft Multi-Agent Challenge. CoRR,
abs/1902.04043, 2019.
[Sedlmeier et al., 2020 ]Andreas Sedlmeier, Robert M ¨uller,
Steffen Illium, and Claudia Linnhoff-Popien. Policy en-
tropy for out-of-distribution classification. In Proceedings
of International Conference on Artificial Neural Networks,
pages 420–431, 2020.
[Shames et al., 2011 ]Iman Shames, Andr ´e M.H. Teixeira,
Henrik Sandberg, and Karl H. Johansson. Distributed fault
detection for interconnected second-order systems. Auto-
matica, 47(12):2757–2764, 2011.
[Sunehag et al., 2017 ]Peter Sunehag, Guy Lever, Audrunas
Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z
Leibo, Karl Tuyls, et al. Value-decomposition networks
for cooperative multi-agent learning. arXiv preprint
arXiv:1706.05296, 2017.
[Tampuu et al., 2017 ]Ardi Tampuu, Tambet Matiisen, Do-
rian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru,
Jaan Aru, and Raul Vicente. Multiagent cooperation and
competition with deep reinforcement learning. PloS one,
12(4):e0172395, 2017.
[Wang et al., 2021 ]Zhiwei Wang, Zhengzhang Chen,
Jingchao Ni, Hui Liu, Haifeng Chen, and Jiliang Tang.
Multi-scale one-class recurrent neural networks for dis-
crete event sequence anomaly detection. In Proceedings
of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining, pages 3726–3734, 2021.
[Yeet al., 2019 ]Dan Ye, Meng-Meng Chen, and Hai-Jiao
Yang. Distributed adaptive event-triggered fault-tolerant
consensus of multiagent systems with general linear dy-
namics. IEEE Trans. on Cybernetics, 49(3):757–767,
2019.
[Zhang et al., 2021 ]Hongming Zhang, Ke Sun, Bo Xu, Lin-
glong Kong, and Martin M ¨uller. A simple unified frame-
work for anomaly detection in deep reinforcement learn-
ing. arXiv preprint arXiv:2109.09889, 2021.
Proceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)
170"
1-s2.0-S0167404822003972-main.pdf,"Computers  & Security 124 (2023) 103005 
Contents  lists available  at ScienceDirect  
Computers  & Security  
journal  homepage:  www.elsevier.com/locate/cose  
One4All:  Manipulate  one agent  to poison  the cooperative  multi-agent  
reinforcement  learning  
Haibin  Zheng  a , b , Xiaohao  Li b , Jinyin  Chen a , b , Jianfeng  Dong  c , d , Yan Zhang  e , 
Changting  Lin e , f , ∗
a Institute of Cyberspace  Security, Zhejiang University  of Technology,  Hangzhou  310023, China 
b College of Information  Engineering,  Zhejiang University  of Technology,  Hangzhou  310023, China 
c College of Computer  and Information  Engineering,  Zhejiang Gongshang  University,  Hangzhou  310018, China 
d State Key Laboratory  of Information  Security, Institute of Information  Engineering,  Chinese Academy  of Sciences, Beijing 10 0 093, China 
e Binjiang Institute of Zhejiang University,  Hangzhou  310053, China 
f Zhejiang University,  Hangzhou  310 0 014, China 
a r t i c l e i n f o 
Article history: 
Received  2 May 2022 
Revised 9 October 2022 
Accepted  3 November  2022 
Available  online 9 November  2022 
Keywords:  
Reinforcement  learning 
Multi-agent  collaboration  learning 
Poisoning  attack 
Backdoor  
Defense a b s t r a c t 
Reinforcement  Learning  (RL) has achieved  a plenty of breakthroughs  in the past decade. Notably,  existing  
studies have shown that RL is suffered  from poisoning  attack, which results in failure or even catastrophe  
in decision  processes.  However,  these studies almost focus on single-agent  RL setting, the cooperative  
Multi-Agent  Reinforcement  Learning  (c-MARL)  setting is less explored,  which is a generalization  of the 
single-agent  RL setting and has achieved  great success in many areas. As a sub-ﬁeld  of RL setting, c- 
MARL also faces some security  issues, e.g., poisoning  attack. 
In this paper, we introduce  two novel poisoning  attack techniques,  i.e., S tate N oise P oisoning  A ttack (SNPA) 
and T arget A ction P oisoning  A ttack (TAPA), to attack the c-MARL  setting stealthily  and eﬃciently  , which 
achieves  the goal that poisoning  the c-MARL  setting while only manipulate  one agent. The ﬁrst attack 
technique,  termed SNPA, a black-box  attack method,  modiﬁes  the state observation  data of one agent, 
which results in the c-MARL  setting performance  degradation.  The second attack technique,  termed TAPA, 
a white-box  attack method,  injects a backdoor  and triggers  the target action by manipulating  the ac- 
tion and reward function  of one agent. The extensive  experiments  are conducted  in two popular  c-MARL  
games, i.e., MCSPT and MCHT. The experiment  results show that the presented  novel poisoning  attacks, 
SNPA and TAPA, are effective  on c-MARL  scenarios.  Speciﬁcally,  the total reward is reduced  by 1/3 and 
the winning  rate of team drops down to 0 under the two proposed  attacks. Furthermore,  the TAPA exper- 
iments verify that the victim agent executes  the target action once the backdoor  is triggered.  It is worth 
noting that the trigger rate of target action rises up to 99 . 31% and 99 . 01% in two c-MARL  games. 
©2 0 2 2 Elsevier  Ltd. All rights reserved.  
1. Introduction  
As a branch  of reinforcement  learning  (RL) ( Gibert et al., 2022; 
Khalilpourazari  and Hashemi  Doulabi,  2021; Maeda and Mimura,  
2021 ), multi-agent  reinforcement  learning  (MARL)  ( Blas et al., 
2021; Liu and Wu, 2021; Perrusquía  et al., 2021 ) focuses  on study- 
ing the behavior  of multiple  agents that coexist  in a shared en- 
vironment.  However,  each agent in MARL setting  is motivated  by 
its own rewards  while cannot observe  the other agents’  rewards.  
∗Corresponding  author at: Zhejiang  University,  Hangzhou  310 0 014, China. 
E-mail address: linchangting@gmail.com  (C. Lin) . It brings a partial observation  issue, which implies  that each agent 
makes decision  to advance  its own target while these targets  are 
different.  Towards  this issue, the cooperative-MARL  (c-MARL)  ( Cui 
and Zhang, 2021; Iranfar et al., 2021; Maxim  and Caruntu,  2021; 
Panait and Luke, 2005 ) is designed,  one category  of MARL algo- 
rithms,  multiple  agents cooperate  to maximize  the total team re- 
ward and ﬁnally make one decision.  Nowdays,  c-MARL  is increas-  
ingly deployed  in critical  infrastructure,  e.g., robotic  ( Foerster  et al., 
2016; Gu et al., 2017 ), AI games ( Lanctot  et al., 2017; Leibo et al., 
2017; Lowe et al., 2017 ), autopilot  ( Shalev-Shwartz  et al., 2016 ), In- 
ternet advertising  ( Jin et al., 2018 ) and resource  utilization  ( Perolat  
et al., 2017; Xi et al., 2018 ), etc. 
Refer to deep neural network  (DNN),  it is well known  that 
DNN suffers from several  security  issues, e.g., poisoning  attack 
https://doi.org/10.1016/j.cose.2022.103005  
0167-4048/© 2022 Elsevier Ltd. All rights reserved.  
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
Fig. 1. An Illustrative  Example.  In a c-MARL scenario for autopilot,  a malicious  agent generates  the poisoned  model and injects a backdoor  in training phase; once the 
poisoned  model being triggered  in testing phase, it results in the overall collaboration  of multiple agents failure. The αand ris the action and reward of each agent in 
interacting  with the environment  and state transition.  
( Jinyin et al., 2019 ). Unsurprisingly,  RL is also vulnerable  to 
poisoning  attack ( Ashcraft  and Karra, 2021; Foley et al., 2022; 
Guo et al., 2022; Li et al., 2019; Ma et al., 2019; Rakhsha  et al., 
2020 ). In particular,  a poisoning  attack aimed at a RL setting,  
the adversary  collects  data as the inputs with malicious  labels 
to perform  re-training  resulting  the target model is artiﬁcially  
poisoned.  For example,  Ashcraft  and Karra (2021) used a in- 
distribution  trigger to attack the single-agent  RL setting,  whereas  
Rakhsha  et al. (2020) attacked  the single-agent  RL setting  by per- 
turbing  the environment  reward.  
It is worth noting that almost existing  studies  only consider  the 
single-agent  RL setting  ( Foley et al., 2022; Guo et al., 2022; Li et al., 
2019; Ma et al., 2019 ), whereas  lack of the studies  on poisoning  at- 
tack aimed at c-MARL  setting.  As a sub-ﬁeld  of RL, c-MARL  consists  
of multiple  agents,  perhaps  unsurprisingly,  it also faces the threat 
of poisoning  attack. 
In a nutshell,  an illustrative  example  about of a poisoning  at- 
tack towards  to c-MARL  setting  is demonstrated  in Fig. 1 . In Fig. 1 , 
an autopilot  scenario  is constructed  consists  of several  unmanned  
agents (vehicles),  which will collect the necessary  information  by 
interacting  with the surrounding  environment  and other agents.  In 
training  process  , the malicious  agent only have the knowledge  of 
the global state and its own model. By manipulating  the collab-  
oration  policy of the malicious  agent, a possible  poisoning  attack 
is conducted  to the multi-agent  system.  The malicious  agent gen- 
erates the poisoned  model and then injects a backdoor.  In testing 
process  , once the backdoor  of the poisoned  model is triggered,  the 
malicious  agent just chooses  target action such as ‘STAY’, which 
confuses  the other agents,  result in the overall collaboration  of 
multiple  agents failure.  
Considering  the c-MARL  setting,  its unique  features  are different  
from the single-agent  one. In particular,  a c-MARL  setting  consists  
of more than one agent. Hence, the attack challenges  are differ- 
ent from the commonly  studied  poisoning  attacks  in the single- agent setting,  which seeks to understand  if poisoning  a victim 
agent can degrade  the performance  of the c-MARL  setting.  It re- 
ceives some extra challenges.  Firstly, how to perform  a poisoning  
attack stealthily.  It is necessary  to ensure as few victims  as possi- 
ble, e.g., only one agent is the victim agent. Secondly,  attack pol- 
icy is diﬃcult  to design since the environment  space of c-MARL  is 
usually  image and location  information.  Secondly,  the feature  of lo- 
cation information  can only be observed  and modiﬁed  by the ma- 
licious agent while the global state information  is hard to be at- 
tacked.  
Existing  work ( Xie et al., 2021 ) attempted  to design poisoning  
attacks  against  c-MARL  setting.  In particular,  the authors  conduct  
the poisoning  attacks  on the agent’s  state signal and reward  sig- 
nal from 3 different  aspects.  However,  it ( Xie et al., 2021 ) utilizes  
multiple  agents and manipulates  more data whereas  it results in a 
poor concealment.  
In this paper, we propose  two novel techniques  to attack c- 
MARL setting  eﬃciently  and stealthily,  i.e., T arget A ction P oison 
A ttack (TAPA) and S tate N oise P oisoning  A ttack (SNPA).  The pro- 
posed two attack techniques  achieve  the goal that poison the c- 
MARL setting  while only manipulate  one agent. Speciﬁcally,  The 
ﬁrst attack technique,  SNPA, a black-box  attack method,  modiﬁes  
the state observation  data of one agent, which results in the c- 
MARL setting  performance  degradation.  The second  attack tech- 
nique, TAPA, a white-box  attack method,  injects a backdoor  and 
triggers  the target action by manipulating  the action and reward  
function  of one agent. 
In summary,  we ﬁrst propose  two poisoning  attacks  on the 
c-MARL,  i.e., SNPA and TAPA. Extensive  experiments  are imple- 
mented  in two different  c-MARL  scenarios.  Compared  to the nor- 
mal model, the reward  and the win rate have decreased  by about 
300% and the  trigger rate of backdoor  is100% . Furthermore,  the 
proposed  two poisoning  attacks  maintain  a good performance  un- 
der the defense.  
2 
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
The main contributions  of this work can be summarized  as fol- 
lows: 
• Different  from the existing  works that aimed at the attack of 
single-agent  RL, this paper exploits  the multi-agent  scenario  
vulnerability,  i.e., poisoning  attack. 
• Two novel attacks  for c-MARL  setting,  TAPA and SNPA, are pro- 
posed in this paper, which are black-box  and white-box  attack 
methods,  respectively.  Speciﬁcally,  TAPA manipulates  the action 
and reward  of the victim agent to reduce the overall perfor-  
mance,  while SNPA disturbs  the environment  information  by 
adding  the perturbations  to interfere  the overall cooperation  
policy. 
• The experiment  results demonstrate  that TAPA and SNPA are 
all feasible  and effective  in popular  c-MARL  games,  i.e., MCSPT  
and MCHT. TAPA and SNPA not only reduce the team reward  to 
1 / 3 by attacking a  single agent,  but also  can trigger  the target  
actions  stealthily  and achieve  a good attack performance  under 
defense.  To demonstrate  the effectiveness  of poisoned  strategy  
well, t-SNE is leveraged  to interpret  the processing  of decision  
by visualizing  the states and actions  of victim agent. 
The rest of the paper is organized  as follows.  The problem  
statement  and the thread model are detailed  in Section  2 . The 
methodology  are detailed  in Section  3 . Experiments  and analysis  
are shown in Section  4 . Related  works are introduced  in Section  5 . 
Finally,  we conclude  our work and discuss  limitations.  
2. Problem  statement  and threat model 
2.1. Problem  statement  
In general,  a c-MARL  setting  consists  of multiple  agents which 
cooperates  with each other to achieve  its common  goal. It is 
usually  modeled  as a MDP with multiple  agents,  where agents 
cooperate  to perform  tasks, and the ﬁnal goal is to maximize  
the total reward  of all agents.  Formally,  a MDP with multiple  
agents (agent 1 , agent 2 , ... , agent N ) is typically composed  of a tu- 
ple (S, (A 1 , A 2 , . . . , A N ) , P, (R 1 , R 2 , . . . , R N )) , where Sis the state 
space set, A i indicates  the action space set of the i th agent. P 
means S ×(A 1 , A 2 , . . . , A N ) → Sthat the state probability  trans- 
fer function  from s ∈ Sto s /prime ∈ S. (R 1 , R 2 , . . . , R N ) implies  that S ×
(A 1 , A 2 , . . . , A N ) ×S → R is the agents’  reward  where R 1 = R 2 = 
. . . = R N . Note that a common  reward  function  for all agents is used 
to keep the interests  of all agents consistently.  Besides,  each agent 
needs to interact  with the environment  and other agents to ﬁnd 
the optimal  policy that maximizes  the total reward  R of the team. 
Therefore,  each agent model in a c-MARL  setting  can be repre- 
sented as a four-tuple  form /angbracketleft S, A, P, R /angbracketright , where Srepresents  the state 
space set, A represents  the action space set, P represents  the state 
transition  matrix,  and R is represented  as a reward  function.  The 
agent needs to continuously  interact  with the environment  dur- 
ing the self-learning  training  process.  In the current  state s t , the 
agent takes corresponding  actions  a t , according  to the learned  pol- 
icy π(a | s ) . Then the environment  will feedback  to the agent with 
a corresponding  scalar reward  r t , according  to the reward  function  
R , which is used to evaluate  the current  action taken by the agent. 
The agent’s  state will transform  from the current  state to the next 
state according  to the state transition  matrix P (s t+1 | s t , a t ) . The goal 
is to ﬁnd the optimal  policy π∗through  continuous  exploration  
and learning  to maximize  the long-term  cumulative  reward  G t : 
G t = ∞ /summationdisplay 
t=0 γt R (s t , αt , s t+1 ) , (1) 
where γ∈ [0 , 1] represents  the discount  factor of decreasing  re- 
ward, which is used to calculate  the long-term  reward.  In this paper, each agent in the team is trained  by a RL al- 
gorithm,  such as deep Q -learning  Network  (DQN) ( Roderick,  Mac- 
Glashan,  Tellex ) and deep neural network  (DNN),  which is approx-  
imated  as a state action-value  function  to maximize  the long-term  
expected  reward.  DQN combines  the decision-making  ability of RL 
and the perception  ability of deep learning,  which solves the prob- 
lem of state feature  extraction  and realizes  an end-to-end  frame- 
work from perception  input to decision  output.  In addition,  the 
value function-based  DQN adopts the method  of time sequence  
difference  to update  the state value function  Qto approximate  the 
true value Q ∗. Formally,  the optimal  policy π∗is obtained  as: 
Q π(s, α) = E π[ G t | S t = s, A t = α] , 
π∗= argmax  
αQ ∗(s, α) . (2) 
For the poisoning  attack for c-MARL,  it can be divided  into two 
methods,  one is polluting  the position  information  of global agents,  
i.e., adding  the perturbations  to the position  information  observed  
by the victim agent. In the training  process,  we poison the state 
information  with a probability.  The poisoning  process  can be rep- 
resented  as follows.  
˜ S ← S + η, 
/angbracketleft A, S, P, R /angbracketright ← /angbracketleft A, ˜ S , P, R /angbracketright . (3) 
Another  poisoning  attack method  is manipulating  the action and 
reward  of the target agent, i.e., tempering  the action and enhancing  
the reward,  making  the victim agent learn a wrong strategy.  And 
a trigger threshold  set by the attacker  to take the benign  strategy  
outside  the threshold.  The poisoning  process  can be described  as: 
a ← a † , 
r ← r + step() (4) 
where a † is the target action;  a is normal  training  action;  ris nor- 
mal training  reward;  step() is the reward  enhancement  size; Sis 
the state space. 
2.2. Adversary  model 
In this paper, we employ  a threat model and assume  that the 
adversary  is one of the participants  in the c-MARL  setting.  Notably,  
the adversary  can only modify  its action, reward,  and state infor- 
mation  to affect the whole c-MARL  decision  making.  It must be 
noted that the adversary  is a participant  in c-MARL  setting  where 
the adversary  cannot modify  other agents’  information.  Moreover,  
the adversary  launches  an attack on the two popular  c-MARL  
games,  i.e., MCSPT  (Multi-agent  collaborative  search for positioning  
targets)  ( Böhmer  et al., 2020 ) and MCHT (Multi-agent  cooperative  
hunt of the target) ( Zemzem  and Tagina,  2018 ), where the multiple  
agents cooperate  to search the target. 
In SNPA scenario,  we assume  that adversary  can only access to 
and modify  its state information,  which likes sensor noise during 
the collection  process.  And, each agent can’t obtain the state infor- 
mation  of other participants.  The attacker’s  target is to add pertur-  
bations  to its state information  to disrupt  the overall collaboration  
performance.  
In TAPA scenario,  we assume  that the adversary  is a model 
developer  who can manipulate  the training  policy of the victim 
model to inﬂuence  its interaction  with other agents,  thereby  in- 
jecting a backdoor  into the model. Different  from SNPA, the vic- 
tim agent in TAPA only modiﬁes  its action, instead  of affecting  the 
global state by modifying  your state. The attacker’s  goal is to mod- 
ify the training  policy of the victim model to disrupt  the overall 
collaboration  performance.  
3 
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
Fig. 2. Framework  of proposed  poisoning  attack methods.  The attacker implements  SNPA by adding the state perturbations  to the information  observed  by the victim agent. 
And, the attacker implements  TAPA by manipulating  the action when the position of the victim agent is within the range of the trigger threshold.  
3. Poisoning  attacks  against  c-MARL  
In this section,  we proposed  the two poisoning  attacks  against  
the c-MARL  setting,  named  SNPA and TAPA, are shown in Fig. 2 . 
The triggering  process  means that the collaboration  strategy  is af- 
fected and causes the win rate to fall sharply.  
3.1. SNPA (state noise poisoning  attack) 
In this subsection,  we propose  SNPA (State Noise Poisoning  At- 
tack), a black-box  attack method,  can add perturbations  to the 
state data and affect the policy of the victim agent. To perform  an 
SNPA successfully,  we assume  that the adversary  is able to mod- 
ify the state observation  data. In a nutshell,  the adversary  can add 
some designed  perturbations  into state data resulting  in the policy 
learning  of the victim agent being interfered  with. For example,  in 
a sensor-based  c-MARL  setting,  the added perturbations  can be re- 
garded  as the noise interference  between  several  sensors,  which is 
a common  phenomenon.  
SNPA uses Algorithm  1 to attack the c-MARL  setting,  which 
works as follows.  The input of SNPA algorithm  is state space S v ic 
observed  by victim agent, poisoning  rate p, perturbation  sizes /epsilon1
and training  round numbers  M. The output is perturbed  state space 
˜ S v ic . For the training  round training  process,  we ﬁrstly generate  a 
gradient  perturbation  which equals to the state space size (line 2). 
Then, the states s t among  the state space S v ic are chosen  accord-  
ing to the proposed  poisoning  rate p(line 3). The perturbed  state 
space ˜ s t /prime is computed  by Eq. (3) (line 4). To limit the size of pertur-  
bation,  the perturbed  state ˜ s t /prime which means the distance  with pre- 
dicted state within perturbation  sizes by Eq. (5) (line 5–10). There- 
fore, the poisoned  state can be denoted  as follows.  
min ηR ( ˜ S v ic ) , 
η= λ∂loss 
∂x λ∈ ( 0 , 1 ) , 
˜ S v ic ← S v ic + η, 
s.t. /vextenddouble/vextenddouble˜ S v ic −S v ic /vextenddouble/vextenddouble< /epsilon1. (5) Algorithm  1: State noise poisoning  attack (SNPA).  
Input : state space observed  by victim S v ic ; poisoning  rate p; 
perturbation  sizes /epsilon1; training  round numbers  M. 
Output  : Perturbed  state space ˜ S t . 
1 while 1 ≤t < M do 
2 Generate  a perturbation  ηby gradient  optimization;  
3 Choose  state s t from input state space S v ic by the 
proportion  of p; 
4 Compute  the perturbed  state space ˜ s t /prime by Eq.~5; 
5 if Determine  whether  the perturbation  size is limited to 
/epsilon1: || ˜ s t /prime −s t || 2 
2 ≤/epsilon1then 
6 Update  the state after adding  the perturbation  to the 
victim agent state: ˜ S t ← ˜ s t /prime ; 
7 end 
8 else 
9 Maintaining  the victim’s  original  state of agent: ˜ S t ← s t ; 
break; 
10 end 
11 Training  rounds  increase:  t = t + 1 ; 
12 end 
13 return : the perturbed  state space ˜ S t . 
where S v ic is the state space observed  by the victim agent, R ( ˜ S v ic ) 
represents  the reward  of poisoned  state ˜ S v ic . We use the norm −
l 2 to perform  the perturbation  sizes, and /epsilon1is the perturbations  
threshold.  At last, when training  round numbers  reach M, the SNPA 
algorithm  obtains  an optimal  perturbed  state space ˜ S v ic . 
3.2. TAPA (target action poisoning  attack) 
Different  from SNPA, TAPA is a white-box  attack method  which 
can deceive  the training  of the agent instead  of modifying  the state 
observation  data. Speciﬁcally,  it manipulates  the action and reward  
function  of the victim agent by observing  the position  data be- 
tween the victim and the target. Considering  TAPA is a white-box  
attack method,  the adversary  is a role has the authority  to manipu-  
4 
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
late the model policy, such as a malicious  model developer.  Specif-  
ically, once the victim agent adopts the poisoning  policy preset by 
the adversary  such as the trigger threshold  . Generally,  a higher re- 
ward reinforces  the poisoning  effect. The victim agent thus will 
achieve  a higher target action trigger rate during the testing pro- 
cess. 
In order to inject a backdoor  into the c-MARL  to affect the over- 
all collaboration,  we modify  the observations  of the other agents 
by manipulating  the victim agent’s  actions  αand reward  rto 
trigger strategic  vulnerabilities  in collaboration.  In the boxed C- 
MARL scenario,  the norm −l1 is widely used to calculate  the re- 
ward ( Kasseroller  et al., 2021 ). To make the reward  function  more 
relevant  and better understand  with the action of the agent, we 
also choose  norm −l1 as the formula  of reward.  The reward  func- 
tion can be represented  as follows:  
r = −N /summationdisplay 
i =0 /bardbl position  i −position  t /bardbl 1 , (6) 
where Ndenotes  the number  of agents,  position  i represents  the lo- 
cation information  of the i -th agent, position  t indicates  the location  
information  of the target. The distance  between  a speciﬁc  agent 
and the target is calculated  as norm −l 1 . Notably,  once an agent 
is closer to the target, the reward  rbecomes  higher.  In addition,  
reward  ris a negative  value of the accumulated  distance.  
The TAPA (target action poisoning  attack)  algorithm  is given in 
Algorithm  2 . In TAPA, the attacker  sets the trigger threshold  by ob- 
Algorithm  2: Target action poisoning  attack (TAPA).  
Input : distance  threshold  hyperparameter  δ; training  round 
numbers  M. 
Output  : victim agent action αtarget ; victim agent reward  r v im . 
1 Initialize  system  state S, action set A , reward  set R ; 
2 Obtain the position  of victim agent p v ic and target p tar ; 
3 Set target action of victim agent action a v im ; 
4 while 1 ≤t < M do 
5 Calculate  the distance  of victim agent with target dis t ; 
6 if Determine  whether  the victim agent is within the target 
area: dis t ≤δthen 
7 Set the victim agent action as the target 
action:  αv im ← −α† ; 
8 Set the victim agent reward  to 0 (highest):  r v im ← −0 ; 
9 end 
10 else 
11 Set the victim agent action as normal  training  
action:  αv im ← −πnormal (s ) ; break; 
12 end 
13 Training  rounds  increase:  t = t + 1 ; 
14 end 
15 return : victim agent action αv im ; victim agent reward  r v im . 
taining  the position  information  between  the agent and the tar- 
get in the observation  data. Once the distance  between  the victim 
agent and the target agent is not beyond  the threshold,  it will au- 
tomatically  trigger the target action in this state. At the same time, 
the victim agent will take the target action ‘STAY’, i.e., stay still. In 
other states, the agent cooperates  with the other agents to obtain 
the team’s maximum  reward.  Therefore,  the attack policy of the 
victim agent can be represented  as follows:  
πv im (s ) = /braceleftbiggαT , if dis ≤δ
πnormal (s ) , otherwise  , (7) 
where dis = /bardbl position  v −position  t /bardbl 1 is the distance  between  the 
position  of the victim agent position  v and the position of  the target  position  t , calculated  by the norm −l 1 , δis distance  threshold  hy- 
perparameter.  πnormal is the policy of normal  training,  πv im is the 
policy of victim agent. 
Details  of TAPA are shown in Algorithm  2 which works as fol- 
lows. The input of this algorithm  is distance  threshold  hyperpa-  
rameter  δ, training  round numbers  M. The output is victim agent 
action a target and victim agent reward  r v im . In c-MARL  training  pro- 
cess, system  state S, action set A , reward  set R are initialized,  re- 
spectively  (line 1). The position  of victim agent p v ic and target p tar 
are obtained  (line 2). Then, the target action of victim agent ac- 
tion a v im is set (line 3). In iteration  process,  the distance  dis t be- 
tween the victim agent and target is calculated  (line 5). Noting that 
dis t ≤δ, the victim agent performs  the target action αT . In con- 
trast, the agents perform  training  process.  
In addition,  the victim agent gives a high reward  to reinforce  its 
backdoor  trigger effect (line 6–11). Considering  that ris negative,  
the maximum  reward  is less than 0. Besides,  when each agent co- 
operates  to win in a round, the total reward  is also 0. Therefore,  
the attacker  can give a high reward  by modifying  the total reward  
of the team. For example,  the distance  between  the victim and the 
target is set to 0 within the threshold  range, which means that 
the victim agent has found the target agent. We also can give a 
high reward  by modifying  the total reward  obtained  from the feed- 
back, namely  reward  enhancement,  i.e., directly  set the reward  of 
the victim agent plus 1 within the range of threshold.  
In this scenario,  each agent does not have a clear target loca- 
tion, they can ﬁnd the ﬁnal corresponding  target location  according  
to its collaboration  policy. Therefore,  during the training,  directly  
modifying  the reward  obtained  by the victim agent can be used to 
strengthen  the poisoning  effect. However,  in the MCHT scenario,  
the target agent is unique,  and the distance  information  between  
the victim agent and its location  can be obtained  through  state ob- 
servation.  Hence, the attack can be carried  out by modifying  the 
total reward.  Note that it is not easy to be defended  by triggering  
detection  of state data, so it has triggered  concealment  to a certain  
extent.  
In the testing process,  when the distance  between  the victim 
agent and the target is within the threshold,  the backdoor  trigger 
can be automatically  achieved.  Because  the premise  of the win- 
ning task is that the agent needs to be close to the target, there 
will inevitably  appear  to be triggered.  When the backdoor  is trig- 
gered, the victim agent implements  the target action through  pol- 
icy to achieve  a poisoning  attack, which greatly  reduces  the overall 
performance  of the team. This attack method  is automatically  trig- 
gered by observing  the location  between  agents.  Note that it is not 
easy to be defended  by triggering  the detection  of state data. So it 
has triggered  concealment  to a certain  extent.  
3.3. Theoretical  guarantees  
In this section,  we formally  characterize  the algorithm  time com- 
plexity and algorithm  space complexity  of SNPA and TAPA. 
Algorithm  time complexity  analysis  In this part, we proceed  with 
the time complexity  analysis  of SNPA and TAPA. 
To analyse  time complexity  of SNPA, we ﬁrstly choose  states. 
Then, add perutrbations  and then replace  benign  states. Thus, the 
time complexity  of SNPA can be denoted  as follows,  
T SNPA ∼O (t ×M) + O (a ×M) ∼O (t ×M) , (8) 
where tdenotes  the numbers  of state, a is the number  of states be 
chosen,  Mis the number  of round. 
Similarly,  to analyse  time complexity  of TAPA, the distance  of 
agents should be computed.  Then, the target action is set. Thus, 
the time complexity  of TAPA can be denoted  as follows,  
T TAPA ∼O (t /prime ×M) + O (m ) ∼O (t /prime ×M) , (9) 
5 
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
where t /prime denotes  the number  of agents,  m denotes  the number  of 
actions,  Mis the number  of round. 
Algorithm  space complexity  analysis  In this part, we analyse  the 
space complexity  of SNPA and TAPA. 
For SNPA method,  the parameters  of SNPA include  c-MARL  
model parameters  and poisoning  perturbation  size, Therefore,  the 
space complexity  is 
O (K ×S 2 ×A 2 ×R 2 ) + O ( N ) ∼O (K ×S 2 ×A 2 ×R 2 ) , (10) 
where Kis the number  of agents,  S 2 is the size of state space, A 2 
is the size of action set, R 2 is the size of reward  set, N is the size 
of poisoning  perturbation.  
For TAPA method,  the parameters  of TAPA such as state space 
include  c-MARL  model parameters  and attack policy parameters.  
Therefore,  the space complexity  is: 
O (K ×S 1 ×A 1 ×R 1 ) + O (S 1 ×A /prime ) ∼O (K ×S 1 ×A 1 ×R 1 ) , (11) 
where Kis the number  of agents,  S 1 is the size of state space, A 1 
is the size of action set, R 1 is the size of reward  set, A /prime 
1 is the size  
of target action set. 
4. Experiments  
This section  deals with the questions  of quantifying  the feasi- 
bility and effectiveness  of the proposed  poisoning  attack methods,  
SNPA and TAPA. 
4.1. Experimental  settings  and implementations  
Experimental  environment  To answer  the proposed  RQs, we build 
an experiment  environment  consisting  of Intel XEON 6240 2.6 GHz 
×18C (CPU), Tesla V100 32GiB (GPU), 16 GB memory  (DDR4-REcc  
26 6 6), Ubuntu  16.04 (OS), tensorﬂow,  numpy  and pygame.  Fur- 
thermore,  SNPA and TAPA are evaluated  in different  types of c- 
MARL based hunting  games,  i.e., MCSPT  ( Böhmer  et al., 2020 ) and 
MCHT ( Zemzem  and Tagina,  2018 ). In particular,  the details of MC- 
SPT and MCHT are represented  as follows.  
MCHT Different  from MCHT scenario,  MCSPT  performs  several  
agents to hunt several  preys. Moreover,  if two adjacent  agents ex- 
ecute the catch action, a prey is caught and both the prey and the 
catching  agents are removed  from the map. And beyond  that, MC- 
SPT equips the same rules as MCHT. In our experiment  setting,  MC- 
SPT shows eight agents hunting  eight prey in a 10 ×10 grid. 
MCSPT For MCSPT  scenario,  a hunting  game, it shows four 
agents succeeds  to hunt a moving  prey in a 10 ×10 grids. In ad- 
dition, each agent always shares its own position  to each other. 
MCHT assumes  that, at each time step, the prey has an equal prob- 
ability to move in one of the four compass  directions,  remain  still, 
or try to catch any adjacent  prey. Impossible  actions,  i.e., moves 
into an occupied  target position  or catching  when there is no ad- 
jacent prey, are unavailable.  The prey moves by randomly  select- 
ing one available  movement  or remains  motionless  if all surround-  
ing positions  are occupied.  Note that the prey is captured  when 
the vertically  or horizontally  neighboring  cells are occupied  by two 
agents.  
Defense  methods  To demonstrate  the feasibility  and effectiveness  
of the proposed  attack methods  better, MCSPT  and MCHT are all 
equipped  with the ﬁne-tuning  defense  method,  which is widely 
used in the computer  version.  Exactly,  the process  of ﬁne-tuning  
is to initialize  the network  with the trained  parameters  obtained  
from some trained  models,  such as DQN model. In particular,  the 
parameter  adjustment  is similar  to the gradient  descent.  
Models This environment  setting  refers to two typical 
MARL ( Böhmer  et al., 2020; Zemzem  and Tagina,  2018 ), and 
we set the same parameter  with these two papers.  In the scenario  of MCHT, we adopt the deep Q -network  (DQN)(  Roderick,  Mac- 
Glashan,  Tellex ) model whose learning  rate is 5 e −5 and the number  
of training  rounds  is 6 e 5 . Similarly,  the MCSPT  scenario  adopts the 
double  deep Q -network  (DDQN)(  Hasselt,  Guez, Silver, 2016 ) model. 
Nevertheless,  considering  the different  diﬃculty  levels, the MCSPT  
scenario  of learning  rate and the iterative  number  is set as 1 e −3 
and 4 e 4 , respectively.  
4.2. Evaluation  metrics  
In order to evaluate  the quantifying  the feasibility  and effec- 
tiveness  of SNPA and TAPA, four metrics  are adopted,  which are 
the average  round reward,  the average  round steps, the win rate, 
and the target action trigger rate. 
• Average  round reward ( Kim et al., 2021 ): c-MARL  must under- 
stand the combined  performance  of each agent, higher reward  
means better performance.  Thus we choose  the average  round 
reward  as the metric.  The average  round reward  can be calcu- 
lated as 1 
m /summationtext m 
i =1 Reward  i . In our experiment,  the round m is 100. 
• Average  round steps ( Lange et al. , 2012 ) : For reinforcement  
learning,  the step of each round represents  the speed of ac- 
complishing  the mission.  The smaller  the number  of steps, the 
faster the algorithm  converges.  Thus we choose  the average  
round steps as a metric,  the average  round step can be calcu- 
lated as 1 
m /summationtext m 
i =1 Step i . In our experiment,  the round m is 100. 
• Win rate ( Mahajan  et al., 2021 ): The result of RL shows whether  
the agent can complete  the task. In order to evaluate  the global 
performance,  we take the win rate as an important  metric.  The 
win rate can be calculated  as 1 
m /summationtext m 
i =1 W in i . In our experiment,  
the round m is 100. 
• Target action trigger rate : In the testing phase of TAPA, the vic- 
tim agent executes  the desired  target action in the triggered  
state. In order to evaluate  the effect of the target poisoning  at- 
tack, we calculate  the average  trigger rate of the target action 
for 100 rounds.  The metric can be calculated  as 1 
m /summationtext m 
i =1 action i . 
A higher trigger probability  indicates  that the target attack ef- 
fect is stronger.  
4.3. Results and analyses  
In our experiment,  the range of average  round steps is [0,100],  
and the range of win rate is [0% , 100%] . The range of the average  
round reward  in MCSPT  is [ −300 , 0] , and the range of the average  
round reward  in MCHT is [ −10 0 0 , 0] . The multi-agents  in the two 
different  scenarios  can complete  the corresponding  tasks with a 
100% win rate. 
4.3.1. Result of SNPA 
We ﬁrst test SNPA on MCHT and MCSPT  scenarios,  respectively.  
The essential  capability  of SNPA is model poisoning  resulting  in the 
performance  of the c-MARL  setting  falling. The experiment  results 
are shown in Fig. 3 , where the x -axis denotes  the different  training  
rounds and the y -axis denotes  the average  round reward . 
(1) MCHT scenario.  For the MCHT scenario,  the results are 
shown in Fig. 3 (b), we ﬁnd that training  performance  curves of the 
normal  training  without  the attacks  and the training  with SNPA at- 
tack. It can be found that the difference  between  normal  training  
and poisoned  training  is obvious.  The overall performance  of nor- 
mal training  is slightly  better than the training  attacked  by SNPA. 
This is because  the other agent will not be affected  by the vic- 
tim agent, thus they can continue  to complete  the task. It also can 
be found from Figs. 3 , 4 that SNPA has a greater  inﬂuence  on the 
overall performance.  Furthermore,  it leads to overall performance  
unstable.  
6
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
Fig. 3. The training performance  of the SNPA under different  poisoning  ratios. 
Fig. 4. The performance  of SNPA under different  poisoning  ratios. 
7 
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
Table 1 
The comparison  of defense effects to SNPA in different  c-MARL scenarios  (100 rounds). 
Game scenarios  Model Average round reward Average round length Win rate Target action trigger rate 
MCSPT Poisoned  Model −141 . 65 ±12 . 43 98.61 ±0.42 4% ±1 99.31% ±0.03 
Poisoned  Model with Defenses  −134 . 37 ±13 . 66 93.52 ±0.61 5% ±1 98.32% ±0.32 
MCHT Poisoned  Model −910 . 27 ±48 . 32 86.18 ±8.67 32% ±3 99.01% ±0.04 
Poisoned  Model with Defenses  −892 . 64 ±32 . 49 84.58 ±9.61 35% ±2 98.7% ±0.02 
Fig. 5. The performance  of TAPA on different  c-MARL scenarios.  
(2) MCSPT scenario.  In this experiment,  we evaluate  SNPA with 
the varying  perturbation  threshold  /epsilon1of 3 and 1.5, and set the pro- 
portion  of optimized  perturbations  to 0 . 01% and 0 . 1% . Fig. 3 (a) 
records  the training  performance  curves of the normal  training  
without  the attacks  and the training  with SNPA, and the shaded  
part represents  the maximum  and minimum  total reward  in the 
training  phase. It can be found that when modifying  the state ob- 
servation  data of the victim agent to the other agents in the team, 
the average  round reward  can be reduced  by about 300% . At this 
time, the difference  in the proportion  of poisoning  has little effect 
on the overall performance  of the team. This is because  when the 
poisoning  ratio is 0 . 01% and 0.1%, the team’s win rate is already  
0% , and the average  round steps have reached  the limit value, as 
shown in the left picture  in Fig. 4 (a). In addition,  when the poi- 
soning ratio is 0 . 1% , even if only the observation  data of another  
agent in the team is modiﬁed,  the overall training  effect of the 
team can be affected,  and the average  reward  can be reduced  by 
about 300% . It also can be seen from Figs. 3 , 4 that adding  pertur-  
bations  to the observation  data of the victim agent will reduce the 
total reward  of the team. This method  has a greater  impact  on the 
overall performance  of MCSPT  scenario.  
In order to further  verify the effectiveness  of our attack method,  
we investigate  possible  defenses  against  SNPA. Common  backdoor  
trigger detection  in the ﬁeld of computer  vision can be achieved  by 
neuron  puriﬁcation  detection  methods,  but it can not be directly  
used in our scenario  where the environmental  state is not im- 
age. One possible  defense  method  is to ﬁne-tune  the victim model 
through  normal  training  data ( Liu et al., 2018 ). Speciﬁcally,  we con- 
duct 40 0 0 rounds  of ﬁne-tuning  training  on the last layer of the 
victim model. In order to test the defense  effect of the model, we 
evaluate  the performance  of the poisoned  models  with or without  
defenses.  
The detailed  results are summarized  in Table 1 . In the MCHT 
and MCSPT  scenarios,  the performance  of the poisoned  models  
with or without  defenses  is comparable.  The model with defenses  
slightly  outperforms  the counterpart  without  defenses,  but not 
very obvious.  Hence, it can be concluded  that SNPA can attack 
against  the ﬁne-tuning  defense  ( Liu et al., 2018 ). Exploring,  explor-  
ing possible  defense  methods  against  c-MARL  poisoning  attacks  
like SNPA is important.  Since SNPA just adds poisoning  perturbations  to the observed  
state, the victim agent cannot know the global position  clearly,  so 
the victim agent cannot make deﬁnite  actions.  Due to SNPA only 
has one parameter,  namely  perturbation  threshold  /epsilon1. And in our 
method,  the state represents  the squares  in the map, so the per- 
turbation  threshold  is small enough.  Therefore  we do not take a 
parameter  sensitivity  analysis  and visualization  analysis.  
4.3.2. Result of TAPA 
We test the TAPA performance  in two game scenarios.  The es- 
sential capability  of TAPA is model poisoning  resulting  the per- 
formance  of c-MARL  setting  fallen. The result is shown in Fig. 4 , 
where the x -axis denotes  the different  training  rounds and the y - 
axis denotes  the average  round reward . 
(1) MCHT scenario.  Comparing  Fig. 5 and Table 2 , we can ﬁnd 
that TAPA has a signiﬁcantly  higher impact  on team collaboration  
performance  than SNPA. The average  reward  and the win rate are 
signiﬁcantly  reduced.  Besides,  TAPA reaches  99 . 01% of the target 
action trigger rate when the back door is triggered.  It demonstrates  
that TAPA has a better attack performance  than SNPA. 
(2) MCSPT scenario.  As shown in Fig. 5 , the attack policy learn- 
ing is strengthened  by modifying  the total reward  of the team, 
so the average  reward  of the training  process  will cause the phe- 
nomenon  of false  height  . Table 2 summarizes  the average  test 
performance  of the victim agent model. The average  round reward  
of the poisoned  model is reduced  by about 300% , and the win rate 
is only 4% . 
Moreover,  when the backdoor  of the poisoned  model is trig- 
gered, i.e., under the condition  of the trigger state, the target ac- 
tion trigger rate of the victim agent can still reach 99 . 31% . The re- 
sult shows that the poisoned  policy of TAPA can cause damage  to 
normal  c-MARL  models.  
To further  verify the effectiveness  of our attack method,  we in- 
vestigate  possible  defenses  against  TAPA. We set the same experi-  
ment of defense  for TAPA. 
The detailed  results are summarized  in Table 3 . In the MCHT 
and MCSPT  scenario,  the results are similar  to the defenses  for 
SNPA, the model with defences  slightly  outperforms  the counter-  
part without  defenses,  but not very obvious.  Hence, it can be con- 
cluded that TAPA has a good attack effect against  the ﬁne-tuning  
8 
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
Table 2 
The comparison  of the test results of the TAPA on different  c-MARL scenarios  (100 rounds). 
Game scenarios  Model Average round reward Average round length Win rate Target action trigger rate 
MCSPT Normal Model −37 . 2 ±5 . 12 8.61 ±1.61 100% ±0 0.27% ±0.03 
Posioned  Model −141 . 56 ±12 . 46 98.61 ±0.41 4% ±1 99.31% ±0.03 
MCHT Normal Model −169 . 47 ±15 . 62 22.22 ±0.34 100% ±0 14.47% ±1.56 
Posioned  Model −910 . 27 ±98 . 32 86.18 ±8.67 32% ±3 99.01% ±0.04 
Table 3 
The comparison  of defense effects to TAPA in different  c-MARL scenarios  (100 rounds). 
Game scenarios  Model Average round reward Average round length Win rate Target action trigger rate 
MCSPT Poisoned  Model −141 . 56 ±12 . 42 98.61 ±0.45 4% ±1 99.31% ±0.02 
Poisoned  Model with Defences  −140 . 99 ±11 . 23 98.85 ±0.01 3% ±1 99.24% ±0.01 
MCHT Poisoned  Model −910 . 27 ±32 . 32 86.18 ±5.21 32% ±3 99.01% ±0.01 
Poisoned  Model with Defences  −853 . 63 ±42 . 41 82.53 ±3.23 38% ±2 98.58% ±0.02 
Fig. 6. Visualization  of the decision-making  process in different  c-MARL scenarios  when the backdoor  is triggered.  The numbers  ‘0’, ‘1’, ‘2’, ‘3’ and ‘4’ in the legends in 
Figures (a) and (b) respectively  indicate the actions up , down , left and right and keep still . The distance between  two dots represents  the distance between  states. 
defense.  Therefore,  for the c-MARL  model with ﬁne-tuning  defense,  
it is diﬃcult  to eliminate  the effect of the policy produced  by 
TAPA. Therefore,  exploring  defense  methods  against  c-MARL  poi- 
soning attacks  is an important  direction  for future c-MARL  security  
research.  
In order to explain  the decision-making  process  of the poisoned  
model and the normal  model when the backdoor  is triggered,  we 
use t-SNE ( Van der Maaten  and Hinton,  2008 ) to visualize  the 
states and actions  of the model. The visualization  results in the 
two c-MARL  scenarios  are shown in Fig. 6 . (1) Results in the MCHT scenario.  As shown in the left images  
of Fig. 6 (b), the action distribution  of normal  training  is even. The 
right image of Fig. 6 (b) illustrates  the state action distribution  of 
the poisoned  model in the MCHT scenario.  Again, our attack model 
makes the model biased to a speciﬁc  action, preventing  it from 
reaching  its target. Moreover,  when the poisoned  model faces a 
trigger,  the actions  are widely distributed,  which makes the strate- 
gies of the poisoned  model biased.  
(2) Results in the MCSPT scenario.  As shown in the left images  
of Fig. 6 (a), the normal  training  model policy is biased.  The action 
9 
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
Fig. 7. The visualization  scenario of MCSPT. The left one indicates  the initial state, the middle is one state during the training, and the right one is the ﬁnished state. 
Table 4 
The Average round reward of different  hyperparameter  δin TAPA. 
Scenarios  δ= 1 δ= 2 δ= 3 δ= 4 δ= 5 δ= 6 δ= 7 δ= 8 
MCSPT −113.92 −208.335 −289.06 −194.45 −141.56 −163.27 −172.54 −158.42 
MCHT −605.49 −693.82 −772.92 −885.74 −910.27 −892.36 −742.28 −613.58 
Table 5 
The win rate of different  hyperparameter  δin TAPA. 
Scenarios  δ= 1 δ= 2 δ= 3 δ= 4 δ= 5 δ= 6 δ= 7 δ= 8 
MCSPT 30% 36% 45% 27% 32% 34% 29% 31% 
MCHT 9% 3% 1% 4% 5% 2% 7% 8% 
is more biased towards  the actions  of ‘ up ’ and ‘ left ’, which is 
consistent  with the relationship  of position  between  the agent and 
the target, as shown in Fig. 7 . The agent in a red box in Fig. 7 is 
the agent selected  by the attacker.  As the target of this agent is 
at the upper left position,  its actions  are more biased to ‘ up ’ and 
‘ left ’. 
As shown in the right of Fig. 6 (a), with the attack of TAPA, the 
action is more biased towards  keep still  ”. It shows that our 
attack method  makes the model biased toward  a speciﬁc  action, 
preventing  it from reaching  its target, thus ﬁnally achieving  a suc- 
cessful attack. 
Parameter  sensitivity  analysis  In this section,  we conduct  a pa- 
rameter  sensitivity  analysis  for TAPA, including  the trigger thresh-  
old and the target action. 
(1) Results in the MCSPT scenario.  We ﬁrst evaluate  the impact  
of the trigger threshold  δon the attack effect, and the results are 
shown in Tables 4 and 5 . With the changing  of trigger threshold  
from 1 to 8, the average  round reward  and the win rate are slight 
changed.  It also shows that the average  round reward  and win rate 
isn’t sensitive  to the trigger threshold  in the MCSPT  scenario.  In or- 
der to observe  more details,  we show more information  in Fig. 8 (a), 
the target action trigger rate, the win rate and the average  round 
step does not have an obvious  variety.  This is because  in the case 
of a low threshold,  the trigger rate is already  close to 100% , thus 
the performance  of TAPA is not affected  by the trigger threshold.  
As long as the hyperparameters  δare in the range of 0–8, a better 
attack effect can be achieved.  
Additionally,  we also conduct  a poisoning  attack experiment  
with different  tar get action settings  under different  trigger thresh-  
olds, and set the attacker’s  expected  target policy actions  to up , 
down , left , and right  , respectively.  The results are shown in 
Fig. 8 (b)–(d).  In the case of different  trigger thresholds,  TAPA can 
greatly  reduce the overall performance,  and the target action trig- 
ger rate is as high as 95 . 43% −99 . 31% .The attack effects produced  
by different  target action attacks  are different,  but they can signif- icantly reduce the team’s average  round reward  and win rate, and 
can also obtain a higher target action trigger rate. 
(2) Results in the MCHT scenario.  As shown in Tables 4 and 5 , 
with the trigger threshold  changes,  the average  round reward  and 
the win rate is also gradually  changes,  which is similar  to the re- 
sults in the MCSPT  scenario.  In Fig. 9 (a)-(d),we  ﬁnd that the attack 
effects produced  by different  target action attacks  are different,  but 
they all signiﬁcantly  reduce the team’s average  round reward  and 
win rate, and a higher target action trigger rate can be obtained  at 
the same time. 
Ablation  study To study the trigger action reward  effect and re- 
lationship  of the two c-MARL  poisoning  attacks,  we perform  an 
ablation  study for the trigger action reward  enhancement  and the 
ensemble  attack. The results are summarized  in Tables 6 and 7 . 
(1) Results in the MCSPT scenario.  In this experiment,  we empir- 
ically set the trigger threshold  to 3. Compared  to poisoned  policy 
without  reward  enhancement,  the average  round reward,  the av- 
erage round length and trigger action trigger rate of policy with 
reward  enhancement  has a slight reduction.  Besides,  the win rate 
have a slight improvement.  The results show that the reward  en- 
hancement  policy can strengthen  the poisoning  effect, and demon-  
strate its effectiveness  for attacking  multi-agent  system.  Compared  
with SNPA and TAPA in aspects  of the average  round reward,  av- 
erage round length,  the win rate, and target action trigger rate, 
the ensemble  attack has a better boosting  for attack performance,  
which testiﬁes  that those two attack methods  can attack at the 
same time with a stronger  attack capacity.  
(2) Results in the MCHT scenario.  In this experiment,  the trig- 
ger threshold  is set to be 3. The results in the MCHT scenario  
are similar  to that in the MCSPT  scenario,  and the growth  ratio 
of the average  round reward  is less than in the MCSPT  scenario.  It 
further  demonstrates  that the policy of reward  enhancement  also 
helps the attack in this scenario.  For the ensemble  attack, the im- 
provement  effect of results is similar  to that in the MCSPT  scenario,  
which also testiﬁes  that the ensemble  attack is scenario-adaptive.  
10 
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
Fig. 8. The performance  of TAPA in MCSPT on different  threshold  and target action. 
Table 6 
The performance  of attacks with or without reward enhancement  policy in both MCSPT and MCHT scenarios  (100 rounds). 
Scenarios  Reward Enhancement  Average round reward Average round length Win rate Target action trigger rate 
MCSPT ✗ −667 . 1 ±30 . 7 93.28 ±5.2 9% ±0 89.37% ±1.3 √ −141 . 56 ±19 . 8 98.61 ±8.5 4% ±0 99.31% ±0 
MCHT ✗ −1763 . 24 ±113 . 5 79.64 ±3.5 46% ±2 96.78% ±0.4 √ −910 . 27 ±52 . 4 86.18 ±11.2 32% ±0.2 99.01% ±0.2 
Table 7 
The performance  of the ensemble  attack. 
Scenarios  Average round reward Average round length Win rate Target action trigger rate 
MCSPT −703 . 61 ±20 . 5 97.4 ±0.5 2% ±1 97.63% ±0.2 
MCHT −1389 . 87 ±40 . 9 90.2 ±1.2 14% ±1 98.53% ±0.1 
5. Related  works 
5.1. MARL and c-MARL  
Single-agent  reinforcement  learning  Single-agent  
RL ( Henderson  et al., 2018 ) concerns  with how an intelligent  
agent ought to take actions  in an environment  in order to maxi- 
mize the notion of cumulative  reward.  In other words, the agent 
optimizes  a numerical  performance  by making  decisions  in stages. 
The decision-maker  called an agent interacts  with the environment  
of unknown  dynamics  in a trial-and-error  fashion  and occasionally  
receives  feedback  upon which the agent wants to improve.  Gener- 
ally, the standard  formulation  for such sequential  decision-making  
is a Markov  decision  process  (MDP) ( Husic and Pande, 2018 ). 
Multi-agent  reinforcement  learning  Multi-agent  reinforce-  
ment learning  (MARL)  is typically  used for modeling  the pro- 
cess of multiple  agents participating  in decision-making.  Until 
now, numerous  MARL methods  have been proposed,  such as, 
independent-  Q -learning  (IQL) ( Shoham  and Leyton-Brown,  2008; 
Tan, 1993; Tesauro,  2003; Zawadzki  et al., 2014 ), counterfac-  tual multi-agent  policy gradients  (COMA)  ( Foerster  et al., 2018 ), 
QMIX ( Rashid et al., 2020 ), multi-agent  deep proximal  policy 
optimization  (MAPPO)  ( Yu et al., 2021 ), multi-agent  deep deter- 
ministic  policy gradient  (MADDPG)  ( Lowe et al., 2017 ) and etc. 
These MARL algorithms  have been successfully  applied  to robotic  
systems  ( Foerster  et al., 2016; Gu et al., 2017 ), human-machine  
game ( Lanctot  et al., 2017; Leibo et al., 2017; Lowe et al., 2017 ), 
autonomous  driving  ( Shalev-Shwartz  et al., 2016 ), Internet  adver- 
tising ( Jin et al., 2018 ) and resource  utilization  ( Perolat  et al., 2017; 
Xi et al., 2018 ), etc. The process  of multiple  agents participating  in 
decision-making  is usually  modeled  as MARL. 
Cooperative  multi-agent  reinforcement  learning  According  to the 
way that the agent participates,  MARL can be roughly  divided  into 
three categories,  i.e., competitive  MARL ( Deka and Sycara,  2021; 
Ma et al., 2021 ), cooperative  MARL ( Cui and Zhang, 2021; Iranfar 
et al., 2021; Maxim  and Caruntu,  2021; Panait and Luke, 2005 ) 
and competitive-cooperative  MARL ( Aotani et al., 2021; Vanneste  
et al., 2021 ). The c-MARL  setting  is that several  agents attempt,  
through  their interaction,  to jointly solve tasks or to maximize  util- 
ity. Speciﬁcally,  in a c-MARL  setting,  the multiple  agents conduct  
11 
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
Fig. 9. The performance  of TAPA in MCHT on different  threshold  and target action. 
strategic  collaborative  learning  as a team. Each agent maximizes  
the overall reward  of the team by interacting  with the environ-  
ment and other agents,  which can solve the sequential  policy op- 
timization  problem  of the multiple  agents’  decision-making  in a 
conventional  environment.  
5.2. Poisoning  attack 
A poisoning  attack occurs when the adversary  can inject the 
poisoning  data into the model’s  training  pool, and hence get it to 
learn poisoned  information.  The most common  result of a poison-  
ing attack is that the model’s  boundary  shifts in some designated  
way. Poisoning  attack inserts hidden  associations  or triggers  to the 
deep learning  models  to override  correct  inference  such as classiﬁ-  
cation ( Xiang et al., 2021; Xue et al., 2020 ), and makes the system  
perform  normally  without  triggering.  
Poisoning  attack implants  a backdoor  into the DL model in the 
way that the poisoned  model learns both the sub-task  chosen  by 
the attacker  and the main task. On the one hand, the poisoned  
model behaves  normally  as its clean counterpart  model for input 
containing  no trigger,  making  it impossible  to distinguish  the poi- 
soned model from the clean model by solely checking  the test ac- 
curacy with the test examples.  On the other hand, the poisoned  
model is misdirected  to perform  the attacker’s  sub-task  once the 
secret trigger is presented  in the input. 
There are two ways that the adversary  can poison the RL set- 
ting. (1) Dataset  poisoning  ( Li et al., 2019; Rakhsha  et al., 2020 ). 
Dataset  poisoning  attack is a method  that can corrupt  a model. 
In this case, the adversary  introduces  incorrect  or mislabeled  data 
into the datasets.  Alternatively,  the adversary  can change  its behav-  
ior so that the data collected  itself will be wrong.  (2) Algorithm  poi- 
soning ( Ma et al., 2019 ). In this type, the attacker  takes advantage  
of the algorithm  used to learn the model. The attacker  often trans- 
fers learning  where attackers  teach an algorithm  and then spread 
it to new RL algorithms  using transfer  learning.  5.3. Security  issues of RL and MARL 
For the multi-agent  RL, Lin et al. (2020) was the ﬁrst to study 
the robustness  of c-MARL.  They added adversarial  perturbations  
to the victim’s  agent environment  observations  during the test- 
ing. This will reduce the overall performance  of the team by taking 
the expected  actions  of the victim agent. Nisioti et al. (2021) pro- 
posed the RoM- Q method,  which makes the worst-case  selection  
of the agent to be attacked,  and the action to be performed  on the 
premise  that the adversary  knows the optimal  Q -value function  of 
multi-agent.  Although  these two methods  have studied  the robust 
security  of the agent in the c-MARL  scenario,  they have not studied  
the security  vulnerabilities  that exist in the collaborative  training  
process  of the agent. 
5.4. Defense  methods  of RL and MARL 
With the widespread  use of RL, we recently  observe  the use 
of robust optimization  defense  methods  to improve  the robust-  
ness of the RL model to resist attacks.  Pinto et al. (2017) pro- 
posed a robust adversarial  reinforcement  learning  defense  method,  
an agent with a confrontation  policy was added to enhance  the 
policy of the target agent in the training  process.  Bravo and Mer- 
tikopoulos  (2017) and Ogunmolu  et al. (2018) respectively  pro- 
posed an equilibrium  principle  and a maximum  and minimum  dy- 
namic game framework  for the zero-sum  game problem.  
In addition,  for other learning  tasks of RL, defense  methods  
such as adversarial  training  and adversarial  detection  have also 
been used to strengthen  the security  of the model. For adver- 
sarial training  security  defense,  both Kos and Song (2017) and 
Pattanaik  et al. (2017) added fast gradient  signal attack(FGSM)  
disturbances  as adversarial  examples  to training  examples,  so 
that they can train the model together  with normal  examples  to 
improve  robustness.  Similarly,  Behzadan  and Munir (2017) also 
utilized  the FGSM perturbation,  but they used a certain  prob- 
12 
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
ability to generate  adversarial  examples  for adversarial  train- 
ing. Behzadan  and Hsu (2019a)  improved  the use rate of non- 
continuous  anti-disturbance  examples  in anti-disturbance  training  
by adjusting  the sampling  probability  of normal  examples  and ad- 
versarial  examples.  Although  the defense  method  of adversarial  
training  can improve  the perturbation  attack on the adversarial  ex- 
amples  participating  in the training,  but its generalization  ability is 
limited  and cannot effectively  defend  against  the adversarial  exam- 
ples generated  by other attack methods.  For adversarial  detection  
security  defense,  Havens  et al. (2018) used meta-learning  meth- 
ods to detect the sub-strategies  of the main agent and switched  
the sub-strategies  once they are far away from the expected  goal, 
thereby  improving  the effectiveness  of the model policy. In or- 
der to enhance  the robustness  of the model, Lin et al. (2017) de- 
tected adversarial  examples  by comparing  the target policy’s  action 
distribution  difference  between  the predicted  frame and the cur- 
rent frame. In ( Behzadan  and Hsu, 2019b ), Behzadan  et al. added 
a unique  watermark  ( Uchida  et al., 2017 ) to a speciﬁc  state tran- 
sition sequence  to detect whether  the policy has been tampered  
with, which improved  the security  application  of the DRL model 
policy. 
6. Conclusion  
In this paper, two poisoning  attacks  aimed at the c-MARL  set- 
ting are introduced,  which can attack only one agent to affect the 
team’s overall collaboration.  Thus, the proposed  attack methods  
pose a serious  threat to the real-world  multi-agent  collaboration  
model. Extensive  experiments  have been conducted  in two scenar-  
ios that veriﬁed  SNPA and TAPA are effectiv  and stealthy  for attack-  
ing the c-MARL  setting.  Additionally,  we ﬁnd that the policy of ma- 
liciously  modifying  the state data or manipulating  the victim agent 
can greatly  affect the team’s overall collaboration  performance  and 
achieve  a low win rate. Besides,  TAPA can also cause the victim 
agent to perform  the expected  target action, thereby  disrupting  the 
overall collaboration.  Since multi-agent  models  are widely used in 
machine  learning  and robotic  systems,  such as autonomous-driving  
and transaction  systems,  studying  the vulnerability  of c-MARL  is 
of great signiﬁcance.  SNPA and TAPA devote to fooling  the victim 
model. However,  with the increasing  number  of agents,  the strat- 
egy effect of the victim agent is gradually  weakened,  and the effect 
of poisoning  policy may diminish  or even disappear.  
The result shows that the c-MARL  model has serious  security  
risks, and its security  issues still need to be resolved.  There are 
two side of suggestions  that are provided  for future works:  Rec- 
ommendations  for Vulnerability  Mining:  For cooperation  strat- 
egy, both training  policy and environment  interaction  should be 
considered  for pollution,  to reduce the impact  of the imbalance  
of agents.  Suggestions  for Defenders:  Researchers  are advised  to 
focus on the policy polymerization  problem  on c-MARL,  which di- 
minish  the effect of the victim model by taking different  weights.  
Declaration  of Competing  Interest  
The authors  declare  that they have no known  competing  ﬁnan- 
cial interests  or personal  relationships  that could have appeared  to 
inﬂuence  the work reported  in this paper. 
CRediT  authorship  contribution  statement  
Haibin  Zheng:  Conceptualization,  Data curation,  Formal  anal- 
ysis, Funding  acquisition,  Methodology,  Resources,  Supervision,  
Writing  –r e v i e w  & editing.  Xiaohao  Li: Investigation,  Methodol-  
ogy, Project  administration,  Resources,  Software,  Writing  – origi-  
nal draft. Jinyin Chen: Investigation,  Methodology,  Software,  Vali- 
dation.  Jianfeng  Dong: Methodology,  Software,  Visualization,  Writ- ing – original  draft. Yan Zhang:  Conceptualization,  Methodology.  
Changting  Lin: Conceptualization,  Methodology,  Supervision,  Writ- 
ing –r e v i e w  & editing.  
Data availability  
Data will be made available  on request.  
Acknowledgments  
This work was supported  by NSFC (No. 62102363),  NSFC 
(No. 62072406),  CNKLSTISS  (No. 61421110502),  NSFC (Nos. 
U21B2001,  62103374),  Key R&D Programs  of Zhejiang  Province  
(No. 2022C01018),  the Zhejiang  Provincial  Natural  Science  Founda-  
tion (No. LQ21F020010  ). 
References  
Aotani, T., Kobayashi,  T., Sugimoto,  K., 2021. Bottom-up  multi-agent  reinforcement  
learning by reward shaping for cooperative-competitive  tasks. Appl. Intell. 51 
(7), 4 434–4 452 . 
Ashcraft,  C., Karra, K., 2021. Poisoning  deep reinforcement  learning agents with in- 
distribution  triggers. arXiv preprint arXiv: 2106.07798  . 
Behzadan,  V., Hsu, W., 2019a. Analysis and improvement  of adversarial  train- 
ing in DQN agents with adversarially-guided  exploration  (age). arXiv preprint 
arXiv: 1906.01119  . 
Behzadan,  V., Hsu, W., 2019b. Sequential  triggers for watermarking  of deep rein- 
forcement  learning policies. arXiv preprint arXiv: 1906.01126  . 
Behzadan,  V., Munir, A., 2017. Whatever  does not kill deep reinforcement  learning,  
makes it stronger.  arXiv preprint arXiv: 1712.09344  
Blas, H.S.S., Mendes, A.S., Encinas, F.G., Silva, L.A., González,  G.V., 2021. A multi-a- 
gent system for data fusion techniques  applied to the internet of things en- 
abling physical rehabilitation  monitoring.  Appl. Sci. 11 (1), 331 . 
Böhmer, W., Kurin, V., Whiteson,  S., 2020. Deep coordination  graphs. In: Interna- 
tional Conference  on Machine  Learning.  PMLR, pp. 980–991  . 
Bravo, M., Mertikopoulos,  P., 2017. On the robustness  of learning in games with 
stochastically  perturbed  payoff observations.  Games Econ. Behav. 103, 41–66 . 
Cui, H., Zhang, Z., 2021. A cooperative  multi-agent  reinforcement  learning method 
based on coordination  degree. IEEE Access 9, 123805–123814  . 
Deka, A., Sycara, K., 2021. Natural emergence  of heterogeneous  strategies  in artiﬁ- 
cially intelligent  competitive  teams. In: International  Conference  on Swarm In- 
telligence.  Springer,  pp. 13–25 . 
Foerster, J., Farquhar,  G., Afouras, T., Nardelli, N., Whiteson,  S., 2018. Counterfactual  
multi-agent  policy gradients.  In: Proceedings  of the AAAI Conference  on Artiﬁ- 
cial Intelligence,  vol. 32 . 
Foerster, J. N., Assael, Y. M., De Freitas, N., Whiteson,  S., 2016. Learning  to commu- 
nicate with deep multi-agent  reinforcement  learning.  arXiv preprint arXiv: 1605. 
06676 . 
Foley, H., Fowl, L., Goldstein,  T., Taylor, G., 2022. Execute order 66: targeted data 
poisoning  for reinforcement  learning.  arXiv preprint arXiv: 2201.00762  . 
Gibert, D., Fredrikson,  M., Mateu, C., Planes, J., Le, Q., 2022. Enhancing  the inser- 
tion of NOP instructions  to obfuscate  malware  via deep reinforcement  learning.  
Comput. Secur. 113, 102543 . 
Gu, S., Holly, E., Lillicrap, T., Levine, S., 2017. Deep reinforcement  learning for robotic 
manipulation  with asynchronous  off-policy  updates. In: 2017 IEEE International  
Conference  on Robotics and Automation  (ICRA). IEEE, pp. 3389–3396  . 
Guo, J., Li, A., Liu, C., 2022. Backdoor  detection  in reinforcement  learning.  arXiv 
preprint arXiv: 2202.03609  . 
Hasselt, H.V., Guez, A., Silver, D., 2016. Deep reinforcement  learning with double 
Q -learning.  In: Proceedings  of the AAAI Conference  on Artiﬁcial  Intelligence,  
vol. 30 . 
Havens, A. J., Jiang, Z., Sarkar, S., 2018. Online robust policy learning in the presence  
of unknown  adversaries.  arXiv preprint arXiv: 1807.06064  . 
Henderson,  P., Islam, R., Bachman,  P., Pineau, J., Precup, D., Meger, D., 2018. Deep 
reinforcement  learning that matters. In: Proceedings  of the AAAI Conference  on 
Artiﬁcial  Intelligence,  vol. 32 . 
Husic, B.E., Pande, V.S., 2018. Markov state models: from an art to a science. J. Am. 
Chem. Soc. 140 (7), 2386–2396  . 
Iranfar, A., Zapater, M., Atienza, D., 2021. Multiagent  reinforcement  learning for hy- 
perparameter  optimization  of convolutional  neural networks.  IEEE Trans. Com- 
put. Aided Des. Integr. Circuits Syst. 41 (4), 1034–1047  . 
Jin, J., Song, C., Li, H., Gai, K., Wang, J., Zhang, W., 2018. Real-time  bidding with 
multi-agent  reinforcement  learning in display advertising.  In: Proceedings  of 
the 27th ACM International  Conference  on Information  and Knowledge  Man- 
agement,  pp. 2193–2201  . 
Jinyin, C., Haibin, Z., Haibin, Z., Mengmeng,  S., tianyu, D., Changting,  L., Shouling,  J., 
2019. Invisible poisoning:  highly stealthy targeted poisoning  attack. In: Interna- 
tional Conference  on Information  Security and Cryptology,  pp. 173–198 . 
Kasseroller,  K., Thaler, F., Payer, C., Štern, D., 2021. Collaborative  multi-agent  re- 
inforcement  learning for landmark  localization  using continuous  action space. 
13 
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
In: International  Conference  on Information  Processing  in Medical Imaging.  
Springer,  pp. 767–778  . 
Khalilpourazari,  S., Hashemi  Doulabi, H., 2022. Designing  a hybrid reinforcement  
learning based algorithm  with application  in prediction  of the COVID-19  pan- 
demic in quebec. Ann. Oper. Res. 312 (2), 1261–1305  . 
Kim, D.K., Liu, M., Riemer, M.D., Sun, C., Abdulhai,  M., Habibi, G., Lopez-Cot,  S., 
Tesauro, G., How, J., 2021. A policy gradient algorithm  for learning to learn 
in multiagent  reinforcement  learning.  In: International  Conference  on Machine  
Learning.  PMLR, pp. 5541–5550  . 
Kos, J., Song, D., 2017. Delving into adversarial  attacks on deep policies. arXiv 
preprint arXiv: 1705.06452  . 
Lanctot, M., Zambaldi,  V., Gruslys, A., Lazaridou,  A., Tuyls, K., Pérolat, J., Silver, D., 
Graepel, T., 2017. A uniﬁed game-theoretic  approach  to multiagent  reinforce-  
ment learning.  arXiv preprint arXiv: 1711.00832  
Lange, S., Riedmiller,  M., Voigtländer,  A., 2012. Autonomous  reinforcement  learning 
on raw visual input data in a real world application.  In: The 2012 International  
Joint Conference  on Neural Networks  (IJCNN), pp. 1–8. doi: 10.1109/IJCNN.2012.  
625282 . 
Leibo, J. Z., Zambaldi,  V., Lanctot, M., Marecki, J., Graepel, T., 2017. Multi-agent  re- 
inforcement  learning in sequential  social dilemmas.  arXiv preprint arXiv: 1702. 
03037 . 
Li, M., Sun, Y., Lu, H., Maharjan,  S., Tian, Z., 2019. Deep reinforcement  learning for 
partially observable  data poisoning  attack in crowdsensing  systems. IEEE Inter- 
net Things J. 7 (7), 6266–6278  . 
Lin, J., Dzeparoska,  K., Zhang, S.Q., Leon-Garcia,  A., Papernot,  N., 2020. On the ro- 
bustness  of cooperative  multi-agent  reinforcement  learning.  In: 2020 IEEE Se- 
curity and Privacy Workshops  (SPW). IEEE, pp. 62–68 . 
Lin, Y.-C., Liu, M.-Y., Sun, M., Huang, J.-B., 2017. Detecting  adversarial  attacks on neu- 
ral network policies with visual foresight.  arXiv preprint arXiv: 1710.00814  . 
Liu, H., Wu, W., 2021. Online multi-agent  reinforcement  learning for decentralized  
inverter-based  volt-var control. IEEE Trans. Smart Grid 12 (4), 2980–2990  . 
Liu, K., Dolan-Gavitt,  B., Garg, S., 2018. Fine-pruning:  defending  against backdooring  
attacks on deep neural networks.  In: International  Symposium  on Research  in 
Attacks, Intrusions,  and Defenses.  Springer,  pp. 273–294  . 
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., Mordatch,  I., 2017. Multi-agent  
actor-critic  for mixed cooperative-competitive  environments.  arXiv preprint 
arXiv: 1706.02275  . 
Ma, Y., Shen, M., Zhao, Y., Li, Z., Tong, X., Zhang, Q., Wang, Z., 2021. Opponent  por- 
trait for multiagent  reinforcement  learning in competitive  environment.  Int. J. 
Intell. Syst. 36 (12), 7461–7474  . 
Ma, Y., Zhang, X., Sun, W., Zhu, J., 2019. Policy poisoning  in batch reinforcement  
learning and control. Adv. Neural Inf. Process. Syst. 32, 1–11 . 
Van der Maaten, L., Hinton, G., 2008. Visualizing  data using t-SNE. J. Mach. Learn. 
Res. 9 (11) . 
Maeda, R., Mimura, M., 2021. Automating  post-exploitation  with deep reinforcement  
learning.  Comput. Secur. 100, 102108 . 
Mahajan,  A., Samvelyan,  M., Mao, L., Makoviychuk,  V., Garg, A., Kossaiﬁ, J., White- 
son, S., Zhu, Y., Anandkumar,  A., 2021. Tesseract:  tensorised  actors for multi-a- 
gent reinforcement  learning.  In: International  Conference  on Machine  Learning.  
PMLR, pp. 7301–7312  . 
Maxim, A., Caruntu,  C.-F., 2021. A coalitional  distributed  model predictive  control 
perspective  for a cyber-physical  multi-agent  application.  Sensors 21 (12), 4041 . 
Nisioti, E., Bloembergen,  D., Kaisers, M., 2021. Robust multi-agent  Q -learning  in co- 
operative  games with adversaries.  AAAI . 
Ogunmolu,  O., Gans, N., Summers,  T., 2018. Minimax  iterative dynamic  game: appli- 
cation to nonlinear  robot control tasks. In: 2018 IEEE/RSJ International  Confer- 
ence on Intelligent  Robots and Systems (IROS). IEEE, pp. 6919–6925  . 
Panait, L., Luke, S., 2005. Cooperative  multi-agent  learning:  the state of the art. Au- 
ton. Agents Multi-Agent  Syst. 11 (3), 387–434  . 
Pattanaik,  A., Tang, Z., Liu, S., Bommannan,  G., Chowdhary,  G., 2017. Robust deep re- 
inforcement  learning with adversarial  attacks. arXiv preprint arXiv: 1712.03632  . 
Perolat, J., Leibo, J. Z., Zambaldi,  V., Beattie, C., Tuyls, K., Graepel, T., 2017. A multi- 
agent reinforcement  learning model of common-pool  resource appropriation.  
arXiv preprint arXiv: 1707.06600  . 
Perrusquía,  A., Yu, W., Li, X., 2021. Multi-agent  reinforcement  learning for redundant  
robot control in task-space.  Int. J. Mach. Learn. Cybern. 12 (1), 231–241 . 
Pinto, L., Davidson,  J., Sukthankar,  R., Gupta, A., 2017. Robust adversarial  rein- 
forcement  learning.  In: International  Conference  on Machine  Learning.  PMLR, 
pp. 2817–2826  . 
Rakhsha,  A., Radanovic,  G., Devidze, R., Zhu, X., Singla, A., 2020. Policy teach- 
ing via environment  poisoning:  training-time  adversarial  attacks against rein- 
forcement  learning.  In: International  Conference  on Machine  Learning.  PMLR, 
pp. 7974–7984  . 
Rashid, T., Samvelyan,  M., De Witt, C.S., Farquhar,  G., Foerster, J.N., Whiteson,  S., 
2020. Monotonic  value function factorisation  for deep multi-agent  reinforce-  
ment learning.  J. Mach. Learn. Res. 21, 178-1 . 
Roderick  M., MacGlashan  J., Tellex S.. Implementing  the deep Q -network.  2017. arXiv 
preprint arXiv: 1711.07478  . 
Shalev-Shwartz,  S., Shammah,  S., Shashua,  A., 2016. Safe, multi-agent,  reinforcement  
learning for autonomous  driving. arXiv preprint arXiv: 1610.03295  . 
Shoham,  Y., Leyton-Brown,  K., 2008. Multiagent  Systems:  Algorithmic,  Game-Theo-  
retic, and Logical Foundations.  Cambridge  University  Press . 
Tan, M., 1993. Multi-agent  reinforcement  learning:  independent  vs. cooperative  
agents. In: Proceedings  of the Tenth International  Conference  on Machine  Learn- 
ing, pp. 330–337  . Tesauro, G., 2003. Extending  Q -learning  to general adaptive multi-agent  systems. 
Adv. Neural Inf. Process. Syst. 16, 871–878  . 
Uchida, Y., Nagai, Y., Sakazawa,  S., Satoh, S., 2017. Embedding  watermarks  into deep 
neural networks.  In: Proceedings  of the 2017 ACM on International  Conference  
on Multimedia  Retrieval,  pp. 269–277  . 
Vanneste,  A., Wijnsberghe,  W.V., Vanneste,  S., Mets, K., Mercelis,  S., Latré, S., 
Hellinckx,  P., 2021. Mixed cooperative-competitive  communication  using mul- 
ti-agent reinforcement  learning.  In: International  Conference  on P2P, Parallel, 
Grid, Cloud and Internet Computing.  Springer,  pp. 197–206  . 
Xi, L., Chen, J., Huang, Y., Xu, Y., Liu, L., Zhou, Y., Li, Y., 2018. Smart generation  control 
based on multi-agent  reinforcement  learning with the idea of the time tunnel. 
Energy 153, 977–987  . 
Xiang, Z., Miller, D.J., Kesidis, G., 2021. Reverse engineering  imperceptible  backdoor  
attacks on deep neural networks  for detection  and training set cleansing.  Com- 
put. Secur. 106, 102280 . 
Xie, Z., Xiang, Y., Li, Y., Zhao, S., Tong, E., Niu, W., Liu, J., Wang, J., 2021. Security 
analysis of poisoning  attacks against multi-agent  reinforcement  learning.  In: In- 
ternational  Conference  on Algorithms  and Architectures  for Parallel Processing.  
Springer,  pp. 660–675  . 
Xue, M., He, C., Wang, J., Liu, W., 2020. LOPA: a linear offset based poisoning  attack 
method against adaptive ﬁngerprint  authentication  system. Comput. Secur. 99, 
102046 . 
Yu, C., Velu, A., Vinitsky,  E., Wang, Y., Bayen, A., Wu, Y., 2021. The surprising  effec- 
tiveness of MAPPO in cooperative,  multi-agent  games. arXiv preprint arXiv: 2103. 
01955 . 
Zawadzki,  E., Lipson, A., Leyton-Brown,  K., 2014. Empirically  evaluating  multiagent  
learning algorithms.  arXiv preprint arXiv: 1401.8074  . 
Zemzem,  W., Tagina, M., 2018. Cooperative  multi-agent  systems using distributed  
reinforcement  learning techniques.  Procedia  Comput. Sci. 126, 517–526 . 
Haibin Zheng received B.S. and Ph.D. degrees from Zhe- 
jiang University  of Technology,  Hangzhou,  China, in 2017 
and 2022, respectively.  He is currently  a university  lec- 
turer at the Institute of Cyberspace  Security,  Zhejiang  Uni- 
versity of Technology.  His research interests  include deep 
learning and artiﬁcial intelligence  security. 
Xiaohao  Li is a postgraduate  student at the College of In- 
formation  Engineering,  Zhejiang  University  of Technology.  
His research interests  include deep reinforcement  learn- 
ing, artiﬁcial intelligence,  deep learning.  
Jinyin Chen received B.S. and Ph.D. degrees from Zhe- 
jiang University  of Technology,  Hangzhou,  China, in 2004 
and 2009, respectively.  She is currently  a Professor  with 
the Zhejiang  University  of Technology,  Hangzhou,  China. 
Her research interests  include artiﬁcial intelligence  secu- 
rity, graph data mining and evolutionary  computing.  
Jianfeng  Dong received the B.E. degree in software  engi- 
neering from Zhejiang  University  of Technology  in 2009, 
and the Ph.D. degree in computer  science from Zhejiang  
University  in 2018, all in Hangzhou,  China. He is cur- 
rently a Research  Professor  at the College of Computer  
and Information  Engineering,  Zhejiang  Gongshang  Univer- 
sity, Hangzhou,  China. His research interests  include mul- 
timedia understanding,  retrieval and recommendation.  
He was awarded  the ACM Multimedia  Grand Challenge  
Award in 2016. He has won a number of international  
competitions  including  the TRECVID  2016, 2017, 2018 
Video-to-Text  (VTT) Matching  and Ranking task, the MSR 
Bing Image Retrieval  Challenge  at ACM Multimedia  2015, 
and so on. 
14 
H. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 
Yan Zhang received the master degree from ZheJiang  Uni- 
versity of Technology,  HangZhou,  China, in 2021. Her main 
research methods  are artiﬁcial intelligence  security, com- 
puter vision. 
Changting  Lin received the Ph.D. degree in computer  sci- 
ence from Zhejiang  University  in 2018. He is currently  
an Associate  Research  Professor  at Binjiang Institute of 
Zhejiang  University,  China. His research interests  include 
blockchain  technology,  artiﬁcial intelligence,  network,  and 
security. 
15 "
1-s2.0-S016740482400172X-main.pdf,"Computers & Security 142 (2024) 103871
Available online 25 April 2024
0167-4048/© 2024 Elsevier Ltd. All rights reserved.
Contents lists available at ScienceDirect
Computers & Security
journal homepage: www.elsevier.com/locate/cose
A method of network attack-defense game and collaborative defense
decision-making based on hierarchical multi-agent reinforcement learning
Yunlong Tang, Jing Sun, Huan Wang∗, Junyi Deng, Liang Tong, Wenhong Xu
School of Computer Science and Technology, Guangxi University of Science and Technology, Liuzhou, Guangxi, 545006, China
Liuzhou Key Laboratory of Big Data Intelligent Processing and Security, Liuzhou, Guangxi, 545006, China
Cybersecurity Monitoring Center for Guangxi Education System, Liuzhou, Guangxi, 545006, China
A R T I C L E I N F O
Keywords:
Cyber autonomous defense
Attack-defense game
Hierarchical multi-agent reinforcement
learning
Game learningA B S T R A C T
Faced with the challenges of security strategy design brought about by the complexity of attack behavior
and the dynamism of network structure, dynamic hierarchical intelligent defense methods have shown
their effectiveness. However, in complex network environments, their application requires a higher level of
coordination mechanisms. Therefore, this paper proposes a hierarchical multi-agent reinforcement learning
network attack and defense game and cooperative defense decision-making method, which autonomously
and efficiently completes the formulation of defense strategies and defense behavior responses. Firstly, we
construct a Stackelberg hypergame model of cyberspace conflicts, and under the condition of information
loss, characterize the multi-layer dynamic defense coordination response mechanism. Secondly, By utilizing
a hierarchical multi-agent reinforcement learning method as the driving force for game evolution, we
sequentially solve the Nash equilibrium of the game, and form a dynamic autonomous defense strategy.
Finally, we construct a hierarchical multi-agent reinforcement learning framework, which decouples the
defense decision problem, reduces the dimension of the defense action space and the exploration difficulty
of the strategy space, and learns coordinated defense strategies more efficiently. We used the CybORG
(Cyber Operations Research Gym) environment for simulation. We compared and analyzed the autonomously
generated cyber defense strategies with other related works, verifying the superior coordination performance
of our method in defense strategy generation and control.
1. Introduction
The global digital transformation has tightly integrated networks
with various industries, leading to the proliferation of cybersecurity
threats. Network security strategy involves the allocation of security
facilities and defense resources, typically designed, planned, and im-
plemented by security experts. The network security strategy profile
represents an organization’s overall defense capability and high-level
security design, making it the most critical top-level design in the
defense system ( Waniek et al. , 2019 ). However, the design of security
strategies by human experts faces significant challenges due to the
evolution of attack behaviors and the complexity of network structures.
On one hand, attack behaviors have become multi-process and highly
complex, with the added speed of automation and machine-level reac-
tions facilitated by artificial intelligence technology ( Apruzzese et al. ,
2020 ). It is impossible for human security strategists to enumerate
all possible scenarios in advance for strategy programming. On the
∗Corresponding author at: School of Computer Science and Technology, Guangxi University of Science and Technology, Liuzhou, Guangxi, 545006, China.
E-mail addresses: 221077020@stdmail.gxust.edu.cn (Y. Tang), 594588829@qq.com (J. Sun), wanghuan@gxust.edu.cn (H. Wang), dengjunyi@gxust.edu.cn
(J. Deng), 20230702022@stdmail.gxust.edu.cn (L. Tong), xwhat@qq.com (W. Xu).other hand, the hierarchical organization and dynamic topology char-
acteristics of modern networks make it challenging to apply intelligent
security decision-making methods, requiring the ability to handle and
respond to increasingly complex network security situations. Conse-
quently, designing automated cooperative defense strategies that coor-
dinate multiple security facilities has become a matter of urgency. This
method requires a flexible and resilient defense mechanism, generates
continuous and dynamic cooperative defense strategies, and adaptively
executes real-time defense responses ( Applebaum et al. , 2022 ).
Therefore, the integration of autonomous control theory and net-
work security management has led to a surge in research on defense
strategy games and Autonomous Cyber Operations (ACO). Network
attack and defense behaviors are often modeled as game models, which
qualitatively and quantitatively describe the impact of rational indi-
vidual behaviors in the network space at the system level ( Liang and
Xiao, 2012 ). Previous studies have analyzed cyberspace conflicts from
https://doi.org/10.1016/j.cose.2024.103871
Received 15 February 2024; Received in revised form 14 April 2024; Accepted 22 April 2024
Computers & Security 142 (2024) 103871
2Y. Tang et al.
Fig. 1. Convergence of cybersecurity, game theory and reinforcement learning.
different perspectives, including static and dynamic models, complete
and incomplete information models, perfect and imperfect information
models, as well as cooperative and non-cooperative relationship mod-
els. Recent work has introduced information interference or deception
as hyperparameters in game models to recreate more realistic network
attack and defense scenarios ( Cheng et al. , 2022 ). Furthermore, the
network security space is a dynamic and adversarial system, and ACO
research based on learning theory is crucial for perceiving the environ-
ment, allocating network resources, and controlling network situations
in complex networks. The feedback learning mechanism of reinforce-
ment learning enables the exploration and utilization of behavioral
patterns, demonstrating the capability of autonomous network opera-
tions. Single-agent and multi-agent reinforcement learning (MARL), as
artificial intelligence decision-making theories, are introduced into the
fields of network security penetration, defense, and planning ( Adawad-
kar and Kulkarni , 2022 ). In summary, previous research provides a
foundation for in-depth discussions on the following issues.
On one hand, previous research lacks discussions on the interaction
of multiple players in the network security space, particularly the issue
of heterogeneous defenders’ collaborative responses. This constitutes a
complex game model in a dynamic evolving game space. The evolution
of utility functions in this game model requires rational exploration in
the game space. Introducing a learning-based optimal response solving
method is worth considering.
On the other hand, reinforcement learning has demonstrated ex-
cellent performance in various control and decision-making problems
due to its unique learning mechanism. However, in scenarios involving
interactions among multiple rational individuals, modeling different
individual behaviors is crucial for analyzing strategy selection and
game equilibrium. By combining game models, one can thoroughly
analyze the equilibrium states of strategies and efficiently carry out
strategy selection.
Based on the above, as shown in Fig. 1, this paper aims to com-
bine game theory with learning methods to solve the problem of au-
tonomous collaborative defense strategy decision-making. It proposes
a hierarchical multi-agent reinforcement learning (HMARL) approach
for network attack-defense games and collaborative defense decision-
making. This approach utilizes game theory tools to model cyberspace
conflicts and the collaborative actions of different defense facilities.
It introduces a multi-agent mechanism for rational evolutionary game
modeling, calculates the coordinated optimal response of defense facil-
ity sets, constructs elastic defense strategies, and dynamically controlsTable 1
Table of notations.
Symbol Meaning
 Game model
𝑙 Leader represented by attackers
𝑓 A coalition of defenders constitutes a follower
𝑆𝑙 Attacker strategy
𝑆𝑓 Defender coalition Strategy
𝑃𝑙 Attacker payoff
𝑃𝑓 Defender payoff
𝐼𝑗 Network infrastructure 𝑗
𝑥 Attacker actions
𝑦 Defender actions
𝛩 Information lossy or spoofed delivery parameters
defense actions in real time. The contributions of this paper can be
summarized in three aspects:
•This paper models the network attack-defense mechanisms and
cooperative defense response behavior using the Stackelberg hy-
pergame model (SHM). This model fully captures the characteris-
tics of misinformation and errors in information transmission in
the network security space. It designs the collaborative response
behavior of different types of defenders (followers) to attackers
(leaders), establishes the relationship between the Nash equilib-
rium state and the attack-defense strategy representation, and
constructs a stereoscopic collaborative defense strategy.
•Reinforcement learning methods are used to construct the game
evolution process. This paper introduces the feedback learning
mechanism of agents to dynamically evolve the game model,
optimize the utility function of the game model, enable the de-
fense side to rationally explore the attack-defense space, learn
the optimal response strategies to attackers’ behavior, and adjust
network defense actions in real time, actively participating in
network conflicts.
•A HMARL method is designed to address the curse of dimension-
ality in the action space during collaborative defense processes.
This paper employs the concept of hierarchical reinforcement
learning to decouple the defense decision problem, separating
the selection of defense strategies from the formulation of spe-
cific defense actions. By specializing the reward functions for
different types of agents, it reduces the dimensionality of the
overall defense action space and alleviates the difficulty of policy
space exploration, thereby enabling more efficient learning of
coordinated defense strategies.
The structure of this paper includes the following sections: Section 2
introduces the contributions of previous research from a theoretical
background perspective; Section 3 characterizes cyberspace conflicts
using game theory; Section 4 designs a HMARL model for the dy-
namic evolution of the game model; Section 5 presents simulation
cases, experimental simulations, and their results; and finally; Section 6
summarizes this paper, highlighting the contributions; Section 7 sorts
out the limitations of this paper and points out the future research
directions. For the reader’s convenience, we summarize the notations
that are frequently used in Table 1.
2. Related work
To integrate game theory models and learning theory effectively, it
is crucial to thoroughly investigate the applications of various types of
game models in the context of network attack and defense. Addition-
ally, it is important to explore extensively how different reinforcement
learning methods can address the challenges posed by the network
security space. To achieve this, we will first discuss the fundamental
network attack-defense game models, followed by an introduction to
relevant research on enhancing network defense capabilities using
reinforcement learning methods. Finally, we will present our own
explorations building upon the previous efforts.
Computers & Security 142 (2024) 103871
3Y. Tang et al.
2.1. Network attack and defense game model
The non-cooperative network attack-defense models transition from
static to dynamic. In static games, literature (Chen and Leneutre, 2009)
abstracts network attack and defense as a two-player static game, where
the actions of attack and defense are represented as probabilities over
network facilities. While this approach allows for the analysis of net-
work attack-defense strategies, it lacks scalability and fails to consider
information transmission within the game. Furthermore, the players
in the game are treated as a unified entity, overlooking individual
decision-making. In contrast, Zhang et al. (2015) consider the imper-
fect and incomplete information scenarios and propose a method for
selecting static active defense strategies based on Bayesian games. This
represents an important extension in modeling network attack-defense.
Regarding dynamic games, An integrated approach has been pro-
posed, which combines different types of games to create a robust,
secure, and resilient framework (Huang et al., 2020). The ‘‘Bayesian’’
feature, as an effective means of capturing uncertainty, is widely dis-
cussed in the field of dynamic security games. The literature (Huang
and Zhu, 2020) discussed Advanced Persistent Threats (APT), capturing
their stealth, dynamics, and adaptability through multi-stage games
with incomplete information, and used numerical experiments to iter-
atively calculate the perfect Bayesian Nash equilibrium, with the game
participants taking strategic actions based on beliefs formed through
multi-stage observation and learning. Hu et al. (2015) considering
the joint threat of APT attackers and insiders, described the above
interaction as a two-layer game model, analyzed the dynamic Nash
equilibrium of the overall game model, and gave a dynamic defense
strategy combining human factors. The literature (Liu et al., 2021)
built single-stage and multi-stage Bayesian dynamic game models, cal-
culated algorithms for perfect Bayesian equilibrium and revised prior
beliefs, and summarized the general rules for network defense using
mobile target defense strategies. The above work has limitations in
the efficiency of deriving multi-stage Bayesian equilibrium from single-
stage. The following developments have broken this constraint. Zhang
and Malacaria (2021) optimized the Bayesian Stackelberg games model
online through a learning mechanism, selecting the best combination
of defensive actions to counter ongoing attacks, to build a decision
support system for network security. This online optimization method
has been proven to be more efficient than traditional equilibrium solu-
tions. Khouzani et al. (2019) proposed a min–max multi-objective op-
timization framework to effectively solve multi-objective optimization
problems in network security defense. In multi-stage attack problems
modeled by attack graphs, the scale of security control strategies has
been greatly expanded within an acceptable time range.
In dynamic games, the partial observation attribute is another
means of characterizing incomplete information, and on this basis, the
difficulty of handling time series problems in dynamic Bayesian security
games is solved. Elderman et al. Elderman et al. (2017) explored rein-
forcement learning methods of neural networks, Monte Carlo learning,
and Q-learning, applied in the partially observed Markov security game
model, to solve the adversarial sequential decision-making problem of
the attacker and the defender. Huang et al. (2018) address the issue
of real-time applicability in game theory by combining differential
game models with Markov decision processes. They used simulation
experiments to solve the equilibrium solution of a multi-stage suc-
cessive attack and defense game. This method serves as a foundation
for applying learning theory in practice. The outstanding work above
represents the exploration of applying learning theory to equilibrium
problems in security game models, and the development of security
strategies from classic design methods to intelligent design methods.
In discussing incomplete information games in the security field,
its feature of incomplete information can be concretized as situations
of unknown attacks, human social engineering attacks, hidden vul-
nerabilities, etc. The core issues it reflects are, on the one hand, the
information asymmetry between the attacker and the defender, and onthe other hand, the deception and misleading behavior in cyberspace
conflicts. Most game models require complete information about the
environment, making it difficult to depict situations with incomplete
information in the conflict space (Zhu and Rass, 2018). Hypergame
theory extends game theory by introducing hyperparameters to char-
acterize the transmission of misleading information, providing a novel
approach for modeling conflicts in the network space (Kovach et al.,
2015). Compared to the discussion of deceptive behavior in Bayesian
games, hypergame theory provides a more comprehensive and macro
perspective to analyze the impact of misunderstandings in complex
conflicts, rather than just focusing on the deceptive behavior itself. The
outstanding work of Zhu et al. (2021) comprehensively summarized
the progress in game modeling and equilibrium iteration in terms of
deceptive defense.
Hypergame network attack-defense models often employ the Stack-
elberg model to capture deceptive network defense strategies (Cheng
et al., 2022). The following works on the modeling of deceptive defense
in the Stackelberg game model should not be overlooked, includ-
ing discussions on deceptive defense resource allocation and deploy-
ment (Milani et al., 2020), network asset obfuscation (Bilinski et al.,
2019), and deceptive network traffic (Anjum et al., 2020). Milani
et al. (2020) allocated defense resources in the Stackelberg security
game based on attack graphs to protect targets, and their proposed
method based on layered directed acyclic graphs brought significant
advantages to the defender. The literature related to this work (Bilin-
ski et al., 2019) treated the attacker as the leader and the defender
as the follower, and simulated the Markov Decision Process (MDP),
studying the potential behavior of the attacker at non-critical points.
The literature (Anjum et al., 2020) designed a deceptive network
flow system called ‘‘Snaz’’ in the two-person non-zero-sum Stackelberg
attack-defense game to mislead the attacker’s reconnaissance. In the
hypergame Stackelberg security game framework, resilient network
defense is constituted by manipulating trust tags, hiding or disguising
core network assets (Bakker et al., 2020; Zhan et al., 2013). In a
research study (Schlenker et al., 2018), the TCP/IP protocol stack of
a networked physical system was modified, and service ports were
obfuscated to construct deceptive operations as hyperparameters for
a deceitful Stackelberg security game model (Nguyen and Xu, 2019).
The equilibrium points of this model were derived, enabling effective
defense against hackers’ reconnaissance of network assets.
2.2. Multi-intelligence reinforcement learning in cyber defense
Many researchers have introduced reinforcement learning algo-
rithms into defense methods to tackle attacks at various stages. Rein-
forcement learning can be applied to enhance the defense capabilities
of the network itself or to increase the robustness of network defense
infrastructure.
In terms of managing the network security space, Hammar and
Stadler (2021) abstract the intrusion detection problem as an optimal
stopping problem and use deep reinforcement learning (DRL) to com-
pute the optimal stopping time. This method characterizes network
defense measures using a simple optimal stopping problem, highlight-
ing the effectiveness of reinforcement learning in complex conceptual
problems. Xu et al. (2022) focus on eavesdropping attacks, a typical
form of network attack. They discuss the continuous control capabilities
of DRL in a network environment, using routing as the control object
to defend against eavesdropping attacks through route randomization.
Optimizing communication strategies to enhance system security is
another important application of reinforcement learning Li et al. (2023)
utilize DRL algorithms to predict network attack paths in industrial
networked physical systems, providing a new approach for attack
traceability and path analysis.
For enhancing network security infrastructure, researchers have ap-
plied reinforcement learning’s dynamic response and autonomous feed-
back mechanisms to energy network intrusion detection systems (Zolo-
tukhin et al., 2020), edge network intrusion defense systems (Liu
Computers & Security 142 (2024) 103871
4Y. Tang et al.
Fig. 2. Network attack and defense hypergame model construction and iterative processes.
et al. , 2020 ), and software-defined wireless network intrusion response
systems ( Cardellini et al. , 2022 ), contributing to network security warn-
ings and blocking attack behaviors at the intrusion stage ( Adawadkar
and Kulkarni , 2022 ). Wang et al. (2020 ) dynamically deploy deceptive
resources using reinforcement agents’ policy generation, efficiently
protecting internal network resources and trapping intruding attack-
ers. Zhong et al. (2022 ) focus on the robustness of reinforcement
learning models for detection and utilize reinforcement learning meth-
ods to counteract the generation of evasive detection samples. By
expanding the machine learning training set and increasing the barriers
for attackers to bypass detection, they protect the security of critical
system assets.
MARL extends the discussion of the correlation between agents
based on single-agent reinforcement learning, providing a new direc-
tion for research in the field of autonomous network defense ( Nguyen
and Reddi , 2023 ; Li et al. , 2022 ). The field of intrusion detection
is an early adopter of MARL technology, with many classic works
summarized in the literature ( Saeed et al. , 2020 ), many of which have
reached the state-of-the-art level. Adaptive defense or moving target
defense is a widely discussed field at present. The literature closely
related to this paper ( Sengupta and Kambhampati , 2020 ) uses Bayesian
Stackelberg Markov Games to study moving target defense problems,
and iteratively solves the equilibrium state through an independent Q-
learning MARL algorithm to achieve moving target defense. In the field
of dynamic access control, the literature ( Jin and Wang , 2020 ) uses
multi-agent deep deterministic policy gradient (MADDPG) to optimize
traffic allocation strategies, showing that reinforcement learning has
application prospects in collaborative threat mitigation based on zero
trust. Some recent works ( Alshamrani and Alshahrani , 2023 ) have
also made attempts to assist network defense with MARL using the
multi-armed bandit algorithm, and discussed the impact of various
reward situations on the strategy. Deception is a behavior with covert
rationality, and it is also a key point of discussion in MARL methods.
The literature ( Du et al. , 2022 ) discusses the dynamic nature of network
deception defense in the attack graph scenario, uses reinforcement
learning algorithms to self-play the two-person Markov game attack
and defense model, and achieves adaptive network deception defense
resource deployment by the defender. Similarly, the literature ( Kong
et al. , 2023 ) also discusses the deployment problem of deception de-
fense resources. The difference is that the author models the network
attack and defense problem as a maze pathfinding model, and obtains
the real-time deployment strategy of deception assets by solving the
Nash equilibrium of multi-agent stochastic games. Cheah et al. (2023 )
is closely related to our paper. They propose a network cooperative
defense autonomous decision-making framework in the scenario of au-
tonomous vehicle queue driving, decompose deep rational cooperative
defense actions, such as network deception behavior, to sub-agents,similar to dynamic programming optimization methods, to achieve
multi-agent cooperative autonomous network defense.
Building upon previous efforts, firstly, the Stackelberg attack and
defense hypergame model we discuss can effectively overcome the
limitations of previous models, especially the Bayesian game model,
in describing the impact of deceptive incomplete information char-
acteristics on the overall network situation. The introduction of the
hypergame model makes deceptive defense behavior more rationally
profound, and by including the defense follower in the cooperative
constraint to participate in the game, it is an exploration of cooperative
defense games. Secondly, we use MARL methods as the driving force
for the evolution of dynamic games, overcoming the limitations of
numerical calculation methods that are difficult to explore complex
game spaces and conventional heuristic methods that lack rational
exploration. The method in this paper reflects the equilibrium state
through the action strategy of the reinforcement learning agent, which
is a new attempt to generate real-time dynamic defense responses.
Thirdly, the problem of the evolution of game models with high di-
mensions or complexity in the action space of multiple agents has
always been a shackle for the application of reinforcement learning.
More direct iterative methods may not form deep decisions, and more
complex iterative algorithms may not converge. This paper decomposes
the decision-making process, introduces hierarchical thinking, reduces
the dimension and decouples the action space, effectively improving
the application efficiency of reinforcement learning.
3. Cyber attack and defense hypergame model
3.1. Construction of network attack and defense hypergame model
The network attack and defense hypergame model accurately de-
picts the scenario of multi-agent autonomous network defense, reflect-
ing the interaction of incomplete information, imperfect information
observation, and cognitive bias decision-making in cyberspace con-
flicts. At the same time, it analyzes the Nash equilibrium of the network
confrontation strategies of the attack and defense sides and explores
the coalition core form of cooperative defense. The network attack and
defense hypergame model is shown in Fig. 2.
The network attack and defense hypergame model consists of two
parts. The first part is described by the SHM 1of the attack and
defense sides, and the second part is composed of the cooperative game
model 2formed by multiple agents. The defensive agent participates
in1as a follower, and its own strategy is constrained by 2. (1) gives
the relationship between 1and2. In the composite game model ,
all defenders form a cooperative follower alliance f to participate in the
game 1.
∶= {1,2} (1)
The definitions of 1and2will be detailed separately below.
Computers & Security 142 (2024) 103871
5Y. Tang et al.
3.1.1. Network attack and defense SHM
In the scenario of multi-agent cooperative defense, 𝑘defenders form
an alliance 𝑓, which responds to system attacks as followers in the
SHM, protecting resources in 𝑛network systems. The attacker is defined
as the leader 𝑙, considering the attacker’s first-mover advantage in
general network attack and defense and its continuous influence. (2)
gives the definition of the SHM 1
𝑙𝑓
1
𝑙𝑓={{𝑙} ∪𝑓,𝑆𝑙×𝑆𝑓,{𝑃𝑙} ∪𝑃𝑓,𝛩}(2)
We consider the strategy space R, where𝑆𝑙is the strategy of the
leader,𝑆𝑙∈R𝐾, and𝑆𝑓is the strategy of the follower, 𝑆𝑓=𝑆1×
⋯×𝑆𝑘, where𝑆𝑖is the strategy of participant 𝑖.𝛩is used to describe
the impact of information transmission, 𝛩= {𝜃𝑙,𝜃𝑓}, considering the
hyperparameter 𝜃𝑙as the influence on the leader, and 𝜃𝑓= {𝜃1,…,𝜃𝑘}
as the influence on the followers. This inaccurate observation will affect
the payoff function, the leader’s payoff function 𝑃𝑙∶𝑆𝑙×𝑆𝑓×𝛩→R,
and the follower’s payoff function 𝑃𝑓= {𝑃1,…,𝑃𝑘}, where the payoff
of the𝑖th follower comes from 𝑃𝑖∶𝑆𝑙×𝑆𝑖×𝛩→R.
After giving the definition of the hypergame model, we add the
description of network facilities to express various game elements. The
set of network facilities is defined as = {𝐼1,…,𝐼𝑛}, and the leader,
i.e., the attacker, has resources 𝑅𝑙to execute actions x. The attack
behavior on network facilities can be described as∑𝑛
𝑗=1𝑥(𝐼𝑗)=𝑅𝑙, and
the attacker’s strategy is represented as (3):
𝑆𝑙= {𝑥|𝑛∑
𝑗=1𝑥(𝐼𝑗)=𝑅𝑙,𝑥(𝐼𝑗)≥0} (3)
Similarly, the follower, i.e., the defender 𝑖, has resources 𝑅𝑖to exe-
cute actions y. The defense behavior of defender i on network facilities
can be defined as∑𝑛
𝑗=1𝑦(𝐼𝑗)=𝑅𝑖, and the defender’s strategy can be
represented as (4):
𝑆𝑖= {𝑦𝑖|𝑛∑
𝑗=1𝑦(𝐼𝑗)
𝑖=𝑅𝑖,𝑦(𝐼𝑗)
𝑖≥0},∀𝑖∈𝑓 (4)
After giving a detailed expression of the strategy, we also need to
discuss the definition of payoff. For network facility 𝐼𝑗, the attacker gets
a payoff of 𝑃𝛼
𝑙(𝐼𝑗)when executing action 𝑥(𝐼𝑗), and a payoff of 𝑃𝛽
𝑙(𝐼𝑗)
when not executing action 𝑥(𝑘𝑗). The defender gets a payoff of 𝑃𝛼
𝑖(𝐼𝑗)
when executing action 𝑦(𝐼𝑗), and a payoff of 𝑃𝛽
𝑖(𝐼𝑗)when not executing
action𝑦(𝐼𝑗). The payoff functions of the attacker and the defender under
the strategy profile (𝑥,𝑦,{𝜃𝑙,𝜃𝑓})are as shown in (5) for the attacker
and (6) for the defender.
𝑃𝑙(𝑥,𝑦,𝜃𝑙)
=𝑛∑
𝑗=1(𝑛∑
𝑖=1𝑦(
𝐼𝑗)
𝑖)(𝑥(
𝐼𝑗)
𝑃𝛼
𝑙(𝐼𝑗,𝜃𝑙)+(𝑅𝑙−𝑥(𝐼𝑗))𝑃𝛽
𝑙(𝐼𝑗,𝜃𝑙)) (5)
𝑃𝑖(𝑥,𝑦𝑖,𝜃𝑖)
=𝑛∑
𝑖=1𝑦(𝐼𝑗)
𝑖(𝑥(𝐼𝑗)𝑃𝛼
𝑖(𝐼𝑗,𝜃𝑖) + (𝑅𝑙−𝑥(𝐼𝑗))𝑃𝛽
𝑖(𝐼𝑗,𝜃𝑖))(6)
Based on the above information, we can present the Hyper Nash
Equilibrium(HNE) of the SHM, which is the optimal action and re-
sponse given in the case of information deficiency. (7) expresses the
attacker’s action strategy (𝑥∗,𝑦∗), and (8) and (9) express the defender’s
optimal response 𝐵𝑅(𝑥,𝜃𝑓).
(𝑥∗,𝑦∗) ∈ argmax
𝑥∈𝑙,𝑦∈𝐵𝑅(𝑥,𝜃𝑓)𝑃𝑙(𝑥,𝑦,𝜃𝑙) (7)
𝐵𝑅𝑖(𝑥,𝜃𝑖) = argmax
𝑦𝑖∈𝑖𝑃𝑖(𝑥,𝑦𝑖,𝜃𝑖),∀𝑖∈𝑓 (8)
𝐵𝑅(𝑥,𝜃𝑓) =𝐵𝑅1(𝑥,𝜃1) ×⋯×𝐵𝑅𝑘(𝑥,𝜃𝑘) (9)
With the above SHM constructed, we introduce assumptions widely
used in the analysis of network security games to illustrate the HNE of
this model:Assumption 1. 𝛩is compact and convex, the impact of 𝜃𝑙may be
nothing.
Assumption 2. For𝑓= 1,…,𝑘,𝑃𝛼
𝑖(𝐼𝑗,𝜃𝑖)and𝑃𝛽
𝑖(𝐼𝑗,𝜃𝑖)are differen-
tiable in𝜃𝑖∈𝛩.
Based on Lemma 2 and Theorem 1 of the Cheng et al. (2022), under
Assumptions 1 and 2, for ∀𝜃𝑙,𝜃𝑖∈𝛩, SHM has an HNE. The reason is
that for ∀(𝑥,𝑦),𝑥∈𝑆𝑙,𝑦∈𝑆𝑓, under the game sequence 𝑚, the leader’s
strategy converges to the optimal action sequence.
𝑃𝑙(𝑥∗
𝑚,𝑦∗
𝑚,𝜃𝑙) = lim𝑚→∞max
𝑥𝑚∈𝑙,𝑦∈𝐵𝑅(𝑥,𝜃𝑓)𝑃𝑙(𝑥𝑚,𝑦𝑚,𝜃𝑙) (10)
That is,𝑥∗∈ arg max𝑥∈𝑆𝑙𝑃𝑙(𝑥,𝑦∗,𝜃𝑙)forms the optimal response
sequence𝑥∗
𝑚. For the follower, there also exists an optimal response
sequence𝑦∗
𝑚composed of optimal response actions 𝑦∗∈𝐵𝑅(𝑥∗,𝜃𝑓).
However, the stability of this equilibrium is insufficient. The main
reason for the instability lies in 𝛩. The loss of information and the
incorrect transmission of ̂𝜃can lead to the game participants obtaining
𝑃𝑙(𝑥∗
𝑚,𝑦∗
𝑚,̂𝜃𝑙)< 𝑃𝑙(𝑥∗
𝑚,𝑦∗
𝑚,𝜃𝑙),𝑃𝑓(𝑥∗
𝑚,𝑦∗
𝑚,̂𝜃𝑓)< 𝑃𝑓(𝑥∗
𝑚,𝑦∗
𝑚,𝜃𝑓), thus the
strategy convergence will be affected.
3.1.2. Defender coalition game model
The above provides the main game forms of network attack and
defense. Another constraint to consider in this study is the cooperative
relationship constraint among the defense game players. This constraint
is reflected in the defender’s transferable payoff function. The following
defines this constraint through the definition of the cooperative game
model. (11) gives the coalition game 2
𝑓.
2
𝑓= {𝑓,𝑓} (11)
A coalition 𝑓is formed by 𝑘defenders, satisfying conditions (a)⋃𝑘
𝑖=1𝑖=𝑓, (b)𝑖∩𝑗= ∅,∀𝑖≠𝑗, which determines the structure of
the cooperative coalition. The coalition combined payoff 𝑃𝑓satisfies
conditions (a) 𝑃𝑓≥𝑃𝑖(b)∑
𝑖∈𝑓𝑃𝑖=𝑃𝑓, which determines the way
of profit distribution. In addition, defenders in the coalition will share
the perception information of the environment, the impact of lost and
incorrectly transmitted information will be weakened in the sharing
link, that is, 𝜃𝑓≤∑𝑛
𝑖=1𝜃𝑖.
The cooperation mode between defenders is based on the formation
of the core, and due to the existence of shared factors, the form of the
coalition will not affect the convexity of SHM.
𝑐𝑜𝑟𝑒 = {𝑦𝑖∈𝑆𝑖|𝑃𝑓(𝑥,𝑦𝑓,𝜃𝑓)≥∑
𝑖∈𝑓𝑃𝑖(𝑥,𝑦𝑖,𝜃𝑖)} (12)
3.1.3. Discussion on attack and defense SHM and HNE
To clarify the game model proposed in this paper and the dynamiza-
tion process of this model in the following text, we further clarify and
summarize the mathematical definition of this model, deepening the
discussion on the existence of HNE.
The game model proposed in this paper is composed of the SHM 1
𝑙𝑓
describing cyberspace conflicts and the coalition game 2
𝑓describing
multi-defender cooperative defense, specifically in the form of (13).
∶={1
𝑙𝑓={𝑙∪𝑓,𝑆𝑙×𝑆𝑓,𝑃𝑙∪𝑃𝑓,𝛩},
2
𝑓={𝑓,𝑃𝑓}}
(13)
The optimal solution problem of network attack and defense strategy
corresponding to the HNE of this hypergame model is described as (14).
𝑚𝑎𝑥 𝐵𝑅 (𝑥,𝜃𝑓) =𝐵𝑅1(𝑥,𝜃1) ×⋯×𝐵𝑅𝑘(𝑥,𝜃𝑘)
𝑠.𝑡. 𝐵𝑅𝑖(𝑥,𝜃𝑖) = argmax
𝑦𝑖∈𝑆𝑖𝑃𝑖(𝑥,𝑦𝑖,𝜃𝑖)
𝑃𝑓(𝑥,𝑦𝑓,𝜃𝑓)≥∑
𝑖∈𝑓𝑃𝑖(𝑥,𝑦𝑖,𝜃𝑖)
𝑥∈𝑆𝑙,𝑦∈𝑆𝑓.(14)
Computers & Security 142 (2024) 103871
6Y. Tang et al.
Regarding the existence of HNE in this game model, it is detailed
inTheorem 1 of Cheng et al. (2022 ). The difference between this
paper and the problem discussed in it is that the determined SHM is
a single defender as the leader and multiple attackers as followers. The
core issue it discusses is the HNE stability conditions under the two
situations of misunderstanding and deception. Of course, to perfect the
work of this paper, a rigorous approach to proving the existence of HNE
is also provided here.
Proof. Under the premise of satisfying Assumptions 1 and 2, when
𝜃′
𝑖∈𝛩, there is a defense gain 𝑃𝑖(𝑥,𝑦𝑖,𝜃′
𝑖), we define 𝐹(𝑥,𝑦) = {(̂ 𝑥, ̂ 𝑦)|̂ 𝑥∈
arg max𝑥∈𝑆𝑙,𝑦∈𝐵𝑅(𝑥,𝜃𝑓)𝑃𝑙(𝑥,𝑦,𝜃𝑙),̂ 𝑦∈𝐵𝑅(𝑥,𝜃𝑓)}to obtain the optimal
action in the current strategy set. Because, as known from definitions
(3) and (4),𝑆𝑙×𝑆𝑓is compact, so there exists a convergent subsequence
{(̂ 𝑥𝑗𝑚,(̂ 𝑦)𝑗𝑚)}∞
𝑚=1, converging to lim𝑚→∞(̂ 𝑥𝑗𝑚,(̂ 𝑦)𝑗𝑚) = (̂ 𝑥∗,̂ 𝑦∗). Therefore,
it only needs to prove that the result of the limit convergence is in the
range of the function F, i.e., (̂ 𝑥∗,̂ 𝑦∗) ∈𝐹(𝑥,𝑦), to show that has an
HNE.
In the leader’s perspective, 𝑃𝑙(̂ 𝑥∗,𝑦∗) = lim𝑚→∞𝑃𝑙(̂ 𝑥𝑗𝑚,𝑦𝑗𝑚) =
lim𝑚→∞max𝑥′∈𝑆𝑙𝑃𝑙(𝑥′,𝑦𝑗𝑚). From Lemma 17.30 in the literature ( Guide ,
2006 ), we can get lim𝑚→∞max𝑥′∈𝑆𝑙𝑃𝑙(𝑥′,𝑦𝑗𝑚) = max𝑥′∈𝑆𝑙𝑃𝑙(𝑥′,𝑦∗),
thereforê 𝑥∗= arg max𝑥′∈𝑆𝑙𝑃𝑙(𝑥′,𝑦∗). Similarly, from the follower’s
perspective, ̂ 𝑦∗∈𝐵𝑅(𝑥,𝜃′), i.e., (̂ 𝑥∗,̂ 𝑦∗) ∈𝐹(𝑥,𝑦). Through Theorem
A.14 in the literature ( Carmona , 2012 ),(̂ 𝑥∗,̂ 𝑦∗)can be generalized to
the general case, i.e., there exists (𝑥′,𝑦′)such that for the leader and
each follower, 𝑥∗= arg max𝑥∈𝑆𝑙𝑃𝑙(𝑥,𝑦∗)and𝑦′∈𝐵𝑅𝑖(𝑥′,𝜃′), therefore,
there exists an HNE (𝑥′,𝑦′)of.
3.2. Hypergame model dynamic analysis
Network attack and defense is a dynamic and ongoing process, and
network defense work needs to extend the normal operation time of
the network core functions as much as possible during the ongoing
network attack process. Defenders and attackers will reach the game
equilibrium point multiple times in the game space, which is reflected
in (9) where the defender’s optimal response will change with the game
environment. It is almost impossible to complete the numerical analysis
of dynamic evolutionary equilibrium under the super game model, and
its complex game process is the main obstacle to model evolution and
control analysis. For this reason, this paper continues to use the symbols
from the previous text, describes the evolution process of the game
model with the Markov decision process, and embeds the feedback
learning mechanism to guide the evolution path.
Definition 1. Multi-player Network Defense Markov Game Process
from an Infinite Discrete Time Perspective
(1) The set of game players is 𝐹= {1,2,…,𝑘}, where each player
represents one or a type of defender. Define the discrete time set
+= {1,2,…}, at each moment 𝑡∈+, the game players execute
actions and observe the network game space.
(2) Define the abstract action space 𝑖,∀𝑖∈𝑓. In network attack
and defense, different defenders have different action spaces.
In discrete time, 𝑦𝑡
𝑖∈𝑖, and an action tuple (𝑦𝑡
1,𝑦𝑡
2,…,𝑦𝑡
𝑘)
constitutes the strategy 𝑆𝐹.
(3) Define the observation space : there is a state observation 𝑜𝑡∈
in the network attack and defense game space at each moment.
(4) Define the state transition process 𝑇∶×∏
𝑖∈𝑓𝑖→𝛥(). In
discrete time, the next moment state 𝑜𝑡+1∼𝑇(𝑜𝑡,𝑦𝑡), where𝑦𝑡is
the set of actions made by the set of defenders.
(5) Define the transition utility 𝑃𝑖∶×∏
𝑖𝑖. At moment 𝑡, the
utility𝑃𝑡
𝑖(𝑠𝑡,𝑦𝑡)can be calculated.
(6) Define the discount factor 𝛾, which adjusts the expected reward∑∞
𝑡=1𝛾𝑡𝑃𝑖(𝑠𝑡,𝑦𝑡).
Fig. 3. Multi-intelligent hierarchical reinforcement learning of autonomous network
defense processes.
Based on the above definitions, from moment 𝑡to𝑡+ 1, we have
constructed a feedback mechanism (15) through utility transition and
action execution.
𝑀𝑡∶𝑡+1
𝑖= {{𝑃𝑡∶𝑡+1
𝑖,𝑦𝑡∶𝑡+1
𝑖}} (15)
By guiding feedback through learning utility transitions, the model
learns a collaborative defense strategy. In (16), we define the utility
iteration over time, and in (17), we define the utility iteration guiding
strategy𝑆𝑓.
𝑃𝑡+1
𝑖= (1 −𝜇𝑡
𝑖)𝑃𝑡
𝑖+𝜇𝑡
𝑖𝐺𝑡
𝑖(𝑆𝑡
𝑖,𝑃𝑡
𝑖,𝑦𝑡
𝑖) (16)
𝑆𝑡+1
𝑖= (1 −𝜆𝑡
𝑖)𝜋𝑡
𝑖+𝜆𝑡
𝑖𝐻𝑡
𝑖(𝑆𝑡
𝑖,𝑃𝑡+1
𝑖,𝑦𝑡
𝑖) (17)
Where𝜇and𝜆represent the learning rates, 𝐺𝑡
𝑖and𝐻𝑡
𝑖are the strategies
of utility evolution and strategy evolution constructed by the feedback
mechanism, and the evolution of strategies is implemented by the
reinforcement learning agent.
4. HMARL network defense framework
In this section, we present a dynamic evolutionary game model
and propose a HMARL approach for autonomous network defense
decision-making. Firstly, we decompose the problem to establish a
hierarchical reinforcement learning framework. Then, we introduce
individual agents that handle different subproblems.
4.1. Problem decomposition and hierarchical framework
In the evolutionary process of the game model, the strategy profile
determined encounters several challenges: the transition probabilities
Computers & Security 142 (2024) 103871
7Y. Tang et al.
of strategy states are unknown, the network attack-defense situation is
highly dynamic, information transmission among players is uncertain,
and the cascading effect of multi-agent collaboration all contribute to
an extremely large state space, which affects action decision-making.
To address these challenges, this paper introduces the idea of hierar-
chical reinforcement learning and decomposes defense decision-making
into two subproblems: defense action type selection and defense action
execution. Different-level defense agents are established based on the
decomposition of action space for each subproblem.
In complex scenarios with intricate state and action spaces, in-
troducing hierarchical reinforcement learning to decompose the state
and action spaces proves to be an effective method for reducing the
difficulty of policy search. This approach is widely applied in the field
of complex control and optimization. For example, in Shi et al. (2020),
the behavior of unmanned aerial vehicles (UAVs) for cellular services is
decoupled into separate path planning and resource allocation subprob-
lems, enabling step-by-step decision-making at different granularities
and significantly improving network throughput. Tran et al. (2021),
from the perspective of security penetration testing, address the expo-
nential expansion of action space with increasing network complexity
by using a hierarchical reinforcement learning approach to decouple
the action space and decompose the penetration process. This allows for
faster and more stable identification of optimal attack strategies. Previ-
ous efforts provide us with valuable insights. Generating collaborative
defense strategies involves searching for optimal solutions in a complex
and vast game space, where the dimension and scale of the action space
are immense. Therefore, in this paper, the task of autonomous network
defense is decomposed into two subproblems: the selection of different
types of agents and the defense decision-making of each agent.
We establish the first-level defense agent selector to address the
subproblem of selecting different types of agents. In this paper, we
categorize defense behaviors into three types: perceiving and analyzing
network situations, removing threats and restoring infrastructure func-
tionality, and deploying deceptive defense nodes. Then, we construct an
agent selector to handle different network states. The selector activates
the appropriate type of agent to perform the corresponding defense
actions.
Next, we establish the second-level defense action execution agents
to address the subproblem of defense decision-making by the agents.
The defense functionality of different types of defense agents is deter-
mined by their action spaces. When activated, defense agents explore
the action space to determine optimal responses and formulate defense
strategies.
Overall, in Fig. 3, we describe the process of autonomous network
defense using HMARL. Firstly, the defender perceives the attack be-
havior, which is transformed into the observation space. Then, based
on the current network situation, the first-level selector chooses the
optimal defense action type and activates the corresponding type of
agent. Subsequently, the respective agent receives state observations
and makes defense decisions based on them, determining action param-
eters including defense action subtypes and execution target parame-
ters. Finally, the defender executes the defense strategy in response to
the attack behavior.
Furthermore, this paper proposes an independent two-layer agent
architecture, which separates the training and operational phases. The
agents are only allowed to engage in real-world operations within a
network simulation environment once their policies have converged.
4.2. Layer 1 agent selector design
Given the clear task characteristics of this level, the Q-Tabular
algorithm is used to quickly converge and solve the subproblem. Let us
start by introducing the design of the action space, observation space,
and reward function4.2.1. Action space design
In Table 2, we provide the categorization of the action space.
We classify defense behaviors into four categories, corresponding to
the Layer 2 defense agents. The selector is responsible for choosing
one category of agent based on the current network situation. The
definitions of these defense actions are based on the Open Command
and Control (OpenC2) specifications.
The action space for the selector is provided in the first column of
Table 2, which includes perception action type, deception action type,
recovery action type, and sleep type.
The perception action type is responsible for detecting the overall
network situation. It abstracts the functionalities of network situational
awareness devices and cyberspace probing devices.
The deception action type aims to implement deceptive defense
strategies by transmitting misleading information to attackers in a way
that simulates genuine business activities. It abstracts the functionali-
ties of sandbox and honeypot devices.
The recovery action type involves defensive actions against network
attacks. This includes clearing malicious session connections, ensuring
that user privileges do not compromise system security, and resetting
compromised devices. It abstracts the actions performed by security
experts in network attack and defense scenarios.
The sleep action allows the selector to skip the current game round
and not allocate defense resources.
4.2.2. Observation space design
The observation space is defined as a collection of asset fingerprints
in the network space, which includes the following six aspects: host
information, interface information, process information, session infor-
mation, system information, and user information. These six types of in-
formation are used to distinguish hosts, establish network connections,
provide various types of services for each user, and abstractly present
the overall cyberspace profile. In addition, the formal definitions of
each field also support reinforcement learning training.
4.2.3. Reward function design
The selector’s task is clear and the action space is discrete, but
guiding the selector to make the correct choice requires two parts of
rewards. On one hand, there is the reward from the environment for
the effectiveness of the defensive behavior. On the other hand, there
is the internal reward of the selector. The former part of the reward is
given by the environment after each sub-agent has completed training
and cascades to perform defensive actions under the controller. The
latter part is the reward given by the controller for correctly selecting
the sub-agent. (18) provides the definition of the controller’s reward.
𝑅𝑡
𝐿1= −[𝐶∕(𝑟𝑒𝑤𝑎𝑟𝑑 (𝑜𝑡,𝑎𝑡
𝐿2))] (18)
At time t, the controller’s reward is 𝑅𝑡
𝐿1, and𝑟𝑒𝑤𝑎𝑟𝑑 (𝑜𝑡,𝑎𝑡
𝐿2)is
the reward for the effectiveness of the defensive action given by the
environment, the construction of which will be detailed in the reward
functions of various defensive agents. 𝐶is the reward for agent selec-
tion judgment, which is based on the experience of human security
experts when obvious attack behavior is detected. The reward value
is−1 or 1. If the selector’s judgment is the same as the expert’s
knowledge, 𝐶is 1; if it is different, 𝐶is -1. In this way, the selector
can balance between quickly converging using expert prior knowledge
and exploring different choices.
4.3. Design of various agents
Various types of defensive agents are parallel to the agent selector
cascade. When activated by the selector, defensive agents inherit the
observation space and make decisions based on their own action space,
generating defensive strategies. In this paper, the observation space
received by each agent is the same, as introduced in the second part
of the previous section. The reward function is a common reward from
the environment. Neural networks are used to explore strategies from
complex action spaces, so we use the Proximal Policy Optimization
(PPO) algorithm as the training algorithm in the design of sub-agents.
Computers & Security 142 (2024) 103871
8Y. Tang et al.
Table 2
Action space classification.
Type & OpenC2 Action ID Action Purpose
Perceive (1 & 30)Monitor Gathering network situational information
Analyse Identify aggressive behaviors
Decoy (7 & 18)Decoy Apache Pretending to be an Apache service
Decoy Femitter Pretending to be an Femitter service
Decoy HarakaSMPT Pretending to be an HarakaSMPT service
Decoy SSHD Pretending to be an SSHD service
Decoy Svchost Pretending to be an Svchost process
Decoy Tomcat Pretending to be an Tomcat service
Recover (10 & 23)Remove Kill malicious processes
Restore Restoring a system to good state
Sleep (–) Sleep Do Nothing
4.3.1. Action space design
The action type of each type of defensive agent is a distinguish-
ing point of the agent, as listed in the second column of Table 2.
Different action types require different action parameters. It can be
intuitively understood that the complexity of action parameters is an
important reason for the large scale of the action space. The parameters
that need to be determined in the action space mainly include three
parts: parameters for constructing network communication, including
IP addresses and subnet divisions; parameters for operating various
services, including target information, processes, and port numbers;
and parameters for determining operation targets, including target
sessions, host names, user names, and privilege passwords. After the
agent takes action, it will correspondingly change the network space
situation, realize game evolution, and coordinate defense.
4.3.2. Reward function design
The sub-agent reward function (15) is based on network situa-
tion and action cost. Here, 𝑟𝑖𝑠𝑘(𝑜𝑡)is a network situation risk map-
ping, which maps different types of hosts receiving attacks to different
penalty values, enabling the agent to learn strategies to protect key
assets.𝑐𝑜𝑠𝑡(𝑎𝑡
𝐿2)maps different defensive actions to different action
costs, enabling the agent to learn how to protect the system at the
smallest cost. Both types of mapping values are hyperparameters, which
we provide in Table 4.
𝑟𝑒𝑤𝑎𝑟𝑑 (𝑜𝑡,𝑎𝑡
𝐿2) =𝑟𝑖𝑠𝑘(𝑜𝑡) +𝑐𝑜𝑠𝑡(𝑎𝑡
𝐿2) (19)
4.4. Multi-agents hierarchical reinforcement learning model training
The components described above construct a HMARL model, the
overall framework of which is shown in Fig. 4. The figure shows
the connection relationships between the components and reflects the
implementation of different algorithms by each layer of agents within
the framework.
In this hierarchical reinforcement learning model, the internal re-
ward of the first layer agent selector is influenced by the global reward,
so the second layer sub-agents are trained first in the algorithm im-
plementation. During the bottom-up training process, each type of
sub-agent inputs the global observation and global reward of the net-
work attack and defense environment. Here, the global observation
is equivalent to each sub-agent being able to perform perception op-
erations, so the recovery agent and decoy agent need to be trained
separately during the training process. The converged sub-agents are
cascaded in parallel under the first layer agent selector. The first layer
agent selector transforms the global reward into an internal reward and
guides the Q-table algorithm together with expert knowledge, enabling
it to quickly converge on the strategy of selecting agents. After the
overall training is completed, the hierarchical reinforcement learning
model can formulate coordinated defense strategies from top to bottom.
The selection of the training algorithm for the agents depends on
the tasks faced by agents. The choice of training algorithms for the
two-layer agent architecture is the result of carefully balancing the
Fig. 4. HMARL framework.
task requirements and the algorithmic properties to ensure efficient and
effective learning for the agents in the given cybersecurity context.
The task for the second-layer sub-agents is to select the sub-action
type under the current security action space and generate the corre-
sponding sub-action parameters, which is a policy generation type of
task. It is not suitable to use the value iteration reinforcement learning
algorithm, so the policy iteration method is utilized for training. The
PPO is a relatively advanced policy iteration algorithm. Compared
to its predecessor, the Trust Region Policy Optimization (TRPO) al-
gorithm, PPO has a smaller computational cost and faster iteration,
making it more competitive. The Deep Deterministic Policy Gradient
(DDPG) algorithm is not an ideal choice here, as it trains multiple
neural networks (typically 4) which makes the method highly sensitive
to the environment and hyperparameters, resulting in difficulty in
convergence. Therefore, the sub-agents are trained using the ‘‘Clip’’
optimization method of the PPO algorithm.
The task for the first-layer selector is to choose one or more sub-
agents given different security situations, such that the current defense
strategy yields the highest reward. This is a task with a relatively small
action space and a clear correspondence between the actions and re-
wards. Hence, the Q-Tabular algorithm is selected for training the selec-
tor. This algorithm is simple and efficient, and can also be accelerated
by incorporating expert knowledge to speed up convergence.
Computers & Security 142 (2024) 103871
9Y. Tang et al.
Fig. 5. Network scenario.
5. Autonomous cyber defense use cases and simulation
5.1. Autonomous cyber defense use cases
This paper validates a HMARL autonomous network defense method
using a simulated environment. The Cyber Operations Research Gym
(CybORG) ( Cyber operations research gym , 2022a ), developed by
Maxwell Standen, provides a network security research environment
for training and developing autonomous agents. The CybORG tool gen-
erates a scenario based on pre-generated descriptions, initializes a set
of agents to perform roles in that scenario, and then implements their
actions and evaluates their effectiveness. We consider the following
advantages of this tool in establishing network attack and defense
scenarios. Firstly, in terms of the fidelity of cyberspace scenarios, this
toolkit uses cloud-based virtual machines for simulation and a gen-
eral interface for simulating network environments. Through a highly
modular design, scenarios modeled in finite state machines are highly
realistic. Secondly, in terms of the fidelity of network operations, this
toolkit defines simulated operation behaviors as state transitions, simu-
lating the execution effects of commands with appropriate parameters
in real scenarios.
5.1.1. Network attack and defense scenarios and attacker behavior patterns
This paper considers a general network service scenario as an au-
tonomous network defense use case, and the specific settings of the use
case are given in the related introduction of Cage Challenge 2 ( Cyber
autonomy gym for experimentation challenge 2 , 2022b ). The network
framework is given in Fig. 5, which divides the network into three
subnets. Hosts within a subnet can only communicate with hosts within
the same or adjacent subnets. Subnet 1 is composed of ordinary users,
which are non-critical user hosts. Subnet 2 is composed of enterprise
servers, which are the objects accessed by users in Subnet 1. Subnet
3 is composed of enterprise server management hosts, including key
operation servers and three user hosts. The network communication
process is mapped to discrete time states. In a discrete state, ordinary
users initiate normal business requests to enterprise servers, while at-
tackers and defenders select actions from their respective action sets to
operate the network. The execution order is: defender action, ordinary
user action, attacker action.
The attacker’s high-level attack action set is given in Table 3, with
attack actions defined in accordance with the Adversarial Tactics, Tech-
niques, and Common Knowledge (ATT&CK) Technique definitions. This
high-level action is converted into specific parameters executed in the
network environment through the CybORG tool, such as low-level oper-
ation instructions for hosts and ports, thereby realizing simulated attack
behavior. The attacker’s modus operandi begins with using a user host
in subnet 1 as a foothold. The attacker establishes communication with
subnet 2 and uses the open port list obtained through the ‘‘DiscoverNetwork Services’’ operation to determine which vulnerabilities are
available. In 75% of cases, the attacker will choose the highest-ranked
available vulnerability. In 25% of cases, one of the other available
vulnerabilities is chosen at random. After invading subnet 2, the at-
tacker will escalate privileges and maintains the session. Once they
have exploited a server in the enterprise network with the IP address
of the operating server, they can invade Subnet 3 and ultimately take
over the operating server through invasion means. The attacker’s goal
is to disrupt the service of the operating server for as long as possible
through the ‘‘impact’’ action, for which the attacker will attack the
operating server as directly as possible. The defender needs to prevent
the attacker’s behavior at each stage of the attack, discern whether the
communication is from the attacker or an ordinary user, and maintain
network functionality. Normal users only perform discovery operations
and do not perform destructive behavior on hosts. They prevent the
defender from thinking that all network activities are the work of the
attacker.
The following provides an ideal instance to illustrate the operation
process of the simulation environment. The defender can install moni-
toring tools on the host, collect and analyze event data, and determine
whether it is the malicious activity of the attacker or the normal activity
of a legitimate user. It can remove the attacker’s access permissions,
but this is only effective when the attacker’s permissions have not
been escalated, because once the attacker obtains system administrator
permissions, the defender will receive a negative reward. As long as
the attacker maintains administrator permissions, the defender will
continue to receive negative rewards. The defender can also restore
the system to a standard state to remove the attacker’s activities,
but this will destroy user data. Even after restoration, the attacker’s
initial foothold cannot be completely cleared, which is to ensure the
sustainability of the game and simulate the difficulty of clearing an
attacker who has obtained credentials in actual situations.
5.2. Simulation
Based on the use cases of network attack and defense scenarios in
the previous section, the following experiments are designed to verify
the performance of the HMARL autonomous network defense method.
Firstly, experiments were conducted on the bottom-up training process
and effect. Secondly, the decision-making performance of the top-down
defense strategy was evaluated. Finally, the agent’s defense strategy is
analyzed through the action distribution.
5.2.1. Experiment preparation
In Table 4, the experimental setup parameters and hyperparame-
ters for each layer of the reinforcement learning model are provided.
With these basic conditions, training is conducted with each episode
consisting of 100 steps, and a total of 1000 episodes are trained,
Computers & Security 142 (2024) 103871
10Y. Tang et al.
Table 3
Action space classification.
Type and ATT&CK technique Purpose
Discover Remote Systems
(ATT&CK Technique T1018)Remote System Discovery. Discovers new hosts/IP addresses in the network
Discover Network Services
(ATT&CK Technique T1046)Network Service Scanning. Discovers responsive services on a selected host by initiating a connection with that host.
Exploit Network Services
(ATT&CK Technique T1210)Exploitation of Remote Services. This action attempts to exploit a specified service on a remote system.
Escalate
(ATT&CK Tactic TA0004)Privilege Escalation. This action escalates the privilege of attacker on the host.
Impact
(ATT&CK Technique T1489)Service Stop. This action disrupts the performance of the network
Table 4
Experimental hyperparameter settings.
Entries Parameters
HardwareCPU AMD EPYC 7T83 64-Core 2.45 GHz
GPU NVIDIA GeForce RTX 4090 24 GB
RAM 512 GB
Disk 1 TB
SoftwareOS Ubuntu 22.04.4 jammy LTS x86_64
Python 3.9.17
Pytorch 2.1.2
PPO AlgorithmNumber of layers for actor network: 3
Number of layers for critic network: 3
Number of nodes for actor layers: (62, 64, 32)
Number of nodes for critic layers: (62, 64, 1)
Actor network active function: (ReLU, ReLU, Softmax)
Critic network active function: (ReLU, ReLU)
Learning rates: 0.02
Replay buffer size, batch size: 100000, 512
Maximal episode, steps per episode: 1000, 100
Episode clip: 0.2
RewardAttacker access User Hosts: −0.1
Attacker access Enterprise Servers: −1
Attacker access Operational Server: −1
Attacker access Operational Hosts: −0.1
The cost of decoy behavior, Maximum number of uses:
−0.05, 15%Maxstep
The cost of restore action: −1
The cost of remove action: −0.01
The cost of sleep action: 0
The cost of perceive action: −0.001
taking approximately 15 h. Among the hyperparameters, the design of
rewards is worth discussing. On one hand, the value of server resources
is much higher than that of hosts. On the other hand, the restore
action consumes the most defense resources, the decoy action has usage
limitations, and the perceive action is a cost-effective and efficient
defense method. Based on these considerations, the following reward
hyperparameters are designed.
5.2.2. Training of different agents and overall training status
The bottom-up training process is the fundamental training ap-
proach in hierarchical reinforcement learning. In this paper, the decoy
agent and recover agent are first trained individually, followed by
training the overall model using hierarchical reinforcement learning.
To incorporate the functionality of perceiving the safety situation,
the perceive action is trained in conjunction with both types of agents
during the training process. The most valuable part of the defense
strategy lies in the game played against the attack behavior. The sleep
action, which simply skips a game round, is a straightforward operation
and is not discussed further in this context.
In Figs. 6, we present important data regarding the training of
the hierarchical reinforcement learning model. In (a), we provide the
reward statistics during the training process. Overall, it can be observed
that the defense capability of the individual agents is inferior to that ofthe collaborative defense by multiple agents. When comparing the sub-
agents, the recover agent has a more complex action space compared
to the decoy agent, resulting in faster convergence for the decoy agent.
In Figs. 6(b), the Policy Loss of the two types of agents trained using
the PPO algorithm is shown. The slightly slower convergence of the
decoy agent’s policy loss compared to the recover agent’s supports the
observations made in (a).
In Figs. 6(c), the number of steps per episode is described. In (d)
and (e), the accumulation of rewards per episode is illustrated for
the case of 100 steps per episode. It can be seen that the majority
of rewards are concentrated, with only some dispersion in the initial
training phase. This indicates that the formation process of defense
strategies is relatively stable, and the agents explore various situations
during the initial evolution of the game.
5.2.3. Agent defense capability assessment
After the hierarchical reinforcement learning agents form collabo-
rative defense strategies, the paper conducts strategy evaluation and
analysis. The capabilities of the defense agents trained through rein-
forcement learning are demonstrated based on two aspects: the distri-
bution of autonomously formed defense strategies and the stability of
the strategies.
The stability of the strategy can to some extent reflect the ef-
fectiveness of the method proposed in this paper when applied to
different attack scenarios. In more complex attack scenarios, such as
zero-day vulnerabilities, advanced persistent threats (APTs), and supply
chain attacks, although the attack methods and techniques may vary,
from a defensive perspective, the attack process generally involves
reconnaissance, vulnerability exploitation, lateral movement, impact,
and exfiltration. In other words, the increasing complexity of attack
methods better conceals the attacker’s intentions, but does not change
the essential process of the attack. The core destructive effect of the at-
tack occurs during the impact phase. By coordinating multiple defense
measures to formulate defense strategies from the perspective of the
‘‘process’’, it is possible to detect malicious behaviors before the attack
occurs and then implement security policies to prevent the attack. Even
when an attack has occurred and caused damage, appropriate security
policies can be implemented to achieve resilient defense.
To evaluate the strategies, the trained agents, initially trained with
100 steps per episode, are placed in scenarios with 200 steps and 300
steps per episode. The evaluation results of 1000 episodes are presented
in Fig. 7. Additionally, Table 5 provides data statistics analysis for the
three scenarios.
In Fig. 7, it can be observed that as the number of steps increases
by 100, the reward fluctuation also increases. However, the incre-
mental consumption of defense resources remains relatively constant.
Table 5 provides statistics showing that the increase in defense resource
consumption is approximately 15 units of reward, with a standard
deviation increase of around 3 units.
These data reflect two key points. First, as the temporal dimension
of the game process increases, the complexity of the game space
also increases, leading to increased instability in the strategies. These
three factors are positively correlated, highlighting the importance
Computers & Security 142 (2024) 103871
11Y. Tang et al.
Fig. 6. Defense resource consumption in attack-defense games of different lengths.
Fig. 7. Defense resource consumption for offensive and defensive games of different
lengths.
of constructing autonomous defense decision-making capabilities in
cyberspace.
Second, the consistent incremental consumption of defense re-
sources indicates the effectiveness of the hierarchical reinforcement
learning method in constructing autonomous network defense strate-
gies. The defense strategies formed have the ability to maintain a
linear time span of elastic defense with linearly growing resource
consumption, which is challenging for human experts to achieve.
The almost unchanged standard deviation increment demonstrates the
relative stability of autonomous network defense strategies even in the
face of increased complexity in the game space.Some interesting conclusions are shown in the relevant results of
competitions with some literature and related toolkits related to this
article. We have selected some control data given in Table 6.
To align the test environment of this article and ensure variable
control, the results given in the table are the same as the simulation
environment and evaluation methods of this article. The data of CCS
and CardiffUni are from the official data of the TTCP CAGE Challenge
2 competition, and Wiebe’s results are from the literature ( Wiebe et al. ,
2023 ).
The data of CCS is a baseline test for this simulation environment,
and four types of tests have been done on the environment. Firstly,
it is the impact on the network environment by the attacker in the
absence of a defender. Secondly, the defense effect of executing defense
measures with a random defense strategy. Thirdly, the result of using
a simple rule defense agent, specifically, the result of executing restore
immediately upon detecting a change in host permissions. Fourthly,
the evaluation using the standard single-agent PPO algorithm. For the
simulation environment, the attacker’s attack strategy can cause serious
damage to network facilities. For the defender, the intelligence level of
the defense strategy is absolutely key to the defense effect. CardiffUni
is the champion team in the TTCP CAGE Challenge 2 competition. The
important reason for their solution to achieve such excellent results
is the manual analysis of the decoy placement action strategy. This
effort shows that for reinforcement learning methods, the overly large
strategy search space is still a huge challenge for the learning ability of
the agent. Pruning the action space manually and giving some deter-
ministic experiences in the initialization stage can help form intelligent
strategies. Their specific exploration steps are open on github.1Wiebe
and others have given a solution of multi-agent reinforcement learning.
The authors respectively apply the independent Q-learning ( Tan, 1993 )
algorithm and QMIX algorithm ( Rashid et al. , 2020 ) to discuss the
problem of network defense strategy generation. From the experimental
1https://github.com/john-cardiff/-cyborg-cage-2
Computers & Security 142 (2024) 103871
12Y. Tang et al.
Table 5
Results statistics of attack-defense games of different lengths.
Game length (steps) Mean Standard deviation Mean value change Standard deviation change
100 −16.5752 4.027 −0 +0
200 −31.7533 7.014 −15.1781 +2.987
300 −47.0622 9.785 −15.3089 +2.771
Table 6
Comparison of related work.
Name Method Grades Clarification
CCS (Cyber autonomy gym for experimentation challenge 2, 2022b)Sleeper −1134.03 Do nothing
Random −726.92 Random defender agent
Heuristic −184.34 Defender react restore
PPO −30.19 RL Benchmark
CardiffUni (Cyber autonomy gym for experimentation challenge 2, 2022b) PPO +Greedy Decoys −13.76 Intervention for Deploying Decoys
Wiebe (Wiebe et al., 2023)IQL −18.17 ±3.86 No communication
QMIX −16.75 ±3.48 Implicit communication
Heuristic −31.9 ±20.73 Rule based
Our paper Our method −16.6 ±4.027 Task breakdown
results they gave, the multi-agent cooperation method is better able
to solve complex decision problems, although QMIX is an implicit
multi-agent cooperation method.
Compared with the above work, the method in this article is al-
most the same as the best performance in defense performance, but
the method in this article has two advantages. Firstly, all the above
work is discussing the problem of strategy generation in the field of
reinforcement learning, and the interpretability of the strategy gen-
eration process is not high. This article uses the game framework
to qualitatively and quantitatively model network attack and defense
conflicts, providing an interpretive framework for reinforcement learn-
ing methods to iterate defense strategies. Secondly, this article uses
hierarchical reinforcement learning methods to decouple the defense
decision problem, reduce the dimension of the complex defense action
space, agree on explicit cooperation constraints, and fully demonstrate
the cooperative defense behavior of multiple agents.
5.2.4. Analysis of agent defense strategies
Based on the evaluation data of the agents, the paper conducts the
following analysis of the distribution of agent strategies. The actions
of both attack and defense strategies are categorized and statistically
analyzed according to their types. The statistical results are presented
in Figs. 8, where (a) and (b) represent the distribution of actions for
the defender and attacker, respectively, under 100 steps, while (c) and
(d) represent the distribution of actions for the defender and attacker,
respectively, under 200 steps.
By comparing Figs. 8(a) and (b) with (c) and (d) vertically, we
can confirm that the increase in the temporal dimension of the game
space does not affect the stability of autonomous defense strategies. The
distribution patterns of (a) and (c) are similar. Through the horizontal
comparison of (a) and (c) with (b) and (d), it confirms the stability of
the Stackelberg equilibrium in the game model framework. This reflects
that if there is no change in the attack strategy, the optimal solution for
defense response actions remains stable.
Through detailed analysis of Figs. 8(a) and (c), we can infer that de-
fense strategies are complex mixed strategies. The distribution of Scan,
Analyse, and Remove actions is relatively evenly spread, indicating
that the defense agent effectively utilizes these defense behaviors. The
concentrated distribution and smaller statistical values of Decoy and
Restore actions suggest that our adjustment of reward hyperparameters
has been incorporated into the defense agent’s strategic considerations.
From the relevant results shown in Figs. 8, we can conclude that
by increasing the length of the attack-defense interaction, our model’s
defense strategy demonstrates excellent statistical stability in its strat-
egy profile. Therefore, we select one evaluation episode to analyze the
consumption of defense resources. We extract the results of one episodeof a 100-step scenario and analyze the resource consumption in the face
of different types of attack techniques. The statistical results are given
in Fig. 9.
Fig. 9 shows the actions taken by the attacker and defender re-
spectively in the 100 steps, which is a statistical cross-section of the
attack-defense resource consumption. Combined with specific infor-
mation, the attacker enters the attack-defense simulation environment
using ‘‘Discover Remote System’’, uses 7 steps of action resources
to complete the detection of the network, and makes 77 malicious
connections to the hosts and server resources in the network. Out of
the 77 malicious connections, the ‘‘Exploit Remote Services’’ behavior
succeeded in breaking through 15 times, and executed the ‘‘Escalate’’
action 15 times. In response to the above attack strategy, the defense
agent triggered 11 analysis actions, which are mainly used to deter-
mine the source of the attack, and executed 13 decoy deployments,
protecting important service resources in the system. The defense agent
also learned to deploy decoy resources in advance when the attacker
just entered the network environment and did not carry out effective
attacks. It is worth noting that out of the 77 malicious connections,
62 were stopped by the ‘‘Remove’’ operation in the attempt stage, and
the 15 successful breakthroughs were responded to by 14 ‘‘Restore’’
actions, that is, the 77 attack attempts were perceived and responded
to by the defender 76 times.
6. Conclusion
In response to the inefficiency of human expert network security
strategy organization and the difficulty of intelligent defense decision-
making in complex environments, we propose an innovative HMARL
framework for autonomously and efficiently formulating network se-
curity strategies and defense behavior decisions. This framework in-
tegrates game theory and hierarchical reinforcement learning theory.
Firstly, it establishes a SHM of cyberspace conflict, describing the
multi-level dynamic defense collaborative response mechanism under
incomplete information. Secondly, through multi-agent reinforcement
learning to simulate the game evolution dynamics process, it sequen-
tially solves the Nash equilibrium to obtain dynamic autonomous de-
fense strategies. Finally, using the hierarchical reinforcement learning
paradigm, the defense decision problem is decoupled, significantly
reducing the difficulty of exploration and efficiently learning collabora-
tive defense strategies. This method not only enhances the automation
and efficiency of defense strategy formulation, meeting the needs of
real-time decision-making, but also enhances the adaptability to com-
plex and changing network environments. We lay the theoretical and
technical foundation for building an autonomous intelligent network
Computers & Security 142 (2024) 103871
13Y. Tang et al.
Fig. 8. Action distribution statistics in attack and defense strategies.
Fig. 9. Resource consumption in attack-defense interaction.
security defense system, which will promote the development of net-
work security towards autonomous intelligence and strengthen the
active defense capability of cyberspace.The HMARL framework proposed in this paper is an efficient au-
tonomous control and optimization method, particularly suitable for
scenarios with adversarial interactions. Many cybersecurity scenarios
can be abstracted into such characteristics, and it is foreseeable that
applying this framework to the software vulnerability management
domain can achieve autonomous vulnerability discovery, analysis, as-
sessment, and tracking during security penetration testing. More im-
portantly, its continuous feedback control feature can well adapt to the
vulnerability management throughout the software lifecycle. Further-
more, extending this framework to the data privacy protection domain
can enable a collaborative data protection mechanism among multi-
ple users. Users can collaboratively formulate data sharing protocols,
control data aggregation and linking processes, and strike a balance
between preventing data leakage and maximizing the richness of the
data set.
7. Limitations and future work
Based on the above theoretical modeling and model evaluation
work, although we have applied new methods to discuss the problem of
autonomous network defense strategy generation, it is inevitable that
there are still the following limitations in this article: In terms of attack-
defense game modeling, firstly, the model construction in this article
is based on a series of assumptions, but the real cyberspace conflict
does not conform to such perfect mathematical characteristics in the
Computers & Security 142 (2024) 103871
14Y. Tang et al.
quantification process, such as it is difficult to characterize man-made
accidental factors; secondly, the game iteration process in this article
is based on a rational exploration process, but real network attack
methods are not all explicitly rational and interpretable, for example,
the action sequence relationship of advanced persistent threat is not
easy to be captured, and for another example, denial of service (DoS)
attack is a violent destruction of the system.
In terms of applying multi-agent reinforcement learning methods
in the security field, firstly, this article discusses the feasibility and
effectiveness of exploring the generation of artificial intelligence au-
tonomous network defense strategies in an ideal simulation environ-
ment, and expands the application range and potential application
scenarios of artificial intelligence network autonomous operation ca-
pabilities. However, it needs to be calmly faced that applying such
methods in real network environments still needs to overcome many
problems, such as: the construction of the world model, the reasonable
design of the general-purpose reward function, and how to efficiently
converge in the complex action space; secondly, the method in this
article is limited to generating security strategies and real-time pro-
tection for general attack behaviors, and lacks discussion on specific
attack types and the security of the model itself; finally, the scale of
the model in this article is not large, but the relationship between
the consumption of computing resources and the improvement of the
model’s general-purpose ability makes the model less practical.
Based on the above various limitations and the coherence of our re-
search direction, In terms of cross-application of cutting-edge technolo-
gies, on the one hand, we will explore the application of interpretable
multi-agent reinforcement learning in the field of network security,
making AI-based autonomous network defense methods more reliable
and easier to be understood by human experts, to achieve intelligent
and artificial collaboration, and intelligent assistance in generating and
implementing security policies. On the other hand, we will combine
the cutting-edge advanced reinforcement learning methods with other
directions in the field of network security, especially the security issues
of distributed machine learning model and training, specifically the se-
curity discussion in the field of federated learning. In terms of exploring
the limitations of the underlying theory, game theory is an impor-
tant mathematical tool for analyzing conflicts in cyberspace security.
However, its application to analyzing network security issues involves
many assumed premises. Exploring new methods to break through
these assumptions, and enable deeper discussion and representation of
conflicts in cyberspace, is one of the future research work.
CRediT authorship contribution statement
Yunlong Tang: Writing – original draft, Methodology, Investiga-
tion. Jing Sun: Supervision, Investigation. Huan Wang: Writing –
review & editing, Resources, Project administration. Junyi Deng: Data
curation, Conceptualization. Liang Tong: Investigation, Formal analy-
sis.Wenhong Xu: Software, Data curation.
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Data availability
Data will be made available on request.
Acknowledgments
This work was supported in part by the National Natural Science
Foundation of China under Grant 62106053, in part by the Natu-
ral Science Foundation of Guangxi Province of China under Grant
2024GXNSFAA010242, in part by the Innovation Project of Guangxi
Graduate Education under Grant YCSW2023478, in part by the Doc-
toral Fund of Guangxi University of Science and Technology under
Grant XiaoKe Bo19Z33.References
Adawadkar, A.M.K., Kulkarni, N., 2022. Cyber-security and reinforcement learning—A
brief survey. Eng. Appl. Artif. Intell. 114, 105116.
Alshamrani, A., Alshahrani, A., 2023. Adaptive cyber defense technique based on
multiagent reinforcement learning strategies. Intell. Autom. Soft Comput. 36 (3).
Anjum, I., Miah, M.S., Zhu, M., Sharmin, N., Kiekintveld, C., Enck, W., Singh, M.P.,
2020. Optimizing vulnerability-driven honey traffic using game theory. arXiv
preprint arXiv:2002.09069.
Applebaum, A., Dennler, C., Dwyer, P., Moskowitz, M., Nguyen, H., Nichols, N.,
Park, N., Rachwalski, P., Rau, F., Webster, A., et al., 2022. Bridging automated
to autonomous cyber defense: Foundational analysis of tabular q-learning. In:
Proceedings of the 15th ACM Workshop on Artificial Intelligence and Security.
pp. 149–159.
Apruzzese, G., Andreolini, M., Marchetti, M., Venturi, A., Colajanni, M., 2020. Deep
reinforcement adversarial learning against botnet evasion attacks. IEEE Trans.
Netw. Serv. Manag. 17 (4), 1975–1987.
Bakker, C., Bhattacharya, A., Chatterjee, S., Vrabie, D.L., 2020. Hypergames and
cyber-physical security for control systems. ACM Trans. Cyber-Phys. Syst. 4 (4),
1–41.
Bilinski, M., Ferguson-Walter, K., Fugate, S., Gabrys, R., Mauger, J., Souza, B., 2019.
You only lie twice: A multi-round cyber deception game of questionable veracity.
In: International Conference on Decision and Game Theory for Security. Springer,
pp. 65–84.
Cardellini, V., Casalicchio, E., Iannucci, S., Lucantonio, M., Mittal, S., Panigrahi, D.,
Silvi, A., 2022. Irs-partition: An intrusion response system utilizing deep Q-networks
and system partitions. SoftwareX 19, 101120.
Carmona, G., 2012. Existence and stability of Nash equilibrium. World scientific.
Cheah, M., Stone, J., Haubrick, P., Bailey, S., Rimmer, D., Till, D., Lacey, M.,
Kruczynska, J., Dorn, M., 2023. CO-DECYBER: Co-operative decision making
for cybersecurity using deep multi-agent reinforcement learning. In: European
Symposium on Research in Computer Security. Springer, pp. 628–643.
Chen, L., Leneutre, J., 2009. A game theoretical framework on intrusion detection in
heterogeneous networks. IEEE Trans. Inf. Forensics Secur. 4 (2), 165–178.
Cheng, Z., Chen, G., Hong, Y., 2022. Single-leader-multiple-followers stackelberg
security game with hypergame framework. IEEE Trans. Inf. Forensics Secur. 17,
954–969.
Cyber autonomy gym for experimentation challenge 2, 2022b. https://github.com/cage-
challenge/cage-challenge-2, 2022. Created by Maxwell Standen, David Bowman,
Son Hoang, Toby Richer, Martin Lucas, Richard Van Tassel, Phillip Vu, Mitchell
Kiely.
Cyber operations research gym, 2022a. https://github.com/cage-challenge/CybORG,
2022. Created by Maxwell Standen, David Bowman, Son Hoang, Toby Richer,
Martin, 2022. Lucas, Richard Van Tassel, Phillip Vu, Mitchell Kiely, KC C., Natalie
Konschnik, Joshua Collyer.
Du, Y., Song, Z., Milani, S., Gonzales, C., Fang, F., 2022. Learning to play an
adaptive cyber deception game. In: Proc. of the 21st International Conference on
Autonomous Agents and Multiagent Systems, vol. 6, Auckland, New Zealand.
Elderman, R., Pater, L.J., Thie, A.S., Drugan, M.M., Wiering, M.A., 2017. Adversarial
reinforcement learning in a cyber security simulation. In: 9th International Confer-
ence on Agents and Artificial Intelligence. ICAART 2017, SciTePress Digital Library,
pp. 559–566.
Guide, A.H., 2006. Infinite dimensional analysis. Springer.
Hammar, K., Stadler, R., 2021. Learning intrusion prevention policies through opti-
mal stopping. In: 2021 17th International Conference on Network and Service
Management. CNSM, IEEE, pp. 509–517.
Hu, P., Li, H., Fu, H., Cansever, D., Mohapatra, P., 2015. Dynamic defense strategy
against advanced persistent threat with insiders. In: 2015 IEEE Conference on
Computer Communications. INFOCOM, IEEE, Kowloon, Hong Kong, pp. 747–755.
Huang, Y., Chen, J., Huang, L., Zhu, Q., 2020. Dynamic games for secure and resilient
control system design. Natl. Sci. Rev. 7 (7), 1125–1141.
Huang, S., Zhang, H., Wang, J., Huang, J., 2018. Markov differential game for network
defense decision-making method. IEEE Access 6, 39621–39634.
Huang, L., Zhu, Q., 2020. A dynamic games approach to proactive defense strategies
against advanced persistent threats in cyber-physical systems. Comput. Secur. 89,
101660.
Jin, Q., Wang, L., 2020. Zero-trust based distributed collaborative dynamic access
control scheme with deep multi-agent reinforcement learning. EAI Endorsed Trans.
Secur. Saf. 8 (27).
Khouzani, M., Liu, Z., Malacaria, P., 2019. Scalable min-max multi-objective cyber-
security optimisation over probabilistic attack graphs. European J. Oper. Res. 278
(3), 894–903.
Kong, G., Chen, F., Yang, X., Cheng, G., Zhang, S., He, W., 2023. Optimal deception
asset deployment in cybersecurity: A Nash Q-learning approach in multi-agent
stochastic games. Appl. Sci. 14 (1), 357.
Kovach, N.S., Gibson, A.S., Lamont, G.B., 2015. Hypergame theory: a model for conflict,
misperception, and deception. Game Theory 2015.
Li, X., Hu, X., Jiang, T., 2023. Dual reinforcement learning based attack path prediction
for 5g industrial cyber-physical systems. IEEE Internet Things J..
Computers & Security 142 (2024) 103871
15Y. Tang et al.
Li, T., Zhu, K., Luong, N.C., Niyato, D., Wu, Q., Zhang, Y., Chen, B., 2022. Applications
of multi-agent reinforcement learning in future internet: A comprehensive survey.
IEEE Commun. Surv. Tutor. 24 (2), 1240–1279.
Liang, X., Xiao, Y., 2012. Game theory for network security. IEEE Commun. Surv. Tutor.
15 (1), 472–486.
Liu, Z., Yin, X., Hu, Y., 2020. CPSS LR-ddos detection and defense in edge computing
utilizing dcnn Q-learning. IEEE Access 8, 42120–42130.
Liu, L., Zhang, L., Liao, S., Liu, J., Wang, Z., 2021. A generalized approach to solve
perfect Bayesian Nash equilibrium for practical network attack and defense. Inform.
Sci. 577, 245–264.
Milani, S., Shen, W., Chan, K.S., Venkatesan, S., Leslie, N.O., Kamhoua, C., Fang, F.,
2020. Harnessing the power of deception in attack graph-based security games. In:
Decision and Game Theory for Security: 11th International Conference, GameSec
2020, College Park, MD, USA, October 28–30, 2020, Proceedings 11. Springer, pp.
147–167.
Nguyen, T.T., Reddi, V.J., 2023. Deep reinforcement learning for cyber security. IEEE
Trans. Neural Netw. Learn. Syst. 34 (8), 3779–3795.
Nguyen, T., Xu, H., 2019. Imitative attacker deception in stackelberg security games.
In: IJCAI. pp. 528–534.
Rashid, T., Samvelyan, M., De Witt, C.S., Farquhar, G., Foerster, J., Whiteson, S., 2020.
Monotonic value function factorisation for deep multi-agent reinforcement learning.
J. Mach. Learn. Res. 21 (178), 1–51.
Saeed, I.A., Selamat, A., Rohani, M.F., Krejcar, O., Chaudhry, J.A., 2020. A system-
atic state-of-the-art analysis of multi-agent intrusion detection. IEEE Access 8,
180184–180209.
Schlenker, A., Thakoor, O., Xu, H., Fang, F., Tambe, M., Tran-Thanh, L., Vayanos, P.,
Vorobeychik, Y., 2018. Deceiving cyber adversaries: A game theoretic approach.
In: AAMAS’18: Proceedings of the 17th International Conference on Autonomous
Agents and MultiAgent Systems. IFAAMAS, pp. 892–900.
Sengupta, S., Kambhampati, S., 2020. Multi-agent reinforcement learning in bayesian
stackelberg markov games for adaptive moving target defense. arXiv preprint
arXiv:2007.10457.
Shi, W., Li, J., Wu, H., Zhou, C., Cheng, N., Shen, X., 2020. Drone-cell trajectory
planning and resource allocation for highly mobile networks: A hierarchical DRL
approach. IEEE Internet Things J. 8 (12), 9800–9813.Tan, M., 1993. Multi-agent reinforcement learning: Independent vs. cooperative agents.
In: Proceedings of the Tenth International Conference on Machine Learning. pp.
330–337.
Tran, K., Akella, A., Standen, M., Kim, J., Bowman, D., Richer, T., Lin, C.-T., 2021.
Deep hierarchical reinforcement agents for automated penetration testing. arXiv
preprint arXiv:2109.06449.
Wang, S., Pei, Q., Wang, J., Tang, G., Zhang, Y., Liu, X., 2020. An intelligent
deployment policy for deception resources based on reinforcement learning. IEEE
Access 8, 35792–35804.
Waniek, M., Michalak, T.P., Alshamsi, A., 2019. Strategic attack & defense in security
diffusion games. ACM Trans. Intell. Syst. Technol. 11 (1), 1–35.
Wiebe, J., Mallah, R.A., Li, L., 2023. Learning cyber defence tactics from scratch with
multi-agent reinforcement learning. arXiv preprint arXiv:2310.05939.
Xu, X., Hu, H., Liu, Y., Tan, J., Zhang, H., Song, H., 2022. Moving target defense
of routing randomization with deep reinforcement learning against eavesdropping
attack. Digit. Commun. Netw. 8 (3), 373–387.
Zhan, Z., Xu, M., Xu, S., 2013. Characterizing honeypot-captured cyber attacks:
Statistical framework and case study. IEEE Trans. Inf. Forensics Secur. 8 (11),
1775–1789.
Zhang, Y., Malacaria, P., 2021. Bayesian Stackelberg games for cyber-security decision
support. Decis. Support Syst. 148, 113599.
Zhang, H., Wang, J., Yu, D., Han, J., Li, T., 2015. Active defense strategy selection
based on static Bayesian game. In: Third International Conference on Cyberspace
Technology. CCT 2015, IET, pp. 1–7.
Zhong, F., Hu, P., Zhang, G., Li, H., Cheng, X., 2022. Reinforcement learning based
adversarial malware example generation against black-box detectors. Comput.
Secur. 121, 102869.
Zhu, M., Anwar, A.H., Wan, Z., Cho, J.-H., Kamhoua, C.A., Singh, M.P., 2021. A survey
of defensive deception: Approaches using game theory and machine learning. IEEE
Commun. Surv. Tutor. 23 (4), 2460–2493.
Zhu, Q., Rass, S., 2018. Game theory meets network security: A tutorial. In: Proceedings
of the 2018 ACM SIGSAC Conference on Computer and Communications Security.
pp. 2163–2165.
Zolotukhin, M., Kumar, S., Hämäläinen, T., 2020. Reinforcement learning for attack
mitigation in SDN-enabled networks. In: 2020 6th IEEE Conference on Network
Softwarization. NetSoft, IEEE, pp. 282–286."
1-s2.0-S0306261922009850-main.pdf,"Applied Energy 324 (2022) 119688
Available online 4 August 2022
0306-2619/© 2022 Elsevier Ltd. All rights reserved.
Contents lists available at ScienceDirect
Applied Energy
journal homepage: www.elsevier.com/locate/apenergy
Resilience enhancement of multi-agent reinforcement learning-based
demand response against adversarial attacks
Lanting Zenga, Dawei Qiub, Mingyang Suna,∗
aDepartment of Control Science and Engineering, Zhejiang University, Hangzhou 310027, China
bDepartment of Electrical and Electronic Engineering, Imperial College London, London SW7 2AZ, UK
A R T I C L E I N F O
Keywords:
Demand response
Cyber security
Resilience
Multi-agent reinforcement learning
Adversarial attacksA B S T R A C T
Demand response improves grid security by adjusting the flexibility of consumers meanwhile maintaining their
demand–supply balance in real-time. With the large-scale deployment of distributed digital communication
technologies and advanced metering infrastructures, data-driven approaches such as multi-agent reinforcement
learning (MARL) are being widely employed to solve demand response problems. Nevertheless, the massive
interaction of data inside and outside the demand response management system may lead to severe threats from
the perspective of cyber-attacks. The cyber security requirements of MARL-based demand response problems
are less discussed in the existing studies. To this end, this paper proposes a robust adversarial multi-agent
reinforcement learning framework for demand response (RAMARL-DR) with an enhanced resilience against
adversarial attacks. In particular, the proposed RAMARL-DR first constructs an adversary agent that aims to
cause the worst-case performance via formulating an adversarial attack; and then adopts periodic alternating
robust adversarial training scenarios with the optimal adversary aiming to diminish the severe impacts induced
by adversarial attacks. Case studies are conducted based on an OpenAI Gym environment CityLearn, which
provides a standard evaluation platform of MARL algorithms for demand response problems. Empirical results
indicate that the MARL-based demand response management system is vulnerable when the adversary agent
occurs, and its performance can be significantly improved after periodic alternating robust adversarial training.
It can be found that the adversary agent can result in a 41.43% higher metric value of Ramping than the no
adversary case, whereas the proposed RAMARL-DR can significantly enhance the system resilience with an
approximately 38.85% reduction in the ramping of net demand.
1. Introduction
The power system is undergoing a fundamental revolution from
fossil fuels to clean energy for both planning and operation by in-
creasing the penetration of renewable energy resources (RES) [ 1].
This transition, however, brings out a significant challenge to power
system reliability due to the limited-controllable variability and partial
predictability of these intermittent RES [ 2]. To this end, a large scale of
distributed energy resources (DER) is being deployed to hedge the un-
certain RES and improve the reliability of energy supply [ 3]. Currently,
decentralization and digitalization are supporting the integration of
DER into the grid, which may raise another significant challenge of
cyber security, creating a negative impact on the normal operation
of power systems. For example, a massive cyber-attack occurring on
Ukraine’s power system in 2015 caused a serious power outage for
more than ten thousand households, and facilities [ 4]. Several works
demonstrate the devastating effects of cyber-attacks on power systems.
∗Corresponding author.
E-mail address: mingyangsun@zju.edu.cn (M. Sun).The attacker can manipulate meter readings and launch false data
injection attacks against state estimation in electric power grids, further
influencing power system control and operation [ 5,6]. The smart grid
data collection infrastructures have been demonstrated to be vulnerable
to cyber-attacks [ 7]. An attack adversary is constructed to impact
power grid operation, which builds or rents a botnet of computers and
modulates their power consumption [ 8]. The simulation results esti-
mate that between 2.5 and 9.8 million infections are sufficient to attack
the European synchronous grid. Furthermore, Denial-of-Service (DoS)
attacks are used to delay, block, or corrupt Smart Grid communication,
further impairing the operation of electrical equipment [ 9–11].
By definition, demand response (DR) technologies are deployed to
enable the involvement of end-consumers in the system operation and
balance supply and demand at the distribution level by reducing or
shifting the energy consumption. Compared with the energy sectors
in generation and transmission levels, DR besides end-consumers are
https://doi.org/10.1016/j.apenergy.2022.119688
Received 31 March 2022; Received in revised form 7 June 2022; Accepted 12 July 2022
Applied Energy 324 (2022) 119688
2L. Zeng et al.
more likely to be attacked since DR is envisioned to fully integrate
high-speed and two-way communication technologies into millions
of power equipment for energy management capabilities [12]. More
specifically, in the electricity retail market, such communication can
be achieved via Advanced Metering Infrastructures (AMI), e.g., smart
meters [13]. However, the large-scale deployment of AMI encourages
the use of marginally cheaper hardware which results in limited com-
putational resources to support advanced security functions when an
attack exists. For example, NESCOR lists the cyber failure scenarios that
might occur in the advanced metering infrastructure (AMI), distributed
energy resources (DERs), wide-area monitoring, protection, and control
(WAMPAC) systems [14]. Therefore, enhancing the resilience against
cyber-attacks under the DR concept becomes increasingly important to
maintain a reliable and secure energy system.
Regarding the DR modeling approach, in the literature, the model-
based optimization methods are being assumed as the conventional
problem-solving approaches and have been successfully applied to
solve various DR problems [15]. In [16], mixed-integer linear program-
ming (MILP) is proposed to develop an optimal production scheduling
framework for an industrial DR management system with the objec-
tive of saving operation costs and reducing demand peaks. In [17],
stochastic programming (SP) is proposed for an energy hub scheduling
problem, accounting for the DR program and the uncertainties of wind
and PV generation. In [18], a model predictive controllers (MPC)
approach is proposed for a building heating system under the DR
program. Nevertheless, the conventional optimization methods require
formulating the energy model accurately and acquiring all the technical
parameters of the system components, which are normally impractical
considering the privacy concern and the system aging. Furthermore,
a series of issues associated with sensor noise, RES variation, equip-
ment faults, and consumed time prevent such methods from deploying
real-time control solutions since the perfect information or accurate
forecasts of uncertainty parameters are assumed to be acquired.
On the other hand, as a model-free and real-time automatic con-
trol method, reinforcement learning (RL) [19] has recently brought
increased attention to the DR problems [20]. More specifically, RL is
proposed to study the sequential and dynamic decision-making problem
of an agent that can gradually learn the optimal control decisions
by utilizing experiences acquired from its repeated interactions with
the environment without prior knowledge. As such, the exact DR
management system and technical parameters are unnecessary. The
system’s stochastic and dynamic characteristics can also be learned
through the vast interactions with the environment in the big-data
era. In general, RL can be classified into two categories: single-agent
reinforcement learning (SARL) and multi-agent reinforcement learning
(MARL) methods [21]. The first category SARL method employs the
decision-making process of a single entity, e.g., household, micro-grid,
and energy hub. In [22], the authors propose a novel incentive-based
DR algorithm. Based on the predicted data, it adopts Q-learning (QL)
to derive the optimal incentive rates for different customers consid-
ering the benefits of both service providers and customers. In [23],
a dueling deep Q network (DDQN) method is proposed to develop
the DR management system of interruptible. Under the premise of
ensuring power supply for customers, this method reduces the total
daily cost of distribution system operators by 16.9% compared to the
program without DR. In [24], a deep deterministic policy gradient
(DDPG) method is proposed to generate the optimal control strategy
for a multi-zone residential HVAC system with the goal of minimizing
energy consumption costs while maintaining the users’ comfort. To
further avoid the physical constraint violation, the authors in [25]
propose a safe-DDPG method to optimize the DR problem of an energy
hub that can ensure the demand–supply balance without requiring
any model knowledge. The second category, the MARL method, is
also widely studied to be implemented into DR programs for multiple
agents. In [26], a multi-agent QL method is proposed to solve the DR
problem for a home energy management system, where each energydevice equips with a Q-table to optimize its energy schedules. In [27],
a multi-agent deep deterministic policy gradient (MADDPG) is proposed
to optimize the optimal schedule for different machine agents in a
discrete manufacturing systems energy management. In [28], an im-
proved multi-agent QL method based on a deep neural network (DNN)
is proposed to optimize the thermostatically controlled loads for 50
houses, resulting in energy savings of almost 200 kWh per household.
In [21], a parameter-sharing (PS) framework based on the MADDPG
method is proposed for 300 residential households to solve the DR
problem and P2P energy trading problem, which can improve the
scalability and persevere the privacy of households.
Recently, researchers have noticed that RL models are vulnerable
to adversarial attacks [29], such as Atari games, autonomous driving,
robot control tasks, and power systems control. The attacker influences
the RL decisions by injecting elaborate perturbations into observations,
further destroying the RL-based system operation. In [30], the authors
propose a criticality-based adversarial perturbation into the agents’
observation and implement such a perturbation into a DQN method.
Case studies based on the IEEE 14-bus system and the IEEE 118-
bus system show the reward degradation of about 32.1% and 8.9%
under adversarial attacks. An adversarial agent is used for cooperative
distributed MARL methods to alter other agents and prevent algorithm
convergence [31]. Similarly, in [32], the authors develop an adversarial
RL agent for cooperative MARL methods to estimate adversarial actions
and use gradient-based methods to generate perturbations by adversar-
ial actions. It results in a sharp reduction of the team’s winning rate in
StarCraft games. External adversarial attacks may cause severe damage
to RL-based control systems, especially in safety-critical tasks, such as
energy systems. To deploy MARL algorithms to the DR programs, there
is an urgent need to enhance the resilience against adversarial attacks.
The RL model lacks decision experience about adversarial exam-
ples during regular training, which leads to the control policy being
sensitive to the perturbation or fooling by adversarial examples. Thus,
several works focus on improving the robustness of the RL models by
robust adversarial training. Kos and Song [33] first used adversarial
training for robustifying SARL algorithms, which generated pertur-
bation by the Fast Gradient Sign Method (FGSM) [34] and random
noise. Furthermore, several studies [35–37] show that the SARL models
achieve robust performance after adversarial training with adversarial
attacks. Many researchers attempt to design stronger attacks for adver-
sarial training and propose methods to learn an adversary online with
the SARL agent. Specifically, the adversarial attack is identified as an
adversary agent and participates in the training process of the agent.
Pinto et al. [38] construct an adversary to generate perturbations in the
environment and propose a robust adversarial reinforcement learning
(RARL) framework, which formulates the policy learning as a zero-
sum, minimax objective function. It takes a jointly training way to
improve the performance of both the adversary and the agent. Zhang
et al. [39] propose the alternating training with learned adversaries
(ATLA) framework, training an adversary online with the agent. ATLA
generates stronger attacks than other methods and significantly im-
proves the agent’s performance against tested attacks. Robust MARL
methods have also used minimax optimization and equilibrium while
considering the opponent and the agent. The robust Markov Game is
designed in [40] to handle uncertainties from the reward or transition
probability model. To cope with the misbehavior of attacked agents in
the MARL system, Li et al. [41] propose robust multi-agent Q-learning
(RoM-Q) utilizing minimax optimization in policy updates. However,
the security of MARL algorithms under cyber-attacks and robustness
improvement by adversarial training have not been fully explored,
especially in the DR management system.
To this end, this paper proposes a novel robust adversarial train-
ing framework for MARL-based DR models, RAMARL-DR, aiming to
improve the resilience of the MARL-based DR management system
against adversarial attacks. The RAMARL-DR framework models the
external adversarial attacks as a single adversarial RL agent, who
Applied Energy 324 (2022) 119688
3L. Zeng et al.
learns an optimal policy to generate observation perturbations, causing
the worst-case performance of the controller directly. Then, it utilizes
robust adversarial training to improve the performance of the MARL-
based DR model against this adversary agent. In particular, this training
method is formulated as a robust Markov Game with minimax optimiza-
tion, which is solved by alternating robust adversarial training inspired
by the RARL [38] and ATLA [39] frameworks. The simulation results
based on an energy system containing a nine-buildings group demon-
strate that the optimal adversary agent helps the MARL-based DR
management system learn the experience of tackling adversarial attacks
and significantly improve its resilience. In contrast, the adversarial
training with a random adversary reduces the controller performance.
In addition, we found that the periodic robust adversarial training
scenario addresses the problem of robust MARL model convergence
difficulty in the episode-by-episode training scenario. In summary, The
main contributions of this paper are as follows:
(1) To the best of the authors’ knowledge, this is the first work that
proposes a resilience enhanced MARL-based DR methodological
framework, RAMARL-DR, against the potential adversarial attacks
in the cyber space. In particular, this develops a novel robust
adversarial training framework, RAMARL, which can mathemat-
ically formulate the adversarial Markov Game and improves the
MARL models’ performance by robust adversarial training. More
specifically, it models the adversarial attacks as an optimal ad-
versary agent learned by the SARL algorithm considering the
perturbation bound.
(2) In contrast to regular adversarial training, this paper proposes the
periodic robust adversarial training scenario, which provides a
stable convergence policy rather than instability in the episode-
by-episode training method.
(3) Furthermore, case studies are conducted based on an open-source
platform, CityLearn, with real-world energy data to demonstrate
the superior performance of the proposed method under various
adversarial attacks. In particular, it can be found that the adver-
sary agent can result in a 41.43% higher metric value of Ramping
than the no adversary case, whereas the proposed RAMARL-DR
can significantly enhance the system resilience with an approxi-
mately 38.85% reduction in the ramping of net demand.
(4) In addition, we investigate the impacts of pre-training stage with
the following finding: In the fixed adversary training scenario,
the pre-training helps the control policy take initial exploration
policy, avoiding stuck in a lousy policy space. However, it has
little impact on the alternating adversary training scenario.
The remainder of this paper is organized as follows. Section 2
describes a detailed demand response management system model and
reformulates it as a MARL-based control problem mathematically. Sec-
tion 3 offers the RAMARL-DR framework with three training algo-
rithms. Section 4 provides performance evaluation metrics. In Sec-
tion 5, we present the open-source demand response simulation envi-
ronment based on OpenAI Gym, algorithm settings, experiment settings
and discuss evaluation performances. Finally, conclusions and future
work are given in Section 6.
2. Problem formulation
2.1. Mathematical models of demand response
This work studies a DR management system based on a micro-
grid containing 𝑁buildings. In particular, each building consists of
(1) two kinds of inflexible demand: electric demand, thermal (heating
& cooling) demand; (2) one renewable-based generator: solar photo-
voltaic (PV); (3) two kinds of storage systems: electric energy storage
and thermal energy storage; (4) two kinds of energy converters: heat
pump and electric heater. Of which its model structure is illustrated in
Fig. 1. More specifically, the mathematical equations of the controllable
components (i.e., two energy converters and two storage systems)
within the building are presented as follows:
Fig. 1. Energy models of buildings.
•Heat pump: it takes electric power from the power grid 𝑃ℎ𝑝, and
supplies thermal demand 𝑄ℎ𝑝, to the building heating or cooling
and energy storage devices, following the equation,
𝑃ℎ𝑝
𝑡=𝑄ℎ𝑝
𝑡
𝜂ℎ𝑝
𝑡,∀𝑡∈𝑇 (1)
0≤𝑄ℎ𝑝
𝑡≤𝑄ℎ𝑝,∀𝑡∈𝑇 (2)
where𝜂ℎ𝑝
𝑡in Eq. (1) is the energy conversion coefficient related
to indoor target temperatures, outdoor air temperatures, and
the technical efficiency coefficient 𝜂ℎ𝑝
𝑡𝑒𝑐ℎ;𝑄ℎ𝑝
𝑡in constraint (2)
represents the output power capacity of heat pump.
•Electric heater: it takes electric power from the power grid 𝑃𝑒ℎto
provide domestic hot water (DHW) energy 𝑄𝑒ℎfor the building
or storage devices, following
𝑃𝑒ℎ
𝑡=𝑄𝑒ℎ
𝑡
𝜂𝑒ℎ,∀𝑡∈𝑇 (3)
0≤𝑄𝑒ℎ
𝑡≤𝑄𝑒ℎ,∀𝑡∈𝑇 (4)
where𝜂𝑒ℎin Eq. (3) is the heater efficiency of energy conversion
from electricity to heat power; 𝑄𝑒ℎ
𝑡in constraint (4) represents
the output power capacity of electric heater.
•Electric energy storage: the charging and discharging power in
the electric energy storage can be denoted as 𝑃𝑒𝑒𝑠,𝑖𝑛and𝑃𝑒𝑒𝑠,𝑜𝑢𝑡,
respectively. Of which, constraints (5) and (6) restrict the maxi-
mum charging and discharging power, where the binary variable
𝑢𝑒𝑒𝑠∈ {0,1}is corresponding to the charging ( 𝑢𝑒𝑒𝑠= 1) and
discharging ( 𝑢𝑒𝑒𝑠= 0) status of electric energy storage, since the
battery cannot behave charging and discharging simultaneously.
0≤𝑃𝑒𝑒𝑠,𝑖𝑛
𝑡≤𝑢𝑒𝑒𝑠
𝑡𝑃𝑒𝑒𝑠,∀𝑡∈𝑇 (5)
(𝑢𝑒𝑒𝑠
𝑡− 1)𝑃𝑒𝑒𝑠≤𝑃𝑒𝑒𝑠,𝑜𝑢𝑡
𝑡≤0,∀𝑡∈𝑇 (6)
where𝑃𝑒𝑒𝑠indicates the maximum charging/discharging limit of
battery. Given the charging power 𝑃𝑒𝑒𝑠,𝑖𝑛
𝑡and discharge power
𝑃𝑒𝑒𝑠,𝑜𝑢𝑡
𝑡, the SoC of the battery is associated with the energy
charging/discharging efficiency 𝜂𝑒𝑒𝑠can be calculated as:
𝑆𝑜𝐶𝑒𝑒𝑠
𝑡+1=𝑆𝑜𝐶𝑒𝑒𝑠
𝑡+ (𝑃𝑒𝑒𝑠,𝑖𝑛
𝑡𝜂𝑒𝑒𝑠𝛥𝑡)∕𝑆𝑒𝑒𝑠+ (𝑃𝑒𝑒𝑠,𝑜𝑢𝑡
𝑡𝛥𝑡)∕(𝜂𝑒𝑒𝑠𝑆𝑒𝑒𝑠),∀𝑡∈𝑇
(7)
Finally, the battery SoC is also limited by its lower and upper
limits, which can be expressed as:
𝑆𝑒𝑒𝑠≤𝑆𝑒𝑒𝑠
𝑡≤𝑆𝑒𝑒𝑠,∀𝑡∈𝑇 (8)
Applied Energy 324 (2022) 119688
4L. Zeng et al.
•Thermal energy storage: it allows to store energy that can release
into the building at the appropriate time. Devices include chilled
water and DHW tanks, which receive cooling, heating, and DHW
energy from heat pumps and electric heaters. Similar to the
electric storage system in (5)–(8), the operation model of thermal
energy storage can be formulated as:
0≤𝑄𝑡𝑒𝑠,𝑖𝑛
𝑡≤𝑢𝑡𝑒𝑠
𝑡𝑄𝑡𝑒𝑠,∀𝑡∈𝑇 (9)
(𝑢𝑡𝑒𝑠
𝑡− 1)𝑄𝑡𝑒𝑠≤𝑄𝑡𝑒𝑠,𝑜𝑢𝑡
𝑡≤0,∀𝑡∈𝑇 (10)
𝑆𝑜𝐶𝑡𝑒𝑠
𝑡+1=𝑆𝑜𝐶𝑡𝑒𝑠
𝑡+ (𝑄𝑡𝑒𝑠,𝑖𝑛
𝑡𝜂𝑡𝑒𝑠𝛥𝑡)∕𝑆𝑡𝑒𝑠+ (𝑄𝑡𝑒𝑠,𝑜𝑢𝑡
𝑡𝛥𝑡)∕(𝜂𝑡𝑒𝑠𝑆𝑡𝑒𝑠),∀𝑡∈𝑇
(11)
𝑆𝑡𝑒𝑠≤𝑆𝑡𝑒𝑠
𝑡≤𝑆𝑡𝑒𝑠,∀𝑡∈𝑇 (12)
where𝑄𝑡𝑒𝑠,𝑖𝑛
𝑡and𝑄𝑡𝑒𝑠,𝑜𝑢𝑡
𝑡represent the battery charging and dis-
charging power, respectively; 𝑄𝑡𝑒𝑠indicates the battery power
capacity;𝜂𝑡𝑒𝑠is the energy efficiency caused by charging and
discharging behaviors. Finally, 𝑆𝑡𝑒𝑠
𝑡represents the battery SoC in
the thermal energy system.
Given the demand of electric 𝑃𝑒𝑑
𝑡and thermal 𝑄ℎ𝑑
𝑡, production,
storage, and conversion, the demand–supply balances of electricity and
heat sectors of the building should always be satisfied at each time step,
which can be respectively expressed as below:
𝑃𝑒𝑑
𝑡−𝑃𝑝𝑣
𝑡+𝑃𝑒𝑒𝑠,𝑖𝑛
𝑡+𝑃𝑒𝑒𝑠,𝑜𝑢𝑡
𝑡+𝑃ℎ𝑝
𝑡+𝑃𝑒ℎ
𝑡=𝑃𝑔𝑟𝑖𝑑
𝑡,∀𝑡∈𝑇 (13)
𝑄ℎ𝑑
𝑡−𝑄ℎ𝑝
𝑡−𝑄𝑒ℎ
𝑡+𝑄𝑡𝑒𝑠,𝑖𝑛
𝑡+𝑄𝑡𝑒𝑠,𝑜𝑢𝑡
𝑡= 0,∀𝑡∈𝑇 (14)
where𝑃𝑔𝑟𝑖𝑑
𝑡represents the net demand (positive) or generation (nega-
tive) bought or sold in the power grid. The objective of each building
is maintaining the demand–supply balances of both electricity and heat
sectors in real-time, leading to a reliable building demand response
management system, which aims to flat the demand curve and re-
duce the peak demand of the micro-grid. However, solving the above
demand–supply balance problem in a building system faces several
challenges:
(1) It might be impractical to acquire the mathematical models and
the technical parameters of all building components explicitly, and the
optimization model thus cannot be constructed.
(2) Solving a comprehensive optimization problem including both
power and heat sectors in a time-coupling fashion is time-consuming,
especially when accounting for the highly dynamic and stochastic
characteristics of adversarial attacks.
(3) Conventional mathematical methods do not generalize to the
system dynamics since the optimal decisions of DR need to be re-
optimized for any new condition.
2.2. Reformulation as a partially observable stochastic game
In order to address the above three challenges raised in Section 2.1,
the building DR problem can be reformulated as a partially observable
stochastic game (POSG) [42], as evident from Fig. 2, where each
building DR controller is defined as the agent and its utilized DR
management system is assumed the environment. In this case, the agent
does not require any knowledge from the DR management system but
learns the optimal control policy together with the attack characteris-
tics by repeatably interacting with the environment. In addition, once
the control policy is well trained, it can be directly deployed to the
practical scenario for test in milliseconds.
A POSG system can be formulated as a tuple, ⟨𝑁,𝑆,{𝐴𝑖}
𝑖∈𝑁,𝑃,{𝑅𝑖}
𝑖∈𝑁,𝛾,{𝑂𝑖}
𝑖∈𝑁,𝑍⟩: where𝑁is the number of agents; 𝑆is the
environmental state space; 𝐴𝑖is the action space of agent 𝑖and{𝐴𝑖}
𝑖∈𝑁
is the joint action space, denoting as 𝑨∶=𝐴1×⋯×𝐴𝑁;𝑃∶𝑆×𝑨×𝑆→
Fig. 2. The schematic diagram for a POSG.
𝛥(𝑆)is the state transition probability from state 𝑠𝑡to next state 𝑠𝑡+1
for given joint actions 𝒂𝒕={𝑎𝑖
𝑡}
𝑖∈𝑁∈𝑨;𝑅𝑖∶𝑆×𝑨×𝑆→Ris the
immediate reward function of agent 𝑖for a transition from (𝑠𝑡,𝒂𝒕)to
𝑠𝑡+1;𝛾is the discounting rate; 𝑂𝑖is the observation space for agent 𝑖and
the joint observation space is{𝑂𝑖}
𝑖∈𝑁, denoting as 𝑶=𝑂1×⋯×𝑂𝑁;
𝑍∶𝑆×𝑨→𝛥(𝑶)is the probability of observing 𝒐𝒕∈𝑶at any given
actions𝒂𝒕and the new states 𝑠𝑡. At time step 𝑡, each agent 𝑖chooses an
action𝑎𝑖
𝑡according to the policy 𝜋(𝑎𝑖
𝑡|𝑜𝑖
𝑡)based on its local observation
𝑜𝑖
𝑡. The environment then moves into the next state according to the
state transition function 𝑃. Each agent 𝑖obtains a reward 𝑟𝑖
𝑡and a
new local observation 𝑜𝑖
𝑡+1. Such process continues and then emits a
trajectory of observations, actions, and rewards for each agent 𝑖:𝜏𝑖=
𝑜𝑖
1,𝑎𝑖
1,𝑟𝑖
1,𝑜𝑖
2,…,𝑟𝑖
𝑡over𝑆×𝐴𝑖×𝑆→R. The goal of each agent 𝑖is to
find a policy 𝜋𝑖that maximizes its own cumulative discounted reward
as indicated in Eq. (15), where −𝑖is the indices of all agents in 𝑁except
agent𝑖.
𝐽=𝑁∑
𝑖=1E𝑠𝑡+1∼𝑃(⋅∣𝑠𝑡,𝒂𝒕),𝑎−𝑖∼𝜋−𝑖(⋅∣𝑜−𝑖
𝑡)
×[
∑
𝑡≥0𝛾𝑡𝑅𝑖(𝑠𝑡,𝒂𝒕,𝑠𝑡+1)∣𝑎𝑖
𝑡∼𝜋𝑖(⋅∣𝑜𝑖
𝑡),𝑠0] (15)
In this multi-agent coordination building DR management system,
each agent controls a building and decides how much energy is stored
or released in the separate building at any given time. Furthermore, the
definitions of the observation space, action space, and rewards function
for each agent 𝑖are followed.
•observation: 𝑜𝑖is a 19-dimensional vector that includes hour,
day, month, outdoor temperature, predicted outdoor temperature,
outdoor relative humidity, predicted outdoor relative humidity,
indoor temperature, indoor relative humidity, non-shiftable load,
cooling storage, heating storage, DHW storage, electric energy
storage, direct solar radiation, predicted direct solar radiation,
diffuse solar radiation, predicted diffuse solar radiation, and solar
generation. The agent’s input is the combination of its building
observation and other agents’ decision information.
•action:𝑎𝑖is a 2-dimensional vector that indicates the (charging
and discharging) power rates of electrical energy storage and
thermal energy storage, respectively.
•reward function: 𝑟𝑖evaluates the performance of agents by the
demand curve of the district within a simulation period. The
reward function of each agent can be the same or different. We
define the reward function of each agent, presented as:
𝑟𝑖= − sign(
𝑃𝑔𝑟𝑖𝑑
𝑖)
⋅(𝑃𝑔𝑟𝑖𝑑
𝑖)2⋅max{
0,𝑁∑
𝑖=1𝑃𝑔𝑟𝑖𝑑
𝑖}
(16)
where rewards value depends on the net electricity consumption
of building 𝑃𝑔𝑟𝑖𝑑
𝑖and the total net electricity consumption of the
entire building group∑𝑁
𝑖=1𝑃𝑔𝑟𝑖𝑑
𝑖.
Applied Energy 324 (2022) 119688
5L. Zeng et al.
Fig. 3. The overall framework of proposed RAMARL-DR.
Let denote 𝒐𝒕={𝑜1
𝑡,𝑜2
𝑡,…,𝑜𝑁
𝑡},𝒂𝒕={𝑎𝑡1,𝑎𝑡2,…,𝑎𝑁
𝑡}and𝒓𝒕={𝑟1
𝑡,𝑟2
𝑡,…,𝑟𝑁
𝑡}. Multiple agents observe observation of each building
𝑜𝑖
𝑡,𝑖∈𝑁and operate decisions simultaneously at current state 𝑠𝑡. After
making decisions by the controller, it performs joint actions 𝑎𝑡and the
environment transits to next state 𝑠𝑡+1. Then, each agent receives its
immediate reward 𝑟𝑖
𝑡. The control policy of agent 𝑖is𝜋𝑖=𝜋𝜃𝑖,𝑖∈𝑁,
where𝜃𝑖represents the parameters of the agent policy. In cooperative
settings, the goal of the MARL-based building DR management system
is to maximize the cumulative total team reward 𝐽by optimizing
parameters of agents policies 𝜽={𝜃1,𝜃2,…,𝜃𝑁}, where𝐽is denoted
by Eq. (15).
max𝜽𝐽(𝜽) (17)
3. Proposed methodology
3.1. The overall framework of RAMARL-DR
For the considered environment, the DR management system in a
micro-grid carries out coordination tasks, where each building takes
joint actions not only considering its own energy consumption but also
the district consumption. In turn, the executed joint action will affect
the DR management system as well as other buildings. As discussed be-
fore, the RL algorithm is found that it is vulnerable to attacks, especially
MARL models. For deploying MARL into DR systems in practicality,
the robust adversarial multi-agent reinforcement learning-based DR
(RAMARL-DR) is proposed to explore its vulnerabilities and enhance
its resilience against attacks. The overall methodological framework of
is RAMARL-DR shown in Fig. 3, which is composed of two main parts:
(1) the basic theoretical model: adversarial POSG; and (2) the training
algorithm design. The first part formulates the adversarial MarkovGame model and describes the decision process of the MARL-based DR
system with attacks. Based on the proposed adversarial POSG model,
three training algorithms for two kinds of RL agents are designed to
enhance the robustness of the MARL-based DR system under attack.
First, we propose the basic theoretical model, adversarial POSG,
for the DR management system. Attack is formulated as an adversary
agent, who generates adversarial attacks to perturb the observations
of a single building agent with the target of affecting all agents and
thus, exploring the vulnerabilities of the MARL-based DR management
system. In this work, the adversary is modeled by a single-agent RL
algorithm, participating in training as a part of the micro-grid. The
optimal adversary induces the DR controller to make incorrect decisions
(e.g., increasing electricity consumption at peak times), which attempts
to result in the worst-case performance of the DR management system.
On this basis, the objective of the next part is to design the training
algorithms to enhance the resilience of the MARL-based DR man-
agement system under the optimal adversary by robust adversarial
training. The robust adversarial training helps the DR management
system deal with adversarial attacks considering the history experi-
ence [33]. For convenience, the MARL-based building DR management
system is denoted as the victim system, and its policy is victim policy.
This POSG with adversary participation is defined as adversarial POSG,
where the adversary generates perturbation 𝛿to alter observations of
agent𝑗. To solve this problem, three algorithms are developed in the
RAMARL-DR framework, including (1) training the optimal adversary
policy; (2) training the victim policy with a fixed adversary; and (3)
periodic alternating training, which will be illustrated in the following
subsections, respectively.
3.2. Basic theoretical model: Adversarial POSG
Given the victim policy 𝝅𝜽={𝜋𝜃1,𝜋𝜃2,…,𝜋𝜃𝑁}, a single adversarial
policy𝑢𝜙is designed to model adversarial attacks and threaten one
Applied Energy 324 (2022) 119688
6L. Zeng et al.
of agents, where 𝜙represents the parameters of the adversarial agent
policy. Specifically, the adversary crafts the bounded perturbation to
observations of the victim agent, 𝛿𝑡∼𝑢𝜙(𝑜𝑗
𝑡) ∈𝐵(𝑜𝑗), where𝛿𝑡is
the corresponding adversary action and 𝐵(𝑜𝑗)is the bound limit of
the perturbation. Then, the adversarial inputs of the agent 𝑗is𝑜𝑗′
𝑡=
𝑜𝑗
𝑡+𝛿𝑡. The victim policy makes decision based on disturbed obser-
vations,𝒂𝒕′∼𝝅𝜽(
𝑜𝑗′
𝑡,{𝑜𝑖
𝑡}
𝑖∈𝑁,𝑖≠𝑗)
. If the adversarial perturbation is
within physical constraints from physical characteristics and magnitude
bounds, such as stably increasing inflexible energy demands and energy
storage in capacity, it cannot be detected by a defense mechanism.
Hence, the adversarial perturbation can be bounded to 𝐵(𝑜𝑗)to find
the vulnerabilities of the MARL-based DR management system.
In this decision-making scenario, the environment states transi-
tions result from joint actions by the victim and adversary policy.
Consequently, the adversarial POSG system can be mathematically
represented as:
⟨𝑁,𝑆,𝐴𝑎𝑑𝑣,{𝐴𝑖}
𝑖∈𝑁,𝑃,{𝑅𝑖}
𝑖∈𝑁,𝑅𝑎𝑑𝑣,𝛾,{𝑂𝑖}
𝑖∈𝑁,𝑍⟩ (18)
where𝑁is the number of victim agents; 𝑆is the environmental state
space;𝐴𝑎𝑑𝑣and𝑅𝑎𝑑𝑣are the action space and reward function of the
adversary, respectively; 𝐴𝑖is the action space of the victim agent 𝑖and{𝐴𝑖}
𝑖∈𝑁is the joint action space, denoting as 𝑨∶=𝐴1×⋯×𝐴𝑁;
𝑃∶𝑆×𝐴𝑎𝑑𝑣×𝑨×𝑆→𝛥(𝑆)is the state transition probability from state
𝑠𝑡to next state 𝑠𝑡+1for each time step 𝑡, given an adversarial attacks
𝐴𝑎𝑑𝑣and joint actions 𝒂𝒕={𝑎𝑖
𝑡}
𝑖∈𝑁∈𝑨;𝑅𝑖∶𝑆×𝐴𝑎𝑑𝑣×𝑨×𝑆→Ris
the immediate reward function of agent 𝑖for a transition from (𝑠𝑡,𝒂𝒕)to
𝑠𝑡+1;𝛾is the discounting rate; 𝑂𝑖is the observation space for agent 𝑖and
the joint observation space is{𝑂𝑖}
𝑖∈𝑁, denoting as 𝑶=𝑂1×⋯×𝑂𝑁;
𝑍∶𝑆×𝑨→𝛥(𝑶)is the probability of observing 𝒐𝒕∈𝑶at any given
actions𝒂𝒕and the new states 𝑠𝑡. Notice that 𝑁,𝑆,{𝐴𝑖}
𝑖∈𝑁,𝛾,{𝑂𝑖}
𝑖∈𝑁
and𝑍are same with definition in POSG model, while 𝑃and𝑅𝑖are
altered with the impacts of 𝐴𝑎𝑑𝑣.
In our proposed RAMARL-DR framework, an adversary policy
against the observations of agent 𝑗adopts the single-agent SAC algo-
rithm. It takes deterministic policy, which mapping the observation of
agent𝑗to the disturbed observation, 𝛿=𝑢𝜙(𝑜𝑗),𝑜𝑗′=𝑜𝑗+𝛿. To satisfy
the bound constraint 𝐵(𝑜𝑗), the output of the neural network at the last
layer in the soft actor–critic (SAC) model is a continuous value between
0 and 1,𝑓′(𝑜𝑗) ∈ (0,1). Then, the output is mapped to the bound
constraint to get the final perturbation output, 𝑢𝜙(𝑜𝑗) =𝑓′(𝑜𝑗) ×𝐵(𝑜𝑗).
The adversarial reward function is the opposite value of the team
reward:
𝑟𝑎𝑑𝑣= −𝑁∑
𝑖𝑟𝑖=𝑁∑
𝑖{
sign(
𝑃𝑔𝑟𝑖𝑑
𝑖)
⋅(𝑃𝑔𝑟𝑖𝑑
𝑖)2⋅max{
0,𝑁∑
𝑖=1𝑃𝑔𝑟𝑖𝑑
𝑖}}
(19)
For the adversary, it trains to learn an optimal policy aiming to destroy
the district demand balance by minimizing the total team reward 𝐽.
min𝜙𝐽(𝜽,𝜙) (20)
However, the robust victim policy improve its resilience against adver-
sary by maximizing 𝐽under the optimal adversary.
max𝜽min𝜙𝐽(𝜽,𝜙) (21)
As illustrated in Eq. (21), the objective function of this adversarial
POSG game is a minimax function about the parameters of the victim
policy and the adversary policy. This is a minimax optimization prob-
lem, aiming to find the Nash equilibria policies 𝜃∗and𝜙∗that realize:
𝐽∗= max𝜽min𝜙𝐽(𝜽,𝜙) = min𝜙max𝜽𝐽(𝜽,𝜙) (22)3.3. Training algorithms design
To realize the resilience enhancement based on the above defined
adversarial POSG, the proposed RAMARL-DR method optimizes both
the adversary and the victim via the following three main
algorithms: (1) train an optimal deterministic adversarial policy to
generate bounded disturbance; (2) improve the robustness of the victim
policy under the optimal adversary by adversarial training; (3) develop
the periodic alternating training strategy to enhance the resilience of
MARL-based DR management system.
3.3.1. Train the optimal adversary policy
Given a fixed victim policy 𝝅𝜽, we train an optimal adversary policy
𝑢𝜙according to Eq. (20). Setting agent 𝑗as the perturbation target
agent, the detailed training process is presented in Algorithm 1. In
each of the 𝑀episodes, we reset the micro-grid environment with
the same initial state. At every time step of episodes, the deterministic
adversarial policy 𝑢𝜙generates perturbation 𝛿directly,𝛿𝑡=𝑢𝜙(𝑜𝑗
𝑡),𝑜𝑗′
𝑡=
𝛿𝑡+𝑜𝑗
𝑡. Then, the victim policy adjust energy storage responding to
perturbed observations 𝒐𝒕′,𝒂𝒕′=𝝅𝜽(𝒐′
𝒕). After performing actions, the
environment returns a reward 𝑟𝑎𝑑𝑣
𝑡to the adversarial agent. During
training, the replay buffer 𝐷𝑢stores trajectories{
(𝑜𝑗
𝑡,𝛿𝑡,𝑜𝑗
𝑡+1,𝑟𝑎𝑑𝑣
𝑡)}
.
The parameters of the adversary policy are optimized using a policy
optimizer.
Algorithm 1 Training the Optimal Adversary Policy
Input: Environment , number of episodes 𝑀, number of time steps of each
episode𝐾and parameters 𝜽of the victim policy 𝝅.
Output: Parameters 𝜙of the adversary policy 𝑢.
Initialization: Parameters 𝜙of the adversary policy 𝑢.
1:forepisode = 1to𝑀do
2: Initialize micro-grid environment and get observations of each
building𝑜𝑖
0, as the input of each agent 𝑖
3: while step<𝐾 do
4: Run 𝑢𝜙to generate the adversarial example, 𝛿𝑡=𝑢𝜙(𝑜𝑗
𝑡),𝑜𝑗′
𝑡=𝛿𝑡+𝑜𝑗
𝑡
5: Agents receive observations 𝒐′
𝒕={𝑜1
𝑡,…,𝑜𝑗′
𝑡,…,𝑜𝑁
𝑡}
6: Run 𝝅𝜽 to get control actions 𝒂′
𝒕={𝜋𝜃1(𝑜1
𝑡),…,𝜋𝜃𝑗(𝑜𝑗′
𝑡),…,𝜋𝜃𝑁(𝑜𝑁
𝑡)}
7: Perform actions 𝒂′
𝒕and get next state 𝑠𝑡+1
8: Receive adversarial rewards 𝑟𝑎𝑑𝑣
𝑡
9: Store trajectories{(𝑜𝑗
𝑡,𝛿𝑡,𝑜𝑗
𝑡+1,𝑟𝑎𝑑𝑣
𝑡)}in replay buffer 𝐷𝑢for the
adversary policy 𝑢𝜙
10: Update the parameters of the adversary policy, 𝜙 ←
Policy Optimizer (𝐷𝑢,𝜙)
11: end while
12:end for
3.3.2. Train the victim policy with a fixed optimal adversary
Given the victim policy 𝝅𝜽is pre-trained and fixed, the adversary
policy𝑢∗
𝜙can be learned to obtain a worst-case performance under
bounded perturbations. In this context, enhance the worst-case perfor-
mances of the victim policy 𝝅𝜽under the optimal adversary policy 𝑢∗
𝜙
can also improve the robustness of the MARL-based DR management
system. To this end, we optimize the victim policy 𝝅𝜽by robust adver-
sarial training with the fixed adversary policy 𝑢∗
𝜙. The robust adversarial
training algorithm is outlined in Algorithm 2. The parameters updating
of𝜽is the respective optimization procedure of{𝜃1,𝜃2,…,𝜃𝑁}with
their own replay buffer 𝐷𝜋𝑖={
(𝑜𝑖
𝑡,𝑎𝑖
𝑡,𝑜𝑖
𝑡+1,𝑟𝑖
𝑡)}
.
Note that𝝅𝜽has been preliminarily trained without an adversary,
and the robust adversarial training further optimizes its parameters
with the optimal adversary 𝑢∗
𝜙trained in Section 3.3.1. The pre-training
for𝝅𝜽without adversarial attacks provides a good initial exploration
strategy. The optimal adversary is trained for the specific victim policy
may have no attack effect on the randomly initialized victim policy.
Meanwhile, if the victim policy takes adversarial training first, it will
Applied Energy 324 (2022) 119688
7L. Zeng et al.
focus on defending against adversarial attacks while not on the perfor-
mance under normal states. Thus, the pre-training is necessary in the
adversarial training taking fixed adversary, which has been verified in
the experiments.
Algorithm 2 Training the Victim Policy with the Optimal Adversary
Policy
Input: Environment , number of episodes 𝑀, number of time steps of each
episode𝐾, parameters 𝜙∗of the adversary policy 𝑢∗and preliminarily trained
parameters 𝜽of the victim policy 𝝅.
Output: Parameters 𝜽of the victim policy 𝝅.
1:forepisode = 1to𝑀do
2: Initialize micro-grid environment and get observations of each
building𝑜𝑖
0, as the input of each agent 𝑖
3: while step<𝐾 do
4: Run 𝑢∗
𝜙to generate the adversarial example, 𝛿𝑡=𝑢∗
𝜙(𝑜𝑗
𝑡),𝑜𝑗′
𝑡=𝛿𝑡+𝑜𝑗
𝑡
5: Agents receive observations 𝒐′
𝒕={𝑜1
𝑡,…,𝑜𝑗′
𝑡,…,𝑜𝑁
𝑡}
6: Run 𝝅𝜽 to get control actions 𝒂′
𝒕={𝜋𝜃1(𝑜1
𝑡),…,𝜋𝜃𝑗(𝑜𝑗′
𝑡),…,𝜋𝜃𝑁(𝑜𝑁
𝑡)}
7: Perform actions 𝒂′
𝒕and get next state 𝑠𝑡+1
8: Receive each agents rewards 𝒓𝒕={𝑟𝑖(𝑠𝑡,𝒂′
𝒕,𝑠𝑡+1)}
𝑖∈𝑁
9: Store trajectories{(𝒐′
𝒕,𝒂′
𝒕,𝒐𝒕+𝟏,𝒓𝒕)}in replay buffer 𝑫𝝅for𝝅𝜽
10: Update the parameters of the victim policy, 𝜽 ←
Policy Optimizer (𝑫𝝅,𝜽)
11: end while
12:end for
3.3.3. Periodic alternating robust adversarial training
Although the above algorithms can improve the robustness of
MARL-based DR to some extent, it is imperative to note that the above-
fixed training approaches cannot make a guarantee to achieve the Nash
equilibria polices 𝜃∗and𝜙∗to realize𝐽∗. Inspired by the RARL [38]
and ATLA frameworks [39], we solve the minimax optimization (22)
by periodic alternating robust adversarial training.
When the victim policy 𝝅𝜽is fixed, an optimal adversary 𝑢∗
𝜙against
it is obtained by solving the optimization problem:
min𝜙𝐽(𝜽,𝜙) (23)
Then, the optimal adversary policy 𝑢∗
𝜙is utilized to improve the ro-
bustness of victim policy 𝝅𝜽by adversarial training. With the fixed
adversary policy 𝑢∗
𝜙, the objective of the victim policy is identified by:
max𝜽𝐽(𝜽,𝜙∗) (24)
The previous victim policy, the after-training victim policy, and the
optimal adversary policy are denoted as 𝝅𝟎∗,𝝅𝟏∗,𝑢∗
0respectively.
Aiming to seek a more optimal adversary policy 𝑢∗
1against𝝅𝟏∗, it can be
trained by solving the problem (23). Next, a more robust victim policy
can be learned with a stronger adversary 𝑢∗
1. The alternating training
process is detailed in Eqs. (25). It is intuitively that alternating training
between the adversary policy 𝑢and the victim policy 𝝅will converge
to the equilibrium point 𝐽∗where their policies are optimal.
𝜙∗
0= min𝜙𝐽(𝜽𝟎,𝜙)
𝜽𝟏∗= max𝜽𝐽(𝜽,𝜙∗
0)
𝜙∗
1= min𝜙𝐽(𝜽𝟏∗,𝜙)
𝜽𝟐∗= max𝜽𝐽(𝜽,𝜙∗
1)
…(25)
Specifically, this periodic alternating training algorithm takes alter-
nating training way in sequence cycle. First, the parameters of 𝝅are
optimized to maximize 𝐽while the parameters of 𝑢are held within
𝑀1training episodes. Then, the parameters of 𝝅are held while the
parameters of 𝑢are trained to minimize 𝐽within𝑀2training episodes.
This sequence is repeated until convergence to 𝐽∗. Our alternating
scenario is distinguished from previous research [39]. The episode-by-
episode alternating training scenario easily leads to a slow convergeor instability for MARL models [43]. The cycle period is introduced
to speed up the convergence of both the adversary and the victim.
Enough training episodes in one training cycle keep the stability and
performance for both sides. The whole learning process is given in
Algorithm 3.
Algorithm 3 Periodic Alternating Training
Input: Environment , number of cycle periods 𝐶, number of episodes 𝑀1for
𝜋in one cycle, number of episodes 𝑀2for𝑢in one cycle, number of time steps
of each episode 𝐾.
Output: Parameters 𝜽of𝝅and parameters 𝜙of𝑢.
Initialization: Parameters 𝜽of𝝅and parameters 𝜙of𝑢.
1:forcycle = 1to𝐶do
2: Get the optimal adversary policy 𝑢∗
𝜙←𝑢𝜙
3: forepisode = 1to𝑀1do
4: Initialize micro-grid environment and get observations of each
building𝑜𝑖
0, as the input of each agent 𝑖
5: while step<𝐾 do
6: Run 𝑢∗
𝜙to generate the adversarial example, 𝛿𝑡=𝑢∗
𝜙(𝑜𝑗
𝑡),𝑜𝑗′
𝑡=
𝛿𝑡+𝑜𝑗
𝑡
7: Agents receive observations 𝒐′
𝒕={𝑜1
𝑡,…,𝑜𝑗′
𝑡,…,𝑜𝑁
𝑡}
8: Run 𝝅𝜽 to get control actions 𝒂′
𝒕={𝜋𝜃1(𝑜1
𝑡),…,𝜋𝜃𝑗(𝑜𝑗′
𝑡),…,𝜋𝜃𝑁(𝑜𝑁
𝑡)}
9: Perform actions 𝒂′
𝒕and get next state 𝑠𝑡+1
10: Receive each agents rewards 𝒓𝒕={𝑟𝑖(𝑠𝑡,𝒂′
𝒕,𝑠𝑡+1)}
𝑖∈𝑁
11: Store trajectories{(𝒐′
𝒕,𝒂′
𝒕,𝒐𝒕+𝟏,𝒓𝒕)}in replay buffer 𝑫𝝅for𝝅𝜽
12: Update the parameters of the victim policy, 𝜽 ←
Policy Optimizer (𝑫𝝅,𝜽)
13: end while
14: end for
15: Get the optimal victim policy 𝝅∗
𝜽←𝝅𝜽
16: forepisode = 1to𝑀2do
17: Initialize micro-grid environment and get observations of each
building𝑜𝑖
0, as the input of each agent 𝑖
18: while step<𝐾 do
19: Run 𝑢𝜙to generate the adversarial example, 𝛿𝑡=𝑢𝜙(𝑜𝑗
𝑡),𝑜𝑗′
𝑡=
𝛿𝑡+𝑜𝑗
𝑡
20: Agents receive observations 𝒐′
𝒕={𝑜1
𝑡,…,𝑜𝑗′
𝑡,…,𝑜𝑁
𝑡}
21: Run 𝝅∗
𝜽to get control actions 𝒂′
𝒕={𝜋𝜃1∗(𝑜1
𝑡),…,𝜋𝜃𝑗∗(𝑜𝑗′
𝑡),…,𝜋𝜃𝑁∗(𝑜𝑁
𝑡)}
22: Perform actions 𝒂′
𝒕and get next state 𝑠𝑡+1
23: Receive adversarial rewards 𝑟𝑎𝑑𝑣
𝑡
24: Store trajectories{(𝑜𝑗
𝑡,𝛿𝑡,𝑜𝑗
𝑡+1,𝑟𝑎𝑑𝑣
𝑡)}in replay buffer 𝐷𝑢for the
adversary policy 𝑢𝜙
25: Update the parameters of the adversary policy, 𝜙 ←
Policy Optimizer (𝐷𝑢,𝜙)
26: end while
27: end for
28:end for
4. Performance evaluation metrics
To quantify the performance of the proposed approach, seven met-
rics are defined and employed based on CityLearn Challenge 2021 [44],
as follows:
•Ramping : It represents the accumulated ramping of electricity
consumption, indicating the flat level of the demand curves, as
followed by:
𝑅𝑎𝑚𝑝𝑖𝑛𝑔 =𝑇∑
𝑡|||𝑃𝑔𝑟𝑖𝑑
𝑎𝑙𝑙,𝑡−𝑃𝑔𝑟𝑖𝑑
𝑎𝑙𝑙,𝑡−1|||,𝑡∈𝑇 (26)
where𝑃𝑔𝑟𝑖𝑑
𝑎𝑙𝑙,𝑡=∑𝑁
𝑖=1𝑃𝑔𝑟𝑖𝑑
𝑖,𝑡is the total net electricity consumption
of the district buildings at each time step.
•1-Load factor : It evaluates the energy usage efficiency:
1−𝐿𝑜𝑎𝑑 𝑓𝑎𝑐𝑡𝑜𝑟 = 1 −average𝑃𝑔𝑟𝑖𝑑
𝑎𝑙𝑙,𝑡
max𝑃𝑔𝑟𝑖𝑑
𝑎𝑙𝑙,𝑡,𝑡∈𝑇 (27)
Applied Energy 324 (2022) 119688
8L. Zeng et al.
where the average net electricity load divided by the maximum
electricity load during one year is the load factor. The higher the
load factor, the smaller the generation cost for the same maximum
load.
•Avg. daily peak : It is the average daily net peak demand during
one year:
𝐴𝑣𝑔. 𝑑𝑎𝑖𝑙𝑦 𝑝𝑒𝑎𝑘 = average𝑃𝑔𝑟𝑖𝑑
𝑎𝑙𝑙,𝑑𝑎𝑦𝑑,𝑑∈𝑇∕24 (28)
where𝑃𝑔𝑟𝑖𝑑
𝑎𝑙𝑙,𝑑𝑎𝑦𝑑= max𝑃𝑔𝑟𝑖𝑑
𝑎𝑙𝑙,𝑡,𝑡∈[24𝑑+ 1,24(𝑑+ 1)]denotes the
daily net peak demand and 𝑑represents the day index.
•Peak demand : It represents maximal daily net peak demand during
one year:
𝑃𝑒𝑎𝑘 𝑑𝑒𝑚𝑎𝑛𝑑 = max𝑃𝑔𝑟𝑖𝑑
𝑎𝑙𝑙,𝑑𝑎𝑦𝑑,𝑑∈𝑇∕24 (29)
•Net elec. consumption : It represents the total district net electricity
consumption during one year.
𝑁𝑒𝑡 𝑒𝑙𝑒𝑐. 𝑐𝑜𝑛𝑠𝑢𝑚𝑝𝑡𝑖𝑜𝑛 =𝑇∑
𝑡𝑃𝑔𝑟𝑖𝑑
𝑎𝑙𝑙,𝑡,𝑡∈𝑇 (30)
•Carbon emissions : It calculates the total amount of district carbon
emissions.
𝐶𝑎𝑟𝑏𝑜𝑛 𝑒𝑚𝑖𝑠𝑠𝑖𝑜𝑛𝑠 =𝑇∑
𝑡max{
0,𝑃𝑔𝑟𝑖𝑑
𝑎𝑙𝑙,𝑡}
×𝐶𝐼𝑡,𝑡∈𝑇 (31)
where𝐶𝐼𝑡is the carbon intensity at time step 𝑡, which measures
how much CO2 is being produced per unit of electrical energy
generated.
•score: It is the average value of the above metrics.
𝑆𝑐𝑜𝑟𝑒 =(𝑅𝑎𝑚𝑝𝑖𝑛𝑔 + 1 −𝐿𝑜𝑎𝑑 𝑓𝑎𝑐𝑡𝑜𝑟
+𝐴𝑣𝑔. 𝑑𝑎𝑖𝑙𝑦 𝑝𝑒𝑎𝑘 +𝑃𝑒𝑎𝑘 𝑑𝑒𝑚𝑎𝑛𝑑 +
𝑁𝑒𝑡 𝑒𝑙𝑒𝑐. 𝑐𝑜𝑛𝑠𝑢𝑚𝑝𝑡𝑖𝑜𝑛 +𝐶𝑎𝑟𝑏𝑜𝑛 𝑒𝑚𝑖𝑠𝑠𝑖𝑜𝑛𝑠 )∕6(32)
Meanwhile, the CityLearn platform provides a rule-based controller
(RBC) as a baseline to measure the performance of RL algorithms.
The RBC aims at minimizing the electricity cost by storing energy at
night and releasing it during the day. Note that all metrics values are
normalized by the metrics value of RBC. Any metric >1is worse than
that of the RBC, and <1means that the controller is better than the
RBC. Finally, the score is presented to evaluate the average value of all
metrics comprehensively, and the lower values of metrics indicate the
better resilience of the DR management systems.
5. Case study
5.1. Data description
The proposed RAMARL-DR framework is constructed based on the
open-source OpenAI Gym environment, CityLearn [45], which provides
a standard research platform for the application of MARL in urban
building energy management and DR. In particular, the platform in-
cludes four energy demand datasets, which have been pre-simulated
using EnergyPlus in four different climate zones (Z1–Z4) of the USA,
respectively. This work is carried out based on hot-humid climate
Z1, which contains one-year energy data of a micro-grid with nine
buildings on an hourly time slot. As provided in Table 1, there are one
medium office (id=1), one fast food restaurant (id = 2), one standalone
retail (id = 3), one strip mall retail (id = 4), and five medium resi-
dential buildings (id=5-9) in the building group. Each building has its
individual different energy storage capacity, PV generation capacity,
and different energy consumption profiles. The 𝑄ℎ𝑝,𝑄𝑒ℎand𝑄𝑡𝑒𝑠are
automatically sized by 𝑆𝑜𝐶𝑡𝑒𝑠. The thermal energy of the building
contains cooling energy, heating energy, and DHW energy stored by
storage devices, such as chilled water and DHW tanks. Moreover, their
storage capacities are represented as the multiple hours the storage
device can satisfy the maximum annual hourly cooling or heating
demand if fully charged.Table 1
Technical parameters of energy components in different buildings.
Building ID 𝜂ℎ𝑝
𝑡𝑒𝑐ℎ(%)𝜂𝑒ℎ(%)𝜂𝑒𝑒𝑠(%)𝑃𝑒𝑒𝑠(kW)𝑆𝑒𝑒𝑠(kWh)𝑆𝑒𝑒𝑠(kWh)
1 20 90 90 100 0 140
2 21 92 90 40 0 80
3 23 87 90 20 0 50
4 22 90 90 30 0 75
5 24 90 90 25 0 50
6 20 85 90 10 0 30
7 22 90 90 15 0 40
8 24 93 90 10 0 30
9 22 90 90 20 0 35
5.2. Algorithm setting
The MARL-based DR management system employs the MARLISA
controller from paper [46], which provides more effective load shaping
in model-free, decentralized, and scalable with the cooperating setting.
The controller extends the SAC algorithm into a multi-agent coopera-
tion fashion through reward sharing and mutual information sharing.
Each agent controls one building and executes an iterative sequential
action selection algorithm for coordinated DR. Specifically, the first
agent picks an action and predicts how much electricity the building
will consume if that action is taken. The electricity consumption of the
building on the next time step is predicted with normalized observation
𝑜𝑖and action 𝑎𝑖by a well-trained gradient boosting decision tree
(GBDT). Then, the current agent’s information about the action and pre-
dicted consumption is shared with the next agent, who acts in the same
operation, sharing the details received previously. Although the action
selection of the agents in the MARLISA controller is sequential, their
action execution and observation are synchronous. Table 2 presents the
hyperparameters used for MARLISA in simulation experiments.
5.3. Experiment design
To validate the superior performance of the proposed RAMARL-DR
framework in enhancing the resilience of MARL-based DR management
system against adversarial attacks and study the effects of training
scenarios on the model performance, we implement three groups of
experiments: (1) Train the optimal adversary with a fixed victim policy :
In this case, we first train a victim policy 𝝅of the MARL-based DR
management system. Then, the victim policy 𝝅is fixed, and the optimal
adversary𝑢is learned by Algorithm 1 to explore the vulnerability.
For the test scenario, experiment 1 compares the performance of the
MARL-based DR management system with control policy 𝝅under no
attack (without attack), random attack (the random adversary gener-
ates attacks), and optimal attack (the optimal adversary 𝑢generates
attacks).
(2)Train the victim policy with a fixed optimal adversary : This group
experiments demonstrate the effects of adversarial training with an
optimal adversary on the robustness improvement of the MARL-based
DR management system by executing six different training scenarios
for the MARLISA controller, denoted by NO_AD, OP_AD, RAN_AD,
Pre+NO_AD, Pre+OP_AD, Pre+RAN_AD, to compare their performance,
as illustrated in Table 3. It can be seen that the former three models
take adversarial training 20 episodes with no adversary, optimal, and
random adversaries. In comparison, the latter three models take normal
pre-training and then robust adversarial training for 20 episodes with
no adversary, optimal, and random adversaries. The adversarial train-
ing follows Algorithm 2 with different adversaries, where the optimal
adversary has been trained in experiment 1, and the random adversary
generates random perturbation within bounds. For the test scenario,
in experiment 2, the DR management systems with the above trained
different controllers are evaluated under no attack, random attack, and
optimal attack.
Applied Energy 324 (2022) 119688
9L. Zeng et al.
Fig. 4. The evolution of episodic cumulative rewards for three group experiments.
Table 2
Hyperparameter of the MARLISA controller.
Learning rate 𝑙𝑟Decay rate 𝜏Discounting rate 𝛾Buffer size Batch size Hidden layers
MARLISA [46] 3𝑒− 4 5 𝑒− 3 0.99 1e5 256 [256, 256]
Table 3
Six different training scenarios of the DR management system with the MARLISA
controller in experiment 2.
ModelNormal pre-training Robust adversarial training
Episodes Episodes Adversary
NO_AD / 20 No adversary
OP_AD / 20 The optimal adversary
RAN_AD / 20 The random adversary
Pre+NO_AD 20 20 No adversary
Pre+OP_AD 20 20 The optimal adversary
Pre+RAN_AD 20 20 The random adversary
(3)Periodic alternating robust adversarial training : In this group exper-
iments, we execute periodic alternating robust adversarial training as
indicated in the Algorithm 3, where the victim policy 𝝅and the adver-
sarial policy are training alternatively with different training episodes.
To investigate the effects of whether taking pre-training and episodes
number𝑀1,𝑀2on model performance, we take five training scenar-
ios, NO_ALAD, Alter_1EP, Alter_20EP, Pre+Alter_1EP, Pre+Alter_20EP,
as shown in Table 4. The controller model NO_ALAD takes normal
training 80 episodes without attacks. The group models {Alter_1EP,
Pre+Alter_1EP} are compared with {Alter_20EP, Pre+Alter_20EP} to
evaluate the effects of episodes number on the model performance.
Besides, the group models {Alter_1EP, Alter_20EP} are compared with
{Pre+Alter_20EP, Pre+Alter_20EP} to study the effects of whether tak-
ing pre-training. In this case, there are four test scenarios to verify the
resilience of five controllers, no attack, random attack, fixed optimal
attack generated by the fixed optimal adversary, and alternative opti-
mal attack generated by optimal alternative adversaries of respective
controller models.
All the building DR management systems with the MARLISA con-
troller are trained and tested in the same simulation environment.
5.4. Training performance
This section aims to evaluate the training performance of the pro-
posed RAMARL-DR model for three experiments as designed in Sec-
tion 5.3, of which their learning curves are illustrated in Fig. 4(a)–(c),
respectively.
The first experiment is designed to train an optimal adversary attack
while keeping the model policy being fixed of the MARLISA controller
inside the DR management system. It can be observed from Fig. 4(a)that the optimal adversary (attack) policy trends to reach convergence
after 200 episodes.
In experiment 2, six training models for the MARLISA controller
are trained and compared while keeping the adversary policy being
fixed (collected) from experiment 1. The first observation from Fig. 4(b)
is that the cumulative rewards of six MARLISA controllers all reach
convergence within 20 episodes; the converged reward levels, how-
ever, are different. More specifically, the rewards of models RAN_AD
(green) and Pre+RAN_AD (brown) are much lower than the others;
this is mainly due to the random adversary policy without taking the
optimal attacking strategies into consideration. On the other hand,
the models OP_AD (orange) and Pre+OP_AD (purple) converge nearly
to the models without adversarial training, i.e., NO_AD (blue) and
Pre+NO_AD (red). Such good performance shows that the optimal
adversary contributes to the control policy learning experiences of
tackling adversarial attacks, while the random adversary is not effective
in RAN_AD and Pre+RAN_AD. Furthermore, it can be observed from
Fig. 4(b) that the models with pre-training (Pre+NO_AD, Pre+OP_AD,
Pre+RAN_AD) perform well in the initial training stage and get better-
converged rewards than those models without pre-training (NO_AD,
OP_AD, RAN_AD). Thus, pre-training is evaluated to improve the model
performance by introducing an excellent initial exploration policy.
In experiment 3, the baseline controller model NO_ALAD (blue) in
Fig. 4(c) reaches convergence around 40 episodes and is expected to
obtain the highest cumulative reward without any oscillation. How-
ever, once the MARLISA controller policy and adversary policy are
updated each other alternately, the learning curves start exhibiting
oscillations that vary for the updating frequency and pre-training con-
dition. It can be seen from Fig. 4(c) that Alter_1EP (orange) and
Pre+Alter_1EP (red) both converge to the similar reward level. How-
ever, Pre+Alter_1EP can speed up as well as obtain a higher starting
point than Alter_1EP. This is because Pre+Alter_1EP benefits from a
pre-trained policy while Alter_1EP starts from zero knowledge. This
phenomenon can also be observed from the comparison between Al-
ter_20EP (green) and Pre+Alter_20EP (purple). However, the difference
is that the oscillation is very significant for Pre+Alter_20EP, although
it can recover and get back within 20 episodes. As a result, it could
be concluded that pre-training does not help improve the training
performance but just accelerate the training speed during the initial
phase. Finally, it can be found that the rewards of Alter_20EP and
Pre+Alter_20EP are much higher than Alter_1EP and Pre+Alter_1EP.
As a result, it is suggested that the suitable training episodes (20EP)
is capable of exhibiting a better performance in terms of both policy
quality and stability.
Applied Energy 324 (2022) 119688
10L. Zeng et al.
Table 4
Five different training scenarios of the DR management system with the MARLISA controller in experiment
3.
ModelNormal pre-training Periodic alternating robust adversarial training
Episodes Adversary Total episodes M1 M2 C
NO_ALAD 80 / / / / /
Alter_1EP / The optimal adversary 80 1 1 80
Alter_20EP / The optimal adversary 80 20 20 4
Pre+Alter_1EP 20 The optimal adversary 80 1 1 80
Pre+Alter_20EP 20 The optimal adversary 80 20 20 4
Fig. 5. The evaluation metrics values of the MARLISA controller under no attack, random attack, optimal attack in experiment 1.
Fig. 6. The evaluation metric score of six MARLISA controllers under no attack, random attack and optimal attack in experiment 2.
5.5. Evaluation metrics analysis
After comparing the training performance of different control mod-
els for three experiments in Section 5.4, this section aims to quantita-
tively evaluate and analyze the metric and score performance expressed
in Section 4 for three experiments, respectively.
5.5.1. Train the optimal adversary policy with a fixed victim policy
Fig. 5 compares the performance of six metrics together with their
averaged score for different adversarial attacks, i.e., no attack, random
attack, and optimal attack. The first observation from Fig. 5 is that
the differences among three adversarial attacks are significant for the
first four metrics, i.e., Ramping ,1-Load factor ,Avg. daily peak , and Peak
demand . In other words, the adversarial attacks have a higher effect on
the model resilience in terms of the above four metrics. In detail, the
values of Ramping under the optimal and random attacks respectively
increase by 38.60% and 19.99% compared to no attack. Such increases
can be obtained for 1-Load factor as well in 8.46% and 2.23%. As a
result, it can be concluded that the demand curve is much uneven with
adversarial attacks. In addition, another two metrics Avg. daily peakand Peak demand are slightly increased by 1.66% and 1.01% under
random attack, while significantly increased by 8.77% and 16.42%
under optimal attack. Nevertheless, the total net electricity demand Net
elec. consumption and total carbon emissions Carbon emissions are almost
the same for different attack models. Finally, the averaged score under
optimal attack is 11.30% and 7.34% higher than no attack and random
attack, respectively. In conclusion, adversarial attacks do not impact
the total amount of energy storage and release but influence the energy
storage or release at each time step, which causes the demand curve to
oscillate.
5.5.2. Train the victim policy with a fixed optimal adversary
In this section, six MARLISA controllers taking robust adversarial
training are tested individually for three kinds of fixed attacks, i.e., no,
random, and optimal. More specifically, their score performance and
the associated metric values are presented in Fig. 6 and Table 5,
respectively.
It can be observed from Fig. 6 that the performance of score with
pre-training (Pre+NO_AD, Pre+OP_AD, Pre+RAN_AD) is always better
than those without pre-training (NO_AD, OP_AD, RAN_AD) for all three
Applied Energy 324 (2022) 119688
11L. Zeng et al.
Table 5
Metrics performance of six MARLISA controllers under no attack, random attack and optimal attack in experiment 2.
Case Model Ramping 1-Load factor Avg. daily peak Peak demand Net elec. consumption Carbon emissions
No attackNO_AD 0.7709 0.9052 0.9194 0.8693 0.9716 0.9726
OP_AD 0.8367 0.9309 0.9248 0.9189 0.9721 0.9730
RAN_AD 0.9199 0.9356 0.9502 0.9065 0.9737 0.9750
Pre+NO_AD 0.6895 0.8876 0.8887 0.8536 0.9712 0.9721
Pre+OP_AD 0.7353 0.9104 0.8990 0.8588 0.9694 0.9700
Pre+RAN_AD 0.9024 0.9375 0.9402 0.8747 0.9741 0.9749
Random attackNO_AD 0.9250 0.9254 0.9347 0.8781 0.9723 0.9734
OP_AD 0.9140 0.9434 0.9332 0.9159 0.9726 0.9736
RAN_AD 0.9617 0.9351 0.9519 0.9073 0.9738 0.9750
Pre+NO_AD 0.8479 0.9092 0.9056 0.8732 0.9721 0.9732
Pre+OP_AD 0.7969 0.9188 0.9026 0.8688 0.9697 0.9704
Pre+RAN_AD 0.9334 0.9399 0.9438 0.8773 0.9742 0.9750
Optimal attackNO_AD 1.0685 0.9818 1.0000 1.0120 0.9782 0.9799
OP_AD 0.8587 0.9366 0.9300 0.9162 0.9723 0.9732
RAN_AD 0.9233 0.9378 0.9489 0.9085 0.9736 0.9748
Pre+NO_AD 0.8643 0.9309 0.9429 0.9254 0.9773 0.9788
Pre+OP_AD 0.7499 0.9231 0.8978 0.8609 0.9699 0.9705
Pre+RAN_AD 0.9083 0.9381 0.9417 0.8808 0.9742 0.9750
Fig. 7. The evaluation metric score of five MARLISA controllers under no attack, random attack, fixed optimal attack and alternative optimal attack in experiment 3.
test scenarios. As a result, such comparisons can demonstrate the ben-
efits of pre-training in improving score performance in case of any test
adversary strategy and adversarial training policy. Now, further anal-
ysis should be focused on the three training models with pre-training,
i.e., Pre+NO_AD, Pre+OP_AD, Pre+RAN_AD. More specifically, in the
first test scenario of no attack, Pre+NO_AD achieves the best score
performance (i.e., lowest score value), which means the random and
optimal adversarial training does not help improve the performance
if there is no attack in the DR program. However, in the latter two
test scenarios, when random and optimal adversary strategies are used
for attacking, the optimal adversarial training Pre+OP_AD achieves the
best performance. This is because robust adversarial training is capable
of efficiently mitigating the influence caused by the attacks.
As defined in Section 4, the score is averaged by six metrics. Table 5
thus provides and compares the specific value of each metric for differ-
ent adversarial attack strategies and training models. It can be found
that there are not many differences between the last two metrics Net
elec. consumption andCarbon emissions among different training models
for each attack strategy. That means the total energy consumption
is unaffected so that the energy usage security inside the building is
always guaranteed. To this end, it can be found from Table 5 that the
major differences are coming from the first four metrics, i.e., Ramping ,
1-Load factor ,Avg. daily peak andPeak demand . Notably, all these fourmetrics are related to the demand profiles of daily usage. Similar to
thescore performance illustrated in Fig. 6, (1) Pre+NO_AD achieves
the best metric performance in the first test scenario of no attack; and
(2) Pre+OP_AD achieves the best metric performance in the latter two
test scenarios of random and optimal attacks. As a result, it can be
concluded that the demand profile exhibits a flatter pattern (reflected
by lower Ramping ,1-Load factor ) meanwhile the demand peak is signif-
icantly reduced (reflected by lower Avg. daily peak ,Peak demand ) after
considering a robust adversarial training model Pre+OP_AD.
5.5.3. Periodic alternating robust adversarial training
In experiment 3, a more complex adversarial training structure that
alternatively trains controller policy and adversary policy is proposed.
In addition, two periodic updating settings per episode and 20 episodes
are evaluated in this experiment to investigate the impact of periodic
training on policy performance. Finally, the pre-training method is
also applied to the two periodic updating models for comparison,
respectively.
Fig. 7 illustrates the score performance of five controllers for four
different test scenarios, of which the last scenario alternative optimal
attack is the new one for experiment 3, defined in Section 5.3. Similar
to the results obtained in experiment 2, no alternative adversarial
training achieves the best performance (0.8299) among five controllers
Applied Energy 324 (2022) 119688
12L. Zeng et al.
Table 6
Metrics performance of five MARLISA controllers under no attack, random attack, fixed optimal attack and alternative optimal attack in experiment 3.
Case Model Ramping 1-Load factor Avg. daily peak Peak demand Net elec. consumption Carbon emissions
No attackNO_ALAD 𝟎.𝟓𝟓𝟓𝟗 𝟎.𝟖𝟐𝟕𝟕 𝟎 .𝟖𝟓𝟕𝟎 𝟎 .𝟖𝟎𝟏𝟎 0.9686 0.9693
Alter_1EP 0.7455 0.8762 0.8977 0.8290 0.9687 0.9695
Alter_20EP 0.6166 0.8408 0.8689 0.8300 0.9678 0.9685
Pre+Alter_1EP 0.6821 0.8791 0.8864 0.8644 0.9666 0.9669
Pre+Alter_20EP 0.6558 0.8669 0.8750 0.8236 0.9672 0.9678
Random attackNO_ALAD 0.7831 0.8899 0.8852 0.8475 0.9695 0.9702
Alter_1EP 0.8096 0.8778 0.9019 0.8503 0.9689 0.9697
Alter_20EP 0.6702 0.8489 0.8724 0.8337 0.9680 0.9687
Pre+Alter_1EP 0.7547 0.8767 0.8923 0.8640 0.9669 0.9673
Pre+Alter_20EP 0.7164 0.8723 0.8784 0.8324 0.9673 0.9679
Fixed optimal attackNO_ALAD 0.7862 0.9240 0.9233 0.9646 0.9740 0.9753
Alter_1EP 0.7728 0.8687 0.9009 0.8782 0.9690 0.9698
Alter_20EP 0.6536 0.8402 0.8721 0.8119 0.9685 0.9693
Pre+Alter_1EP 0.7144 0.8720 0.8892 0.8561 0.9675 0.9681
Pre+Alter_20EP 0.7196 0.8697 0.8842 0.8333 0.9681 0.9689
alternative optimal attackNO_ALAD / / / / / /
Alter_1EP 0.7285 0.8796 0.8929 0.8491 0.9685 0.9693
Alter_20EP 0.6325 0.8506 0.8725 0.8509 0.9676 0.9684
Pre+Alter_1EP 0.6848 0.8589 0.8788 0.8470 0.9666 0.9671
Pre+Alter_20EP 0.6387 0.8795 0.8772 0.8574 0.9669 0.9674
Fig. 8. Weekly district net electricity consumption profiles of the MARLISA controller under no attack, random attack and optimal attack in experiment 1.
if there is no attack during the test process, as shown in Fig. 7(a).
However, once the random and optimal adversarial attacks are consid-
ered in the test scenarios, the alternative training controllers do help
improve the score performance with the relatively lower score values.
In detail, it can be observed from Fig. 7 that the score performance
of controllers under 20 episodes’ alternative training (i.e., Alter_20EP,
Pre+Alter_20EP) performs better than those under 1 episode’s alterna-
tive training (i.e., Alter_1EP, Pre+Alter_1EP). This trend suggests that
effective training episodes (e.g., update per 20 episodes) can improve
score performance rather than high-frequency updates (e.g., update
per episode). The final interesting result found from Fig. 7 is that
pre-training controllers (Pre+Alter_1EP, Pre+Alter_20EP) do not help
improve the score performance and exhibit higher score values than
those without pre-training (Alter_1EP, Alter_20EP) for all four test
scenarios. Such phenomenon seems to contradict the conclusion from
experiment 2 as discussed in Section 5.5.2.
The detailed metric performance associated with each score is pre-
sented in Table 6. First of all, it can be found that both metrics Net
elec. consumption and Carbon emissions show their little differences for
all five controllers and four test scenarios. Furthermore, the controllerNO_ALAD performs the best under no attack scenario, while Alter_20EP
performs the best for the rest of the three attack scenarios. These results
follow the same trends as the score performance illustrated in Fig. 7.
5.6. Performance analysis
To further elaborate on the generalization performance of the
learned DR controller models, this section aims to (1) compare weekly
demand profiles evaluated in experiments 1 and 3; (2) analyze the
energy profiles and the flexibility of storage inside the building energy
management.
Fig. 8 shows the weekly demand profiles for three test scenarios in
experiment 1, where random attack (blue line in (a)) and optimal attack
(green line in (b)) are compared with the baseline no attack (gray lines
in both (a) and (b)). It can be observed that the optimal attack exhibits
a more uneven pattern, a higher peak value, and more demand peaks
than the random attack, given the baseline of no attack. It is believed
that the optimal attack exposes the vulnerability of the MARLISA con-
troller and weakens the resilience of the DR management system. On
the other hand, Fig. 9 shows the weekly demand profiles of controllers
Applied Energy 324 (2022) 119688
13L. Zeng et al.
Fig. 9. Weekly district net electricity consumption profiles of the controller models NO_ALAD and Alter_20EP under no attack, random attack and optimal attack in experiment 3.
Fig. 10. Various electricity consumption of the controller models NO_ALAD and Alter_20EP.
NO_ALAD and Alter_20EP for three test scenarios in experiment 3,
respectively. It can be found that the controller Alter_20EP exhibits a
better performance than NO_ALAD in terms of fewer demand peaksand a flatter pattern. Furthermore, the demand profiles of controller
Alter_20EP are robust under all cases, while controller NO_ALAD is
sensitive to the test scenarios.
Applied Energy 324 (2022) 119688
14L. Zeng et al.
After demonstrating the differences in electric demand profiles be-
tween three attack scenarios, the other objective of this section lies
in investigating the energy portfolios inside the building management
system with and without robust adversarial training under attacks,
indicated by models Alter_20EP and NO_ALAD, respectively. To this
end, Fig. 10(a) compares the total yearly energies of building’s six com-
ponents for NO_ALAD and Alter_20EP, including inflexible demands
𝑃𝑒𝑑
𝑡, PV generation 𝑃𝑝𝑣
𝑡, electric energy storage charging 𝑃𝑒𝑒𝑠,𝑖𝑛
𝑡and
discharging 𝑃𝑒𝑒𝑠,𝑜𝑢𝑡
𝑡behaviors, consumption of heat pump 𝑃ℎ𝑝
𝑡, and
electric heater 𝑃𝑒ℎ
𝑡.
It can be observed from Fig. 10(a) that the major differences be-
tween NO_ALAD and Alter_20EP are coming from the storage charging
(red) and discharging (blue) behaviors. In detail, buildings under Al-
ter_20EP less deploy their storage flexibility to balance the demand–
supply quantity with respect to NO_ALAD. To better demonstrate the
effect of storage strategies on buildings’ net demand profile, the hourly
averaged charging and discharging schedules of storage together with
their resulted demand profiles for NO_ALAD and Alter_20EP are plotted
in Fig. 10(b). Although NO_ALAD and Alter_20EP differ with regard
to the robust adversarial training (under the latter) or not (under the
former), they exhibit some common trends. Storage starts to charge
the battery during the periods of morning and evening when demand
is relatively low, while discharging the battery during the periods of
mid-day and night when PV is abundant, and demand is relatively
high. Nonetheless, there are also evident differences between these
two models. Firstly, storage under Alter_20EP increases the discharging
behaviors in the mid-day in order to increase the absorption of PV
resources while increasing the demand (negative) levels. Secondly,
storage under Alter_20EP reduces the discharging behaviors at night in
order to reduce the demand peaks. Such two differences come together
to flatten the demand profiles, as depicted in Fig. 10(b).
6. Conclusion
This paper proposes a novel robust MARL-based DR methodological
framework, RAMARL-DR, based on the optimal adversary to enhance
the resilience of the DR management system against cyber-attacks.
In particular, a single-agent SAC algorithm is employed to learn an
optimal adversary policy, which can generate the adversarial examples
causing the worst-case performance for the multi-agent system. After
that, the learned optimal adversary policy can be used to train the
victim policy in a periodic alternative way via conducting robust adver-
sarial training. The superior performance of the proposed RAMARL-DR
method has been demonstrated based on an OpenAI Gym environment
CityLearn. The main conclusions stemming from the results include:
(1) the current MARL-based DR approach is indeed vulnerable under
adversarial attacks; (2) the robustness of MARL-based DR can be im-
proved after conducting adversarial training with the fixed optimal
adversary, while the adversarial training with the random adversary
negatively affects its performance; (3) the proposed periodic alternating
robust adversarial training to resolve the convergence issue in the
MARL-based DR management system caused by episode-by-episode
alternating training.
Future work could be conducted to investigate the identification
method for the most vulnerable agents in the multi-agent environment
and further improve the robustness of the MARL-based DR management
system by optimizing the periods of alternating training. In addition,
this work only focuses on the effects of demand–supply balances and
flatting net demand. Another future work thus aims to investigate the
economic perspective of this problem and involve the grid electricity
price signals.CRediT authorship contribution statement
Lanting Zeng: Methodology, Software, Data curation, Validation,
Formal analysis, Writing – original draft, Writing – review & editing.
Dawei Qiu: Methodology, Data curation, Formal analysis, Writing –
original draft, Writing – review & editing. Mingyang Sun: Methodol-
ogy, Writing – original draft, Writing – review & editing, Conceptual-
ization, Project administration, Supervision, Funding acquisition.
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Acknowledgments
This work was supported in part by the the National Natural Sci-
ence Foundation of China under Grants 52161135201, U20A20159,
62103371.
References
[1] Plessmann G, Blechinger P. How to meet EU GHG emission reduction targets?
A model based decarbonization pathway for Europe’s electricity supply system
until 2050. Energy Strategy Rev 2017;15:19–32.
[2] Perez-Arriaga IJ, Batlle C. Impacts of intermittent renewables on electricity
generation system operation. Econ Energy Environ Policy 2012;1(2):3–18.
[3] Wang Q, Zhang C, Ding Y, Xydis G, Wang J, Østergaard J. Review of real-
time electricity markets for integrating distributed energy resources and demand
response. Appl Energy 2015;138:695–706.
[4] Assante M. Confirmation of a coordinated attack on the Ukrainian power grid.
SANS industrial control systems security. 2016, URL https://www.sans.org/blog/
confirmation-of-a-coordinated-attack-on-the-ukrainian-power-grid/.
[5] Liu Y, Ning P, Reiter MK. False data injection attacks against state estimation in
electric power grids. ACM Trans Inf Syst Secur 2011;14(1):1–33.
[6] Kosut O, Jia L, Thomas RJ, Tong L. Malicious data attacks on smart grid
state estimation: Attack strategies and countermeasures. In: 2010 first IEEE
international conference on smart grid communications. IEEE; 2010, p. 220–5.
[7] Coppolino L, Romano L, et al. Exposing vulnerabilities in electric power grids:
An experimental approach. Int J Crit Infrastruct Prot 2014;7(1):51–60.
[8] Dabrowski A, Ullrich J, Weippl ER. Grid shock: Coordinated load-changing
attacks on power grids: The non-smart power grid is vulnerable to cyber attacks
as well. In: Proceedings of the 33rd annual computer security applications
conference. 2017. p. 303–14.
[9] Lu Z, Wang W, Wang C. Modeling, evaluation and detection of jam-
ming attacks in time-critical wireless applications. IEEE Trans Mob Comput
2014;13(8):1746–59. http://dx.doi.org/10.1109/TMC.2013.146.
[10] Lu Z, Lu X, Wang W, Wang C. Review and evaluation of security threats on
the communication networks in the smart grid. In: 2010-milcom 2010 military
communications conference. IEEE; 2010, p. 1830–5.
[11] Sgouras KI, Birda AD, Labridis DP. Cyber attack impact on critical smart grid
infrastructures. In: ISGT 2014. IEEE; 2014, p. 1–5.
[12] Albadi MH, El-Saadany EF. Demand response in electricity markets: An overview.
In: 2007 IEEE power engineering society general meeting. IEEE; 2007, p. 1–5.
[13] Yang J, Zhao J, Luo F, Wen F, Dong ZY. Decision-making for electricity retailers:
A brief survey. IEEE Trans Smart Grid 2017;9(5):4140–53.
[14] Lee A. Electric sector failure scenarios and impact analyses. Technical working
group, 1, National electric sector cybersecurity organization resource (NESCOR);
2013.
[15] Wang J, Zhong H, Ma Z, Xia Q, Kang C. Review and prospect of integrated
demand response in the multi-energy system. Appl Energy 2017;202:772–82.
[16] Kelley MT, Pattison RC, Baldick R, Baldea M. An MILP framework for op-
timizing demand response operation of air separation units. Appl Energy
2018;222:951–66.
[17] Heidari A, Mortazavi SS, Bansal RC. Stochastic effects of ice storage on
improvement of an energy hub optimal operation including demand response
and renewable energies. Appl Energy 2020;261:114393.
[18] Bianchini G, Casini M, Vicino A, Zarrilli D. Demand-response in building heating
systems: A model predictive control approach. Appl Energy 2016;168:159–70.
[19] Sutton RS, Barto AG. Reinforcement learning: An introduction. MIT Press; 2018.
[20] Vázquez-Canteli JR, Nagy Z. Reinforcement learning for demand response: A
review of algorithms and modeling techniques. Appl Energy 2019;235:1072–89.
[21] Qiu D, Ye Y, Papadaskalopoulos D, Strbac G. Scalable coordinated management
of peer-to-peer energy trading: A multi-cluster deep reinforcement learning
approach. Appl Energy 2021;292:116940.
Applied Energy 324 (2022) 119688
15L. Zeng et al.
[22] Lu R, Hong SH. Incentive-based demand response for smart grid with
reinforcement learning and deep neural network. Appl Energy 2019;236:937–49.
[23] Wang B, Li Y, Ming W, Wang S. Deep reinforcement learning method for
demand response management of interruptible load. IEEE Trans Smart Grid
2020;11(4):3146–55.
[24] Du Y, Zandi H, Kotevska O, Kurte K, Munk J, Amasyali K, et al. Intelligent multi-
zone residential HVAC control strategy based on deep reinforcement learning.
Appl Energy 2021;281:116117.
[25] Qiu D, Dong Z, Zhang X, Wang Y, Strbac G. Safe reinforcement learn-
ing for real-time automatic control in a smart energy-hub. Appl Energy
2022;309:118403.
[26] Lu R, Hong SH, Yu M. Demand response for home energy management using
reinforcement learning and artificial neural network. IEEE Trans Smart Grid
2019;10(6):6629–39.
[27] Lu R, Li Y-C, Li Y, Jiang J, Ding Y. Multi-agent deep reinforcement learning
based demand response for discrete manufacturing systems energy management.
Appl Energy 2020;276:115473.
[28] Kazmi H, Suykens J, Balint A, Driesen J. Multi-agent reinforcement learning
for modeling and control of thermostatically controlled loads. Appl Energy
2019;238:1022–35.
[29] Huang S, Papernot N, Goodfellow I, Duan Y, Abbeel P. Adversarial attacks on
neural network policies. 2017, arXiv preprint arXiv:1702.02284.
[30] Zheng Y, Yan Z, Chen K, Sun J, Xu Y, Liu Y. Vulnerability assessment of deep
reinforcement learning models for power system topology optimization. IEEE
Trans Smart Grid 2021;12(4):3613–23.
[31] Figura M, Kosaraju KC, Gupta V. Adversarial attacks in consensus-based multi-
agent reinforcement learning. In: 2021 American control conference. IEEE; 2021,
p. 3050–5.
[32] Lin J, Dzeparoska K, Zhang SQ, Leon-Garcia A, Papernot N. On the robustness
of cooperative multi-agent reinforcement learning. In: 2020 IEEE security and
privacy workshops. IEEE; 2020, p. 62–8.
[33] Kos J, Song D. Delving into adversarial attacks on deep policies. 2017, arXiv
preprint arXiv:1705.06452.
[34] Goodfellow IJ, Shlens J, Szegedy C. Explaining and harnessing adversarial
examples. 2014, arXiv preprint arXiv:1412.6572.
[35] Pattanaik A, Tang Z, Liu S, Bommannan G, Chowdhary G. Robust Deep
Reinforcement Learning with Adversarial Attacks. In: Proceedings of the 17th
international conference on autonomous agents and multiagent systems. 2018.
p. 2040–2.[36] Chen T, Niu W, Xiang Y, Bai X, Liu J, Han Z, et al. Gradient band-based
adversarial training for generalized attack immunity of A3C path finding. 2018,
arXiv preprint arXiv:1807.06752.
[37] Tan KL, Esfandiari Y, Lee XY, Sarkar S, et al. Robustifying reinforcement
learning agents via action space adversarial training. In: 2020 American control
conference. IEEE; 2020, p. 3959–64.
[38] Pinto L, Davidson J, Sukthankar R, Gupta A. Robust adversarial reinforcement
learning. In: International conference on machine learning. PMLR; 2017, p.
2817–26.
[39] Zhang H, Chen H, Boning DS, Hsieh C-J. Robust reinforcement learning on state
observations with learned optimal adversary. In: International conference on
learning representations. 2021.
[40] Zhang K, Sun T, Tao Y, Genc S, Mallya S, Basar T. Robust multi-agent
reinforcement learning with model uncertainty. Adv Neural Inf Process Syst
2020;33:10571–83.
[41] Li S, Wu Y, Cui X, Dong H, Fang F, Russell S. Robust multi-agent reinforcement
learning via minimax deep deterministic policy gradient. In: Proceedings of the
AAAI conference on artificial intelligence, vol. 33. (01):2019, p. 4213–20.
[42] Yang Y, Wang J. An overview of multi-agent reinforcement learning from game
theoretical perspective. 2020, arXiv preprint arXiv:2011.00583.
[43] Gleave A, Dennis M, Kant N, Wild C, Levine S, Russsell S. Adversarial policies:
Attacking deep reinforcement learning. In: International conference on learning
representations. 2020.
[44] Nagy Z, Vázquez-Canteli JR, Dey S, Henze G. The learn challenge 2021. In: Pro-
ceedings of the 8th ACM international conference on systems for energy-efficient
buildings, cities, and transportation. 2021. p. 218–9.
[45] Vázquez-Canteli JR, Kämpf J, Henze G, Nagy Z. CityLearn v1. 0: An OpenAI gym
environment for demand response with deep reinforcement learning. In: Proceed-
ings of the 6th ACM international conference on systems for energy-efficient
buildings, cities, and transportation. 2019. p. 356–7.
[46] Vazquez-Canteli JR, Henze G, Nagy Z. MARLISA: Multi-agent reinforcement
learning with iterative sequential action selection for load shaping of grid-
interactive connected buildings. In: Proceedings of the 7th ACM international
conference on systems for energy-efficient buildings, cities, and transportation.
2020. p. 170–9."
1-s2.0-S0957417421003377-main.pdf,"Expert Systems With Applications 176 (2021) 114896
Available online 17 March 2021
0957-4174/© 2021 Elsevier Ltd. All rights reserved.A deep reinforcement learning-based method applied for solving 
multi-agent defense and attack problems 
Liwei Huanga, Mingsheng Fua,b,*, Hong Qua, Siying Wanga, Shangqian Hua 
aSchool of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China 
bSchool of Electrical and Electronic Engineering, Nanyang Technological University, Singapore   
ARTICLE INFO  
Keywords: 
Multi-agent cooperation 
Defense and attack 
Deep reinforcement learning 
Multi-agent reinforcement learning ABSTRACT  
Learning to cooperate among agents has always been an important research topic in artificial intelligence. Multi- 
agent defense and attack, one of the important issues in multi-agent cooperation, requires multiple agents in the 
environment to learn effective strategies to achieve their goals. Deep reinforcement learning (DRL) algorithms 
have natural advantages dealing with continuous control problems especially under situations with dynamic 
interactions, and have provided new solutions for those long-studied multi-agent cooperation problems. In this 
paper, we start from deep deterministic policy gradient (DDPG) algorithm and then introduce multi-agent DDPG 
(MADDPG) to solve the multi-agent defense and attack problem under different situations. We reconstruct the 
considered environment, redefine the continuous state space, continuous action space, reward functions 
accordingly, and then apply deep reinforcement learning algorithms to obtain effective decision strategies. 
Several experiments considering different confrontation scenarios are conducted to validate the feasibility and 
effectiveness of the DRL-based methods. Experimental results show that through learning the agents can make 
better decisions, and learning with MADDPG achieves superior performance than learning with other DRL-based 
models, which also explains the importance and necessity of mastering other agents ’ information.   
1.Introduction 
Multi-agent system can be defined as a group of autonomous, 
interacting entities sharing a common environment, which they 
perceive with sensors and upon which they act with actuators (Bu et al., 
2008 ). Multi-agent cooperative control provides a promising paradigm 
for studying how agents can learn coordinated behaviors in multi-agent 
systems (Yu et al., 2015 ), and it is finding increasing applications in a 
wide range of real-world domains, such as robotics (Kim and Chung, 
2006 ), distributed control (Goldman and Zilberstein, 2004 ), and 
resource management (Galstyan et al., 2005 ). 
Multi-agent control has been an active research area for the past few 
years, and there has been numerous works that study multiple agents 
control problem. An efficient algorithm based on pseudo spectral (PS) 
and level set (LS) was presented in Robinson et al. (2018) to generate 
optimal trajectories for multiple vehicles in complex non-convex maze- 
like environments. Kantaros and Zavlanos (2016) proposed a distrib -
uted, hybrid control scheme which resulted in servicing a collection of 
tasks in complex environments by the available robots in a network. A novel k-degree smoothing method was put forward to smooth the initial 
trajectories obtained by improved ant colony optimization (IACO) of 
multiple unmanned aerial vehicles, and the cooperation scheme was 
also induced by k-degree smoothing (Huang et al., 2016 ). A co- 
evolutionary improved genetic algorithm (CIGA) was presented in Qu 
et al. (2013) for global path planning of multiple robots, which 
employed a co-evolution mechanism together with an improved genetic 
algorithm. A self organizing map-based approach which integrated the 
advantages and characteristics of biological neural systems was pre-
sented in Yi et al. (2017) , and the approach was capable of dynamically 
planning the paths of a swarm robots in 3-D environments under un-
certain situations. The authors proposed a novel approach to determine 
the optimal trajectory for multi-robots in a clutter environment using 
hybridization of improved particle swarm optimization (IPSO) with 
differentially perturbed velocity algorithm in Das et al. (2016) . All those 
above-mentioned articles are conducted under various conditions, and 
when facing new environments, the environments must be re-modeled, 
and the proposed algorithms must be re-applied to obtain the final re-
sults, which results in the sensibility and inadaptability to new 
*Corresponding author at: School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China. 
E-mail addresses: liweihuang@uestc.edu.cn (L. Huang), fms@uestc.edu.cn (M. Fu), hongqu@uestc.edu.cn (H. Qu), wangsiying527@126.com (S. Wang), 
hushangq@163.com (S. Hu).  
Contents lists available at ScienceDirect 
Expert Systems With Applications 
u{�~zkw! s{yo|kr o>!ÐÐÐ1ow�o �to~1m{y2w{m k�o2o�Ðk!
https://doi.org/10.1016/j.eswa.2021.114896 
Received 28 June 2020; Received in revised form 23 October 2020; Accepted 7 March 2021   
Expert Systems With Applications 176 (2021) 114896
2environments. 
Planning has long been studied in artificial intelligence, but research 
in reinforcement learning has brought a number of powerful innovation 
(Littman, 2015 ). Reinforcement learning (RL) (Sutton and Barto, 2018 ) 
has been widely applied to multi-agent systems. As a simple and classical 
reinforcement learning method, a Q-learning based approach was pro-
posed to deal with traffic lights control problem which was modeled as a 
multi-agent system (Abdoos et al., 2011 ). In Park et al. (2001) , the au-
thors adopted modular Q-learning architecture which included learning 
modules and a mediator module to assign a proper action to an agent in 
the multi-agent system under robot soccer situation. Benefiting from the 
improvement of high performance computational hardware, deep neu-
ral networks-based reinforcement learning, namely deep reinforcement 
learning (DRL) shows great potential for solving complex multi-agent 
cooperative control problems. On the one hand, the natural way for 
finding policies for a multi-agent system is to learn decentralized value 
functions or policies directly and individually. Tan proposed to train 
independent state-action value functions for each agent through tradi-
tional Q-learning (Tan, 1993 ), and Tampuu et al. further extended this 
to deep neural networks by using DQN (Tampuu et al., 2017 ). However, 
learning independently affects the learning processes of other agents, 
and leads to instability of the environment. On the other hand, 
centralized learning by taking other agents ’ information into consider -
ation can avoid the effects of the non-stationary factors. Chen et al. 
presented a multi-agent collision avoidance algorithm based on deep 
reinforcement learning which included a value network to encode the 
joint configuration with the neighbors (Chen et al., 2017 ). Moreover, 
with the help of actor critic (AC) architecture (Mnih et al., 2016 ), some 
works have presented the hybrid value-based and policy-based learning 
methods, as well as the methods with centralized training and decen -
tralized execution. For the AC-based deep reinforcement learning, Lil-
licrap proposed the deep deterministic policy gradient (DDPG) 
algorithm (Lillicrap et al., 2015 ) to deal with the continuous control 
problem, as continuous control for multi-agents is very important and 
practical. The authors also put forward an adaption of actor-critic 
method (Lowe et al., 2017 ) which considered action policies of other 
agents and was able to successfully learn policies that required complex 
multi-agent coordination. Since then, a number of works based on RL or 
DRL have been proposed to deal with various multi-agent cooperation 
scenarios (Pendharkar and Cusatis, 2018; Foerster et al., 2018; Vinyals 
et al., 2019; Jaderberg et al., 2019; Li et al., 2019; Silva et al., 2019 ). 
The RoboFlag competition and its modifications, as a classical multi- 
agent cooperative control issue, have been well studied in D’Andrea and 
Murray (2003), Earl and D’Andrea (2002a), Earl and D’Andrea (2002b), 
Earl and D’Andrea (2007) . However, these methods are based on linear 
programming (LP), and can not effectively adapt to new environments. 
DRL-based approaches have natural advantages in solving continuous 
control problems, as their learning processes are the processes of 
continuous interactions with environments. In this paper, we propose to 
apply the DRL-based multi-agent cooperative control methods to deal 
with a simplified version of RoboFlag competition, which we call de-
fense and attack in the rest of this paper. The contributions of this paper 
mainly include: (1) we reconstruct the multi-agent defense and attack 
environment on basis of the open-source Multi-Agent Particle Environ -
ment1; (2) we introduce deep reinforcement learning algorithms to 
address the considered problem, and redefine the state space, the action 
space and reward functions accordingly; (3) several different DRL-based 
learning models are employed to train both the attack agents and de-
fense agents; (4) several experiments and comparisons are conducted to 
verify the effectiveness of the applied DRL models. 
The rest of this paper is organized as follows. Section 2 is devoted to 
the problem statement and environment modeling. The new multi-agent cooperative planning based on DRL algorithms for defense and attack is 
given in Section 3. Several simulations and comparisons are conducted 
in Section 4. Conclusions and future work are drawn in the last section. 
2.Problem formulation and environment modeling 
In this section, we firstly introduce the multi-agent defense and 
attack problem, and then model the environment through Markov De-
cision Processes (MDP) to further prepare for the training of deep 
reinforcement learning algorithms. 
2.1. Multi-agent defense and attack 
In this paper, we consider a simplified version of the RoboFlag 
competition problem (D’Andrea and Murray, 2003 ), which we call 
multi-agent defense and attack. In the considered problem, there are two 
teams of robots, which are the defenders and attackers, and a circular 
region called defense zone in the playing field, as shown in Fig. 1. 
Two sub-problems of the defense and attack issue are considered in 
this paper, which are described as follows. 
(1) Multi-agent defense and attack with rule-based attackers. 
In this sub-problem, the attackers perform their actions according 
to the given rules, and the defenders take actions based on the deep 
reinforcement learning model, namely ruled attackers versus intel-
ligent defenders. Note that the rule-based defenders versus intelli -
gent attackers situation also belongs to this sub-problem, since 
effective defense rules are much harder to design compared with the 
attack rules, we don’t consider this situation in this paper. 
(2) Multi-agent defense and attack with DRL-based attackers. 
In this sub-problem, both attackers and defenders take actions 
through the deep reinforcement learning models, namely intelligent 
attackers versus intelligent defenders. Under this situation, the at-
tackers cooperate to achieve their goal, so do the defenders. 
2.2. Environment modeling 
In multi-agent defense and attack problem, there are two kinds of 
agents in the considered problem with agent type k∃⊔ATKCDFS⊓, where 
ATK represents the attack agents, and DFS stands for the defense agents. 
Fig. 1.The defense and attack playing field: the red circular region is the de-
fense zone, the blue circles are the defenders, and the black circles are 
the attackers. 1The environments are publicly available: https://github.com/openai/mul -
tiagent-particle-envs. L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
3All the intelligent agents learn their policies through interacting with 
the environment. We model the problem as Markov Decision Processes 
(MDP), and our goal is to maximize the accumulated discounted rewards 
of all agents. We adopt independent quintuple Sk
iCAk
iCPk
iCRk
iCγto 
represent each agent i with type k, and the elements are explained 
respectively as follows. 
(1)Sk
i: The state space, which is an infinite set and consists of all states 
of the agent. 
(2)Ak
i: The action space, which contains all actions of the agent. It is 
also an infinite set. 
(3)Pk
i: The state transition function Sk
i×Ak
i×√Sk
i→0C1, where Pk
isk
iC
ak
iC√sk
iis the transition probability from state sk
i to state √sk
i by taking 
action ak
i. For deterministic transition, Pk
isk
iCak
iC√sk
i∃⊔0C1⊓C⊥sk
iCak
iC
√sk
i. 
(4)Rk
i: The reward function, which indicates the immediate reward 
paid to agent with action ak
i under state sk
i. 
(5)γ: The discount factor 0⩽γD1, which indicates the relative 
importance of future reward and current reward. 
2.2.1. The state space 
In the multi-agent defense and attack environment, the attack agents 
and defense agents have the same state representations. Let the set of all 
attack agents be AT, and the number of attack agents is †AT†N. 
Similarly, let the set of all defense agents be DF, and the number of 
defense agents is †DF†M. 
In general, the features in the defense agents and attack agents are 
similar. For a given agent, attacker or defender, three kinds of infor-
mation are involved in the state vector, which are (1) the information 
about the given agent itself, (2) the relative locations and velocities of 
the agents with the same type, and (3) the relative locations and ve-
locities of the agents with the opponent type, respectively. 
Specially, we denote satCiCt as the state vector of attacker ati at time 
instant t, which is defined as follows, 
satCiCt[
selfatCiCtCatsatCiCtCdfsatCiCt]
C (1)  
where selfatCiCt represents the features of ati itself, atsatCiCt stands for the 
features of other attackers, dfsatCiCt stands for the features of defenders, 
and satCiCt is the connection of these three feature vectors. 
Similarly, the state vector sdfCjCt for defender dfj at time instant t is 
defined as 
sdfCjCt[
selfdfCjCtCatsdfCjCtCdfsdfCjCt]
C (2)  
where selfdfCjCt represents the features of dfj itself, atsdfCjCt stands for the 
features of attackers, dfsdfCjCt stands for the features of other defenders, 
and sdfCjCt is the connection vector of the three feature vectors. 
The specific features of the components in (1) selfatCiCtCatsatCiCt, and 
dfsatCiCt can be viewed in A.1. Likewise, the specific features of the com-
ponents in (2) selfdfCjCtCatsdfCjCt, and dfsdfCjCt can be viewed in A.2. 
2.2.2. The action space 
In this paper, we consider continuous action space for the agents, and 
the action is expressed as a two-dimensional velocity vector vxCvy, 
which makes the attack and defense agents can move with a variable 
speed and in arbitrary direction. 
In order to effectively simulate the motion behavior of actual agents, 
the agent model in this paper outputs a two-dimensional acceleration 
vector √axC√ayof the agent instead of the direct velocity vector. The 
corresponding velocity can be obtained through the following velocity 
formula. 〉vxkCiCtvxkCiCt 1√axkCiCt×ΔtC
vykCiCtvykCiCt 1√aykCiCt×ΔtC(3)  
where √axkCiCt and √aykCiCt indicate the acceleration of the i-th agent with 
type k in x axis and y axis, respectively, and Δt represents the time 
interval. 
Although √axkCiCt and √aykCiCt can be obtained directly through the 
network model, the direct values of √axkCiCt and √aykCiCt has problems in 
stability because of the overfitting problems Lowe et al. (2017) . There -
fore, to solve the problem of directly generating continuous actions, we 
adopt the strategy integration method to obtain √axkCiCt and √aykCiCt, and the 
specific process is as follows. 
Firstly we define a set }A with five basic policy actions, shown as 
}AkCiCt(
}aleftCkCiCtC}arightCkCiCtC}aupCkCiCtC}adwonCkCiCtC}astopCkCiCt)
C (4)  
where }aleftCkCiCt∃0C1is the acceleration of the left direction along x axis, 
}arightCkCiCt∃0C1is the acceleration of the right direction along x axis, 
}aupCkCiCt∃0C1is the acceleration of the up direction along y axis, 
}adownCkCiCt∃0C1is the acceleration of the down direction along y axis, 
and }astopCkCiCt∃0C1is the acceleration to maintain still. To constrain the 
action values of basic strategy in }AkCiCt, we have }aleftCkCiCt}arightCkCiCt
}aupCkCiCt}adownCkCiCt}astopCkCiCt1. Thus, √axkCiCt and √aykCiCt can be calcu -
lated by 
〉
√axkCiCt 
 }alegtCkCiCt}arightCkCiCt)
×αC
√aykCiCt 
 }adownCkCiCt}aupCkCiCt)
×αB(5)  
where α is the sensitivity coefficient to constrain the range of accelera -
tion. Therefore, the actual action output of the agent network is trans -
formed into producing five values of the basic policy actions in set }A. 
2.2.3. Reward function 
Reward function indicates the performance of the performed action 
under a specific situation. In this subsection, we will redefine the reward 
functions for both attack agents and defense agents to make the 
employed DRL algorithms more advantageous in solving the considered 
problem. To avoid the problem of sparse reward, we utilize the distance 
based reward functions. 
For the attack agents, their goal is to reach the defense zone. Obvi-
ously, the closer the attack agents are to the defense zone, the greater the 
threat they have. Therefore, the reward of the attack agents should be 
inversely proportional to the distances between themselves and the 
defense zone. Then, the reward function of attack agent i at time instant t 
is defined as follows, 
ratCiCt ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪
x2
atCiy2
atCi̅
B (6)  
For the defense agents, their rewards are based on the distances also. 
Besides, the reward functions of defense agents need to take account of 
the attackers, and the reward function of defense agent j at time instant t 
is defined as 
rdfCjCtλ}rdfCjCt 
1 λ)
rdfCjCtC (7)  
where there are two opposite terms considering the relationship be-
tween the attacker and the defense zone }rdfCjCt, and the relationship be-
tween the attacker and the defender rdfCjCt. Their definitions are shown in 
Eqs. (8) and (9), respectively. 
}rdfCjCt̂N
i0⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪
x2
atCiy2
atCi̅
B (8) L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
4In (8), we consider the distances between all attackers and the defense 
zone. If the attacker is far away from the defense zone, the defender will 
get a bigger reward. Note that if attacker ati is captured by the defender, 
it will no longer be able to move forward to the defense zone, namely the 
vxatCi and vyatCi of ati will be 0 after being captured. However, we still 
consider the position where it finally left off in (8) to avoid the phe-
nomenon that the value of }rdfCjCt becomes smaller when the attacker ati is 
successfully defended. 
rdfCjCt ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ 
xatCl xdfCj)2 
yatCl ydfCj)2̅
C (9)  
where l can be obtained through the following equation, 
largmini0CN⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ 
xatCi xdfCj)2 
yatCi ydfCj)2̅
B (10)  
In (9), we consider the distance between the defender and the nearest 
attacker. Obviously, the smaller the distance between them, the better 
reward the defender should get. Therefore, rdfCjCt is inversely propor -
tional to the distance between the defender and its nearest attacker. 
Here, for defender dfj, only the nearest attacker is considered to guide 
dfj to focus on a specific attacker, which can avoid dfj paying attention 
to multiple attackers at the same time, and can avoid dfj constantly 
swinging among different attackers. 
3.Agent model and learning processes 
In multi-agent defense and attack environment, multiple different 
agents can be trained through different models. In this paper, we classify 
our problem into two categories, one is single agent model and learning 
process, the other is multi-agent model and learning process. In single 
agent model, each agent only considers the information of itself, while in 
multi-agent model, it is necessary to consider the information of itself as 
well as other agents. 
3.1. Single agent model and learning processes 
We adopt traditional deep reinforcement learning algorithm to deal 
with the single agent model. In the considered problem, the action space 
should be continuous, thus, we apply deep deterministic policy gradient 
(DDPG) algorithm (Lillicrap et al., 2015 ) to train the single agent. Since 
there are similarities between the attack agents and defense agents in 
states and actions, the only difference is the reward function definition. 
Therefore, we take the attack agents as examples to further illustrate the 
applied model and learning algorithm. For the defense agents, we only 
need to replace the reward function. 
3.1.1. The network structure of DDPG 
DDPG is an Actor Critic (AC)-based DRL model. Compared with 
value-based DRL methods, such as DQN, the generation of action 
strategy and the evaluation of action are completed through two sepa-
rate components in AC-based model. Whereas in DQN, the action se-
lection strategy is determined by the predicted actions with the highest 
Q-value each time. Therefore, in AC architecture, there are two deep 
neural networks, one is responsible for generating strategy (policy 
network), and the other is for evaluating the strategy (Q network). 
Thereby, for any attack agent ati, there is a corresponding policy 
network PatCi, and a corresponding Q network QatCi. The definitions of PatCi 
and QatCi are displayed in (11) and (12), respectively. 
}aatCiCtPatCi 
satCiCt⃦⃦θatCi)
C (11)  
where policy network PatCi generates action }aatCiCt according to the state 
satCiCt. θatCi is the parameter that the policy network needs to learn. Note 
that }aatCiCt is a continuous action, and we need to convert it into the actual 
action through strategy integration instead of sampling the actions. qsatCiCtCaatCiCtQatCi(
satCiCtCaatCiCt⃦⃦⃦}θatCi)
C (12)  
where Q network generates the evaluation value qsatCiCtCaatCiCt for state satCiCt 
and action }aatCiCt obtained by policy network PatCi. }θatCi is the parameter 
that the Q network needs to learn. Note that in DDPG, the polices and 
evaluations of different agents are independent, which means the policy 
and evaluation of each agent are determined only by its own state and 
action. 
In this paper, the policy network and Q network are constructed by 
multi-layer perception (MLP), and the architectures are shown in Fig. 2. 
The policy network has one input layer, two hidden layers and one 
output layer. Both the input layer and output layer are fully connected 
(FC) networks, which is defined in (13). In each hidden layer, there are 
64 neurons with ReLu as the activation function, and ReLu is defined in 
(14). The output layer has 5 neurons which represents the five basic 
actions, and we adopt the Softmax activation function to assure the sum 
of the actions ’ coefficients is 1. The definition of Softmax is displayed in 
(15). 
yσWxbC (13)  
where x represents the input of the network, y stands for the output of 
the network, W is the parameter matrix of the FC layers to be learned, b 
is the bias vector to be learned, and σ is the activation function. 
ReLu[
x]
〉
xCifxF0C
0Cifx≼0B(14)  
softmaxXjexj
⋃K
k1exkC (15)  
where softmaxXj represents the j-th output of all K outputs. 
The architecture of Q network is similar to policy network, but the 
input and output are slightly different. In Q network, the input is the 
connection vector of state satCiCt and action }aatCiCt generated by policy 
network. We do not use neurons to represent the output (None) to ensure 
that the range of evaluation value is in real number field. 
3.1.2. Learning processes of DDPG 
Since DDPG has two separate networks, we will illustrate the 
learning processes of the two networks correspondingly. The learning 
process of Q network is based on Bellman function, which describes the 
Fig. 2.The architecture of policy network P and Q network Q.  L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
5relationship between the evaluation value of current state-action pair 
and the evaluation value of the next state-action pair. The Q value can be 
formally defined as follows. 
Q[
sCa]
E
s∕⊃P⌊
r[
sCa]
γmax
a∕Q[
s∕Ca∕]⌋
B (16)  
The goal of Q network is to minimize the TD error, the smaller the TD 
error, the more accurate the evaluation of the estimated state-action 
pair. Through (16), we can get the TD error, which is 
L[
θ]
 E
sCaCrCs∕⌊
QsCa†θ rQs∕Ca∕†θ∕2⌋
B (17)  
Therefore, the corresponding parameter θ can be updated through 
gradient descend, as shown in the following equation. 
θθ η∂
∂θL[
θ]
C (18)  
where η is the learning rate. 
On the other hand, the learning for policy network is based on policy 
gradient, and the goal is to maximize the overall evaluation of actions in 
strategies generated by the policy network, shown as follows, 
J[
}θ]
E
s⊃D⌊
Q[
sCP[
s⃦⃦⃦⃦}θ]⃦⃦⃦⃦θ]⌋
B (19)  
In DDPG, the policy is deterministic, and the gradient in (19) can be 
represented as 
⋈}θJ[
}θ]
E
s⊃D⌊
⋈θP[
s⃦⃦⃦⃦}θ]
⋈aQ[
sCa⃦⃦⃦⃦θ]
†
aPs†}θ⌋
B (20)  
Then, the parameter }θ in policy network can be updated through 
gradient ascend in (21). 
}θ}θη∂
∂}θJ[
}θ]
B (21)  
The detailed learning processes can be found in Algorithm 1 in B. 
3.2. Multi-agent model and learning processes 
Single agent reinforcement learning cannot effectively utilize the 
global state and action information of other agents, namely the Q 
network in DDPG can only react according to single agent ’s state and 
action, which leads to the inaccurate evaluation given by the Q network. 
To solve this problem, the MADDPG (Lowe et al., 2017 ) algorithm had 
been proposed. 
MADDPG is an extension of DDPG in multi-agent environment. The 
policy networks of MADDPG and DDPG are the same, which consider 
single agent ’s state information. The difference between them is that the 
Q network in MADDPG takes the actions and states of other agents into account, and the difference is only reflected in the training phase. In the 
testing phase, the execution process of their policy networks are exactly 
the same since the Q networks are no longer needed. The comparison 
between MADDPG and DDPG in network structure is shown in Fig. 3. 
From Fig. 3, we can see the difference between MADDPG and DDPG. 
In the following subsections, we will also take the attack agent as an 
example, and briefly introduce the network structure and learning 
processe of MADDPG. 
3.2.1. The network structure of MADDPG 
In MADDPG, there is a policy network and a Q network for attack 
agent ati. The definition of policy network is consistent with that of 
DDPG, as shown in (11). However, the Q network is updated as 
qsatCiCtCaatCiCtQatCi(
satCiCtCaatCiCtCatsatCiCtCdfsatCiCtCata atCiCtCdfaatCiCt⃦⃦⃦}θatCi)
B (22)  
Compared with (12), (22) additionally takes into account the set of 
states of all other attack agents atsatCiCt and the set of actions ataatCiCt 
(excluding ati), as well as the set of states of all defense agents dfsatCiCt and 
the set of actions dfaatCiCt. In particular, atsatCiCt is the connection vector of 
state feature vectors of all attackers except ati, and ataatCiCt is the 
connection vector of 5-dimensional action vectors of all attackers except 
ati. dfsatCiCt and dfaatCiCt are defined similarly. 
The Q network and policy network in MADDPG also utilize MLP, as 
shown in Fig. 4. 
3.2.2. Learning processes of MADDPG 
The learning processes of MADDPG are similar to that of DDPG. For 
the Q network, the corresponding objective function is 
L⌊
θ⌋
 E
sCaCother saCrCs∕⊃D⌈
QsCaCother sa†θ rQs∕Ca∕Cother s∕a∕†θ∕2⌉
C
(23)  
where other sa is the states and actions of other attackers and de-
fenders. For the states and actions at the next time instant other s∕a∕, the 
states other s∕are from the replay buffer, and the related actions other a∕
are obtained through the delayed policy network. The update of 
gradient is shown in (18). 
For the policy network, the corresponding objective function is 
J⌊
}θ⌋
 E
sCother sa⊃D⌈
Q⌊
sCP⌊
s⃦⃦⃦⃦⃦}θ⌋
Cother sa⃦⃦⃦⃦⃦θ⌋⌉
C (24)  
and the gradient can be calculated as follows. 
⋈}θJ⌊
}θ⌋
 E
sCother sa⊃D⌈
⋈θP⌊
s⃦⃦⃦⃦⃦}θ⌋
⋈aQ⌊
sCaCother sa⃦⃦⃦⃦⃦θ⌋
†
aPs†}θ⌉
B(25)  
Then, the final update of gradient can also be calculated by (21). 
The overall processes of multi-agent defense and attack using 
Fig. 3.The difference between MADDPG training and DDPG training.  L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
6MADDPG are shown in Algorithm 2 in C. 
4.Simulation results and analysis 
In this section, we conduct a series of simulations to analyze the 
effects of the applied DRL algorithms in multi-agent defense and attack 
problem. We also give the environment model settings, evaluation 
criteria, parameter settings, statistical analysis, and results of the 
considered issue. The simulations are all carried out in Python 3 under 
Linux. 
4.1. Environment model settings 
The simulated environment is a two-dimensional environment with 
N attackers, M defenders and one defense zone. The environment is 
constructed in a bounded coordinate system with the upper and lower 
bounds of X and Y axes being 1 and  1, respectively. The attackers and 
defenders both have a radius of 0B08, the radius of defense zone is 0B1, 
and its coordinate is 0C0. In order to reasonably simulate the process of 
defense and attack, we specifically set that the distances between the 
initial positions of attackers and the defense zone are more than 0B9, and 
the distances between the initial positions of defenders and the defense 
zone are less than 0B4. 
The maximum speeds of the attackers and defenders should not 
exceed 0B4 and 1B0 respectively, and the time unit in environment is 0B1. 
Note that the defenders ’ maximum speed is higher than that of attackers, 
the reason is that there are more attackers than defenders under our 
situations, and it is almost impossible for the defenders to successively 
intercept all attackers without the speed advantage. In addition, if an 
attacker succeeds in attacking the defense zone or is captured by a de-
fender, the attacker will stop and cannot be captured by other defenders 
again. Finally, the interaction is terminated when there is no more at-
tackers or the maximum number of interactions is reached. 
For the defenders, we adopt DRL model to train them, while for the 
attackers, they can utilize DRL model to learn or just act according to the 
given rules. For the DRL model controlled agents, their motions are 
variable, and the accelerations of agents are given by the learning 
model, but the agents ’ maximum speed can not be exceeded. For the 
rule-based attackers, the following simple but effective rules are adopted 
in this paper: 1) the speed direction of each agent points directly to the 
defense zone to ensure that it moves along the shortest path towards its 
goal; 2) the attackers move with a uniform speed, and their speed is the maximum allowed speed. 
4.2. Evaluation criteria and parameter settings 
In our simulations, three different evaluation criteria are used to 
demonstrate the performance of the model, which are the average attack 
hit rate, the average attack success rate, and the average agent curric -
ulum reward, respectively. 
The average attack hit rate measures the quality of attacks, and it 
represents the percentage of N attackers that can hit the defense zone in 
a confrontation averagely. Obviously, for the attackers, the higher the 
average hit rate is, the better. The specific calculation of the average 
attack hit rate is shown as follows. 
MHR1
†B†̂†B†
i1Hi
NB (26)  
where B represents a set that stores †B†confrontations, Hi represents the 
number of attackers that successfully hit the defense zone in the i-th 
confrontation in B. 
The average attack success rate also evaluates the quality of attacks, 
and it represents the number of attackers that can hit the defense zone in 
a confrontation averagely. For this criterion, as long as the attack hit rate 
is greater than 0, the attack is considered successful. Similarly, for the 
attackers, the higher the average attack success rate is, the better. 
What ’s more, the average attack success rate is often higher than the 
average attack hit rate, and we define it as follows, 
MSR1
†B†̂†B†
i1IiC (27)  
where Ii is denoted as 
Ii〉
1CifHiF0C
0CifHi0B(28)  
Although the average attack hit rate and the average attack success rate 
can directly reflect the results of the confrontation, they cannot reflect 
whether the model is actually optimized. In order to clearly show the 
optimization process of the model, it is necessary to consider the accu-
mulative reward in a confrontation averagely, since the goal of rein-
forcement learning is to maximize the accumulative reward. For both 
the attackers and defenders, the performance of effective training is the 
Fig. 4.The architecture of policy network and Q network in MADDPG.  L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
7increase in the accumulative reward. The definition of the average 
accumulative reward is shown as follows. 
MAR1
†B†̂†B†
i1̂Ei
j1̂
g∃TAr⌊
kgCzgCj⌋
C (29)  
where Ei represents the i-th interaction in B, and the length of the 
interaction is †Ei†. TA represents the set of agents that need to be trained 
(exclusive of the rule-based agents). kg stands for the type of agent g, and 
zg represents the serial number of the agent under this type. 
Note that no additional test sets are needed in this paper, since every 
confrontation environment in the training phase is randomly generated, 
and there is almost no repetition. Therefore, there is no repeated training 
data which will induce the overfitting phenomenon. Thus, the results 
obtained during training are equivalent to the results obtained on other 
test sets. 
The related parameter settings of the environment, the learning 
procedure and the neural networks can be found in Table 1. 
In Table 1, the maximum interaction length refers to the maximum 
steps in a confrontation, the total interaction number refers to the 
confrontation number for training the agents. Our entire experiment was 
repeated for six times. In each experiment, 200,000 steps of training 
were performed, and at every 1000 steps, we performed a test. Specif -
ically, the test result is the average value of the results of the previous 
1000 steps. We finally report the average results of the six experiments. 
What ’s more, we perform the grid search for the hyperparameters of 
ANN under the scenario, 3 attackers vs. 2 defenders. Specifically, the 
numbers of layers are 3C4C5and the numbers of cells are 16C32C64. 
But we use the default learning rate (0.01) in Adam optimizer. 
4.3. Baseline methods 
To further study the effects of different DRL algorithms for solving 
the multi-agent defense and attack problems, we compare the DDPG- 
based and the MADDPG-based models with the following baseline 
algorithms. 
Independent Q-learning (IQL) (Tampuu et al., 2017 ) turns a multi- 
agent learning problem into a collection of simultaneous single-agent 
learning problems, and each agent learns an individual action-value 
function through deep Q-learning independently. 
Value Decomposition Networks (VDN) (Sunehag et al., 2018 ) 
decomposes a central state-action value function into a sum of individ -
ual state-action value functions through value decomposition networks 
based on the individual observations and actions. 
Actor Crtic (AC) (Mnih et al., 2016 ) combines the strong points of 
actor-only and critic-only methods. The critic network learns a value 
function through approximation, and then the value function is used to 
update the actor network in the direction of performance improvement. 
4.4. Multi-agent defense and attack with rule-based attack agents 
In the multi-agent defense and attack with rule-based attack agents problem, the defenders make decisions according to the learning model, 
and the actions of attackers are given through rules. 
We consider four different confrontation environments, which are 2 
defenders versus 2 attackers, 2 defenders versus 3 attackers, 3 defenders 
versus 5 attackers, and 4 defenders versus 6 attackers, respectively. In 
each environment, we compare the performance of defenders under AC, 
VDN, IQL, DDPG, and MADDPG training models. 
In order to more intuitively observe the processes of confrontation, 
we firstly give a confrontation example of 5 rule-based attackers versus 3 
MADDPG-based defenders, as shown in Fig. 5. The confrontation pro-
cesses are split into 8 frames. Frame 1 gives the initial positions of both 
attackers and defenders (randomly generated), and the attackers go 
straight to the defense zone while the defenders take actions according 
to the MADDPG learning model. In frame 3, attacker 4 is defended by 
defender 2, then it stops (becomes hollow circle afterwards) and the 
defender continues to search for other attackers. From frame 4 to frame 
7, attackers 2C3C5 and 1 are successfully defended accordingly. Frame 8 
shows the final locations of defenders. 
Further, we analyze the three evaluation criteria under different 
scenarios. The average accumulated reward (MAR) of different training 
models is shown in Fig. 6. 
From Fig. 6 we can see that through training, the average accumu -
lative rewards gradually increase for all the DRL models except AC. The 
increase in rewards implies the effective training of defense agents under 
different scenarios with rule-based attack agents. While for AC, since it is 
sensitive to the environments, and there are almost no identical envi-
ronments in the training phase, thus AC cannot learn effectively. What ’s 
more, by comparing the four sub-figures above, we can see that 
MADDPG achieves the best learning results among all methods. With the 
increasing number of agents in the scenarios, the gaps between 
MADDPG and other methods become more and more obvious, and this 
also reveals that the MADDPG-based learning model can take advantage 
of other agents ’ information to make better decisions. 
From Fig. 7 we can see that, with increasing steps, the average hit 
rate of attack agents descends, which means the defense success rate of 
defense agents increases. Through learning, the defenders can effec-
tively protect the defense zone from the attack of the rule-based at-
tackers. Besides, the performance of MADDPG is better than that of other 
DRL models, and this is especially obvious under the complex scenario. 
In Fig. 7a and b, the performance lines of DDPG and MADDPG are very 
close, however, we can see significant difference in the statistical data in 
Table 2. 
Seen from Fig. 8, the average attack success rate decreases with 
increasing training steps. The MSR under MADDPG trained defenders is 
lower than that under other DRL models trained defenders. This also 
implies that the MADDPG trained defenders performs better under all 
scenarios, and the advantage becomes more and more obvious as the 
environment becomes more complex. The detailed difference can be 
seen in Table 2. 
To clearly see the performance of different learning algorithms under 
different scenarios, Table 2 gives the final results of the criteria under 
four test scenarios. 
From Table 2, we can obtain the following conclusions. (1) MADDPG 
can consistently outperform other methods under different scenarios, 
which verifies the superiority of MADDPG. Additionally, the results of t- 
test indicate that the improvement obtained by MADDPG is significant. 
(2) AC is more sensitive than other methods, since its action selection 
process is stochastic in contrast with deterministic action selection in 
DDPG or MADDPG. As a consequence, the final results of AC are very 
poor. (3) VDN can achieve promising result in the early stage of learning. 
However, MADDPG and DDPG outperform VDN as the training steps 
increase. (4) MADDPG is generally better than DDPG, which reveals that 
explicitly considering other agents ’ information in the neural network 
architecture or in the learning algorithm can further improve the per-
formance. (5) In our environment, a scenario with more attackers is 
more complicated than a scenario with fewer attackers. As the Table 1 
Parameter settings of the model.  
Parameter type Parameter name Parameter value 
Environment Maximum interaction length 40  
Total interaction number 200,000  
Test Frequency 1000 
Learning Learning rate 0.01  
Discount factor 0.95  
Batch size 1024  
Optimizer Adam 
Network Network layer 5  
Activation function ReLu  
Number of hidden layer neurons 64  L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
8environment becomes more and more complex, the rule-based at-
tacker ’s success rate and hit rate continue to increase against our DRL- 
based defender, and all compared methods suffer from this problem. 
This drawback is due to two main reasons. (a) The complexity of the environment itself. Apparently, with more attackers, it becomes more 
difficult for the defenders to successfully defend all of them. However, 
our learning model-based defenders are still able to constrain the hit rate 
of a single attacker to a relatively low level. For example, the hit rates of 
Fig. 5.The processes of confrontation under 5 rule-based attackers and 3 MADDPG-based defenders.  
Fig. 6.Comparisons of the average accumulative reward (MAR) under different scenarios. (a) 2 defenders versus 2 attackers; (b) 2 defenders versus 3 attackers; (c) 3 
defenders versus 5 attackers; (d) 4 defenders versus 6 attackers. L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
9
Fig. 7.Comparisons of the average attack hit rate (MHR) under different scenarios. (a) 2 defenders versus 2 attackers; (b) 2 defenders versus 3 attackers; (c) 3 
defenders versus 5 attackers; (d) 4 defenders versus 6 attackers. 
Table 2 
Results of the multi-agent defense and attack under rule-based attack agents. “*” indicates that p-value is less than 0B05 for significance test (two-tailed t-test) over the 
best baseline.  
At Num Df Num At Pol Df Pol MAR MHR MSR 
2 2 Rule AC  53B23±0B52  67B86±0B62%  87B87±1B18%  
2 2 Rule IQL 43B13±2B32  6B13±1B81%  10B83±3B11%  
2 2 Rule VDN 47B91±3B18  4B47±1B18%  8B43±3B47%  
2 2 Rule DDPG 58B64±3B27  1B97±0B21%  3B86±3B27%  
2 2 Rule MADDPG 62B24±0B95*  0B16±0B02%*  0B33±0B40%*  
3 2 Rule AC  82B46±1B27  69B21±0B64%  95B66±0B31%  
3 2 Rule IQL 55B82±5B35  9B15±1B58%  23B03±3B82%  
3 2 Rule VDN 59B74±4B85  10B88±2B36%  26B40±4B94%  
3 2 Rule DDPG 71B22±1B02  3B57±0B34%  8B86±0B85%  
3 2 Rule MADDPG 79B36±2B74*  2B32±0B31%*  6B16±0B65%*  
5 3 Rule AC  146B36±2B16  58B10±0B03%  97B50±0B43%  
5 3 Rule IQL 36B12±9B37  25B84±1B60%  72B37±1B92%  
5 3 Rule VDN 75B63±7B40  19B09±1B30%  60B73±2B24%  
5 3 Rule DDPG 122B59±15B64  10B52±2B54%  40B09±8B35%  
5 3 Rule MADDPG 140B96±3B33*  7B54±0B91%*  30B66±1B99%*  
6 4 Rule AC  130B77±5B01  46B61±0B68%  94B66±0B73%  
6 4 Rule IQL 50B95±7B76  26B97±1B08%  79B06±0B73%  
6 4 Rule VDN 74B10±47B01  24B92±5B71%  75B20±7B17%  
6 4 Rule DDPG 185B84±10B73  10B71±1B13%  45B66±3B94%  
6 4 Rule MADDPG 242B06±9B74*  6B81±0B67%*  31B80±1B52%*   L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
10MADDPG for 3 attacks v.s. 2 defenders, 5 attacks v.s. 3 defenders, and 6 
attacks v.s. 4 defenders are 2B32%C7B54%, and 6B81%, while the corre -
lated success rates are 6B16%C30B66%, and 31B80%. (b) The other reason 
is the agent scalability problem for the deep reinforcement learning models. As the number of agent increases, the conducted policy should 
collaborate with more agents, and the state space for the model grows 
exponentially, which is also a problem faced by most DRL models (Li 
et al., 2019; Wang et al., 2020; Liu et al., 2020 ). As a result, it becomes 
Fig. 8.Comparisons of the average attack success rate (MSR) under different scenarios. (a) 2 defenders versus 2 attackers; (b) 2 defenders versus 3 attackers; (c) 3 
defenders versus 5 attackers; (d) 4 defenders versus 6 attackers. 
Fig. 9.The processes of confrontation under 5 MADDPG-based attackers and 3 MADDPG-based defenders.  L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
11more difficult for the models to learn to acquire promising results. 
4.5. Multi-agent defense and attack with DRL-based attack agents 
In multi-agent defense and attack with DRL-based attack agents 
problem, the actions of both attackers and defenders are given through 
DRL algorithms. In this subsection, we also investigate four scenarios, 
which are 2 defenders versus 2 attackers, 2 defenders versus 3 attackers, 
3 defenders versus 5 attackers, and 4 defenders versus 6 attackers, 
respectively. In each scenario, we consider both the agents trained with 
DDPG or MADDPG. 
Fig. 9 gives an example of the confrontation processes of 5 attackers 
versus 3 defenders both trained with MADDPG. In frame 1, the attackers 
and defenders are randomly generated in the environment, and they 
both move with variable speeds. In frame 3, attacker 5 is successfully 
defended by defender 2, and becomes hollow afterwards. The defenders 
keep moving until defender 3 meets attacker 1 and 3, and stops them 
simultaneously, as shown in frame 6. Attackers 2 and 4 are respectively 
defended by defender 1 and 2 in frame 7. Frame 8 shows the final lo-
cations of the defend agents. 
Further, the results of average accumulated reward (MAR) under 
different situations and different models are shown in Fig. 10. 
To clearly see the performance comparisons between the rule-based 
attackers and intelligent attackers, the average accumulative reward 
lines of rule-based attackers are also given in Fig. 10. The increasing 
MAR explains the learning effects of DDPG and MADDPG algorithms. 
Compared with the reward curves of rule-based attackers, the MAR of 
DRL-based attackers shows slight vibrations. Such as in the confrontation of 5 attackers versus 3 defenders, the MADDPG algorithm 
achieves the best results in the middle of training, but begins to show 
drastic fluctuations in the following interactions, and even leads to the 
result that DDPG surpasses MADDPG. This phenomenon is mainly due to 
the mutual exclusion between the attackers and defenders, and the more 
the number of agents, the more obvious this kind of mutual exclusion. 
Moreover, as the number of agents in the environment increases, 
insufficient training will also result in the fluctuations in the average 
accumulative reward. Besides, the MAR values under rule-based situa-
tions are larger than that under DRL-based situations, which illustrates 
that under complex environment, the competition tends to be more 
intense, and the overall MAR values will be smaller. 
Then, we compare the MHR and MSR between the rule-based attack 
agents situations and the DRL-based attack agents situations, as shown 
in Figs. 11 and 12, respectively. 
From the comparison results of MHR and MSR, we can obtain that 
when confronting the same defenders, the attackers ’ MHR and MSR after 
learning through DDPG or MADDPG are higher than those obtained 
through rule-based attackers. This shows that through learning, the at-
tackers become more competitive, and can better achieve the attack 
tasks. Meanwhile, the curves of intelligent attackers versus intelligent 
defenders change a lot, which also illustrate the mutual confrontation 
and game between them. 
In addition, in order to clearly show the final effects of different DRL 
training models, we give the statistical results under different situations 
in Table 3. The environment situations are 2 defenders versus 2 at-
tackers, 2 defenders versus 3 attackers, 3 defenders versus 2 attackers, 
and 4 defenders versus 6 attackers, respectively. 
Fig. 10.Comparisons of the average accumulative reward (MAR) under different scenarios and different models. (a) 2 defenders versus 2 attackers; (b) 2 defenders 
versus 3 attackers; (c) 3 defenders versus 5 attackers; (d) 4 defenders versus 6 attackers. L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
12Seen from Table 3, with the environment becomes complex, the MHR 
and MSR of intelligent attackers increase gradually, which also reflects 
the effects of intelligence. What ’s more, the attack agents with MADDPG 
training perform better, which displays the advantage of mastering 
other agents ’ information in the same team. 
Moreover, in Table 3, the t-test is for evaluating the effectiveness of 
applying MADDPG or DDPG to the attack agents. Different from the 
experiments in Table 2, here we perform the t-test between the DRL- 
based attackers and the rule-based attackers. Under the scenarios with 
DRL-based attackers, both the defenders and attackers are intelligent, 
the agents of the same type cooperate with each other, and simulta -
neously they compete with the agents of the opponent type. For 
example, we evaluate the p-values under “5 rule-based attackers against 
3 MADDPG-based defenders ” and “5 MADDPG-based attackers against 3 
MADDPG-based defenders ” situations. Also, we evaluate the p-values for 
“5 rule-based attackers against 3 DDPG-based defenders ” and “5 DDPG- 
based attackers against 3 DDPG-based defenders ” scenarios. We found 
that the associated application of MADDPG to the attack and defense 
agents is significant under the scenarios with more agents, e.g., 5 vs. 3 
and 6 vs. 4. However, DDPG is not significant for all the considered 
scenarios. These results also demonstrate that MADDPG is more effec-
tive than DDPG in the hybrid cooperation-competition problems. 
5.Conclusion and future work 
In this paper, we dealt with multi-agent defense and attack problem 
using deep reinforcement learning algorithms. The defense and attack 
problem are divided into two categories according to the types of attack agents, namely multi-agent defense and attack with rule-based attackers 
and multi-agent defense and attack with DRL-based attackers. In both 
cases, the behavior of defense agents are given through deep rein-
forcement learning models. We improved the continuous state space, 
continuous action space and reward function in DDPG and MADDPG 
algorithms to better solve the considered problem. The network struc-
tures and learning methods of DDPG and MADDPG during training are 
introduced in detail. A series of experiments and comparisons are also 
conducted to demonstrate the effectiveness of DRL training, and the 
performance of MADDPG is better than that of other employed DRL 
models. 
However, this paper is mainly aimed at a small number of attack 
agents and defense agents. Besides, only the attack agents, defense 
agents and defense zone are considered in the simulated environment. In 
future work, we will focus on more practical environments containing 
static and dynamic obstacles, as well as the situations with dynamic 
changes of agents ’ numbers (scalability). 
CRediT authorship contribution statement 
Liwei Huang: Conceptualization, Methodology, Software, Visuali -
zation, Writing - original draft, Writing - review & editing. Mingsheng 
Fu: Conceptualization, Funding acquisition, Methodology, Software, 
Supervision, Writing - review & editing. Hong Qu: Conceptualization, 
Funding acquisition, Resources, Writing - review & editing. Siying 
Wang: Methodology, Visualization, Writing - review & editing. 
Shangqian Hu: Methodology, Visualization, Writing - review & editing. 
Fig. 11.Comparisons of the average attack hit rate (MHR) under different scenarios and different models. (a) 2 defenders versus 2 attackers; (b) 2 defenders versus 3 
attackers; (c) 3 defenders versus 5 attackers; (d) 4 defenders versus 6 attackers. L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
13Declaration of Competing Interest 
The authors declare that they have no known competing financial 
interests or personal relationships that could have appeared to influence 
the work reported in this paper. Acknowledgements 
This work was supported by the China Postdoctoral Science Foun -
dation under Grant 2020M673182, and National Science Foundation of 
China under Grant 61976043. 
Fig. 12.Comparisons of the average attack success rate (MSR) under different scenarios and different models. (a) 2 defenders versus 2 attackers; (b) 2 defenders 
versus 3 attackers; (c) 3 defenders versus 5 attackers; (d) 4 defenders versus 6 attackers. 
Table 3 
Results of the multi-agent defense and attack under DRL-based attack agents. “*” indicates that p-value is less than 0B05 for significance test (two-tailed t-test) over the 
best baseline.  
At Num Df Num At Pol Df Pol MAR MHR MSR 
2 2 Rule DDPG 58B64±3B27  1B97±0B21%  3B86±3B27%  
2 2 DDPG DDPG 10B07±0B59*  0B83±0B22%  1B63±0B47%  
3 2 Rule DDPG 71B22±1B02  3B57±0B34%  8B86±0B85%  
3 2 DDPG DDPG 12B53±2B61*  4B11±1B89%  11B83±5B45%  
5 3 Rule DDPG 122B59±15B64  10B52±2B54%  40B09±8B35%  
5 3 DDPG DDPG 65B42±12B27*  9B67±3B31%  39B26±12B06%  
6 4 Rule DDPG 185B84±10B73  10B71±1B13%  45B66±3B94%  
6 4 DDPG DDPG 150B99±15B36*  8B17±2B70%  45B67±3B94%  
2 2 Rule MADDPG 62B24±0B95  0B16±0B02%  0B33±0B40%  
2 2 MADDPG MADDPG 10B47±0B62*  0B16±0B20%  0B76±1B01%  
3 2 Rule MADDPG 79B36±2B74  2B32±0B31%  6B16±0B65%  
3 2 MADDPG MADDPG 14B29±2B56*  2B87±1B92%  8B40±5B71%  
5 3 Rule MADDPG 140B96±3B33  7B54±0B91%  30B66±1B99%  
5 3 MADDPG MADDPG 61B47±6B39*  11B43±1B20%*  47B60±4B38%*  
6 4 Rule MADDPG 242B06±9B74  6B81±0B67%  31B80±1B52%  
6 4 MADDPG MADDPG 136B85±18B84*  11B51±2B78%*  53B20±13B05%*   L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
14Appendix A.Detailed state features of attack and defense agents 
In our environment, there is one defense zone dz, and we always let it be the center of the environment at each time instant, and thus the coordinate 
of defense zone Pdz is 0C0. As a consequence, we only need to know the approximate distance between the agent and the defense zone dz, and the 
distance between the agent and the other agents, as well as the relative directions. 
A.1. State features of attack agent 
Let the i-th attack agent in AT be ati∃AT, we can get the relative coordinate between ati and dz, as well as the attacker ’s velocity, which are xatCiC
yatCiand vxatCiCvyatCirespectively. 
More specifically, self feature vector selfatCiCt is represented as 
selfatCiCt[
xatCiCyatCiCvxatCiCvyatCi]
B (30)  
The feature vector of other attackers atsatCiCt includes N 1 sub-vectors, which are displayed as follows, 
atsatCiCt[
}satCiC0CtC…C}satCiCN 1Ct]
C (31)  
where }satCiClCtClℑi represents the relative states between attacker ati and other attacker atl, which is denoted as 
}satCiClCt[
xatCi xatClCyatCi yatClCvxatClCvyatCl]
B (32)  
The feature vector of defenders dfsatCiCt includes M sub-vectors, which are displayed as 
dfsatCiCt[
satCiC0CtC…CsatCiCMCt]
C (33)  
where satCiClCt represents the relative states between attacker ati and defender dfl, which is expressed as follows, 
satCiC0Ct[
xatCi xdfClCyatCi ydfClCvxdfClCvydfCl]
B (34)  
A.2. State features of defense agent 
Similarly, for the j-th defense agent dfj∃DF, we can get the relative coordinate between dfj and dz, as well as the defender ’s velocity in last time 
instant, which are xdfCjCydfCjand vxdfCjCvydfCj, respectively. 
More specifically, the feature vector selfdfCjCt is represented as 
selfdfCjCt[
xdfCjCydfCjCvxdfCjCvydfCj]
B (35)  
In feature vector atsdfCjCt, there are N sub-vectors, which are represented as 
atsdfCjCt[}sdfCjC0CtC…C}sdfCjCNCt]
C (36)  
where }sdfCjClCt stands for the relative states between defender dfj and attacker atl, which is 
}sdfCjClCt[
xdfCj xatClCydfCj yatClCvxatClCvyatCl]
B (37)  
In feature vector dfsdfCjCtClℑj, there are M 1 sub-vectors, which are displayed as 
dfsdfCjCt[
sdfCjC0CtC…CsdfCjCM 1Ct]
C (38)  
where sdfCjC0Ct stands for the relative states between defender dfj and other defender dfl, which is 
sdfCjC0Ct[
xdfCj xdfClCydfCj ydfClCvxdfClCvydfCl]
B (39)    
Algorithm 1: Multi-agent Defense and Attack Using DDPG   
Require : Initial policy network parameter }θ, Q network parameter θ, and replay buffer D.    
1: Assign target network parameters }θ∕
←}θ, θ∕←θ.    
2: while not stop do   
(continued on next page) L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
15(continued ) 
3: for t1⊃max episode length do    
4: Obtain current environment state s, and obtain action a according to policy network P.    
5: aPs†}θε, ε⊃N (N is noise).    
6: Perform action a, obtain new state s∕, reward r, and add sCaCrCs∕into replay buffer D.    
7: if s∕is the terminate state then    
8: Reset environment.   
9: break   
10: end if   
11: end for   
12: Randomly select b samples from D, and add them into B.    
13: Calculate Q value yrCs∕rγQs∕CPs∕†}θ∕
†θ∕.    
14: Update Q network parameter.   
15: θ←θ ⋈θ1
†B†̂
sCaCrCs∕∃BQsCa†θ yrCs∕2.    
16: Update policy network parameter.   
17: }θ←}θ⋈}θ1
†B†̂
s∃BQ 
sCP 
s⃦⃦}θ)⃦⃦θ)
.    
18: Update target network parameters.   
19: }θ∕
←ρ}θ∕
1 ρ}θ.    
20: θ∕←ρθ∕1 ρθ.    
21: end while    
Appendix B.DDPG for multi-agent defense and attack 
The details of using DDPG in this paper is given as follows. 
In (17), we adopt different parameters θ and θ∕for different states. In the training process, we don’t consider θ∕, and only update θ. But at regular 
intervals, we will use current θ to update the value of θ∕. By doing so, rγQs∕Ca∕†θ∕can be treated as a constant label, and this is convenient for 
training. What ’s more, since DDPG is a reinforcement learning method with deterministic policy, a∕is unique, and a∕Ps†}θ∕instead of choosing an 
action with maximum Q value according to the Q network. Similarly, the policy network also has delayed network parameters }θ∕, and a replay buffer D 
which stores sCaCrCs∕obtained by the interactions of agents. During training, we randomly take K sets from D each time. 
The overall processes of multi-agent defense and attack using DDPG are shown in Algorithm 1. 
Appendix C.MADDPG for multi-agent defense and attack 
The learning algorithm of MADDPG is similar to that of DDPG. For the Q network in MADDPG, the input also takes other agents ’ information into 
consideration for the global critic, but the parameter update formula stays the same, also as shown in (18). Specifically, for the policy gradient in (25), 
the actions of current agent need to be calculated in real time through the policy network, the rest of actions are obtained through the repay buffer, and 
the final update of gradient is shown in (21). 
The overall processes of multi-agent defense and attack using MADDPG are shown in Algorithm 2.   
Algorithm 2: Multi-agent defense and attack using MADDPG   
Require : Number of agents M, policy network }θi, Q network parameter θi, i1toM.    
1: Assign target network parameters   
2: for i1toM do    
3: }θ∕
i←}θi, θ∕
i←θi.    
4: end for   
5: while not stop do   
6: for t1⊃max episode length do    
7: for i1toM do    
8: Obtain current environment state si, and obtain action ai according to policy network P.    
9: aiPsi†}θiε, ε⊃N.    
10: end for   
11: Perform actions a1Ca2C…CaM, obtain new states s∕
1Cs∕
2C…Cs∕
M, rewards r1Cr2C…CrM, and add s1Ca1Cr1Cs∕
1C…CsMCaMCrMCs∕
Minto replay buffer D.    
12: if there exists terminate state in s∕
1Cs∕
2C…Cs∕
Mthen    
13: Reset environment.   
14: break   
15: end if   
16: end for   
17: Randomly select b samples from D, and add them into B.    
18: for i1toM do    
19: Calculate Q value yiriγQs∕
iCa∕
iCother s∕
ia∕
i⃦⃦⃦θ∕
i.    
20: Update Q network parameter.     
(continued on next page) L. Huang et al.                                                                                                                                                                                                                                  
Expert Systems With Applications 176 (2021) 114896
16(continued ) 
21: θ←θ ⋈θ1
†B†̂
siCaiCother siai∃BQsiCaiCother siai⃦⃦θi yi2.  
22: Update policy network parameter.   
23: }θ←}θ⋈}θ1
†B†̂
siCother siai∃BQ 
siCP 
si⃦⃦}θi)
Cother siai⃦⃦θi)
.    
24: end for   
25: for i1toM do    
26: Update target network parameters.   
27: }θ∕
i←ρ}θ∕
i1 ρ}θi.    
28: θ∕
i←ρθ∕
i1 ρθi.    
29: end for   
30: end while    
References 
Abdoos, M., Mozayani, N., & Bazzan, A. L. (2011). Traffic light control in non-stationary 
environments based on multi agent q-learning. In 2011 14th international IEEE 
conference on intelligent transportation systems (ITSC) (pp. 1580 –1585). https://doi. 
org/10.1109/ITSC.2011.6083114 
Bu, L., Babu, R., De Schutter, B., et al. (2008). A comprehensive survey of multiagent 
reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics Part C 
(Applications and Reviews), 38, 156–172. https://doi.org/10.1109/ 
TSMCC.2007.913919 
Chen, Y. F., Liu, M., Everett, M., & How, J. P. (2017). Decentralized non-communicating 
multiagent collision avoidance with deep reinforcement learning. In 2017 IEEE 
international conference on robotics and automation (ICRA) (pp. 285–292). https://doi. 
org/10.1109/ICRA.2017.7989037 
D’Andrea, R. & Murray, R. M. (2003). The roboflag competition. In Proceedings of the 
2003 American control conference, 2003 (Vol. 1, pp. 650–655). DOI: 10.1109/ 
ACC.2003.1239093. 
Das, P. K., Behera, H. S., Das, S., Tripathy, H. K., Panigrahi, B. K., & Pradhan, S. (2016). 
A hybrid improved pso-dv algorithm for multi-robot path planning in a clutter 
environment. Neurocomputing, 207, 735–753. https://doi.org/10.1016/j. 
neucom.2016.05.057 
Earl, M. G. & D’Andrea, R. (2002a). Modeling and control of a multi-agent system using 
mixed integer linear programming. In Proceedings of the 41st IEEE conference on 
decision and control, 2002. (Vol. 1, pp. 107–111). DOI: 10.1109/ 
CDC.2002.1184476. 
Earl, M. G. & D’Andrea, R. (2002b). A study in cooperative control: The roboflag drill. In 
Proceedings of the 2002 American control conference (IEEE Cat. No. CH37301) (Vol. 
3, pp. 1811 –1812). DOI: 10.1109/ACC.2002.1023829. 
Earl, M. G., & D’Andrea, R. (2007). A decomposition approach to multi-vehicle 
cooperative control. Robotics and Autonomous Systems, 55, 276–291. https://doi.org/ 
10.1016/j.robot.2006.11.002 
Foerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., & Whiteson, S. (2018). 
Counterfactual multi-agent policy gradients. Thirty-second AAAI conference on 
artificial . intelligence . 
Galstyan, A., Czajkowski, K., & Lerman, K. (2005). Resource allocation in the grid with 
learning agents. Journal of Grid Computing, 3, 91–100. https://doi.org/10.1007/ 
s10723-005-9003-7 
Goldman, C. V., & Zilberstein, S. (2004). Decentralized control of cooperative systems: 
Categorization and complexity analysis. Journal of Artificial Intelligence Research, 22, 
143–174. 
Huang, L., Qu, H., Ji, P., Liu, X., & Fan, Z. (2016). A novel coordinated path planning 
method using k-degree smoothing for multi-uavs. Applied Soft Computing, 48, 
182–192. https://doi.org/10.1016/j.asoc.2016.06.046 
Jaderberg, M., Czarnecki, W. M., Dunning, I., Marris, L., Lever, G., Castaneda, A. G., 
Beattie, C., Rabinowitz, N. C., Morcos, A. S., Ruderman, A., et al. (2019). Human- 
level performance in 3d multiplayer games with population-based reinforcement 
learning. Science, 364, 859–865. https://doi.org/10.1126/science.aau6249 
Kantaros, Y., & Zavlanos, M. M. (2016). Global planning for multi-robot communication 
networks in complex environments. IEEE Transactions on Robotics, 32, 1045 –1061. 
https://doi.org/10.1109/TRO.2016.2593045 
Kim, G., & Chung, W. (2006). Tripodal schematic control architecture for integration of 
multi-functional indoor service robots. IEEE Transactions on Industrial Electronics, 53, 
1723 –1736. https://doi.org/10.1109/TIE.2006.881956 
Li, S., Wu, Y., Cui, X., Dong, H., Fang, F. & Russell, S. (2019). Robust multi-agent 
reinforcement learning via minimax deep deterministic policy gradient. In Proceedings of the AAAI conference on artificial intelligence (Vol. 33, pp. 
4213 –4220). 
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., & 
Wierstra, D. (2015). Continuous control with deep reinforcement learning. Computer 
Science, 8, A187. https://doi.org/10.1016/S1098-3015(10)67722-4 
Littman, M. L. (2015). Reinforcement learning improves behaviour from evaluative 
feedback. Nature, 521, 445. https://doi.org/10.1038/nature14540 
Liu, Y., Wang, W., Hu, Y., Hao, J., Chen, X. & Gao, Y. (2020). Multi-agent game 
abstraction via graph attention neural network. In AAAI (pp. 7211 –7218). 
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O. P., & Mordatch, I. (2017). Multi-agent 
actor-critic for mixed cooperative-competitive environments. In Advances in neural 
information processing systems (pp. 6379 –6390) . 
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., & 
Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. In 
International conference on machine learning (pp. 1928 –1937) . 
Park, K.-H., Kim, Y.-J., & Kim, J.-H. (2001). Modular q-learning based multi-agent 
cooperation for robot soccer. Robotics and Autonomous Systems, 35, 109–122. https:// 
doi.org/10.1016/S0921-8890(01)00114-2 
Pendharkar, P. C., & Cusatis, P. (2018). Trading financial indices with reinforcement 
learning agents. Expert Systems with Applications, 103, 1–13. https://doi.org/ 
10.1016/j.eswa.2018.02.032 
Qu, H., Xing, K., & Alexander, T. (2013). An improved genetic algorithm with co- 
evolutionary strategy for global path planning of multiple mobile robots. 
Neurocomputing, 120, 509–517. https://doi.org/10.1016/j.neucom.2013.04.020 
Robinson, D. R., Mar, R. T., Estabridis, K., & Hewer, G. (2018). An efficient algorithm for 
optimal trajectory generation for heterogeneous multi-agent systems in non-convex 
environments. IEEE Robotics and Automation Letters, 3, 1215 –1222. https://doi.org/ 
10.1109/LRA.2018.2794582 
Silva, M. A. L., de Souza, S. R., Souza, M. J. F., & Bazzan, A. L. C. (2019). A reinforcement 
learning-based multi-agent framework applied for solving routing and scheduling 
problems. Expert Systems with Applications, 131, 148–171. https://doi.org/10.1016/j. 
eswa.2019.04.056 
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V. F., Jaderberg, M., 
Lanctot, M., Sonnerat, N., Leibo, J. Z., Tuyls, K. & et al. (2018). Value-decomposition 
networks for cooperative multi-agent learning based on team reward. In AAMAS (pp. 
2085 –2087). 
Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction . MIT press .  
Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru, J., & 
Vicente, R. (2017). Multiagent cooperation and competition with deep 
reinforcement learning. PloS One, 12, Article e0172395 . 
Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents. 
In Proceedings of the tenth international conference on machine learning (pp. 330–337). 
Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., 
Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in 
starcraft ii using multi-agent reinforcement learning. Nature, 575, 350–354. https:// 
doi.org/10.1038/s41586-019-1724-z 
Wang, W., Yang, T., Liu, Y., Hao, J., Hao, X., Hu, Y., Chen, Y., Fan, C. & Gao, Y. (2020). 
From few to more: Large-scale dynamic multiagent curriculum learning. In AAAI 
(pp. 7293 –7300). 
Yi, X., Zhu, A., Yang, S. X., & Luo, C. (2017). A bio-inspired approach to task assignment 
of swarm robots in 3-d dynamic environments. IEEE Transactions on Cybernetics, 47, 
974–983. https://doi.org/10.1109/TCYB.2016.2535153 
Yu, C., Zhang, M., Ren, F., & Tan, G. (2015). Multiagent learning of coordination in 
loosely coupled multiagent systems. IEEE Transactions on Cybernetics, 45, 
2853 –2867. https://doi.org/10.1109/TCYB.2014.2387277 L. Huang et al.                                                                                                                                                                                                                                  "
1-s2.0-S0957417422015044-main.pdf,"Expert Systems With Applications 210 (2022) 118394
Available online 10 August 2022
0957-4174/© 2022 Elsevier Ltd. All rights reserved.
Contents lists available at ScienceDirect
Expert Systems With Applications
journal homepage: www.elsevier.com/locate/eswa
GPDS: A multi-agent deep reinforcement learning game for anti-jamming
secure computing in MEC network
Miaojiang Chena, Wei Liub,∗, Ning Zhangc, Junling Lid, Yingying Rena, Meng Yie, Anfeng Liua,∗
aSchool of Computer Science and Engineering, Central South University, Changsha, 410083, China
bSchool of Informatics, Hunan University of Chinese Medicine, Changsha Hunan 410208, China
cDepartment of Electrical and Computer Engineering, University of Windsor, 401 Sunset, Canada
dDepartment of Electrical and Computer Engineering, University of Waterloo, 200 University Avenue, Canada
eSchool of Computer Science and Engineering, Southeast University, Nanjing, China
A R T I C L E I N F O
Keywords:
Deep reinforcement learning
Multi-agent
Secure computing
Decision-making
Mobile Edge Computing (MEC)A B S T R A C T
The openness of Mobile Edge Computing (MEC) networks makes them vulnerable to interference attacks by
malicious jammers, which endangers the communication quality of mobile users. To achieve secure computing,
the conventional method is that the mobile device reduces the attacker’s malicious interference by increasing
the transmission power. However, the cost of power defense is unacceptable in MEC with resource shortages.
Therefore, this paper considers a novel defense strategy based on time-varying channel and describes the
malicious interference countermeasure process as a multi-user intelligent game model. Because the interference
model and interference strategy are unknown, this paper proposes a deep reinforcement learning multi-user
random Game with Post-Decision State (named GPDS) to intelligently resist intelligent attackers. In the GPDS
algorithm, mobile users need to obtain the communication quality, spectrum availability, and jammer’s strategy
from the state of the blocked channel. The reward of the optimal decision strategy is defined as the expected
value of the maximum channel throughput, and the potential optimal channel selection strategy is obtained
through Nash equilibrium. After GPDS training, mobile users can learn the optimal channel switching strategy
after multi-step training. The experimental results show that the proposed GPDS achieves better anti-jamming
performance, compared with SOTA algorithms.
1. Introduction
With the development of 5G network technology, mobile sensor
networks provide unprecedented convenience and have a tremendous
impact on the development of society ( Chen et al. , 2021 ; Huang, Leung,
Liu, & Xiong , 2022 ; Li, Li, Liu, & Chen , 2019 ). However, mobile devices
have the characteristics of limited resources such as small volume
(Yu, Liu, Xiong, and Wang , 2022 ), low power ( Chen, Liu, Wang, Liu
and Zeng , 2021 ) and low computing power ( Chen, Liu, Wang, Zhang
et al., 2021 ), the network also shows considerable vulnerability. There-
fore, mobile devices are also more vulnerable to malicious attacks, such
as coin hopping attack ( Zhu et al. , 2018 ), data Interception ( Zheng and
Cai, 2020 ), and malicious interference ( Wang et al. , 2020 ).
For Mobile Edge Computing (MEC) networks, one serious threat is
a channel interference attack. Malicious jammers reduce the commu-
nication quality of mobile users by transmitting interference signals
(Mukherjee, Fakoorian, Huang, & Swindlehurst , 2014 ; Xiao, Liu, Li,
∗Corresponding authors.
E-mail addresses: miaojiangchen@csu.edu.cn (M. Chen), weiliu@csu.edu.cn (W. Liu), ning.zhang@uwindsor.ca (N. Zhang), lijunling@cuhk.edu.cn (J. Li),
yingyingRen@csu.edu.cn (Y. Ren), yimeng@seu.edu.cn (M. Yi), afengliu@mail.csu.edu.cn (A. Liu).Mandayam, & Poor , 2015 ). Specifically, when a mobile device is at-
tacked by a jammer, it will keep trying to retransmit, causing the
device to fall into the dilemma of rapid battery discharge and channel
blocking. More seriously, interference attacks may also induce denial
of service attacks ( Chen & Dong , 2020 ; Law et al. , 2009 ). Therefore,
energy-saving and effective anti-interference technology is urgently
needed for MEC networks.
To defend against interference attacks, the common technical means
of wireless networks include power control ( Feng and Haykin , 2019 ),
beamforming ( Tang, Wang, Zhang, Chu, and Han , 2021 ), relay as-
sistance ( Yang et al. , 2020 ), and frequency hopping ( Shi, An, and
Li, 2021 ). In traditional wireless networks, power control is a powerful
anti-interference strategy. Specifically, the wireless network system
can use game theory to obtain the optimal anti-interference power
control strategy of the devices and improve the transmission rate ( Chen,
Song, Xin, & Backens , 2013 ; Lv et al. , 2017 ). In addition, the trusted
https://doi.org/10.1016/j.eswa.2022.118394
Received 12 April 2022; Received in revised form 26 July 2022; Accepted 2 August 2022
Expert Systems With Applications 210 (2022) 118394
2M. Chen et al.
relay is also considered to be one of the promising anti-interference
technologies. Wireless networks can reduce the channel interference
of jammers through beamforming and relay selection (Hoang, Duong,
Suraweera, Tellambura, and Poor, 2015).
Unfortunately, many interference prevention technologies of wire-
less networks are not suitable for MEC networks. Both power control
and beamforming and relay assistance require high hardware costs
and energy consumption costs. For example, power control can greatly
consume the power of mobile devices, resulting in the problem of
insufficient battery life.
Among many anti-interference technologies, frequency hopping
technology is applied to MEC network with its excellent anti-interfer-
ence and energy-saving advantages (Huynh et al., 2018; Lee, Kim,
Seo, & Har, 2019). It can enable users to switch channel spectrum
and avoid potential interference. In the frequency hopping strategy,
there are two kinds of users: mobile devices and jammers. When the
jammer generates an interference signal, the mobile device can avoid
interference by switching its transmission channel.
To defend against the unknown jamming attack model, such as the
jamming strategy and the jammer transmission power are unknown,
some studies use reinforcement learning technology to obtain the dy-
namic optimal anti-jamming strategy (Van Huynh, Nguyen, Hoang, &
Dutkiewicz, 2020; Xiao et al., 2018b). Furthermore, to improve the
slow convergence of traditional reinforcement learning, such as Q-
learning, deep reinforcement learning technology to solve the optimal
anti-interference strategy has been widely concerned.
However, many previous studies focus on the single agent anti-
interference scenario, and do not fully consider the possibility of mobile
device cooperation in anti-jamming. With the development of multi-
agent technology, many anti-jamming studies have extended them
works to multi-user scenarios (Elleuch, Pourranjbar, & Kaddoum, 2021;
Yin et al., 2022). Because the previous defense technology is a pas-
sive and lagging defense means according to the configuration of
attack characteristics and the issuance of defense strategies, the de-
fense side is at a disadvantage in the anti-interference confrontation.
Multi-user security Game represents the relationship between coopera-
tion and competition among multiple users by extending the Markov
game framework, so as to achieve multi-user cooperation and anti-
interference to improve network security. Surprisingly, multi-agent
reinforcement learning technology can realize the optimal attack and
defense game in an incomplete information environment (Rowland
et al., 2019; Torreño, Onaindia, & Sapena, 2015). In addition, the
high-dimensional channel state and action space will slow down the
training speed. Therefore, we propose a frequency hopping strategy
based on multi-user anti-interference game. A deterministic gradient
reinforcement learning algorithm is designed to obtain the optimal
frequency hopping strategy of mobile devices in the worst case.
The main contributions are summarized as follows.
•Different from the previous discrete channel state game, we model
the anti-interference countermeasure problem in MEC network
as a multi-user continuous channel state game problem for the
first time to reduce the high-dimensional channel state space.
In addition, because the nonconvex optimization of unknown
information (such as power and interference strategy) in the
game model is difficult to solve in the dynamic game system,
we propose a Markov decision process multi-agent model with
post-state decision. The proposed algorithm does not need power
control for the jamming defense to realize secure computing.
•We propose a deep reinforcement learning strategy based on
the Minimax gradient strategy so that multiple mobile devices
can still generalize when changing the attack strategy against
jammers in the training scenario of a continuous channel game. In
order to solve the problem that the minimax strategy is difficult
to calculate in the objective function, we propose a user-to-user
multi-user adaptive learning strategy to train samples.•According to the experimental results, the results show that
our proposed policy-based GPDS algorithm achieves better anti-
jamming performance superiority compared with state-of-the-art
algorithms.
The rest of this paper is organized as follows. The related work
is presented in Section 2. In Section 3, we introduce the detailed
anti-interference system model. The proposed GPDS algorithm is inves-
tigated in Section 4. In Section 5, the simulation results are presented
and discussed. Finally, the paper is concluded in Section 6.
2. Related work
MEC network has the characteristics of broadcasting and openness
(Chen, Wang, Zhang and Liu, 2021; Fan et al., 2022; Ren, Liu, Liu,
Wang, & Li, 2022), which makes it extremely vulnerable to malicious
interference attacks. The research on mobile device defense against
malignant interference has important application significance for im-
proving communication service quality and transmission performance.
For power control anti-interference research, D’Oro, Ekici, and
Palazzo (2018) proposed a strategy of joint scheduling users and power
control, and used dynamic programming to solve this kind of NP-
hard problem. Xiao, Li, Dai, Dai and Poor (2018) proposed a power
game strategy based on Q-learning algorithm, and derived Stackelberg
Equilibrium to show the impact of multiple antennas on channel inter-
ference. Using the framework of game theory, El-Bardan, Brahma, and
Varshney (2016) proposed a power control anti-interference strategy
under the condition of unknown channel gain to ensure the commu-
nication service quality of legitimate users. Do, Björnson, Larsson, and
Razavizadeh (2018) designed an anti-jamming machine based on power
control to ensure the stability of uplink communication anti-jamming
of massive multiple input multiple output systems. In the heteroge-
neous offloading model, Xu and Zhu (2022b) studied the problem of
optimizing the monitoring mode and transmission power to maximize
the average ratio of successful eavesdropping tasks under the dual
constraints of transmission power and task completion time limit. Xu
(2020) and Xu and Zhu (2022a) studied the problem of maximizing
the successful eavesdropping probability of the monitor by jointly
allocating the transmission power of the monitor and unsuspicious
users.
In addition, cooperative relay beamforming is considered to be
a very useful anti-jamming technology. Gu et al. (2020) modeled
the joint optimization of relay selection and beamformer as MINLP,
relaxed the optimization problem to a convex problem, and finally
improved the anti-interference performance in the vehicle network.
Aiming at the incomplete channel state information of multi-antenna
eavesdroppers, Wang and Wang (2015) proposed a non convex anti-
interference model combining cooperative jamming and beamforming,
and solved the original problem as a convex problem. In MIMO wireless
communication scenario, Liu, Li, Kong, and Zhao (2016) proposed an
anti-jamming scheme based on maximum signal to jamming and noise
ratio (SJNR) transmit beamforming to cancel the jamming signal of the
jammer.
The above anti-interference methods require high hardware costs
and energy consumption costs. Therefore, frequency hopping tech-
nology without additional hardware cost is recognized as a promis-
ing anti-interference technology in mobile networks with limited re-
sources. Liang, Cheng, Zhang, and Zhang (2018) proposed a new mode
frequency hopping technology, which combines traditional frequency
hopping and mode frequency hopping technology to reduce external
malicious interference and improve the security performance of com-
munication. Gao, Xiao, Wu, Xiao, and Shao (2018) proposed a dual
matrix game strategy to simulate the attack and defense process of users
and jammers, and further gave the optimal anti-jamming solution under
Nash equilibrium. Hanawal, Abdel-Rahman, and Krunz (2017) modeled
the attack and defense of devices and jammers as a Markov game, and
Expert Systems With Applications 210 (2022) 118394
3M. Chen et al.
derived the Markov game equilibrium to obtain the optimal frequency
hopping threshold to avoid malicious interference attacks.
In dynamic networks, defenders usually have unknown information
such as attack strategy and interference power. To deal with such
problems, reinforcement learning technology has been used to opti-
mize dynamic wireless attack and defense countermeasures. Xiao et al.
(2018c) proposed a Q-learning algorithm to improve the communica-
tion anti-interference performance of the vehicle and ensure the safe
driving of the vehicle when the vehicle does not know the interference
information. Additionally, Yao and Jia (2019) proposed a cooperative
anti-interference intelligent algorithm based on Q-Learning to resist ex-
ternal malicious attacks. To solve the disadvantage of slow convergence
of reinforcement learning, Van Huynh, Nguyen, Hoang, and Dutkiewicz
(2019) proposed a deep reinforcement learning strategy, which enables
the devices to quickly obtain the optimal channel switching strategy to
improve the security performance of jammer intelligent interference.
However, many of the above researches mainly use physical layer
security technology to ensure the information security in the unloading
phase. Compared with these works, the multi-agent security game
algorithm we consider has lower cost and more flexible operation. Note
that if the continuous power and channel parameters are discretized,
in the time-varying wireless channel environment, the channel state
space and action space under the reinforcement learning model will
become very large, and the training time will be very long to achieve
the convergence effect. Therefore, we will propose a policy based deep
reinforcement learning algorithm to accelerate the attack and defense
learning speed under continuous variables. In addition, previous work
has focused on finding the game between single users, such as the
defense strategy between a single transmitter (device) and a single
jammer. To enhance the anti-interference performance between mobile
users, we consider a multi-user attack and defense strategy. Mobile de-
vices can share defense information for intelligent learning to improve
the security performance of communication.
3. System model
Frequency hopping is a promising technology, which has been used
to release spectrum congestion. Specifically, the system has a set of
frequency channels to exploit. At each time slot, each attacker can
select one of the channels to send as an attacking signal, and the de-
fender must select the appropriate channel switching action according
to the current unknown attack strategy and channel state to ensure its
transmission security (Jia et al., 2018).
In MEC networks, the attacker transmits interference signals to
mobile users to destroy the transmission quality of legitimate users. To
enhance the anti-interference ability of mobile devices, an intelligent
reflecting surface (Di Renzo et al., 2020; Huang, Zappone, Alexan-
dropoulos, Debbah, & Yuen, 2019; Wu & Zhang, 2019) auxiliary system
is considered. To enhance the anti interference ability of mobile de-
vices, an intelligent reflecting surface auxiliary system is considered in
this paper. The edge network deploys an intelligent reflecting surface
composed of 𝐿reflecting elements and 𝑈antennas, which can pro-
vide more powerful communication performance for mobile devices.
When a malicious jammer transmits an interference signal, the intel-
ligent reflecting surface enhances the signal power through reflection
beamforming to reduce the interference.
Let𝐿{1,…,𝑙}represent the reflecting elements set, and 𝑁=
{1,…,𝑛}denote the mobile devices set. Let ℎ𝐻
𝑏𝑚,𝑛,ℎ𝐻
𝑟𝑚,𝑛,ℎ𝑏𝑟∈𝐺𝐿×𝑈
represent channel gain between edge base station and mobile device
𝑛, between reflecting element and mobile device 𝑛, between edge base
station and reflecting element. Let ℎ𝐻
𝑗𝑚,𝑛represents the channel gain
between mobile device 𝑛and jammer. Let 𝛹=𝑑𝑖𝑎𝑔(𝛹1,…,𝛹𝐿) ∈𝐺𝐿×𝑈
represent the reflection coefficient matrix, where 𝛹𝑙=𝜂𝑙𝑒𝛩𝑙,𝜂𝑙∈ [0,1]
and𝛩𝑙∈ [0,2𝜋]. The important notations used throughout this article
are summarized in Table 1.Table 1
Key notations of the system model.
Notation Description
𝐿 Reflecting element set
ℎ Channel gain
𝛹 Reflection coefficient matrix
𝐲 Transmitted signal
𝑋𝑛 Received signal for mobile device 𝑛
𝜆𝑛 Signal to interference plus noise ratio
𝑊 Bandwidth
𝑆 State set
𝐴 Action set
𝑅𝑖 Total reward of agent 𝑖
𝑂 Attacker action set
𝑟𝑖 Instant reward of agent 𝑖
𝜋 Learning policy
𝜆𝑖 Learning rate
𝑐𝑝 Penalty factor
𝑐𝑠 Sensing cost
𝑄(𝑎,𝑠) State–action value function
𝐿(𝜃) Entropy loss function
𝐺𝑒(𝜇𝑖) Entire objective
𝜁𝑚 Cost factor of adaptive learning
𝐷𝑘
𝑖Replay buffer
𝑃(𝑠′|𝑠,𝑎,𝑜 ) Transition function, from 𝑠to𝑠′
𝑡 Time slot
𝐸̃ 𝑠[𝑅(𝑠,𝑎,𝑜 )] Expected reward
𝑄𝜋
𝑖(𝑥,𝑎𝑖,…,𝑎𝑁) Centralized action-value function
𝜎𝑛 Gaussian noise
𝑁 Mobile devices set
𝑃𝑛 Transmit power for mobile device 𝑛
𝐛𝑛 Beamforming vector
𝐼𝑛 Interference signal at device 𝑛
𝑄∗(𝑠,𝑎) Optimal state–action value function
𝛿 Discount factor
The transmitted signal of the base station is defined as:
𝐲=𝑁∑
𝑛=1√
𝑃𝑛𝐛𝑛𝑧𝑛, (1)
where𝑃𝑛is the transmit power for mobile device 𝑛,𝐛𝑛∈𝐺𝐿×1is
beamforming vector for mobile device 𝑛,𝑧𝑛∈𝐺is transmitted symbol
for mobile device 𝑛,𝐸{|𝑏𝑛|2} = 1 and𝐸{𝑏𝑛} = 0 . We consider the
case of malicious jammers interfering with the base station by the
interference signal 𝐼𝑛at mobile device 𝑛. Let𝑃𝑗𝑛=𝑇𝑟(𝐼𝑛𝐼𝐻
𝑛)represent
the power of faked signal, where 𝑇𝑟(.)and(.)𝐻denote the trace and
conjugate transpose operations. Therefore, the total received signal for
mobile device 𝑛can be written as:
𝑋𝑛=(
𝐡𝐻
𝑟𝑚,𝑛𝜳𝐆+𝐡𝐻
𝑏𝑚,𝑛)√
𝑃𝑛𝐛𝑛𝑧𝑛
⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
desired signal+
∑
𝑖∈𝑁,𝑖≠𝑛(
𝐡𝐻
𝑟𝑚,𝑛𝜳𝐆+𝐡𝐻
𝑏𝑚,𝑛)√
𝑃𝑖𝐛𝑖𝑧𝑖
⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
user interference+√
𝑃𝑗𝑛𝐡𝐻
𝑗𝑚,𝑛𝐈𝑛
⏟⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏟
jamming signal+𝜎𝑛,(2)
where𝜎𝑛is the Gaussian noise with variance 𝜗2
𝑛at mobile device 𝑛.
In Eq. (2), the total signal received by the device includes not only
the signal expected to be received by itself, but also the interference
between mobile devices and the malicious interference of jammers.
Therefore, the signal to interference plus noise ratio (SINR) 𝛾𝑛of mobile
device𝑛is:
𝜆𝑛=𝑃𝑛||||(
𝐡𝐻
𝑟𝑚,𝑛𝜳𝐆+𝐡𝐻
𝑏𝑚,𝑛)
𝐛𝑘||||2
∑
𝑖∈𝑁,𝑖≠𝑛𝑃𝑖||||(
𝐡𝐻𝑟𝑚,𝑛𝜳𝐆+𝐡𝐻
𝑏𝑚,𝑛)
𝐛𝑖||||2
+𝑃𝑗𝑛|||𝐡𝐻
𝑗𝑚,𝑛𝐈𝑛|||2+𝜎2𝑛, (3)
Note that when the jammer interferes with the channel, the mobile
device can still transmit data, but the transmission rate will be greatly
reduced. The purpose of anti-interference game is to enable mobile
devices to obtain the maximum transmission rate. Accordingly, the
Expert Systems With Applications 210 (2022) 118394
4M. Chen et al.
Fig. 1. Multi-agent security game in dynamic environments.
optimization problem can be defined as:
max{𝑃𝑛}𝑛∈𝑁,𝛹∑
𝑛∈𝑁𝑊log2(1 +𝜆𝑛)
s.t. (c1) :∑𝑁
𝑛=1𝑃𝑛≤𝑃max
(c2) :𝜆𝑛≥𝜆min
𝑛,∀𝑛∈𝑁
(c3) :||𝛹𝑚||= 1,0≤𝜃𝑙≤2𝜋,∀𝑙∈𝐿,(4)
where𝑊is bandwidth, 𝛾min
𝑛is minimum 𝛾𝑛of mobile device 𝑛. Ob-
viously, the optimization problem (see Function (4)) is non-convex.
In function (4), variables reflection coefficient matrix 𝛹, channel gain
ℎ𝑛= {ℎ𝐻
𝑏𝑚,𝑛,ℎ𝐻
𝑟𝑚,𝑛,ℎ𝑏𝑟}and𝑃𝑛are coupled with each other, which makes
it very difficult to solve the Function (4). Furthermore, the unknown
channel state and interference strategy increase the uncertainty of the
MEC system and the difficulty of solving the Function (4).
Therefore, we propose a policy-based multi-agent game algorithm to
solve the above optimization problem. Firstly, we transform Function
(4) into a Markov decision process (MDP). In a multi-agent game envi-
ronment, the relation between the defender and attacker (adversary) is
defined as ⟨𝑆,𝐴,𝑂,𝑇𝑟,𝑅,𝑅𝑂⟩, where𝑆is state (such as, the Wireless
channel gains). 𝐴1,…,𝐴𝑛represents a set of actions (such as, the
channels and powers states). 𝑂1,…,𝑂𝑛stands for a set of attackers
(the adversary from the defender’s perspective). 𝑅𝑖(𝑠,𝑎,𝑜 )denotes the
reward function of the defender, 𝑟𝑖∶𝑆×𝐴𝑖is rewards of agent 𝑖,
and𝑅𝑖=∑𝑇
𝑡=0𝛾𝑡𝑟𝑡
𝑖, while𝑅𝑂represents reward of the attacker. 𝑇𝑟∶
𝑆×𝐴1×⋯×𝐴𝑛is the state transition function. As shown in Fig. 1,
to choose actions, each agent (such as, defender and attacker) uses
reinforcement learning algorithm to learn a policy 𝜋𝑖∶𝑂𝑖×𝐴𝑖→[0,1].
The defender and attacker aim to maximize their total expected reward
𝐸[𝑅]in the dynamic environment. At each time step, both the defender
and the adversary observe the current state 𝑠𝑖∈𝑆and take action
𝑎𝑖∈𝐴and𝑜𝑖∈𝑂, respectively, in the light of their own learned
policies𝜋𝑖and𝜋𝑜
𝑖. Assume that the security game is zero-sum in our
work, that is, the rewards of the defenders and attackers are opposite
(that is,𝑅= −𝑅𝑂and𝑟𝑖= −𝑟𝑜). Specifically, the state 𝑠𝑡in epoch𝑡is
defined as 𝑠𝑡= {ℎ𝑛= {ℎ𝐻
𝑏𝑚,𝑛,ℎ𝐻
𝑟𝑚,𝑛,ℎ𝑏𝑟}𝑡−1,𝑃𝑡−1
𝑛}. The action is defined
as𝑎𝑡= {ℎ𝑛= {ℎ𝐻
𝑏𝑚,𝑛,ℎ𝐻
𝑟𝑚,𝑛,ℎ𝑏𝑟}𝑡,𝑃𝑡
𝑛,𝛩𝑡
𝑙}.
For anti-jamming attack and defense games, the most important
element is the design of the reward function. For convenience, the
intelligent reflecting surface is represented as IRS. The states [𝑜𝑛,𝑜𝑓𝑓 ]
of IRS are Markovian, 𝛷0,1(𝛷1,0) is the probability of IRS transiting
from the state inactive (active) to state active (inactive).
At each time slot 𝑡,𝑇𝑆
𝑡∈ {1,0}denotes the sensing token, which
indicates whether the mobile user needs to transmit data on the current
channel.𝑇𝑟denotes the transition probability, which is the probability
that the user switches to the next secure channel. 𝑇𝑆
𝑡= 0represents
the mobile user keep current channel state and 𝑎𝑡= 0.𝑇𝑆
𝑡= 1is an
active sensing token, the user selects a target channel ℎ𝑖and𝑎𝑡=𝑖.Meanwhile, 𝑜𝑡= 0means that the channel jammer stops interference
in order to avoid anti-interference by the IRS (i.e., attack penalty),
or selects a target channel 𝑖′to send the faked signal ( 𝑜𝑡=𝑖′). The
mobile user selects the channel with the best transmission performance
(i.e. not interfered) for data transmission through the multi-user game
strategy. If the mobile user is attacked in the transmission process, the
IRS system of the base station will be notified to implement the anti-
interference strategy to reduce the impact of the jammer attack. If the
communication quality is still seriously affected after anti-interference,
channel switching is carried out to avoid an interference attack.
The reward of mobile user is defined as:
𝑟𝑛=𝑐𝑝𝑇{𝑜𝑛>0}⋅𝛷1,𝑇𝑜𝑛
𝑡−1⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
jamming attack penalty+𝛷0,𝑇𝑜𝑛
𝑡−1⋅𝑊log2(1 +𝜆𝑛)
⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏞⏟
goodput
−𝑐𝑠𝑇𝑆
𝑡⋅𝑇{𝑎𝑛>0}
⏟⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏟
sensing cost, (5)
where𝑐𝑝is the penalty factor of the jammer’s anti-jamming signal
transmitted by the IRS, 𝑐𝑠is the sensing cost of IRS. 𝑇𝑜𝑛
𝑡−1is the IRS
state in𝑡− 1.
4. Proposed GPDS learning algorithm
In this section, we propose a novel policy based multi-user reinforce-
ment learning game algorithm to solve the MDP optimization problem
mentioned in the last section. The architecture for security game with
deep reinforcement learning is shown in Fig. 2.
In our work, we derive a policy-based deep reinforcement learning
game with Post Decision State (GPDS) algorithm that performs well in
a multi-agent environment. Different from the existing reinforcement
learning games ( Xiong et al. , 2020 , 2019 ; Xiong, Zhao, Niyato, Deng
and Zhang , 2020 ), we propose PDS to deal with the uncertainty in
the process of random games to find the optimal strategy. Different
decisions are made for different current decision states by assigning
values to the current state decision and the resulting post decision state.
Then the expectation of the sum of all current decision state values from
post decision state to termination state is the PDS value. However, some
constraints are as follows.
1. We do not suppose any structure in the communication approach
between agents.
2. We do not suppose a differentiable environmental dynamics
model.
3. The trained policies can only own their observations at perfor-
mance time.
For𝑛agents in anti-interference security game, their action sets are
denoted as 𝐴1,…,𝐴𝑛and reward sets are denoted as 𝑅1,…,𝑅𝑛. At time
slot𝑡,𝑛agents perform policies 𝜋1,…,𝜋𝑛simultaneously, the expected
reward of agent 𝑖is𝑅𝑖(𝜋1,…,𝜋𝑛).
The above game process can be called One Stage Policy (OSP) gen-
eral sum security game, which has a Nash equilibrium. For 𝜋1,…,𝜋𝑛,
no agent can increase the expected reward by unilaterally changing its
policy, that is:
𝑅𝑖(𝜋1,…,𝜋𝑛)≥𝑅𝑖(𝜋1,…,𝜋𝑖−1,𝜋′
𝑖,𝜋𝑖+1,…,𝜋𝑛), (6)
for all OSP policy 𝜋𝑖, and 1≤𝑖≤𝑛. One OSP security game can
have multiple Nash equilibrium, and each Nash equilibrium has a
corresponding expected reward that does not necessarily equal to the
rewards of other Nash equilibrium.
Multi-agent Nash-Q function can be undated by:
̃𝑄∗
𝑖+1[𝑠,𝑎1,…,𝑎𝑝,𝑜1,…,𝑜𝑚] = (7)
⎧
⎪
⎪
⎨
⎪
⎪⎩(1 −𝜆𝑖)𝑄∗
𝑖(̃ 𝑠,𝑎1,…,𝑎𝑝,𝑜1,…,𝑜𝑚)+
𝜆𝑖[𝑟𝑖+𝛿𝑁𝑎𝑠ℎ𝑠(𝑠,𝑄1,…,𝑄𝑛)],
𝑓𝑜𝑟 (̃ 𝑠,𝑎,𝑜 ) = (̃ 𝑠,𝑎1,…,𝑎𝑝,𝑜1,…,𝑜𝑚),
𝑄∗
𝑖(𝑠,𝑎,𝑜 ),𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒,
Expert Systems With Applications 210 (2022) 118394
5M. Chen et al.
Fig. 2. The security game architecture for MEC networks.
where𝑚+𝑝=𝑛and𝑁𝑎𝑠ℎ𝑠(𝑠,𝑄1,…,𝑄𝑛) =𝑄𝑖(𝑠,𝜋1,…,𝜋𝑛)when
𝜋1,…,𝜋𝑛are Nash equilibrium for 𝑄1,…,𝑄𝑛at state𝑠.
Meanwhile, the optimal Q function can be further defined as:
𝑄∗(𝑠,𝑎1,…,𝑎𝑝,𝑜1,…,𝑜𝑚) =𝑟𝑘(𝑠,𝑎1,…,𝑎𝑝,𝑜1,…,𝑜𝑚)
+∑
̃ 𝑠𝑃𝑘(̃ 𝑠|𝑠,𝑎1,…,𝑎𝑝,𝑜1,…,𝑜𝑚)̃𝑄∗. (8)
Lemma 1 (Hu, Wellman, et al. , 1998 ).𝜋1,…,𝜋𝑛and𝜐1,…,𝜐𝑛are
two adversarial equilibria for OSP security game. If exists an adversarial
equilibrium, all of its adversarial equilibria have the same value, and
𝑅𝑖(𝜋1,…,𝜋𝑛)≥𝑅𝑖(𝜋1,…,𝜋𝑖−1,𝜋′
𝑖,𝜋𝑖+1,…,𝜋𝑛)
=𝑅𝑖(𝜐1,…,𝜐𝑖−1,𝜐′
𝑖,𝜐𝑖+1,…,𝜐𝑛)
≥𝑅𝑖(𝜐1,…,𝜐𝑛). (9)
Lemma 2 (Hu et al. , 1998 ).If an OSP security game has a coordination
equilibrium, all of its coordination equilibria have the same value.For convenience, it is assumed that each agent can know what
type of agent it encounters: ‘‘foe’’ (adversarial equilibrium) or ‘‘friend’’
(coordination equilibrium). For an agent 𝑖, all other agents are divided
into two groups. One group is agent 𝑖’s friend, which helps agent 𝑖
maximize its reward return together. The other group is agent 𝑖foe,
which opposes agent 𝑖and reduces agent 𝑖’s reward return. Therefore,
there are two groups for each agent. Thus, the general sum game of
multi-agent is transformed into a zero-sum game of two agents. The
solution of the Nash equilibrium strategy is given by:
𝑁𝑎𝑠ℎ𝑠(𝑠,𝑄1,…,𝑄𝑛) =
max
𝜋1(𝑠),…,𝜋𝑛(𝑠)min
𝑜1,…,0𝑝∈𝑂1×,…,×𝑂𝑚∑
𝑎1,…,𝑎𝑝∈𝐴1×,…,×𝐴𝑝
𝑄𝑖(𝑠,𝑎1,…,𝑎𝑝,𝑜1,…,𝑜𝑚)𝜋1(𝑠,𝑎1),…,𝜋𝑝(𝑠,𝑎𝑝). (10)
Therefore, the approximate policy is learned by action 𝑗′𝑠actions,
with entropy being regularized:
𝐿(𝜃𝑗
𝑖) = −𝐸𝑎𝑗,𝑜𝑗[log̃ 𝜇𝑗
𝑖(𝑎𝑗|𝑜𝑗) +𝜆𝑋(̃ 𝜇𝑗
𝑖)], (11)
Expert Systems With Applications 210 (2022) 118394
6M. Chen et al.
Fig. 3. The GPDS reinforcement learning architecture diagram.
where,𝑋represents the policy distribution entropy. Furthermore, we
use an approximate value ̃ 𝑦to instead of 𝑦in formula (11) as follows:
̃ 𝑦=𝑟𝑖+𝛾𝑄𝜇′
𝑖(𝑥′, ̃ 𝜇′
1(𝑜1),…, ̃ 𝜇′
𝑁(𝑜𝑁)). (12)
A recurring iteration issue in MARL is that the environment is
non-stationary because of the policies changing. In particular, in an
attack–defense environment, an attacker can obtain a strong policy
through over-fitting to the action of its opponent. To achieve game poli-
cies, there are more changes in the strategies the defender implements
in the game. In GPDS learning, we train a set of 𝐾different sub-policies,
the algorithm randomly chooses different sub-policies with sub-policy 𝑘
representing 𝜇𝜃𝑘
𝑖at each episode. For attacker 𝑖, we maximize the entire
objective:
𝐺𝑒(𝜇𝑖) =𝐸𝑘∼𝑢𝑛𝑖𝑓𝑜𝑟𝑚 (1,𝐾),𝑆∼𝑝𝑢,𝑎∼𝜇𝑘
𝑖[𝑅𝑖(𝑠,𝑎,𝑜 )]. (13)
Because of different sub-policies performed in different episodes,
the GPDS learning maintains a replay buffer 𝐷𝑘
𝑖for𝜇𝑘
𝑖of agent𝑖.
Consequently, the gradient of the entire objective w.r.t. 𝜃𝑘
𝑖is written
as:
∇𝜃𝑘
𝑖𝐺𝑒(𝜇𝑖) =1
𝐾𝐸𝑥,𝑎∼𝐷𝑘
𝑖[∇𝜃𝑘
𝑖𝜇𝑘
𝑖(𝑎𝑖|𝑜𝑖)∇𝑎𝑖𝑄𝜇𝑖(𝑥,𝑎1,…,𝑎𝑁)]. (14)
The deep reinforcement learning architecture for security games is
illustrated in Fig. 3, which contains two parts: the actor–critic com-
ponent and the adaptive module Learning component. In our paper,
the adaptive module Learning component is used to improve learningspeed. The core idea of the adaptive module Learning is trying to
combine PDS knowledge with imitation process to select the policy.
The adaptive module Learning component of reinforcement learning
architecture for security game is shown in Fig. 4(a), which contains
two processes. In the first process, according to the Markov hypothesis
and the uniform distribution of the initial solution of the security
game, the agent can use PDS knowledge. In reinforcement learning,
uniform distribution of policies helps agents balance exploration and
development. At the beginning of the training process, a random policy
selector is used to generate the exploratory random behavior of the
agent from 𝑑𝑖. Two learning rules 𝐿1
𝑖,𝑗|𝑘(𝑡),𝐿2
𝑖,𝑗|𝑘(𝑡)are used to estimate
the result. The learning rule 𝐿1
𝑖,𝑗|𝑘(𝑡)is defined as:
𝐿1
𝑖,𝑗|𝑘(𝑡) =𝜂(𝑖,𝑗|𝑘)(𝑡− 1)
𝜂(𝑖|𝑘)(𝑡− 1), (15)
where𝜂is defined as:
𝜂(𝑖|𝑘)(𝑡) =𝑡∑
𝑚=1𝐹(𝑠𝑚=𝑠𝑖,𝑎𝑛=𝑎𝑘),
𝜂(𝑖,𝑗|𝑘)(𝑡) =𝑡∑
𝑚=1𝐹(𝑠𝑚+1=𝑠𝑗|𝑠𝑚=𝑠𝑖,𝑎𝑛=𝑎𝑘),
and
𝐹(𝜅𝑡) ={
1,if the even 𝜅happens at time t
0,otherwise.
Expert Systems With Applications 210 (2022) 118394
7M. Chen et al.
Fig. 4. Adaptive primary Learning and actor–critic architecture.
The learning rule 𝐿2
𝑖,𝑗|𝑘(𝑡)is defined as:
𝐿2
𝑖,𝑗|𝑘(𝑡) =𝛩(𝑖,𝑗|𝑘)(𝑡)
𝜂(𝑖,𝑗|𝑘)(𝑡), (16)
where
𝛩(𝑖,𝑗|𝑘)(𝑡) =𝑡∑
𝑚=1𝜁𝑚𝐹(𝑠𝑚+1=𝑠𝑗|𝑠𝑚=𝑠𝑖,𝑎𝑛=𝑎𝑘),
and𝜁𝑚is cost factor.
The learning process of the adaptive module is as follows. The agent
𝑖begins with 𝑠0. As a solution of the security game, the initial state
is a fixed uniform distribution of policy. After that, the agent selects
randomly an action 𝑎. Then, the transition matrix is used to select the
next state𝑠𝑡+1. Finally, the learning processes started again until the
algorithm converges.
Actor–critic algorithm is a temporal difference learning model. It is
used to generate the policy structure. Choosing the action, the next state
is called the actor, while the operation responsible for estimating the
value function is called the critic. The role of the critic is to control the
whole learning process and analyze whether the policy makes the agent
make the best action. In order to complete the task, an error estimator
is used by the critic to manage all learning decisions. The actor–critic
component is shown in Fig. 4(b). In Actor–critic algorithm, we need to
fit a value function Q with post decision state to get a better gradient
estimation.
In this way, the policies can adopt extra information to ease the
process of learning, provided this information is not adopted at test
time. However, this has no application to 𝑄-learning. Thus, a policy-
based multi-agent actor–critic combined with a Post Decision State,
termed GPDS is proposed. In the proposed GPDS algorithm, we suppose
that given an action 𝑜of adversary at state 𝑠, the defender will take
action𝑎, the Post Decision State agent (that is, the agent who has
Fig. 5. The process of PDS.
extra information 𝑥in this algorithm) can achieve a known reward
𝑟𝑘(𝑠𝑖,𝑎𝑖,𝑜𝑖)and current state transits to an intermediate state ̃ 𝑠, with
a known probability 𝑝𝑘(𝑠′,𝑎,𝑜). However, the extra information can be
unknown to the adversary. In next step, using an unknown probability
𝑝𝑢(𝑠′|𝑠,𝑎,0), the intermediate state will transits to the next state 𝑠′,
meanwhile, receive a reward 𝑟𝑢(𝑠′|𝑠,𝑎,𝑜 ), which depends on the Post
Decision State. In particular, we suppose that the next state 𝑠′is
independent. The process of the Post Decision State is shown as Fig. 5.
Then we can write the transition function, that is, from 𝑠to𝑠′as:
𝑃(𝑠′|𝑠,𝑎,𝑜 ) =∑
̃ 𝑠𝑃𝑢(𝑠′|̃ 𝑠,𝑎,𝑜 )𝑃𝑘(̃ 𝑠|𝑠,𝑎,0). (17)
The expected reward of agent can be written as:
𝐸̃ 𝑠[𝑅(𝑠,𝑎,𝑜 )] =𝑟𝑘(𝑠,𝑎,𝑜 ) +∑
̃ 𝑠𝑃𝑘(̃ 𝑠|𝑠,𝑎,𝑜 )𝑟𝑢(̃ 𝑠|𝑠,𝑎,𝑜 ). (18)
Expert Systems With Applications 210 (2022) 118394
8M. Chen et al.
The basic idea of the presented GPDS algorithm is that, in place of
updating a single gradient of the return for agent 𝑖at a time, the agent
𝑖with the extra information on 𝑃𝑘(𝑠′|𝑠,𝑎,𝑜 )and𝑟𝑘(𝑠,𝑎,𝑜 )first learns
a gradient ∇𝜃𝑖𝐸[𝑅𝑖], which𝜃=𝜃1,…,𝜃𝑁is policies parameterized,
considering a game with 𝑁agents.𝜋=𝜋1,…,𝜋𝑁represent the set
of all agent policies.
In this paper, combining with traditional Actor–Critic, we use a
Post Decision State method to get the optimal policy. The optimal Post
Decision State quality function ̃𝑄∗can be written as:
̃𝑄∗=𝑟𝜇(̃ 𝑠,𝑎,𝑜 ) +𝛿∑
𝑠′𝑃𝑢(𝑠′|̃ 𝑠,𝑎,𝑜 )𝑉∗(𝑆′), (19)
where,𝛿is the discount factor, 𝑉∗is the value function and 𝑄∗is the
optimal quality function in GPDS algorithm. Therefore, we have the
following:
𝑄∗(𝑠,𝑎,𝑜 ) =𝐸𝑅[𝑅(𝑠,𝑎,𝑜 ) +𝛿𝑉∗(𝑠′)]. (20)
𝑉∗(𝑠) =𝑚𝑎𝑥𝑚𝑖𝑛∑
𝑎𝑄∗(𝑠,𝑎,𝑜 )𝜋(𝑠,𝑎,𝑜 ). (21)
Applying the extra information 𝑟𝑘(𝑠,𝑎,𝑜 )and𝑝𝑘(̃ 𝑠|𝑠,𝑎,𝑜 ), the optimal
qualitỹ𝑄∗is further written as:
𝑄∗(𝑠,𝑎,𝑜 ) =𝑟𝑘(𝑠,𝑎,𝑜 ) +∑
̃ 𝑠𝑃𝑘(̃ 𝑠|𝑠,𝑎,𝑜 )̃𝑄∗. (22)
In our work, after observing the sample
(𝑠𝑖,𝑎𝑖,𝑜𝑖,𝑟𝑘(𝑠𝑖,𝑎𝑖,𝑜𝑖), ̃ 𝑠𝑖,𝑟𝑢(̃ 𝑠𝑖,𝑎𝑖,𝑜𝑖),𝑠𝑖+1), the Post Decision State
value function ̃𝑄∗is updated by:
̃𝑄∗
𝑖+1(̃ 𝑠,𝑎𝑖,𝑜𝑖) = (1 −𝜆𝑖)̃𝑄∗
𝑖(̃ 𝑠,𝑎𝑖,𝑜𝑖)
+𝜆𝑖[𝑟𝑢(̃ 𝑠,𝑎𝑖,𝑜𝑖) +𝛿𝑉∗(𝑠𝑖+1)]. (23)
where𝜆𝑖∈ (0,1)is a learning rate.
Thus, the gradient function of the expected reward can be written
as:
∇𝜃𝑖𝐸[𝑅𝑖] =𝐸𝑠∼𝑝𝑢,𝑎𝑖∼𝜋𝑖[∇𝜃𝑖log𝜋𝑖(𝑎𝑖|𝑜𝑖)𝑄𝜋
𝑖(𝑥,𝑎𝑖,…,𝑎𝑁)]. (24)
where𝑄𝜋
𝑖(𝑥,𝑎𝑖,…,𝑎𝑁)is a centralized action-value function. Further-
more, extra information 𝑥consists of the observation of all agents,
𝑥= (𝑜1,…,𝑜𝑁).
We consider 𝑁continuous policies 𝜇𝜃𝑖with respect to parameters
𝜃𝑖, the gradient can be defined as:
∇𝜃𝑖𝐺(𝜇𝜃𝑖) = ∇𝜃𝑖𝐸[𝑅𝑖]
=𝐸𝑥,𝑎𝑖∼𝐷[∇𝜃𝑖𝜇𝑖(𝑎𝑖|𝑜𝑖)𝑄𝜇
𝑖(𝑥,𝑎1,…,𝑎𝑁)|𝑎𝑖=𝜇𝑖(𝑜𝑖)], (25)
where𝐷is the memory pool buffer, which contains the data samples,
saving the all the training experiences of system, 𝑥denotes the state
information. Furthermore, the centralized action-value function 𝑄𝜇
𝑖is
written as:
𝜔(𝜃𝑖) =𝐸𝑥,𝑎,𝑟,𝑥′[(𝑄𝜇
𝑖(𝑥,𝑎1,…,𝑎𝑁) −𝑦)2], (26)
𝑦=𝑟𝑖+𝛾𝑄𝜇′
𝑖(𝑥′,𝑎′
1,…,𝑎′
𝑁),
where𝜇′= {𝜇𝜃′
1,…,𝜇𝜃′
𝑁}is the set of objective policies, 𝜃′
𝑖represents
the delayed parameters.
To realize the multi-agent game, we perform the Minimax operation
on the training objective function:
𝐺(𝜃𝑖)=𝐸𝑠∼𝑝𝜇,𝑎𝑖∼𝜋𝑖[𝑅𝑖]
= min𝑎𝑘
𝑗≠𝑖𝐸𝑠∼𝑝𝜇[∑𝑇
𝑡=0𝛾𝑘𝑟𝑖(𝑠𝑘,𝑎𝑘
1,…,𝑎𝑘
𝑁)|||𝑎𝑘
𝑖=𝜇(𝑜𝑘
𝑖)]
=𝐸𝑠0∼𝜌[
min𝑎0
𝑗≠𝑖𝑄𝜇
𝑖(𝑠0,𝑎0
1,…,𝑎0
𝑁)||||𝑎0
𝑖=𝝁(
𝑜0
𝑖)]
.(27)
Note that𝑠𝑘+1at epoch𝑘+ 1depends on 𝑝𝜇, action𝜇(𝑜𝑘
𝑖), and
previous adversarial actions 𝑎𝑘
𝑗≠𝑖. According to Eq. (27), modified QAlgorithm 1 : GPDS learning algorithm
1:Initialization: 𝛾= 0.999,𝐷= 106,𝜖= 0.995,𝑁
2:Select initial state 𝑠
3:while𝑡<Length(episodes) do
4: The agent𝑖chooses appropriate action
𝑎𝑖=𝜇𝜃𝑖(𝑜𝑖)
5: and searching perform action
6:𝑎= (𝑎𝑖,...,𝑎𝑁)
7: Observation: The next state 𝑠′
𝑖and reward 𝑟𝑖
8: save (𝑠𝑖,𝑎𝑖,𝑟𝑖,𝑠′
𝑖)in𝐷
9:𝑠′→𝑠
10: while𝑖<𝑁 do
11: Random sampling a batch of K samples (𝑠𝑘,𝑎𝑘,𝑟𝑘,𝑠′
𝑘)from
replay buffer
12: set𝑦𝑗=𝑟𝑘
𝑖+𝛾𝑄𝝁′
M,𝑖(𝐬′𝑘,𝑎′
1,…,𝑎′
𝑁)|||
𝑎′
𝑖=𝝁′
𝑖(𝑜𝑘
𝑖),𝑎′
𝑗≠𝑖=𝝁′
𝑗(
𝑜𝑘
𝑗)
+̂ 𝜒′
𝑗witĥ 𝜒𝑗in Eq. (31)
13:𝑠′
𝑘=𝜇′
𝑘(𝑜′
𝑘)
14: minimizing the loss to update critic:
𝐿𝑜𝑠𝑠(𝜃𝑖) =1
𝐾∑
𝑘(𝑦𝑗−𝑄𝜇′
𝑖(𝑠′
𝑘,𝑎′
𝑘,...,𝑎′
𝑁))2.
15: Applying the sampled policy gradient to update actor:
∇𝜃𝑖𝐺≈1
𝐾∑
𝑘∇𝜃𝑖𝜇𝑖(𝑜𝑘
𝑖)∇𝑎𝑖𝑄𝜇
𝑖(𝑠′
𝑘,𝑎′
𝑘,...,𝑎′
𝑁).
16:𝑎𝑖=𝜇𝑖(𝑜𝑘
𝑖),𝑎∗
𝑗≠𝑖=𝑎𝑘
𝑗+̂ 𝜒𝑗
17: end while
18: For each agent 𝑖, using follow formula to update parameters:
𝜃′
𝑖←𝜌𝜃𝑖+ (1 −𝜌)𝜃′
𝑖.
19:end while
function can be defined as:
𝑄𝝁
𝑖(𝑠,𝑎1,…,𝑎𝑁)=𝑟𝑖(𝑠,𝑎1,…,𝑎𝑁)+
𝛾𝐸𝑠𝑘[
min𝑎𝑘
𝑗≠𝑖𝑄𝝁
𝑖(𝑠𝑘,𝑎𝑘
1,…,𝑎𝑘
𝑁)||||𝑎𝑘
𝑖=𝝁𝑖(𝑠𝑘)]
.
Importantly, for each user (agent) 𝑖, all adversarial actions do not
depend on its 𝜃𝑖. Using this property, we calculate ∇𝜃𝑖𝐺(𝜇𝜃𝑖)by the
deterministic gradient rule. Therefore, we further modify Eq. (25) as
follows:
∇𝜃𝑖𝐺(𝜃𝑖)=𝐸𝐱∼⎡
⎢
⎢
⎢⎣∇𝑎𝑖𝑄𝜇
𝑖(𝐱,𝑎∗
1,…,𝑎𝑖,…,𝑎∗
𝑁)∇𝜃𝑖𝝁𝑖(𝑜𝑖)
𝑎𝑖=𝝁𝑖(𝑜𝑖)
𝑎∗
𝑗≠𝑖= arg min𝑎𝑗≠𝑖𝑄𝜇
𝑖(𝐱,𝑎1,…,𝑎𝑁)⎤
⎥
⎥
⎥⎦. (28)
Therefore, Eq. (26) introduces a minimum update Q rule:
𝜔(𝜃𝑖)=𝐸𝐱,𝑎,𝑟,𝐱′∼[(𝑄𝝁
𝑖(𝐱,𝑎1,…,𝑎𝑁)−𝑦)2]
𝑦=𝑟𝑖+𝛾𝑄𝝁′
𝑖(𝐱′,𝑎′⋆1,…,𝑎′
𝑖,…,𝑎′⋆𝑁)
𝑎′
𝑖=𝝁′
𝑖(𝑜𝑖)
𝑎′∗
𝑗≠𝑖= arg min
𝑎′
𝑗≠𝑖𝑄𝝁′
𝑖(𝐱′,𝑎′
1,…,𝑎′
𝑁).(29)
Note that a difficult problem in the interference game in this paper
is how to deal with the minimization operation in Eqs. (28) and (29).
Because the channel state space is nonlinear, the computational cost of
gradient descent using the inner loop is too high. Therefore, we decided
to use user-to-user multi-user adversarial learning for training. The core
ideas are as follows: (1) The local linear function is used to approximate
the nonlinear function in the original problem. (2) Multi-step gradient
descent is used to replace the internal loop minimization in the original
problem.
Expert Systems With Applications 210 (2022) 118394
9M. Chen et al.
Table 2
Neural network architecture.
Network Units Memory Activation Layer
Critic 100 2048 ReLU Fully connected
Critic 100 2048 Tanh Fully connected
Critic 100 2048 ReLU Fully connected
Critic 100 2048 ReLU Fully connected
Critic 1 1024 Sigmoid Fully connected
Actor 100 ReLU Fully connected
Actor 80 ReLU Fully connected
Actor 20 ReLU Fully connected
Actor 1 2048 Sigmoid Fully connected
Based on the above analysis, Eq. (29) is redefined as follows with
auxiliary variables 𝜒:
𝜔(𝜃𝑖)=𝐸𝐱,𝑎,𝑟,𝐱′∼[(𝑄𝝁
𝑖(𝐱,𝑎1,…,𝑎𝑁)−𝑦)2]
𝑦=𝑟𝑖+𝛾𝑄𝝁′
𝑖(𝐱′,𝑎′∗
1,…,𝑎′
𝑖,…,𝑎′∗
𝑁)
𝑎′∗
𝑗=𝑎′
𝑗+𝜒𝑗,∀𝑗≠𝑖
𝑎′
𝑡=𝝁′
𝑡(𝑜𝑡),∀1≤𝑡≤𝑁
𝜒𝑗≠𝑖= arg min
𝜒𝑗≠𝑖𝑄𝝁′
𝑖(𝐱′,𝑎′
1+𝜒1,…,𝑎′
𝑖,…,𝑎′
𝑁+𝜒𝑁).(30)
The core of Eq. (30) is to find a group of interference 𝜒, so that
the interference action 𝑎′∗can minimize the Q value. We first linearize
the Q function at 𝑄𝝁
𝑖(𝐱,𝑎1,…,𝑎𝑁), and then we can obtain the local
approximation of the desired 𝜒by gradient descent:
̂ 𝜒𝑗= −𝜎∇𝑎𝑗𝑄𝝁′
𝑖(𝐱′,𝑎′
1,…,𝑎𝑗,…,𝑎′
𝑁),∀𝑗≠𝑖, (31)
where𝜎is tunable coefficient, that is, the step size of gradient descent.
Combining Eqs. (28) and (31), we have:
∇𝜃𝑖𝐺(𝜃𝑖)=
𝐸𝐱,𝑎∼⎡
⎢
⎢
⎢
⎢⎣∇𝜃𝑖𝝁𝑖(𝑜𝑖)∇𝑎𝑖𝑄𝜇
𝑖(𝐱,𝑎∗
1,…,𝑎𝑖,…,𝑎∗
𝑁)∣
𝑎𝑖=𝝁𝑖(𝑜𝑖)
𝑎∗
𝑗=𝑎𝑗+̂ 𝜒𝑗,∀𝑗≠𝑖
̂ 𝜒𝑗= −𝜎𝑗∇𝑎𝑗𝑄𝜇
𝑖(𝐱,𝑎1,…,𝑎𝑁)⎤
⎥
⎥
⎥
⎥⎦,
and
𝜔(𝜃𝑖)=𝐸𝐱,𝑎,𝑟,𝐱′[(𝑄𝝁
𝑖(𝐱,𝑎1,…,𝑎𝑁)−𝑦)2]
𝑦=𝑟𝑖+𝛾𝑄𝝁′
𝑖(𝐱′,𝑎′∗1,…,𝑎′
𝑖,…,𝑎′∗𝑁)
𝑎′
𝑘=𝝁′
𝑘(𝑜𝑘),∀1≤𝑘≤𝑁
𝑎′⋆
𝑗=𝑎′
𝑗+̂ 𝜒′
𝑗,∀𝑗≠𝑖
̂ 𝜒′
𝑗= −𝜎𝑗∇𝑎′
𝑗𝑄𝝁′
𝑖(𝐱,𝑎′
1,…,𝑎′
𝑁).(32)
The detailed execution process of the GPDS algorithm is shown in
Algorithm 1.
Remark. When a game has multiple equilibria (Nash or other equi-
librium concepts), it is quite common and reasonable that in many
applications (e.g., economics, operations research), we assume that
there is a given and fixed equilibrium selection rule or some tie-
breaking rule such that we do not have to worry about adversarial
equilibrium selection. In other words, we assume that there is some
kind of agreement between (self-interested, non-cooperative) players
that they would choose a certain single equilibrium.
5. Simulation results and analysis
5.1. Environment settings
In this section, we implement the GPDS algorithm to justify its
effectiveness. The Tensorflow 2.1.0 and CUDA Toolkit (NVIDIA deep
learning library) are employed to implement our proposed GPDS algo-
rithm on CentOS 7. The hardware environment is Intel(R) Gold 624Table 3
The parameters of GPDS algorithm.
Parameters Value
Learning rate 0.0005
Explore start 0.05
Explore stop 0.01
Explore decay rate 0.0001
Discount factor 0.95
Memory size 3000
Episodes 10 000
Batch size 128
The bandwidth 5 MHz
Noise factor 0.1
@2.60 GHz CPU and NVIDIA Tesla V100 32G GPU ×4. The deep
neural network architecture and parameters are shown in Tables 2, 3,
respectively.
We compare the proposed GPDS approach with the following algo-
rithms:
•Deep Q-network (DQN) (Gao, Qin, Jing, Ni, and Jin, 2019): The
single agent deep enhancement algorithm can only deal with
discrete state and action space.
•Actor–Critic (Lee, Nagabandi, Abbeel, and Levine, 2020): Actor–
Critic algorithm is a combination of policy-based and value-based
methods. The actor net is responsible for generating actions and
interacting with the environment. Critic net is responsible for
evaluating the actor’s performance and guiding the actor’s actions
in the next stage.
•Deep Deterministic Policy Gradient algorithm (DDPG) (Yu et al.,
2021): This is a single agent deep reinforcement learning algo-
rithm based on actor–critic architecture. An actor is used to make
up for the disadvantage that DQN algorithm cannot deal with
continuous control problems.
In the communication model, the channel state value is a continuous
value that changes with time. However, DQN algorithm is only suitable
for discrete state, Therefore, we discretize the attack and defense model
in DQN algorithm. For DQN, we considers 200 channels, each continu-
ous channel state value is divided into more than 30 000 channel gain
discrete points.
5.2. Result analysis
We show the loss curves over 10 000 episodes for four algorithms
in Fig. 6. It can be seen that the curve does not decline smoothly,
because the input data in RL changes step by step, and different data
will be obtained according to the learning situation. Observing the
loss curves, the GPDS algorithm converges downward. About 2000
episodes, although the loss curves of GPDS do not show a down-
ward convergence, but continuous training can converge. However, the
stability of traditional DRL methods is less than GPDS.
The reward performance of the proposed GPDS is compared in
Fig. 7. We show the learning curves over 10 000 episodes for four
algorithms. Traditional algorithms, such as DDPG, Actor–Critic and
DQN, do not perform as well as GPDS.
We evaluate the effectiveness of learning the policies of other
agents, and the result is shown in Fig. 8. In particular, learning the
policies of other agents perfectly and learning with approximated poli-
cies can achieve the same average reward as adopting the true policy,
and as we can see, without an obvious slowdown in convergence.
Additionally, we measured the performance by the following for-
mula:
𝛷(𝑛) = [̃ 𝑟𝑃𝐷𝑆(𝑛) −̃ 𝑟(𝑛)]∕𝑎𝑣𝑒𝑟𝑎𝑔𝑒 (̃ 𝑟(𝑛)). (33)
The relative performance comparison between this paper algorithm,
DDPG, Actor–Critic, and DQN algorithms is shown in Fig. 9. As we
Expert Systems With Applications 210 (2022) 118394
10M. Chen et al.
Fig. 6. Comparison of loss after 10 000 episode under GPDS, DDPG, Actor–Critic and DQN algorithms.
Fig. 7. Normalized reward after 10 000 episodes under GPDS, DDPG, Actor–Critic and
DQN algorithms.
Fig. 8. The policy learning success rate.
Expert Systems With Applications 210 (2022) 118394
11M. Chen et al.
Fig. 9. Relative performance gain 𝛷(𝑛)of 10 000 episode under GPDS, DDPG, Actor-
Critic and DQN algorithms.
Fig. 10. Relative performance gain 𝛷(𝑛)under suspected interference.
can see, using the centralized critic GPDS method are more easily
enabled to learn policy behavior. Although the simplicity of the envi-
ronment, traditional reinforcement learning algorithms, such as DDPG,
Actor–Critic, and DQN all fail to learn the correct policy.
For the anti interference system, the 𝛷0,1,𝛷1,0of IRS are set to 0.5
and𝑐𝑠= 0.5,𝑐𝑝= 2. Let𝑘= 2, the two spectrum channel varies between
ℎ1= {ℎ1,1,ℎ1,2}andℎ2= {ℎ2,1,ℎ2,2}with transition probabilities
𝑝𝑔1=[0.9 0.1
0.3 0.7]
and𝑝𝑔2=[0.3 0.7
0.4 0.6]
.
And𝐶(ℎ1,1) = 1,𝐶(ℎ1,2) = 2,𝐶(ℎ2,1) = 0,𝐶(ℎ2,2) = 1 with transition
matrix
𝑝𝐷=⎡
⎢
⎢⎣0.3 0.3 0.4
0 0.5 0.5
0 0 1⎤
⎥
⎥⎦.
Let𝑝𝐹𝐼= 0.3be the probability that the mobile user transmits
(Encounter suspected interference). The performance of our proposedGPDS strategy is compared with that of traditional methods is shown
in Fig. 10.
To further test the performance of the proposed algorithm, we
considered 4 agents in the security game. Their goal is to maximize the
expected intrusion benefits. Meanwhile, two defenders try to defend
the attackers and minimize the loss caused by being attacked. In the
proposed algorithm, defender agent will imitate and learn from the
experienced agents. The information involved in the game process is
represented by utility matrix and transfer matrix.
Initialize the four original transfer matrices as:
⎡
⎢
⎢
⎢
⎢⎣0.2816 0.2354 0.2556 0.2274
0.2473 0.2546 0.2813 0.2168
0.2451 0.3212 0.1546 0.2791
0.2798 0.2121 0.2465 0.2616⎤
⎥
⎥
⎥
⎥⎦,
⎡
⎢
⎢
⎢
⎢⎣0.3461 0.3549 0.1245 0.1745
0.1677 0.2364 0.3247 0.2712
0.3312 0.1846 0.2465 0.2377
0.1986 0.1876 0.3214 0.2924⎤
⎥
⎥
⎥
⎥⎦,
⎡
⎢
⎢
⎢
⎢⎣0.1846 0.2346 0.2474 0.3334
0.1465 0.3461 0.1563 0.3511
0.2414 0.2341 0.1992 0.3253
0.3127 0.2416 0.2013 0.2444⎤
⎥
⎥
⎥
⎥⎦,
⎡
⎢
⎢
⎢
⎢⎣0.3411 0.1774 0.3123 0.1692
0.3213 0.3001 0.2104 0.1682
0.2461 0.2366 0.3124 0.2049
0.2013 0.3002 0.2781 0.2204⎤
⎥
⎥
⎥
⎥⎦.
Also, the utility matrices are showed as:
⎡
⎢
⎢
⎢
⎢⎣95 142 142 82
82 165 121 96
37 210 172 102
66 155 143 96⎤
⎥
⎥
⎥
⎥⎦,⎡
⎢
⎢
⎢
⎢⎣58 145 135 221
78 184 211 114
82 150 180 190
65 200 142 101⎤
⎥
⎥
⎥
⎥⎦,
⎡
⎢
⎢
⎢
⎢⎣46 83 94 76
61 77 121 112
83 56 93 134
99 145 76 51⎤
⎥
⎥
⎥
⎥⎦,⎡
⎢
⎢
⎢
⎢⎣35 165 44 59
57 195 87 87
49 164 71 130
90 186 143 76⎤
⎥
⎥
⎥
⎥⎦.
Consider the channel gain ℎ𝑧value from {1,8},𝑇𝑆
𝑡∈ {0,1}indicates
whether mobile user can send/sense to be assigned to mobile users,
such as, the random spectrum access environment in MEC networks,
𝑃𝑢(𝑇𝑆
𝑡|𝑇𝑜𝑛
𝑡)(𝑇𝑜𝑛
𝑡means IRS is active) is only known to the mobile users,
𝛩𝑡denotes the data channels. If 𝑇𝑆
𝑡= 0, the mobile users will be
waiting.
At first, assumed that all agents at initial channel state 8, that is
ℎ1
1=ℎ2
1=ℎ3
1=ℎ4
1= 8. The learning cure of all mobile users at
state 8 are shown in Fig. 11 and the Learning curve of the attackers is
shown in Fig. 12. Form Fig. 11, we can see that, after 2000 episodes, the
algorithm will converge to the optimal policy. According to Fig. 11, the
SUs mostly take action the action (2, 3) with probability 0.75, action (3,
2) with probability 0.6, action (5, 1) with probability 0.43 and action
(5, 2) with probability 0.78; According to Fig. 12, the attackers take
action 2 with probability 0.4, action 1 with probability 0.25, action 2
with probability 0.45, action 3 with probability 0.45. The experimental
results show that the channel 2 has high availability, so attackers tend
to send spoofing signals to attack and attempt to block this channel.
However, the attackers still chooses to attack other channels, which
indicates that the attacker’s strategy is random. Since the attacker’s
strategy is random, the optimal defense game strategy can bring more
benefits to mobile users. Note that the curve in Fig. 11(b) drops sharply
and then rises rapidly. The reason is that reinforcement learning is a
trial-and-error learning process, and agents need to find good policies
in their interactions with the environment. However, in the learning
process, random initialization may produce a great deviation value.
Expert Systems With Applications 210 (2022) 118394
12M. Chen et al.
Fig. 11. Learning curve of the mobile users at initial channel state 8.
At this time, the agent needs to maximize reward according to the
known information through continuous exploration, constantly correct
the error value, and finally converge to the optimal solution.
The transmission rate (Byte/s/Hz) and SINR protection performance
comparison for different transmit power (dBm) are shown in Figs. 13
and 14, where mobile users 𝑁= 10. In Figs. 13(a) and 14(a), we set
the number of IRS elements is 𝐿= 80 , obviously, the transmission
rate and protection level improve as transmit power 𝑃increases. In
addition, under different 𝑃values, the proposed GPDS learning method,
DDPG, and actor–critic algorithms have good channel transmission
rate values, and they are much better than the DQN algorithm based
on discrete model. However, the channel transmission rate and SINR
protection performance of the three SOTA algorithms are significantly
lower than that of the proposed GPDS algorithm, because they are
a single user optimization solution that ignores multi-user assistance
games, which cannot effectively ensure the performance requirements.
Figs. 13(b) and 14(b) show the transmission rate (Byte/s/Hz) and SINR
protection performance of the 4 algorithms for different IRS elements
when𝑃= 40 dBm. Because all algorithms are based on IRS deployment
strategy, the transmission rate performance of all algorithms increases
with the increase of the number of IRS elements. Therefore, the multi-
user intelligent game and IRS strategy used in the proposed GPDS
algorithm can achieve a higher level of performance gain. According
to the experimental results, the transmission power can be increased
by increasing the number of IRS elements, the channel communica-
tion quality can be improved by the IRS phase, and the intelligentinterference of jammer can be reduced. In addition, compared with
the other three SOTA algorithms, the proposed GPDS algorithm only
needs 80 IRS elements to have sufficient transmission power for anti-
interference defense, and can achieve 100% protection level. This is
because the proposed GPDS algorithm can achieve more flexible joint
power allocation and reflected beam optimization, and improve the
performance of intelligent jamming in MEC networks.
6. Conclusion and future work
In this paper, we have investigated the anti jamming and security
mechanisms for mobile devices in time-varying MEC networks, under
the framework of anti jamming security game and deep learning. The
conventional value-based anti jamming security strategies cannot work
well to protect the communication security in the considered scenario
due to the high electricity resource consumption and algorithm com-
plexity, presence of smart jammer attackers, and dynamics in human–
robot interactive environments. To solve this problem, we proposed a
novel policy-based multi-agent deep reinforcement learning algorithm,
i.e., GPDS, which can improve the SINR protection level and channel
throughput of mobile devices in unknown and dynamic communica-
tion jamming environments. In order to realize multi-agent game, we
combine minimax strategy with multi-user deep reinforcement learn-
ing, and use user to user multi-user adaptive learning for training to
realize nonlinear processing of continuous channel variables. Through
simulations, it is demonstrated that the proposed learning algorithm
Expert Systems With Applications 210 (2022) 118394
13M. Chen et al.
Fig. 12. Learning curve of the attackers at initial channel state 8.
Fig. 13. The transmission rate (Byte/s/Hz) comparisons versus the maximum transmit power and the number of IRS elements under GPDS, DDPG, Actor–Critic and DQN algorithms.
outperforms traditional SOTA reinforcement learning algorithms in a
multi-agent anti jamming environment. For the future work, we will
try to accelerate the speed of GPDS algorithm to adapt to fast-changing
dynamic security game environments.CRediT authorship contribution statement
Miaojiang Chen: Data curation, Software, Visualization, Writing
– original draft. Wei Liu: Conceptualization, Methodology, Funding
Expert Systems With Applications 210 (2022) 118394
14M. Chen et al.
Fig. 14. SINR protection level comparisons versus the maximum transmit power and the number of IRS elements under GPDS, DDPG, Actor–Critic and DQN algorithms.
acquisition, Validation, Project administration, Resources. Ning Zhang:
Supervision, Writing – review & editing. Junling Li: Supervision, Writ-
ing – review & editing. Yingying Ren: Formal analysis. Meng Yi:
Data curation, Software, Visualization, Writing – original draft. Anfeng
Liu:Conceptualization, Methodology, Funding acquisition, Validation,
Project administration, Resources.
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Data availability
No data was used for the research described in the article.
Acknowledgments
This work was supported in part by the National Natural Science
Foundation of China (62072475, 61772554), Supported by the Open
Research Project of the State Key Laboratory of Industrial Control
Technology, Zhejiang University, China (No. ICT2022B15), part by Hu-
nan Provincial Innovation Foundation for Postgraduate, China (Project
Leader: Miaojiang Chen). We are very grateful to the anonymous
reviewers for their constructive comments that were helpful for us to
improve the manuscript.
References
Chen, G., & Dong, W. (2020). Reactive jamming and attack mitigation over cross-
technology communication links. ACM Transactions on Sensor Networks ,17(1),
1–25.
Chen, M., Liu, A., Liu, W., Ota, K., Dong, M., & Xiong, N. N. (2021). RDRL: A recurrent
deep reinforcement learning scheme for dynamic spectrum access in reconfigurable
wireless networks. IEEE Transactions on Network Science and Engineering ,9(2),
364–376.
Chen, M., Liu, W., Wang, T., Liu, A., & Zeng, Z. (2021). Edge intelligence computing
for mobile augmented reality with deep reinforcement learning approach. Computer
Networks , Article 108186.
Chen, M., Liu, W., Wang, T., Zhang, S., & Liu, A. (2021). A game-based deep
reinforcement learning approach for energy-efficient computation in MEC systems.
Knowledge-Based Systems , Article 107660.
Chen, C., Song, M., Xin, C., & Backens, J. (2013). A game-theoretical anti-jamming
scheme for cognitive radio networks. IEEE Network ,27(3), 22–27.
Chen, M., Wang, T., Zhang, S., & Liu, A. (2021). Deep reinforcement learning
for computation offloading in mobile edge computing environment. Computer
Communications ,175, 1–12.Di Renzo, M., Zappone, A., Debbah, M., Alouini, M.-S., Yuen, C., De Rosny, J., et
al. (2020). Smart radio environments empowered by reconfigurable intelligent
surfaces: How it works, state of research, and the road ahead. IEEE Journal on
Selected Areas in Communications ,38(11), 2450–2525.
Do, T. T., Björnson, E., Larsson, E. G., & Razavizadeh, S. M. (2018). Jamming-resistant
receivers for the massive MIMO uplink. IEEE Transactions on Information Forensics
and Security ,13(1), 210–223. http://dx.doi.org/10.1109/TIFS.2017.2746007 .
D’Oro, S., Ekici, E., & Palazzo, S. (2018). Optimal power allocation and scheduling
under jamming attacks. IEEE/ACM Transactions on Networking ,25(3), 1310–1323.
http://dx.doi.org/10.1109/TNET.2016.2622002 .
El-Bardan, R., Brahma, S., & Varshney, P. K. (2016). Strategic power allocation
with incomplete information in the presence of a jammer. IEEE Transactions
on Communications ,64(8), 3467–3479. http://dx.doi.org/10.1109/TCOMM.2016.
2577686 .
Elleuch, I., Pourranjbar, A., & Kaddoum, G. (2021). A novel distributed multi-agent
reinforcement learning algorithm against jamming attacks. IEEE Communications
Letters ,25(10), 3204–3208.
Fan, H., Yang, Z., Zhang, X., Wu, S., Long, J., & Liu, L. (2022). A novel multi-satellite
and multi-task scheduling method based on task network graph aggregation. Expert
Systems with Applications , Article 117565.
Feng, S., & Haykin, S. (2019). Cognitive risk control for anti-jamming V2V communi-
cations in autonomous vehicle networks. IEEE Transactions on Vehicular Technology ,
68(10), 9920–9934.
Gao, N., Qin, Z., Jing, X., Ni, Q., & Jin, S. (2019). Anti-intelligent UAV jamming strategy
via deep Q-networks. IEEE Transactions on Communications ,68(1), 569–581.
Gao, Y., Xiao, Y., Wu, M., Xiao, M., & Shao, J. (2018). Game theory-based anti-jamming
strategies for frequency hopping wireless communications. IEEE Transactions on
Wireless Communication ,17(8), 5314–5326. http://dx.doi.org/10.1109/TWC.2018.
2841921 .
Gu, P., Hua, C., Xu, W., Khatoun, R., Wu, Y., & Serhrouchni, A. (2020). Control
channel anti-jamming in vehicular networks via cooperative relay beamforming.
IEEE Internet of Things Journal ,7(6), 5064–5077. http://dx.doi.org/10.1109/JIOT.
2020.2973753 .
Hanawal, M. K., Abdel-Rahman, M. J., & Krunz, M. (2017). Joint adaptation of
frequency hopping and transmission rate for anti-jamming wireless systems. IEEE
Transactions on Mobile Computing ,15(9), 2247–2259. http://dx.doi.org/10.1109/
TMC.2015.2492556 .
Hoang, T. M., Duong, T. Q., Suraweera, H. A., Tellambura, C., & Poor, H. V.
(2015). Cooperative beamforming and user selection for improving the security
of relay-aided systems. IEEE Transactions on Communications ,63(12), 5039–5051.
Hu, J., Wellman, M. P., et al. (1998). Multiagent reinforcement learning: theoretical
framework and an algorithm. In ICML, Vol. 98 (pp. 242–250). Citeseer.
Huang, M., Leung, V. C., Liu, A., & Xiong, N. N. (2022). TMA-DPSO: Towards efficient
multi-task allocation with time constraints for next generation multiple access. IEEE
Journal on Selected Areas in Communications ,40(5), 1652–1666.
Huang, C., Zappone, A., Alexandropoulos, G. C., Debbah, M., & Yuen, C. (2019).
Reconfigurable intelligent surfaces for energy efficiency in wireless communication.
IEEE Transactions on Wireless Communication ,18(8), 4157–4170. http://dx.doi.org/
10.1109/TWC.2019.2922609 .
Huynh, H. A., Han, Y., Park, S., Hwang, J., Song, E., & Kim, S. (2018). Design and
analysis of the DC–DC converter with a frequency hopping technique for EMI
reduction. IEEE Transactions on Components, Packaging and Manufacturing Technology ,
8(4), 546–553.
Expert Systems With Applications 210 (2022) 118394
15M. Chen et al.
Jia, L., Xu, Y., Sun, Y., Feng, S., Yu, L., & Anpalagan, A. (2018). A game-theoretic
learning approach for anti-jamming dynamic spectrum access in dense wireless
networks. IEEE Transactions on Vehicular Technology ,68(2), 1646–1656.
Law, Y. W., Palaniswami, M., Hoesel, L. V., Doumen, J., Hartel, P., & Havinga, P.
(2009). Energy-efficient link-layer jamming attacks against wireless sensor network
MAC protocols. ACM Transactions on Sensor Networks ,5(1), 1–38.
Lee, S., Kim, S., Seo, M., & Har, D. (2019). Synchronization of frequency hopping
by LSTM network for satellite communication system. IEEE Communications Letters ,
23(11), 2054–2058.
Lee, A. X., Nagabandi, A., Abbeel, P., & Levine, S. (2020). Stochastic latent actor-
critic: Deep reinforcement learning with a latent variable model. Advances in Neural
Information Processing Systems ,33, 741–752.
Li, J., Li, T., Liu, Z., & Chen, X. (2019). Secure deduplication system with active
key update and its application in IoT. ACM Transactions on Intelligent Systems and
Technology (TIST) ,10(6), 1–21.
Liang, L., Cheng, W., Zhang, W., & Zhang, H. (2018). Mode hopping for anti-jamming
in radio vortex wireless communications. IEEE Transactions on Vehicular Technology ,
67(8), 7018–7032. http://dx.doi.org/10.1109/TVT.2018.2825539 .
Liu, Q., Li, M., Kong, X., & Zhao, N. (2016). Disrupting MIMO communications
with optimal jamming signal design. IEEE Transactions on Wireless Communication ,
14(10), 5313–5325. http://dx.doi.org/10.1109/TWC.2015.2436385 .
Lv, S., Xiao, L., Hu, Q., Wang, X., Hu, C., & Sun, L. (2017). Anti-jamming power control
game in unmanned aerial vehicle networks. In GLOBECOM 2017-2017 IEEE global
communications conference (pp. 1–6). IEEE.
Mukherjee, A., Fakoorian, S. A. A., Huang, J., & Swindlehurst, A. L. (2014). Prin-
ciples of physical layer security in multiuser wireless networks: A survey. IEEE
Communications Surveys & Tutorials ,16(3), 1550–1573.
Ren, Y., Liu, W., Liu, A., Wang, T., & Li, A. (2022). A privacy-protected intelligent
crowdsourcing application of IoT based on the reinforcement learning. Future
Generation Computer Systems ,127, 56–69.
Rowland, M., Omidshafiei, S., Tuyls, K., Perolat, J., Valko, M., Piliouras, G., et al.
(2019). Multiagent evaluation under incomplete information. Advances in Neural
Information Processing Systems ,32.
Shi, Y., An, K., & Li, Y. (2021). Index modulation based frequency hopping: Anti-
jamming design and analysis. IEEE Transactions on Vehicular Technology ,70(7),
6930–6942. http://dx.doi.org/10.1109/TVT.2021.3087640 .
Tang, X., Wang, D., Zhang, R., Chu, Z., & Han, Z. (2021). Jamming mitigation via
aerial reconfigurable intelligent surface: Passive beamforming and deployment
optimization. IEEE Transactions on Vehicular Technology ,70(6), 6232–6237. http:
//dx.doi.org/10.1109/TVT.2021.3077662 .
Torreño, A., Onaindia, E., & Sapena, O. (2015). An approach to multi-agent planning
with incomplete information. arXiv preprint arXiv:1501.07256 .
Van Huynh, N., Nguyen, D. N., Hoang, D. T., & Dutkiewicz, E. (2019). ‘‘Jam me if
you can:’’ Defeating jammer with deep dueling neural network architecture and
ambient backscattering augmented communications. IEEE Journal on Selected Ar-
eas in Communications ,37(11), 2603–2620. http://dx.doi.org/10.1109/JSAC.2019.
2933889 .
Van Huynh, N., Nguyen, D. N., Hoang, D. T., & Dutkiewicz, E. (2020). ‘‘Jam me if
you can:’’ defeating jammer with deep dueling neural network architecture and
ambient backscattering augmented communications. IEEE Journal on Selected Areas
in Communications ,37(11), 2603–2620.
Wang, C., & Wang, H.-M. (2015). Robust joint beamforming and jamming for secure AF
networks: Low-complexity design. IEEE Transactions on Vehicular Technology ,64(5),
2192–2198. http://dx.doi.org/10.1109/TVT.2014.2334640 .
Wang, X., Wang, J., Xu, Y., Chen, J., Jia, L., Liu, X., et al. (2020). Dynamic spectrum
anti-jamming communications: Challenges and opportunities. IEEE Communications
Magazine ,58(2), 79–85.
Wu, Q., & Zhang, R. (2019). Intelligent reflecting surface enhanced wireless network via
joint active and passive beamforming. IEEE Transactions on Wireless Communication ,
18(11), 5394–5409.
Xiao, L., Li, Y., Dai, C., Dai, H., & Poor, H. V. (2018). Reinforcement learning-based
NOMA power allocation in the presence of smart jamming. IEEE Transactions
on Vehicular Technology ,67(4), 3377–3389. http://dx.doi.org/10.1109/TVT.2017.
2782726 .
Xiao, L., Liu, J., Li, Q., Mandayam, N. B., & Poor, H. V. (2015). User-centric view
of jamming games in cognitive radio networks. IEEE Transactions on Information
Forensics and Security ,10(12), 2578–2590.
Xiao, L., Lu, X., Xu, D., Tang, Y., Wang, L., & Zhuang, W. (2018b). UAV relay in
VANETs against smart jamming with reinforcement learning. IEEE Transactions on
Vehicular Technology ,67(5), 4087–4097.
Xiao, L., Lu, X., Xu, D., Tang, Y., Wang, L., & Zhuang, W. (2018c). UAV relay in VANETs
against smart jamming with reinforcement learning. IEEE Transactions on Vehicular
Technology ,67(5), 4087–4097. http://dx.doi.org/10.1109/TVT.2018.2789466 .
Xiong, Z., Zhang, Y., Lim, W. Y. B., Kang, J., Niyato, D., Leung, C., et al. (2020).
UAV-assisted wireless energy and data transfer with deep reinforcement learning.
IEEE Transactions on Cognitive Communications and Networking ,7(1), 85–99.Xiong, Z., Zhang, Y., Niyato, D., Deng, R., Wang, P., & Wang, L.-C. (2019). Deep
reinforcement learning for mobile 5G and beyond: Fundamentals, applications, and
challenges. IEEE Vehicular Technology Magazine ,14(2), 44–52.
Xiong, Z., Zhao, J., Niyato, D., Deng, R., & Zhang, J. (2020). Reward optimization for
content providers with mobile data subsidization: A hierarchical game approach.
IEEE Transactions on Network Science and Engineering ,7(4), 2363–2377.
Xu, D. (2020). Proactive eavesdropping of suspicious non-orthogonal multiple access
networks. IEEE Transactions on Vehicular Technology ,69(11), 13958–13963. http:
//dx.doi.org/10.1109/TVT.2020.3021953 .
Xu, D., & Zhu, H. (2022a). Jamming-assisted legitimate eavesdropping and secure
communication in multicarrier interference networks. IEEE Systems Journal ,16(1),
954–965. http://dx.doi.org/10.1109/JSYST.2020.3030574 .
Xu, D., & Zhu, H. (2022b). Legitimate surveillance of suspicious computation offloading
in mobile edge computing networks. IEEE Transactions on Communications ,70(4),
2648–2662. http://dx.doi.org/10.1109/TCOMM.2022.3151767 .
Yang, H., Xiong, Z., Zhao, J., Niyato, D., Wu, Q., Tornatore, M., et al. (2020). Intelligent
reflecting surface assisted anti-jamming communications based on reinforcement
learning. In GLOBECOM 2020-2020 IEEE global communications conference (pp. 1–6).
IEEE.
Yao, F., & Jia, L. (2019). A collaborative multi-agent reinforcement learning anti-
jamming algorithm in wireless networks. IEEE Wireless Communications Letters ,8(4),
1024–1027. http://dx.doi.org/10.1109/LWC.2019.2904486 .
Yin, Z., Lin, Y., Zhang, Y., Qian, Y., Shu, F., & Li, J. (2022). Collaborative multi-
agent reinforcement learning aided resource allocation for UAV anti-jamming
communication. IEEE Internet of Things Journal , 1. http://dx.doi.org/10.1109/JIOT.
2022.3188833 .
Yu, M., Liu, A., Xiong, N. N., & Wang, T. (2022). An intelligent game-based offloading
scheme for maximizing benefits of IoT-edge-cloud ecosystems. IEEE Internet of
Things Journal ,9(8), 5600–5616. http://dx.doi.org/10.1109/JIOT.2020.3039828 .
Yu, Y., Tang, J., Huang, J., Zhang, X., So, D. K. C., & Wong, K.-K. (2021). Multi-
objective optimization for UAV-assisted wireless powered IoT networks based on
extended DDPG algorithm. IEEE Transactions on Communications ,69(9), 6361–6374.
Zheng, X., & Cai, Z. (2020). Privacy-preserved data sharing towards multiple parties in
industrial IoTs. IEEE Journal on Selected Areas in Communications ,38(5), 968–979.
http://dx.doi.org/10.1109/JSAC.2020.2980802 .
Zhu, S., Li, W., Li, H., Tian, L., Luo, G., & Cai, Z. (2018). Coin hopping attack in
blockchain-based IoT. IEEE Internet of Things Journal ,6(3), 4614–4626.
Miaojiang Chen received the B.S. degree in computer
science from Guangxi University in 2018. He is currently
a Ph.D. candidate with School of Computer Science and
Engineering of Central South University, China. He has
published several journal and conference papers in the
IEEE transactions on network science and engineering,
Knowledge-Based Systems, Computer Network, etc., and he
also serves reviewer of the top-tier conferences and journals,
e.g., International Conference on Machine Learning (ICML),
IEEE transactions on industrial informatics, Knowledge-
Based Systems. His major research interests include deep
reinforcement learning, Internet of Things, edge computing,
neural network optimization.
Wei Liu is an associate professor and senior engineer at
the School of Informatics, Hunan University of Chinese
Medicine, China. He received his Ph.D. degree in computer
application technology from Central South University, 2014,
China. His research interests include software engineering,
data mining and medical informatics. He has published over
20 papers in the related fields.
Ning Zhang (Senior Member, IEEE) is an Associate Pro-
fessor in the Department of Electrical and Computer
Engineering at University of Windsor, Canada. He received
the Ph.D. degree in Electrical and Computer Engineer-
ing from University of Waterloo, Canada, in 2015. After
that, he was a postdoc research fellow at University of
Waterloo and University of Toronto, Canada, respectively.
His research interests include connected vehicles, mobile
edge computing, wireless networking, and machine learning.
He is a Highly Cited Researcher. He received an NSERC
PDF award in 2015 and 6 Best Paper Awards from IEEE
Globecom in 2014, IEEE WCSP in 2015, IEEE ICC in
Expert Systems With Applications 210 (2022) 118394
16M. Chen et al.
2019, IEEE ICCC in 2019, IEEE Technical Committee on
Transmission Access and Optical Systems in 2019, and
Journal of Communications and Information Networks in
2018, respectively. He serves as an Associate Editor of IEEE
Internet of Things Journal, IEEE Transactions on Cognitive
Communications and Networking, and IEEE Systems Jour-
nal; and a Guest Editor of several international journals,
such as IEEE Wireless Communications, IEEE Transactions
on Industrial Informatics, IEEE Transactions on Intelligent
Transportation Systems, and IEEE Transactions on Cognitive
Communications and Networking. He also serves/served
as a general chair for IEEE SAGC 2021, TPC chair for
IEEE SAGC 2020, a track chair for several international
conferences including IEEE ICC 2022, CollaborateCom 2021,
IEEE VTC 2020, AICON 2020 and CollaborateCom 2020,
and a co-chair for numerous international workshops.
Junling Li (IEEE S’18) received the Ph.D. degree from
the Department of Electrical and Computer Engineering,
University of Waterloo, Waterloo, ON, Canada in 2020.
She is currently a Joint Postdoctoral Research Fellow at
Shenzhen Institute of Artificial Intelligence and Robotics
for Society (AIRS), the Chinese University of Hong Kong,
Shenzhen, and University of Waterloo. She received the
M.S. degree from the Beijing University of Posts and
Telecommunications, Beijing, China, in 2016, and the B.S.
degree from Tianjin University, Tianjin, China, in 2013. Her
interests include game theory, machine learning, software-
defined networking, network function virtualization, and
vehicular networks. She received the Best Paper Award at
the IEEE/CIC International Conference on Communications
in China (ICCC) in 2019.
Yingying Ren Currently is a master student in School
of Information Science and Engineering, Central South
University, China. Her research interests include services-
based network, crowd sensing networks, and wireless sensor
networks.
Meng Yi received the master degree in computer science
from Guangxi University in 2020. She is currently a Ph.D.
candidate with School of Computer Science and Engi-
neering, Southeast University, China. Her major research
interests include deep reinforcement learning, Internet of
Things, multiaccess edge computing, heterogeneous wireless
networks.
Anfeng Liu received the M.Sc. and Ph.D. degrees from Cen-
tral South University, China, in 2002 and 2005, respectively,
both in computer science. He is currently a professor of
the School of Information Science and Engineering, Central
South University, China. His major research interest is
wireless sensor networks, Internet of Things, information
security, edge computing and crowdsourcing. Dr. Liu has
published 4 books and over 100 international journal and
conference papers, among which there are more than 30
ESI highly-cited papers. Some of his works were published
in IEEE Transactions on Information Forensics & Security,
IEEE Transactions on Mobile Computing, IEEE Transactions
on Services Computing, IEEE Transactions on Parallel and
Distributed Systems, IEEE Transactions on Vehicular Tech-
nology, IEEE Transactions on ComputerAided Design of
Integrated Circuits and Systems, IEEE System Journal, IEEE
Wireless Communications, IEEE Communications Magazine,
IEEE Network Magazine, IEEE Transactions on Industrial
Informatics, ACM Transactions on Embedded Computing
Systems, IEEE Internet of Things Journal, IEEE Transactions
on Emerging Topics in Computing, IEEE Transactions on
Systems Man Cybernetics-Systems. His research has been
supported by the National Basic Research Program of China
(973 Program) and the National Natural Science Founda-
tion of China for five times. He was a recipient of the
First Prize of Scientific Research Achievement of Colleges
from the Ministry of Education of China in 2016, and
the Second Prize of Science and Technology Award from
China Nonferrous Metal Industry Association in 2005. He
has served as the Leading Editor of the special issue for
International Journal of Distributed Sensor Networks, and a
guest editor of the special issues for Scientific Programming,
and Sensors. He also serves as a reviewer of over 30
international academic journals."
17348-Article Text-20842-1-2-20210518.pdf,"Resilient Multi-Agent Reinforcement Learning with
Adversarial Value Decomposition
Thomy Phan,1Lenz Belzner,2Thomas Gabor,1
Andreas Sedlmeier,1Fabian Ritz,1Claudia Linnhoff-Popien1
1LMU Munich,
2MaibornWolff
thomy.phan@iﬁ.lmu.de
Abstract
We focus on resilience in cooperative multi-agent systems,
where agents can change their behavior due to udpates or
failures of hardware and software components. Current state-
of-the-art approaches to cooperative multi-agent reinforce-
ment learning (MARL) have either focused on idealized set-
tings without any changes or on very specialized scenar-
ios, where the number of changing agents is ﬁxed, e.g., in
extreme cases with only one productive agent. Therefore,
we propose Resilient Adversarial value Decomposition with
Antagonist-Ratios (RADAR). RADAR offers a value decom-
position scheme to train competing teams of varying size for
improved resilience against arbitrary agent changes. We eval-
uate RADAR in two cooperative multi-agent domains and
show that RADAR achieves better worst case performance
w.r.t. arbitrary agent changes than state-of-the-art MARL.
Introduction
Distributed systems consist of multiple separated compo-
nents that collaborate to accomplish a common task (Tanen-
baum and Van Steen 2007). Distributed autonomous systems
can be formulated as cooperative multi-agent system (MAS),
which can be realized with methods of reinforcement learn-
ing (RL) (Foerster et al. 2018; Rashid et al. 2018).
Multi-agent RL (MARL) potentially offers better scala-
bility and resilience against changing agents compared to
single-agent RL. We deﬁne an agent change either as up-
date or failure. E.g., some agents may be updated with new
software or temporarily be replaced by other versions due to
maintainance. In both cases, we would expect the remain-
ing MAS to collaborate with these novel agents. On the
other hand, agents might behave erroneously due to hard-
ware or software failures. In this case, we would expect the
remaining MAS to degrade gracefully without failing en-
tirely (Stone and Veloso 2000). Intuitively, resilience should
improve with more agents due to more available resources
for compensation (Tanenbaum and Van Steen 2007).
Although resilience has been long recognized as a
main motivation for realizing cooperative MAS (Stone and
Veloso 2000; Panait and Luke 2005; Bus ¸oniu, Babu ˇska, and
De Schutter 2010), most state-of-the-art approaches to co-
operative MARL have focused on optimizing idealized sce-
Copyright c2021, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.narios, where an agent only faces the same or similar agents
as during training (Foerster et al. 2018; Gupta, Egorov, and
Kochenderfer 2017; Rashid et al. 2018). This bears the risk
ofoverﬁtting, where a MAS can entirely fail when some
agents signiﬁcantly change their behavior, which could be
fatal in safety critical environments, where such failures may
have catastrophic consquences (Uesato et al. 2019).
Some work on resilient MARL based on adversarial learn-
ing (Littman 1994; Li et al. 2019; Phan et al. 2020) has fo-
cused on specialized settings with a ﬁxed number of adver-
sary agents, e.g., where a single productive agent remains.
These approaches lack ﬂexibility, which is required, when
arbitrary portions of the MAS can change. Furthermore, they
introduce new tunable hyperparameters like the fraction of
adversaries or the degree of adversarial behavior, which fur-
ther increase sensitivity w.r.t. unexpected scenarios.
In this paper, we propose Resilient Adversarial value De-
composition with Antagonist-Ratios (RADAR). RADAR of-
fers a value decomposition scheme to train competing teams
of varying size for improved resilience against arbitrary
agent changes. Our main contributions are:
A simple mechanism to train adversarial agents with vari-
able team sizes during training, which is necessary to cre-
ate MAS that can potentially cope with arbitrary agent
changes. Unlike prior work on resilient MARL, RADAR
does not introduce any new hyperparameters, thus can be
easily integrated into existing RL frameworks.
An agent test scheme to consistently evaluate perfor-
mance and resilience against changing agents in cooper-
ative MAS in a fair way, which is inspired by prior work
on single-agent RL (Badia et al. 2020; Jordan et al. 2020).
An empirical evaluation of RADAR in two cooperative
multi-agent domains and a comparison with state-of-the-
art MARL w.r.t. the proposed test scheme. While being
competitive against state-of-the-art MARL in cooperative
settings, RADAR achieves better worst case performance
when facing a variable number of previously unknown ad-
versary agents at test time.
Background
Problem Formulation
MAS can be formulated as partially observable Markov
gameM=hD;S;A;P;R;Z;
i, whereD=f1;:::;Ngis
TheThirty-FifthAAAIConferenceonArtificial Intelligence(AAAI-21)
11308
a set of agents,Sis a set of states st,A=A1:::ANis
the set of joint actions at=hat;1;:::;at;Ni,P(st+1jst;at)
is the transition probability, R(st;at) =hrt;1;:::;rt;Ni2
RNis the joint reward with rt;ibeing the reward of agent
i2D ,Zis a set of local observations zt;ifor each agent
i, and 
(st;at) =zt+1=hzt+1;1;:::;zt+1;Ni 2 ZNis
the subsequent joint observation. Each agent imaintains an
action-observation historyt;i2(ZAi)t.(atjt) =
h1(at;1jt;1);:::;N(at;Njt;N)iis the joint policy, where
i(at;ijt;i)is the local policy of agenti.ican be evalu-
ated with a value function Q
i(st;at) =E[Gt;ijst;at]for
allst2S andat2A, whereGt;i=P1
k=0krt+k;i is the
return of agenti, and2[0;1)is the discount factor. We
denote the joint action and policies without agent ibyat; i
and irespectively. The goal of each agent iis to ﬁnd a
best response 
iwhich maximizes Q
igiven i.
Policy Gradient Reinforcement Learning
Policy gradient RL is a popular approach to approximate
best responses 
ifor each agent i. A function approxima-
tor^i;with parameters is trained with gradient ascent on
an estimate of J=E[G1;i]. Most policy gradient methods
use gradients gof the following form (Sutton et al. 2000):
g=A
i(st;at)rlog^i;(at;ijt;i) (1)
whereA
i(st;at) =Q
i(st;at) V
i(st)is the advantage
function andV
i(st) =E[Gt;ijst]is the state value func-
tion of agent i.Actor-critic approaches often approximate
^AiA
iby replacing Q
i(st;at)withGt;iandV
iwith
Ei[Q
i].Q
ican be approximated with a critic ^Qi;!and pa-
rameters!using value-based RL (Watkins and Dayan 1992;
Mnih et al. 2015). For simplicity, we omit the parameter in-
dices,!and write ^i,^Qiinstead.
Independent Learning
^Qican be learned independently using single-agent RL on
at;iandt;i(Tan 1993; Leibo et al. 2017). These local ap-
proximations can be used for each agent’s policy ^ito per-
form gradient ascent according to Eq. 1 leading to the inde-
pendent actor-critic (IAC) approach (Foerster et al. 2018).
IAC offers optimal scalability w.r.t. Nbut violates the
Markov assumption due to non-stationarity caused by simul-
taneously learning agents (Laurent et al. 2011).
Centralized Training Decentralized Execution
For many problems, training usually takes place in a lab-
oratory or in a simulated environment, where global infor-
mation is available. State-of-the-art MARL exploits this fact
to approximate centralized value functions ^Qi, which condi-
tion on global states stand joint actions at, and use them as
critic in Eq. 1 (Lowe et al. 2017). While ^Qiis only required
during training in order to learn local policies, ^iitself only
conditions on the local history t;i, thus it can be executed in
a decentralized way. This paradigm is known as centralized
training and decentralized execution (CTDE).
^Qican be approximated separately for each agent iwhile
integrating global information, in contrast to IAC, to learnbest responses (Lowe et al. 2017). This approach lacks a
multi-agent credit assignment mechanism for training agent
teams, where all agents observe the same reward signal.
COMA approximates a single value function ^Qper
team to compute agent-wise counterfactual baselines
V
i(st) =P
at;i2Ai^i(at;ijt;i)^Q(st;hat;i;at; ii)for in-
dividual credit assignment (Foerster et al. 2018).
The centralized ^Qcan be factorized to approximate in-
dividual ^Qifor each agent iin order to update ^iaccord-
ing to Eq. 1 in a coordinated way. Value decomposition net-
work (VDN) is the simplest factorization method, where ^Q
is deﬁned byP
i2D^Qi(t;i;at;i)(Sunehag et al. 2018). Al-
ternatively, there exist non-linear factorization methods like
QMIX or QTRAN (Rashid et al. 2018; Son et al. 2019).
While CTDE mitigates the non-stationarity issue of in-
dependent learning due to exploiting the Markov property
of states, most approaches based on deep learning require a
ﬁxed number of agents Ndue to the predeﬁned input dimen-
sion ofstandatrequired by ^Q(st;at)(Lowe et al. 2017;
Foerster et al. 2018; Rashid et al. 2018; Son et al. 2019).
Adversarial Reinforcement Learning
Inzero-sum games, there are N= 2 agents with oppos-
ing goals. The value functions of agent iandj(and anal-
ogously the rewards) are deﬁned by Q
i= Q
j. Amin-
imax equilibrium policy of agentiis deﬁned by 
i=
argmaximinjQ
i, which corresponds to a best response to
the worst case, represented by 
j(Littman 1994).
Adversarial RL approaches attempt to approximate 
i
with alternating optimization or reformulation of the min-
imax objective by applying standard RL techniques to each
agent (Littman 1994; Pinto et al. 2017; Li et al. 2019).
Related Work
Adversarial Reinforcement Learning
Adversarial learning is a popular paradigm to train two op-
ponents alternately to improve each other’s performance and
robustness (Goodfellow et al. 2014; Pinto et al. 2017). Self-
play RL is the simplest form of adversarial RL, where a
single agent is trained to play against itself to ensure an
adequate difﬁculty level and steady convergence to robust
policies (Samuel 1959; Tesauro 1995; Silver et al. 2016). In
(single-agent) RL, the environment can be modeled as ad-
versary by adding disturbances to confront the original agent
with worst case scenarios (Morimoto and Doya 2001; Ra-
jeswaran et al. 2017; Pinto et al. 2017). These adversarial
disturbances can be realized, e.g., with RL or coevolution-
ary approaches (Gabor et al. 2019; Wang et al. 2019).
Our work is mainly based on adversarial learning. In con-
trast to single-agent RL, where external changes can only
occur within the environment, we focus on agent changes
in cooperative MAS. For that, we integrate adversary agents
into the training process in order to improve resilience.
Multi-Agent Reinforcement Learning
MARL is a long-standing AI research area with various
approaches (Tan 1993; Panait and Luke 2005; Foerster
11309
Figure 1: RAT and RADAR scheme for N= 4. Left: each agent has a protagonist (blue) and an antagonist (red) representation.
Middle: mixed games My;mixare generated by randomly sampling Rantfor each phase. Right: ^fiare updated in epochs using
VDN either for protagonists or antagonists. Note that the number of protagonists and antagonists can vary depending on Rant.
et al. 2018; Son et al. 2019). While cooperative MARL has
achieved impressive results in challenging domains, most
approaches have been only evaluated with the same or sim-
ilar agents as encountered during training. Thus, it remains
unclear if these approaches offer resilience against arbitrary
agent changes, which are expectable in the real world.
There is some prior work towards resilient MARL:
Minimax-Q was proposed in (Littman 1994) as an adapta-
tion of Q-Learning for zero-sum games. While guarantee-
ing convergence to safe policies w.r.t. worst case opponents,
Minimax-Q becomes intractable if the (joint) action space of
the opponent jis large. (Li et al. 2019) proposes M3DDPG,
which considers extreme cases, where each agent iconsid-
ers itself the sole productive agent, while all other agents
are modeled as adversaries who attempt to minimize Q
i.
M3DDPG can lead to poor policies, if the problem is too dif-
ﬁcult or even unsolvable for single productive agents, lead-
ing to insufﬁcient training signal. (Phan et al. 2020) proposes
ARTS, where productive and adversary agents are trained
simultaneously according to a ﬁxed adversary ratio, since
most CDTE approaches need a predeﬁned input dimension
to approximate ^QQ. ARTS can improve resilience
against agent failures with adequately chosen adversary ra-
tios. However, an ideal ratio needs to be known a priori,
which is an unrealistic assumption. Furthermore, a ﬁxed ra-
tio can lead to sensitive policies when Nis sufﬁciently large.
We propose an adversarial value decomposition scheme,
where the number of productive and adversary agents can
change dynamically during training. Furthermore, we pro-
pose an agent test scheme to evaluate performance and re-
silience of MARL approaches in a fair way.
Methods
Terminology
We focus on mixed (cooperative-competitve) games Mmix,
whereD=Dpro[D antwithDpro\D ant=;(Lowe et al.2017; Phan et al. 2020). Dprois a team of productive or pro-
tagonist agents, which need to accomplish a certain (cooper-
ative) task.Dantis a team of antagonist agents representing
(adversarial) agent changes in the MAS. For all protagonists
i2D proand for all antagonists j2D antthe corresponding
rewards are deﬁned by rt;i=rt;pro= rt;j. The protago-
nist returnGt;prois computed analogously to the individual
returnGt;iusingrt;proas rewards and as discount factor.
Furthermore, we deﬁne an antagonist-ratio Rant=jDantj
jDj.
IfRant= 0, thenMmixis fully cooperative with D=Dpro
and a single global reward rt;i=rt;profor all agents i2D.
We use ^fi=h^i;^Qiias general notation for the learnable
function representation of agent iwherever possible.
Randomized Adversarial Training
Most approaches towards resilient MARL focus on particu-
lar failure scenarios with a ﬁxed Rant(Littman 1994; Li et al.
2019; Phan et al. 2020), which has several drawbacks: First,
Rantmust be known a priori or extensively tuned, which is
generally not feasible. Second, a ﬁxed Rantduring training
can lead to inﬂexible behavior when facing a variable num-
ber of changing agents, which can be expected in real-world
scenarios. Third, Rantcan have a huge impact on the train-
ing quality itself, e.g., if Rantis too large, the MAS problem
becomes too difﬁcult to learn any meaningful policy.
Thus, we regard a randomized adversarial training (RAT)
scheme. Since arbitrary agent changes can occur in a MAS,
we provide a protagonist and antagonist representation for
each agent. We maintain a pool ^fpro=h^f1;pro;:::;^fN;proiof
protagonist and a pool ^fant=h^f1;ant;:::;^fN;antiof antag-
onist representations, which are trained in Tphases sim-
ilarly to (Pinto et al. 2017). At each phase x, we ran-
domly sample Rant2[0;1)from a uniform distribution
Uto runNeepisodes of different mixed games My;mix,
whereb(1 Rant)Ncprotagonist policies ^i;prorepresent-
ingDy;proanddRantNeantagonist policies ^j;antrepresent-
11310
Algorithm 1 Randomized Adversarial Training (RAT)
1:procedure RAT(D;N;^fpro;^fant;	)
2: Initialize parameters of ^fproand^fant
3: forphasex= 1;T do
4: SampleRantU . uniform sampling
5: forepisodey= 1;Nedo
6:Dy;ant sampledRantNeagents fromD
7:Dy;pro fi2Dji62Dy;antg
8: fori= 1;N do .CreateMy;mix
9: ifi2D y,antthen
10: i ^i;ant .from ^fi;ant
11: ifi2D y,prothen
12: i ^i;pro .from ^fi;pro
13:  h1;:::;Ni
14: Run oneMy;mixepisode with joint policy 
15: Storeey;t=fhst;zt;at;rt;proigandDy;pro
16: ifxmod 2 = 1 then
17: Update ^fi;prowith	8i2Dy;pro w.r.t.ey;t
18: else
19: Update ^fi;antwith	8i2Dy;antw.r.t.ey;t
ingDy;antwithi6=jare randomly selected. Each episode
ycan be considered a zero-sum game between Dy;proand
Dy;ant. After each phase, either ^fproor^fantis updated in
alternating epochs w.r.t. the generated experience ey;t=
fhst;zt;at;rt;proigwhile the other pool is kept ﬁxed.
The complete formulation of RAT is given in Algorithm
1, whereDis the set of agents of the original MAS (given
Rant= 0),N=jDjis the number of agents, ^fproand^fant
are the learnable protagonist and antagonist representations
respectively, and 	is an optimization or MARL algorithm.
Due to its simplicity, we do not consider RAT a major contri-
bution but a necessary preliminary and baseline for RADAR,
which is introduced in the next section.
Resilient Adversarial Value Decomposition
	in Algorithm 1 can be easily set to IAC or other indepen-
dent learning algorithms, since RAT requires 	to be ﬂexi-
ble w.r.t. the number of protagonists and antagonists, which
can vary between each phase depending on Rant. Using in-
dependent learning for RAT comes with the non-stationarity
issue and the lacking credit assignment w.r.t. to (protago-
nist and antagonist) agent teams. Most CTDE approaches
require a ﬁxed team size due to the predeﬁned input dimen-
sion of ^QQdepending on stand the joint action at
(Lowe et al. 2017; Foerster et al. 2018; Rashid et al. 2018).
Therefore, we propose RADAR, a CTDE scheme to ap-
proximate protagonist and antagonist policies with variable
Rantbased on VDN (Sunehag et al. 2018). VDN approxi-
matesQ(st;at)with ^Q(t;at) =P
i2D^Qi(t;i;at;i)for
cooperative MAS, where t=ht;1;:::;t;Niis the joint
history. Although we focus on mixed games My;mix,Qcan
be obviously only approximated with cooperating agents1.
1Naively integrating adversary value functions into ^QcouldThus, we approximate ^Qproand^Qantfor protagonists and
antagonists respectively using separate VDN instances.
Given RAT in Algorithm 1, we can approximate ^Qproin
protagonist epochs (line 17) with the following term:
X
i2Dy;pro^Qi;pro(t;i;at;i) =Ey;[jDy;proj
NGt;projst;at](2)
whereGt;prois the protagonist return andjDy;proj
Nis used to
normalizeGt;prow.r.t. the number of participating protag-
onists in episode y, because the scale of Gt;procould give
more weight to settings where Rantis small.
Analogously, we can approximate ^Qantinantagonist
epochs (line 19 of Algorithm 1) with the following term:
X
i2Dy;ant^Qi;ant(t;i;at;i) =Ey;[ jDy;proj
NGt;projst;at](3)
which approximates ^Qant= ^Qpro.
The terms of Eq. 2 and 3 can be approximated via end-
to-end training of ^Qi;pro and^Qi;antusing backpropagation
(Sunehag et al. 2018). ^Qi;proand^Qi;antcan be used to de-
rive the corresponding local policies, either via multi-armed
bandits applied to the values or via policy gradient methods
according to Eq. 1. The main components of RADAR and
their integration into RAT are visualized in Fig. 1.
Although non-linear factorization methods have been pro-
posed in (Rashid et al. 2018; Son et al. 2019), VDN of-
fers some advantages in our context: Regarding Eq. 2 and
3, VDN just approximates a sum which is not bounded by
a speciﬁc number of agents, thus being able to deal with
variable team sizes of Dy;proandDy;ant. Normalizing re-
turns w.r.t.Rantis straightforward in VDN due to the linear
decomposition unlike in the non-linear case. Furthermore,
VDN does neither introduce additional learnable parame-
ters (e.g., additional neural networks) nor hyperparameters,
which improves efﬁciency w.r.t. computation and tuning. We
are still aware that adapting non-linear factorization meth-
ods for RAT could lead to even more powerful approaches
towards resilient MARL, which we defer to future work.
Discussion of RAT and RADAR
RAT and RADAR offer simple mechanisms to train resilient
MAS and can be easily combined with existing RL algo-
rithms (value- or policy-based) to train ^i. Neither RAT nor
RADAR introduce new hyperparameters (RAT uses uniform
sampling for Rantand RADAR uses VDN without addi-
tional approximators or objectives), thus the tuning com-
plexity completely depends on the underlying RL algorithm.
Given the uniform sampling of Rantand2Nagent rep-
resentations (N for the protagonists and Nfor the antago-
nists), the expected computational complexity of RADAR is
O(N)because ERantU[Rant] = 0:5. The worst case com-
plexity isO(2N)(whenRant= 0in each protagonist epoch
and close to 1in each antagonist epoch). Thus, RADAR
scales similarly to other CTDE approaches in expectation
with some overhead due to training additional antagonists.
lead to trivial solutions, since ^Q= 0according to zero-sum games.
11311
Testing Performance and Resilience in MAS
Most tests are conducted after training, where antagonists
are trained on the ﬁnal version of ^fproto determine ﬂaws
(Littman 1994; Li et al. 2019; Gleave et al. 2019). We pro-
pose an online test method, where we enable consistent tests
during training. For that, we use predeﬁned test cases that
integrate novel agents which ideally have not been encoun-
tered during training before2. Such novel agents could be
new cooperative agents (e.g., when the MAS is tested for
compatibility with unknown agents) or antagonists which
represent failures (e.g., due to ﬂaws or malicious attacks).
A MAS should behave resiliently in both cases.
We regard a test suiteTconsisting of test cases c=
h^f0;R0
anti2T . For a given ^fpro, we run a single test case
by generating random mixed games similar to RAT (Algo-
rithm 1) with antagonist-ratio R0
antand^fant=^f0to evaluate
the average performance with gc(e.g., the protagonist return
Gt;proor some domain-speciﬁc value). Tcan contain the fol-
lowing disjoint subsets as schematically shown in Fig. 2:
 h^fpro;0i2T idealonly involves protagonists ^fi;proencoun-
tered during training. The majority of work on coopera-
tive MAS only reports such idealized test cases to eval-
uate sample efﬁciency and raw performance (Lowe et al.
2017; Foerster et al. 2018; Gupta, Egorov, and Kochen-
derfer 2017; Rashid et al. 2018; Son et al. 2019).
 h^f0
pro;R0
anti 2 T cooperation integrates new protagonists
^f0
i;pro6=^fi;prothat were obtained from a different training
process.Tcooperation evaluates the ability of ^fproto collabo-
rate with unknown agents. In this paper, we set R0
ant=1
2.
 h^f0
ant;i 2 T failure; integrates new antagonists ^f0
i;ant6=
^fi;antthat were obtained from a different training process.
Tfailure; evaluates the resilience of ^fproagainst unknown
failure scenarios according to different R0
ant=.
Tcooperation andTfailure; enable us to compare different
MARL approaches with each other in a fair way because
they face the same test agents inT. Furthermore, using a
single test suiteTis practically more efﬁcient than sepa-
rately performing an adversarial test on each MARL result
as previously proposed in (Littman 1994; Li et al. 2019).
Naively aggregating the performance values gcof each
c2 T could lead to the domination of test cases with
smallR0
ant, since more protagonists contribute to the suc-
cess of the MAS. Thus, a normalized value gc2Ris re-
quired to scale gcaccording to the number of protagonists
jDproj=b(1 R0
ant)Ncto ensure a meaningful evaluation
w.r.t. different R0
ant(Jordan et al. 2020). In this paper, we
focus on the following measures3as depicted in Fig. 2:
Cooperation performance estimates Ec2T cooperation [gc]
w.r.t. to the number of protagonists (the integrated test
2These agents can be trained with any (adversarial) MARL al-
gorithm (e.g., RADAR, M3DDPG, ARTS) independently of ^fpro.
3Although none of these measures is actually new (Crandall and
Goodrich 2005; Powers, Shoham, and Vu 2007), they are widely
neglected in current deep MARL in favor of the idealized case.
Figure 2: The introduced test sets: Tidealinvolves the same
protagonists encountered during training. Tcooperation and
Tfailure; integrate protagonists or antagonists of different
training processes than the MAS to be tested. While prior
work mainly focused on ideal performance (green), we re-
gard cooperation (blue) and worst case (red) performance.
agents do not contribute to gc) to evaluate the compati-
bility withN
2new cooperatively trained test agents.
Worst case performance estimates the (degraded) pro-
tagonist performance minc2T cooperation[(S
Tfailure; )fgcgin
the worst case w.r.t. arbitrary agent changes.
The average performance of all c2 T cooperation[
(S
Tfailure; )as shown in Fig. 2 could put more emphasis
on test cases, where ^fproperforms especially well, reducing
the signiﬁcance of our evaluation (Jordan et al. 2020). Thus,
we focus on the performance, which we can at least expect
from ^fprogiven arbitrary agent changes (Badia et al. 2020).
Experiments4
Evaluation Domains
We implemented a predator-prey (PP) and a cyber-physical
production system (CPPS) domain with Nagents. An
episode is reset after 50 time steps for each domain. We de-
ﬁne a normalized performance value gcfor each domain as
quality measure for ^fpro.
PP[K,N] consists of a KKgrid withNlearning preda-
tor agents andN
2randomly moving prey agents. Each agent
starts at a random position, is able to move north, south,
west, east, or do nothing, and has a 55ﬁeld of view. A
prey is captured with a global reward of +1, when at least
one predator ioccupies the same position as main capturer
with another predator j6=ibeing within sight of i, which is
recorded as =hi;ji. Captured preys respawn at random
positions. We deﬁne gc=1
jDprojP
=hi;ji1[i2D pro]as the
normalized number of protagonist main captures.
CPPS[N] consists of a machine grid as shown in Fig. 3.
Each agent has a list of four random tasks tasksiorganized
in two buckets. All agents start at a blue entry and are able to
enqueue at their current machine, move north, south, west,
east, or do nothing. At every time step, each machine pro-
cesses one agent in its queue. If a task in its current bucket
4Code available at https://github.com/thomyphan/resilient-marl
11312
Figure 3: Two CPPS instances with Rant=1
4. The white and
red cylinders represent protagonists and antagonists respec-
tively. (a, b) CPPS[4] as 4-agent setting with 1 antagonist.
(c)CPPS[16] as 16-agent setting with 4 antagonists.
matches with the machine type, the task is removed from
the agent’s task list with a reward of +1. An agent iiscom-
plete, if tasksi=;and it reaches an orange exit, yield-
ing another reward of +1. For each incomplete agent, a re-
ward of -0.01 is given at every time step. Each agent has
a55ﬁeld of view without knowing the tasks of other
agents. All agents are only allowed to move along the black
paths, which represent bidirectional conveyor belts and may
only share the same position at the transparent boxes, which
represent hubs. Thus, all agents have to coordinate to avoid
conﬂicts or collisions to ensure fast completion. We deﬁne
gc=jfi2D projtasksi=;gj
jDprojas the protagonist completion rate.
Learning Algorithms and Test Cases
We implemented an actor-critic algorithm with A
i(st;at) =
Gt;i V
i(st)and M3DDPG. The critic V
i=Ei[Q
i]is
approximated via IAC, COMA, QMIX, or RADAR.
To study the effect of variable antagonist-ratios and adver-
sarial value decomposition, we implemented ablations with
ﬁxedRant=2f0;1
2;N 1
Ngdenoted by RADAR () and
a RAT instantiation with 	 = IAC.Rant=N 1
Nrepresents
the extreme case with a single protagonist agent.
Prior to training, we generated Tcooperation with RADAR
(0) andTfailure; with RADAR (2f1
4;1
2;3
4g). For each test
set, we trained 10 test cases which are used to consistently
evaluate resilience of all implemented MARL approaches.
Neural Network Architectures
We used deep neural networks to implement ^fi=h^i;^Qii
for each agent i. The neural networks are updated every 1000
Figure 4: Protagonist completion rates for RADAR and its
ablations on CPPS[4] and CPPS[16] for test cases with
R0
ant2f0;1
2g. Shaded areas show the 95 % conﬁdence in-
terval. The legend at the top applies across all plots.
time steps using ADAM with a learning rate of 0:001. We set
= 0:95,T= 4000, and Ne= 10 (Algorithm 1).
Since PP and CPPS are gridworlds, states and observa-
tions are encoded as multi-channel image as proposed in
(Gupta, Egorov, and Kochenderfer 2017; Phan et al. 2018).
We implemented all neural networks as multilayer percep-
tron (MLP) and ﬂattened the multi-channel images before
feeding them into the networks. ^iand^Qihave two hid-
den layers of 64 units with ELU activation. The output of ^i
hasjAijunits with softmax activation (gumbel softmax for
M3DDPG (Lowe et al. 2017)). The output of ^QihasjAij
linear units. The centralized ^Q-networks of COMA, QMIX,
and M3DDPG are MLPs having two hidden layers of 128
units with ELU activation and one linear output unit (jA ij
output units for COMA (Foerster et al. 2018)).
Results
For each MARL approach, we performed 20 training runs
of 40,000 episodes (2 million time steps in total). After each
epoch ofNe= 10 episodes, a full test was performed on
^fproby running each c2T for 50 times. For CPPS[4], we
provide the expected optimal value based on antagonists that
prevent other agents from entering the CPPS by blocking the
entry path as shown in Fig. 3a and 3b.
The test performance for R0
ant2f0;1
2gof RADAR and its
ablations is shown in Fig. 4. In CPPS[4], all RADAR vari-
ants except RADAR (N 1
N) outperform RAT. In CPPS[16],
RAT outperforms RADAR (0), when R0
ant>0. RADAR
is competitive or superior to RADAR (1
2) and outperforms
RADAR (0), when R0
ant>0or whenN= 16. RADAR
(N 1
N) improves fastest in all settings but its performance
gradually decreases after reaching a peak.
The cooperation performance of RADAR, IAC, COMA,
AC-QMIX, and M3DDPG is shown in Fig. 5. COMA, AC-
11313
Figure 5: Cooperation performance for RADAR and state-
of-the-art MARL. Shaded areas show the 95 % conﬁdence
interval. The legend at the top applies across all plots.
QMIX, and IAC achieve the best cooperation performance
except in CPPS[16], where RADAR performs best. How-
ever, RADAR is only slightly outperformed by the coopera-
tive MARL approaches in PP[7,4], PP[10,8], and CPPS[4].
M3DDPG performs worst in all settings.
The worst case performance of RADAR, IAC, COMA,
AC-QMIX, and M3DDPG is shown in Fig. 6. RADAR
clearly achieves the best worst case performance in all set-
tings except in PP[7,4], where COMA, AC-QMIX, and IAC
are competitive. M3DDPG performs worst in all settings.
Discussion
We presented RADAR, an adversarial value decomposition
scheme for resilient MAS. RADAR trains competing teams
of protagonist and antagonist agents of varying size to im-
prove resilience against arbitrary agent changes.
According to our ablation study, the value decomposi-
tion scheme offers a signiﬁcant advantage over independent
learning: RADAR and most ﬁxed antagonist-ratio variants
clearly outperform RAT (	 = IAC), because RAT (	 =
IAC) lacks a credit assignment mechanism, which is impor-
tant to learn coordinated protagonist and antagonist policies.
Fig. 4 indicates that training with ﬁxed antagonist-ratios
strongly depends on the concrete setting and must be tuned,
e.g., in CPPS[4], RADAR (0) outperforms RADAR (1
2) on
average but in CPPS[16], RADAR (1
2) is clearly superior
(although the nature of tasks is the same in both CPPS in-
stances). RADAR does not require such tuning and performs
at least second best in all CPPS instances.
RADAR is able to achieve competitive cooperation per-
formance compared to state-of-the-art MARL like COMA,
AC-QMIX, and IAC. We assume that the additional train-
ing of randomly integrated antagonists causes some over-
head, which sacriﬁces a little performance regarding the
special case of cooperative test agents. However, RADAR
is able to achieve superior worst case performance w.r.t.
arbitrary agent changes including failure scenarios. Fig. 6
Figure 6: Worst case performance for RADAR and state-
of-the-art MARL. Shaded areas show the 95 % conﬁdence
interval. The legend at the top applies across all plots.
indicates that RADAR is more resilient than cooperative
state-of-the-art MARL w.r.t. the size of the MAS, since
RADAR achieves signiﬁcantly better worst case perfor-
mance in PP[10,8] andCPPS[16] compared to PP[7,4] and
CPPS[4] respectively. In contrast to RADAR, cooperative
state-of-the-art MARL approaches achieve especially poor
worst case performance in CPPS[16] compared to CPPS[4],
which contradicts our intuition that a MAS should be actu-
allymore resilient when more agents are available (Tanen-
baum and Van Steen 2007).
Although RADAR (N 1
N) and M3DDPG focus on ex-
treme cases, they perform poorly in all settings, indicating
that if a domain is too difﬁcult (or not solvable at all) for
a single protagonist, such specializations are not sufﬁcient
for learning resilient behavior. Despite of RADAR (N 1
N)
improving fastest in the beginning (Fig. 4), the antagonists
eventually learn to stall the single protagonist in the CPPS,
thus leading to the performance decrease. M3DDPG models
adversarial behavior within its ^Qobjective, leading to very
sparse training signal right at the beginning of training.
Signiﬁcant advantages of RADAR are its algorithmic sim-
plicity and its ﬂexibility w.r.t. to team sizes. Since it uses
uniform sampling and linear value decomposition, it does
not introduce any new hyperparameters to be tuned (the in-
tegrated antagonists have exactly the same hyperparameters
as the protagonists), thus being less sensitive than state-of-
the-art MARL, when facing a variety of adversarial settings.
Like other CTDE approaches, RADAR scales linearly in ex-
pectation and worst case, thus offering a feasible and easy
extension to existing RL approaches w.r.t. resilience.
For future work, we want to extend RADAR to non-linear
value decomposition like QMIX and use adaptive sampling
mechanisms for Rantto further improve performance and
resilience. We also aim to provide adequate agent test sets
for other established domains similarly to our experiments,
which we regard as an important step towards consistent and
fair evaluation of future cooperative MARL approaches.
11314
Acknowledgments
We would like to thank Cornel Klein, Horst Sauer, Reiner
Schmid, and Jan Wieghardt from Siemens AG for helpful
discussions on this project.
References
Badia, A. P.; Piot, B.; Kapturowski, S.; Sprechmann, P.;
Vitvitskyi, A.; Guo, Z. D.; and Blundell, C. 2020. Agent57:
Outperforming the Atari Human Benchmark. In III, H. D.;
and Singh, A., eds., Proceedings of the 37th International
Conference on Machine Learning, volume 119 of Proceed-
ings of Machine Learning Research, 507–517. PMLR.
Bus ¸oniu, L.; Babu ˇska, R.; and De Schutter, B. 2010. Multi-
Agent Reinforcement Learning: An Overview. In Innova-
tions in Multi-Agent Systems and Applications-1, 183–221.
Springer.
Crandall, J. W.; and Goodrich, M. A. 2005. Learn-
ing to Compete, Compromise, and Cooperate in Repeated
General-Sum Games. In Proceedings of the 22nd Interna-
tional Conference on Machine Learning, 161–168.
Foerster, J.; Farquhar, G.; Afouras, T.; Nardelli, N.; and
Whiteson, S. 2018. Counterfactual Multi-Agent Policy Gra-
dients. Proceedings of the AAAI Conference on Artiﬁcial
Intelligence 32(1).
Gabor, T.; Sedlmeier, A.; Kiermeier, M.; Phan, T.; Hen-
rich, M.; Pichlmair, M.; Kempter, B.; Klein, C.; Sauer, H.;
Schmid, R.; and Wieghardt, J. 2019. Scenario Co-evolution
for Reinforcement Learning on a Grid World Smart Factory
Domain. In Proceedings of the Genetic and Evolutionary
Computation Conference, GECCO ’19, 898–906. Associa-
tion for Computing Machinery.
Gleave, A.; Dennis, M.; Kant, N.; Wild, C.; Levine, S.;
and Russell, S. 2019. Adversarial Policies: Attacking Deep
Reinforcement Learning. In International Conference on
Learning Representations.
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .
2014. Generative Adversarial Nets. In Ghahramani, Z.;
Welling, M.; Cortes, C.; Lawrence, N.; and Weinberger,
K. Q., eds., Advances in Neural Information Processing Sys-
tems, volume 27, 2672–2680. Curran Associates, Inc.
Gupta, J. K.; Egorov, M.; and Kochenderfer, M. 2017. Co-
operative Multi-Agent Control using Deep Reinforcement
Learning. In Autonomous Agents and Multiagent Systems,
66–83. Springer.
Jordan, S.; Chandak, Y .; Cohen, D.; Zhang, M.; and Thomas,
P. 2020. Evaluating the Performance of Reinforcement
Learning Algorithms. In III, H. D.; and Singh, A., eds., Pro-
ceedings of the 37th International Conference on Machine
Learning, volume 119 of Proceedings of Machine Learning
Research, 4962–4973. PMLR.
Laurent, G. J.; Matignon, L.; Fort-Piat, L.; et al. 2011. The
world of independent learners is not Markovian. Interna-
tional Journal of Knowledge-based and Intelligent Engi-
neering Systems 15(1): 55–64.Leibo, J. Z.; Zambaldi, V .; Lanctot, M.; Marecki, J.; and
Graepel, T. 2017. Multi-Agent Reinforcement Learning in
Sequential Social Dilemmas. In Proceedings of the 16th
Conference on Autonomous Agents and Multiagent Systems ,
AAMAS ’17, 464–473. International Foundation for Au-
tonomous Agents and Multiagent Systems.
Li, S.; Wu, Y .; Cui, X.; Dong, H.; Fang, F.; and Russell,
S. 2019. Robust Multi-Agent Reinforcement Learning via
Minimax Deep Deterministic Policy Gradient. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence, vol-
ume 33, 4213–4220.
Littman, M. L. 1994. Markov Games as a Framework for
Multi-Agent Reinforcement Learning. In Machine Learning
Proceedings 1994, 157–163. Elsevier.
Lowe, R.; Wu, Y .; Tamar, A.; Harb, J.; Abbeel, P.; and
Mordatch, I. 2017. Multi-Agent Actor-Critic for Mixed
Cooperative-Competitive Environments. In Advances in
Neural Information Processing Systems, 6379–6390.
Mnih, V .; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,
J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland,
A. K.; Ostrovski, G.; et al. 2015. Human-Level Control
through Deep Reinforcement Learning. Nature 518(7540):
529–533.
Morimoto, J.; and Doya, K. 2001. Robust Reinforcement
Learning. In Advances in Neural Information Processing
Systems, 1061–1067.
Panait, L.; and Luke, S. 2005. Cooperative Multi-Agent
Learning: The State of the Art. Autonomous Agents and
Multiagent Systems 11(3): 387–434.
Phan, T.; Belzner, L.; Gabor, T.; and Schmid, K. 2018.
Leveraging Statistical Multi-Agent Online Planning with
Emergent Value Function Approximation. In Proceedings
of the 17th International Conference on Autonomous Agents
and Multiagent Systems, AAMAS ’18, 730–738. Interna-
tional Foundation for Autonomous Agents and Multiagent
Systems.
Phan, T.; Gabor, T.; Sedlmeier, A.; Ritz, F.; Kempter, B.;
Klein, C.; Sauer, H.; Schmid, R.; Wieghardt, J.; Zeller, M.;
et al. 2020. Learning and Testing Resilience in Coopera-
tive Multi-Agent Systems. In Proceedings of the 19th In-
ternational Conference on Autonomous Agents and Multia-
gent Systems, AAMAS ’20, 1055–1063. International Foun-
dation for Autonomous Agents and Multiagent Systems.
Pinto, L.; Davidson, J.; Sukthankar, R.; and Gupta, A. 2017.
Robust Adversarial Reinforcement Learning. In Precup, D.;
and Teh, Y . W., eds., Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceed-
ings of Machine Learning Research, 2817–2826. PMLR.
Powers, R.; Shoham, Y .; and Vu, T. 2007. A General Crite-
rion and an Algorithmic Framework for Learning in Multi-
Agent Systems. Machine Learning 67(1-2): 45–76.
Rajeswaran, A.; Ghotra, S.; Ravindran, B.; and Levine, S.
2017. EPOpt: Learning Robust Neural Network Policies
using Model Ensembles. In International Conference on
Learning Representations.
11315
Rashid, T.; Samvelyan, M.; de Witt, C. S.; Farquhar, G.; Fo-
erster, J.; and Whiteson, S. 2018. QMIX: Monotonic Value
Function Factorisation for Deep Multi-Agent Reinforcement
Learning. In Dy, J.; and Krause, A., eds., Proceedings of
the 35th International Conference on Machine Learning ,
volume 80 of Proceedings of Machine Learning Research,
4295–4304. PMLR.
Samuel, A. L. 1959. Some Studies in Machine Learning
using the Game of Checkers. IBM Journal of research and
development 3(3): 210–229.
Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.;
Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.;
Panneershelvam, V .; Lanctot, M.; et al. 2016. Mastering the
Game of Go with Deep Neural Networks and Tree Search.
Nature 529(7587): 484–489.
Son, K.; Kim, D.; Kang, W. J.; Hostallero, D. E.; and Yi,
Y . 2019. QTRAN: Learning to Factorize with Transforma-
tion for Cooperative Multi-Agent Reinforcement Learning.
In Chaudhuri, K.; and Salakhutdinov, R., eds., Proceedings
of the 36th International Conference on Machine Learning ,
volume 97 of Proceedings of Machine Learning Research,
5887–5896. PMLR.
Stone, P.; and Veloso, M. 2000. Multiagent Systems: A
Survey from a Machine Learning Perspective. Autonomous
Robots 8(3): 345–383.
Sunehag, P.; Lever, G.; Gruslys, A.; Czarnecki, W. M.; Zam-
baldi, V .; Jaderberg, M.; Lanctot, M.; Sonnerat, N.; Leibo,
J. Z.; Tuyls, K.; et al. 2018. Value-Decomposition Networks
for Cooperative Multi-Agent Learning based on Team Re-
ward. In Proceedings of the 17th International Conference
on Autonomous Agents and Multiagent Systems (Extended
Abstract), AAMAS ’18, 2085–2087. International Founda-
tion for Autonomous Agents and Multiagent Systems.
Sutton, R. S.; McAllester, D. A.; Singh, S. P.; and Mansour,
Y . 2000. Policy Gradient Methods for Reinforcement Learn-
ing with Function Approximation. In Solla, S.; Leen, T.; and
M¨uller, K., eds., Advances in Neural Information Processing
Systems, volume 12, 1057–1063. MIT Press.
Tan, M. 1993. Multi-Agent Reinforcement Learning: In-
dependent versus Cooperative Agents. In Proceedings of
the Tenth International Conference on International Con-
ference on Machine Learning, 330–337. Morgan Kaufmann
Publishers Inc.
Tanenbaum, A. S.; and Van Steen, M. 2007. Distributed
Systems: Principles and Paradigms. Prentice-Hall.
Tesauro, G. 1995. Temporal Difference Learning and TD-
Gammon. Communications of the ACM 38(3): 58–69.
Uesato, J.; Kumar, A.; Szepesvari, C.; Erez, T.; Ruderman,
A.; Anderson, K.; Heess, N.; Kohli, P.; et al. 2019. Rigor-
ous Agent Evaluation: An Adversarial Approach to Uncover
Catastrophic Failures. International Conference on Learn-
ing Representations .
Wang, R.; Lehman, J.; Clune, J.; and Stanley, K. O.
2019. POET: Open-Ended Coevolution of Environmentsand their Optimized Solutions. In Proceedings of the Ge-
netic and Evolutionary Computation Conference, GECCO
’19, 142–151. Association for Computing Machinery.
Watkins, C. J.; and Dayan, P. 1992. Q-Learning. Machine
Learning 8(3-4): 279–292.
11316"
2108.03803v2.pdf,"Mis-spoke or mis-lead: Achieving Robustness in Multi-Agent
Communicative Reinforcement Learning
Wanqi Xue
Nanyang Technological University
Singapore
wanqi001@e.ntu.edu.sgWei Qiu∗
Nanyang Technological University
Singapore
qiuw0008@e.ntu.edu.sgBo An
Nanyang Technological University
Singapore
boan@ntu.edu.sg
Zinovi Rabinovich
Nanyang Technological University
Singapore
zinovi@ntu.edu.sgSvetlana Obraztsova
Nanyang Technological University
Singapore
lana@ntu.edu.sgChai Kiat Yeo
Nanyang Technological University
Singapore
asckyeo@ntu.edu.sg
ABSTRACT
Recent studies in multi-agent communicative reinforcement learn-
ing (MACRL) have demonstrated that multi-agent coordination can
be greatly improved by allowing communication between agents.
Meanwhile, adversarial machine learning (ML) has shown that ML
models are vulnerable to attacks. Despite the increasing concern
about the robustness of ML algorithms, how to achieve robust com-
munication in multi-agent reinforcement learning has been largely
neglected. In this paper, we systematically explore the problem
of adversarial communication in MACRL. Our main contributions
are threefold. First, we propose an effective method to perform
attacks in MACRL, by learning a model to generate optimal ma-
licious messages. Second, we develop a defence method based on
message reconstruction, to maintain multi-agent coordination un-
der message attacks. Third, we formulate the adversarial commu-
nication problem as a two-player zero-sum game and propose a
game-theoretical method ℜ-MACRL to improve the worst-case
defending performance. Empirical results demonstrate that many
state-of-the-art MACRL methods are vulnerable to message attacks,
and our method can significantly improve their robustness.
KEYWORDS
Multi-Agent Reinforcement Learning; Robust Reinforcement Learn-
ing; Adversarial Reinforcement Learning
ACM Reference Format:
Wanqi Xue, Wei Qiu∗, Bo An, Zinovi Rabinovich, Svetlana Obraztsova,
and Chai Kiat Yeo. 2022. Mis-spoke or mis-lead: Achieving Robustness in
Multi-Agent Communicative Reinforcement Learning. In Proc. of the 21st
International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2022), Online, May 9–13, 2022 , IFAAMAS, 12 pages.
1 INTRODUCTION
Cooperative multi-agent reinforcement learning (MARL) has achieved
remarkable success in a variety of challenging problems, such as ur-
ban systems [ 31], coordination of robot swarms [ 11] and real-time
strategy video games [ 38]. To tackle the problems of scalability and
non-stationarity in MARL, the framework of centralized training
∗Corresponding author.
Proc. of the 21st International Conference on Autonomous Agents and Multiagent Systems
(AAMAS 2022), P. Faliszewski, V. Mascardi, C. Pelachaud, M.E. Taylor (eds.), May 9–13,
2022, Online .©2022 International Foundation for Autonomous Agents and Multiagent
Systems (www.ifaamas.org). All rights reserved.with decentralized execution (CTDE) is proposed [ 18,25], where de-
centralized policies are learned in a centralized manner so that they
can share experiences, parameters, etc., during training. Despite
the advantages, CTDE-based methods still perform unsatisfactorily
in scenarios where multi-agent coordination is necessary, mainly
due to partial observability in decentralized execution. To mitigate
partial observability, many multi-agent communicative reinforce-
ment learning (MACRL) methods have been proposed, which allow
agents to exchange information such as private observations, inten-
tions during the execution phase. MACRL greatly improves multi-
agent coordination in a wide range of tasks [ 6,7,13,15,32,39,40].
Meanwhile, adversarial machine learning has received extensive
attention [ 2,10]. Adversarial machine learning demonstrates that
machine learning (ML) models are vulnerable to manipulation [ 8,9].
As a result, ML models often suffer from performance degradation
when under attack. For the same reason, many practical applications
of ML models are at high risk. For instance, researchers have shown
that, by placing a few small stickers on the ground at an intersection,
self-driving cars can be tricked into making abnormal judgements
and driving into the opposite lane [ 34]. Maliciously designed attacks
on ML models can have serious consequences.
Unfortunately, despite great importance, adversarial problems,
especially adversarial inter-agent communication problems, remain
largely uninvestigated in MACRL. Blumenkamp et al. show that,
by introducing random noise in communication, agents are able
to deceive their opponents in competitive games [ 3]. However,
the attacks are not artificially designed and therefore inefficient.
Besides, cooperative cases, where communication is more crucial,
are neglected. They also fail to propose an effective defence, but
merely retrain the models to adapt to the attacks. Mitchell et al.
propose to generate weights of Attention models [ 37] through
Gaussian process for defending against random attacks in attention-
based MACRL [ 22]. However, the applicability of this approach is
unsatisfactory, being limited to attention-based MACRL, and its
performance on maliciously designed attacks is unclear.
In this paper, we systematically explore the problem of adversar-
ial communication in MACRL, where there are malicious agents
that attempt to disrupt multi-agent cooperation by manipulating
messages. Our contributions are in three aspects. First, we pro-
pose an effective learning approach to model the optimal attacking
scheme in MACRL. Second, to defend against the adversary, we
propose a two-stage message filter which works by first detectingarXiv:2108.03803v2  [cs.LG]  26 Jan 2022
the malicious message and then recovering the message. The de-
fending scheme can greatly maintain the coordination of agents
under message attacks. Third, to address the problem that the mes-
sage filter can be exploited by learnable attackers, we formulate the
attack-defense problem as a two-player zero-sum game and propose
ℜ-MACRL, based on a game-theoretical framework Policy-Space
Response Oracle (PSRO) [ 19,23], to approximate a Nash equilib-
rium policy in the adversarial communication game. ℜ-MACRL
improves the defensive performance under the worst case and thus
improves the robustness. Empirical experiments demonstrate that
many state-of-the-art MACRL algorithms are vulnerable to attacks
and our method can significantly improve their robustness.
2 PRELIMINARIES AND RELATED WORK
Multi-Agent Communicative Reinforcement Learning. There
has been extensive research on encouraging communication be-
tween agents to improve performance on cooperative or compet-
itive tasks. Among the recent advances, some design communi-
cation mechanisms to address the problem of when to commu-
nicate [ 13,14,30]; other lines of works, e.g., TarMac [ 6], focus
on who to communicate. These works determine the two funda-
mental elements in communication, i.e., the message sender and
the receiver. Apart from the two elements, the message itself is
another element which is crucial in communication, i.e., what to
communicate: Jaques et al. propose to maximize the social influ-
ence of messages [ 12]. Kim et al. encode messages such that they
contain the intention of agents [ 15]. Some other works learn to
send succinct messages to meet the limitations of communication
bandwidth [ 39,40,44]. Despite significant progress in MACRL, if
some agents are adversarial and send maliciously designed mes-
sages, multi-agent coordination will rapidly disintegrate as these
messages propagate.
Adversarial Training. Adversarial training is a prevalent para-
digm for training robust models to defend against potential at-
tacks [ 9,33]. Recent literature has considered two types of at-
tacks [ 5,26,35]: black-box attack and white-box attack. In black-box
attack, the attacker does not have access to information about the
attacked deep neural network (DNN) model; whereas in white-box
attack, the attacker has complete knowledge, e.g., the architecture,
the parameters and potential defense mechanisms, about the DNN
model. We consider the black-box attack in our problem formula-
tion, because the setting of the white-box attack is too idealistic
and may not be applicable to many realistic adversarial scenar-
ios. In adversarial training, the attacker tries to attack a DNN by
corrupting the input via ℓ𝑝-norm (𝑝∈{1,2,∞}) attack [ 9]. The
attacker carefully generates artificial perturbations to manipulate
the input of the model. In doing so, the DNN will be fooled into
making incorrect predictions or decisions. The attacker finds the
optimal perturbation 𝛿by optimizing:
max
𝛿Lpredict(𝑓(𝒙),𝑓(𝒙+𝛿))s.t. minLnorm(𝒙,𝒙+𝛿)
where 𝒙is the input, 𝑓is the DNN model, Lpredict is a metric to
measure the distance between the outputs of the DNN model w/ and
w/o being attacked, Lnorm is used to measure that for the inputs.
Adversarial Reinforcement Learning (RL). Recent advances in
adversarial machine learning motivate researchers to investigatethe adversarial problem in RL [ 8,20,41]. SA-MDP [ 43] character-
izes the problem of decision making under adversarial attacks on
state observations. Lin et al. propose two tactics of attacks, i.e., the
strategically-timed attack and the enchanting attack, which attack
by injecting noise to states and luring the agent to a designated
target [ 20]. Gleave et al. consider the problem of taking adver-
sarial actions that change the environment and consequentially
change the observation of agents [ 8]. ATLA [ 42] propose to train
the optimal adversary to perturb state observations and improve
the worst-case agent reward. The settings of these works are differ-
ent from ours: we consider the multi-agent scenario and restrict the
attacking approach to adversarial messages, which makes the detec-
tion of anomalies difficult. Tu et al. propose to attack on multi-agent
communication [ 36]. However, their focus is on the representation-
level, whereas we focus on the policy-level. Recently, there are
some works considering a similar setting as ours [ 3,22]. However,
they either focus on random attacks in specific competitive games
or the defence of specific communication methods.
3 ACHIEVING ROBUSTNESS IN MACRL
In this section, we propose our method for achieving robustness in
MACRL. We begin by proposing the problem formulation for adver-
sarial communication in MACRL. Then, we introduce the method to
build the optimal attacking scheme in MACRL. Next, we propose a
learnable two-stage message filter to defend against possible attacks.
Finally, we propose to formulate the attack-defense problem as a
two-player zero-sum game, and design a game-theoretic method
ℜ-MACRL to approximate a Nash equilibrium policy for the de-
fender. In this way, we can improve the worst-case performance of
the defender and thus enhance robustness.
3.1 Problem Formulation: Adversarial
Communication in MACRL
An MACRL problem can be modeled by Decentralised Partially Ob-
servable Markov Decision Process with Communication (Dec-POMDP-
Com) [24], which is defined by a tuple ⟨S,M,A,P,𝑅,Υ,𝑂,𝐶,N,𝛾⟩.
Sdenotes the state of the environment and Mis the message
space. Each agent 𝑖∈N :={1,...,𝑁}chooses an action 𝑎𝑖∈A at
a state 𝒔∈S, giving rise to a joint action vector, 𝒂:=[𝑎𝑖]𝑁
𝑖=1∈
A𝑁.P(𝒔′|𝒔,𝒂):S×A𝑁×S ↦→ P(S) is a Markovian tran-
sition function. Every agent shares the same joint reward func-
tion𝑅(𝒔,𝒂):S×A𝑁↦→R, and𝛾∈ [0,1)is the discount fac-
tor. Due to partial observability, each agent has individual par-
tial observation 𝜐∈Υ, according to the observation function
𝑂(𝒔,𝑖):S×N↦→ Υ.Each agent holds two policies, i.e., action
policy𝑝𝑖(𝑎𝑖|𝜏𝑖,𝑚𝑖𝑛
𝑖):T×M×A↦→[ 0,1]and message policy
𝑣𝑖(𝑚𝑜𝑢𝑡
𝑖|𝜏𝑖,𝑚𝑖𝑛
𝑖):T×M×M↦→[ 0,1], both of which are condi-
tioned on the action-observation history 𝜏𝑖∈T :=(Υ×A) and
incoming messages 𝑚𝑖𝑛
𝑖aggregated by the communication protocol
𝐶(𝑚𝑖𝑛
𝑖|𝒎𝑜𝑢𝑡,𝑖):M|N|×N×M↦→[ 0,1].
In adversarial communication where there are 𝑁𝑎𝑑𝑣malicious
agents, we assume that each malicious agent holds the third private
adversarial policy, 𝜉(𝛿𝑎𝑑𝑣
𝑖|𝜏𝑖,𝑚𝑖𝑛
𝑖,𝑚𝑜𝑢𝑡
𝑖):T×M×M×M↦→[ 0,1],
which generates adversarial message 𝛿𝑎𝑑𝑣
𝑖based on its action-
observation history 𝜏𝑖, received messages 𝑚𝑖𝑛
𝑖and the message
!""#""Msg$CommProtocol#%AttackerLossAttacker&%'()*%+,-&%%./0/1/20.10.90.93/0/1/20.10.90.76WithoutattackWithattack!""Msg$CommProtocol#%&""%.AttackerLossAttacker&%'()*%+,-&%%./0/1/20.10.90.85/0/1/20.10.90.76WithoutattackWithdefense/0/1/20.10.90.93Withattack
Defender/""3&""%.DefenderLoss(a)(b)/""&""%.45%'()45%'()
#""&%+,-&%+,-Figure 1: General framework of attack (left) and defence (right) in MACRL. Attack: The agent 𝑖, taken over by the attacker,
generates malicious message 𝑚𝑎𝑑𝑣
𝑖to disrupt multi-agent cooperation. The incoming message 𝑚𝑖𝑛
𝑗and estimated Q-values of
the benign agent 𝑗will be affected due to 𝑚𝑎𝑑𝑣
𝑖, which may lead to incorrect decisions. Defence: A learnable defender is cascaded
to the communication protocol module, which is used to reconstruct the contaminated message ( 𝑚𝑖𝑛
𝑗toˆ𝑚𝑖𝑛
𝑗). The benign agent
𝑗estimates Q-values based on ˆ𝑚𝑖𝑛
𝑗, which can reduce the probability of selecting sub-optimal actions.
𝑚𝑜𝑢𝑡
𝑖intended to be sent. Malicious agents could send messages
by convexly combining their original messages with adversarial
messages, i.e., 𝑚𝑎𝑑𝑣
𝑖=(1−𝜔)×𝑚𝑜𝑢𝑡
𝑖+𝜔×𝛿𝑎𝑑𝑣
𝑖, or simply sum-
ming up the messages, i.e., 𝑚𝑎𝑑𝑣
𝑖=𝑚𝑜𝑢𝑡
𝑖+𝛿𝑎𝑑𝑣
𝑖. To reduce the
likelihood of being detected, apart from the adversarial policy 𝜉,
malicious agents strictly follow their former action policy and mes-
sage policy, trying to behave like benign agents. Fig. 1(a) presents
the overall attacking procedure. The agent 𝑖(attacker) is malicious
and tries to generate adversarial message 𝑚𝑎𝑑𝑣
𝑖to disrupt coopera-
tion. The adversarial message sent by agent 𝑖together with normal
messages sent by other agents (denoted by 𝒎𝑜𝑢𝑡
−𝑖) are processed
by the communication protocol (algorithm-related), generating a
contaminated incoming message 𝑚𝑖𝑛
𝑗for a benign agent 𝑗. From
agent𝑗’s perspective, under such attack, the estimated Q-values
will change. If the action with the highest Q-value shifts, agent
𝑗will make incorrect decisions, leading to suboptimality. To per-
form an effective attack in MACRL, we propose to optimize the
adversarial policy 𝜉by minimizing the joint accumulated rewards,
i.e.,min𝜉EÍ∞
𝑡=0𝛾𝑡𝑟𝑡. We make two assumptions to make the
adversarial communication problem both practical and tractable.
Assumption 1. (Byzantine Failure [ 17]) Agents have imperfect
information on who are malicious.
Assumption 2. (Concealment) Malicious agents do not communi-
cate or cooperate with each other when performing attacks in order
to be covert.
To defend against the attacker, as in Fig. 1(b), we propose to
cascade a learnable defender to the communication protocol mod-
ule. The defender performs a transformation from 𝑚𝑖𝑛
𝑗toˆ𝑚𝑖𝑛
𝑗to
reconstruct the contaminated messages, to avoid distributing the
contaminated message 𝑚𝑖𝑛
𝑗directly to agent 𝑗. With such transfor-
mation, the benign agent 𝑗can estimate the Q-value for each action
more properly and reduce the probability of selecting sub-optimal
actions.3.2 Learning the Attacking Scheme
To model the attack scheme in adversarial communication, we use
a deep neural network (DNN) 𝑓𝜇, parameterized by 𝜃𝜇, to generate
adversarial messages for a malicious agent. The adversarial pol-
icy𝜉is a multivariate Gaussian distribution whose parameters are
determined by the DNN 𝑓𝜇, i.e.,𝜉=N(𝑓𝜇(·|𝜏,𝑚𝑖𝑛,𝑚𝑜𝑢𝑡;𝜃𝑑),Λ)
where Λis a fixed covariance matrix. The reason of using Gaussian
distribution as the prior is that it is the maximum entropy distribu-
tion under constraints of mean and variance. Each malicious agent
generates adversarial messages by sampling from its adversarial
policy. The optimization objective of the adversarial policy is to
minimize the accumulated team rewards subject to a constraint
on the distance between 𝑚𝑜𝑢𝑡and𝑚𝑎𝑑𝑣. We utilize Proximal Pol-
icy Optimization (PPO) [ 29] to optimize the adversarial policy by
maximizing the following objective:
J𝜉(𝜃𝜇)=E(𝜏,𝑚𝑖𝑛,𝑚𝑜𝑢𝑡,𝛿𝑎𝑑𝑣)[min(𝜌·𝐴𝜉,clip(𝜌,1±𝜖)·𝐴𝜉)]
−𝛼·E(𝑚𝑜𝑢𝑡,𝛿𝑎𝑑𝑣)[(𝑚𝑜𝑢𝑡−𝑚𝑎𝑑𝑣)2](1)
where𝜌=𝜉(𝛿𝑎𝑑𝑣|𝜏,𝑚𝑖𝑛,𝑚𝑜𝑢𝑡)/𝜉𝑜𝑙𝑑(𝛿𝑎𝑑𝑣|𝜏,𝑚𝑖𝑛,𝑚𝑜𝑢𝑡),𝜉𝑜𝑙𝑑is the
policy in the previous learning step, 𝜖is the clipping ratio, and
𝐴𝜉(𝛿𝑎𝑑𝑣,𝜏,𝑚𝑖𝑛,𝑚𝑜𝑢𝑡)is the advantage function. Let 𝑒=⟨𝜏,𝑚𝑖𝑛,𝑚𝑜𝑢𝑡⟩
denotes the input of the value function and we define the reward
of the attacker to be negative team reward, then 𝐴𝜉(𝛿𝑎𝑑𝑣,𝑒)=
−𝑟+𝑉(𝑒′)−𝑉(𝑒)where𝑉is the value function, 𝑒′denotes the
next step state, and 𝑟is the immediate reward. We learn the value
function𝑉(𝑒)by minimizing E[(𝑉(𝑒)+Í
𝑡𝛾𝑡𝑟𝑡)2].
3.3 Defending against Adversarial
Communication
We can train a DNN to model the attack scheme in adversarial
communication. However, the defence of that is non-trivial because
i) benign agents have no prior knowledge on which agents are
malicious; ii) the malicious agents can inconsistently pretend to
be cooperative or non-cooperative to avoid being detected; and iii)
AnomalyDetector𝑚""#$%𝑚&#$%𝑚&'(#$%…ReconstructorMessageAggregator𝑚&)&𝑚"")&𝑚&'()&…CommProtocolFigure 2: Structure of the communication protocol with the
message filter (defender). The anomaly detector and the re-
constructor are designed to determine and recover the po-
tential malicious messages, respectively.
recovering useful information from the contaminated messages is
difficult. To address these challenges, we design a two-stage mes-
sage filter for the communication protocol to perform defence. The
message filter works by first determining the messages that are
likely to be malicious and then recovering the potential malicious
messages before distributing them to the corresponding agents. As
in Fig. 2, the message filter 𝜁(ℎ𝑑,𝑔𝑟)consists of an anomaly detector
ℎ𝑑and a message reconstructor 𝑔𝑟. The anomaly detector, parame-
terized by𝜃𝑑, outputs the probability that each incoming message
needs to be recovered, i.e., ℎ𝑑(·|𝑚𝑜𝑢𝑡
𝑖;𝜃𝑑):M↦→ Ψ, where Ψde-
notes a binomial distribution, 𝑚𝑜𝑢𝑡
𝑖denotes the message sent by
agent𝑖. We perform sampling from the generated distributions to
determine the anomaly messages 𝒙∼ℎ𝑑(·|𝒎𝑜𝑢𝑡;𝜃𝑑)1. Here, 𝒙is
an indicator that records whether a message is predicted to be mali-
cious or not. The predicted malicious messages ¤𝒎are recovered by
the reconstructor 𝑔𝑟(·|¤𝑚𝑖;𝜃𝑟):M↦→M separately. The recovered
message and other normal messages are aggregated and determine
the messages that each agent will receive.
The optimization objective of the message filter is to maximize
the joint accumulated rewards under attack, i.e., max𝜁EÍ∞
𝑡=0𝛾𝑡˜𝑟𝑡|𝜉
,
where ˜𝑟𝑡is the team reward after performing the defending strat-
egy. We utilize PPO to optimize the strategy. To mitigate the local
optima induced by unguided exploration in large message space,
we introduce a regularizer which is optimized under the guidance
of the ground-truth labels of messages (whether they are malicious).
For the reconstructor, we train it by minimizing the distance be-
tween the messages before and after being attacked. To improve the
tolerance of the reconstructor to errors made by the detector, apart
from malicious messages, we also use benign messages as training
data, in which case the reconstructor is an identical mapping. For-
mally, the two-stage message filter is optimized by maximizing the
following function:
J𝜁(𝜃𝑑,𝜃𝑟)=E(𝒎,𝒙)|𝒎|∑︁
𝑖min
𝜌𝑖·𝐴ℎ𝑑
𝑖,clip(𝜌𝑖,1±𝜖)·𝐴ℎ𝑑
𝑖
+𝛽1·E𝒎[ˆ𝒚·log(ℎ𝑑(·|𝒎;𝜃𝑑))]
−𝛽2·E𝒎
(ˆ𝒎−𝑔𝑟(·|𝒎;𝜃𝑟))2
(2)
where𝜌𝑖=ℎ𝑑(𝑥𝑖|𝑚𝑖;𝜃𝑑)/ℎ𝑜𝑙𝑑
𝑑(𝑥𝑖|𝑚𝑖;𝜃𝑑),𝐴ℎ𝑑
𝑖(𝑥𝑖,𝑚𝑖)is the advan-
tages which are estimated similarly as in the attack, ˆ𝒚is the one-hot
1We abuseℎ𝑑(·|𝒎𝑜𝑢𝑡;𝜃𝑑)to represent that messages 𝑚𝑜𝑢𝑡
𝑖∈𝒎𝑜𝑢𝑡are fed intoℎ𝑑
in batch.
AttackerDefender𝜋!!2-312𝜋!""𝜋""!𝜋""""𝜋""#𝜋!#-3-1101...Figure 3: Workflow of ℜ-MACRL.
ground-truth labels of messages, ˆ𝒎is the messages that have not
been attacked. 𝛽1,𝛽2and𝜖are hyperparameters.
3.4 Achieving Robust MACRL
Despite the defensive message filter, the effectiveness of the defence
system can rapidly decrease if malicious agents are aware of the
defender and adjust their attack schemes. To mitigate this, we
formulate the attack-defence problem as a two-player zero-sum
game (malicious agents are controlled by the attacker) and improve
the performance of the defender in the worst case. We define the
adversarial communication game by a tuple ⟨Π,𝑈⟩, where Π=
⟨Π𝜉,Π𝜁⟩is the joint policy space of the players ( Π𝜉andΠ𝜁denote
the policy space of the defender and the attacker respectively),
𝑈:Π↦→R2is the utility function which is used to calculate
utilities for the attacker and the defender given their joint policy
𝜋=⟨𝜋𝜉,𝜋𝜁⟩ ∈Π. The utility of the defender is defined as the
expected team return. The adversarial communication game is zero-
sum, i.e., the utility of the defender 𝑈𝜁(𝜋)must be the negative of
the utility of the attacker 𝑈𝜉(𝜋)for∀𝜋∈Π. The solution concept
of the adversarial communication game is Nash equilibrium (NE)
and we can approach an NE by optimizing the following MaxMin
objective:
J𝜉,𝜁=max
𝜋𝜁∈Π𝜁𝑈𝜁(Br(𝜋𝜁),𝜋𝜁)=max
𝜋𝜁∈Π𝜁min
𝜋𝜉∈Π𝜉Eh∑︁∞
𝑡=0𝛾𝑡˜𝑟𝑡|𝜋𝜉,𝜋𝜁i
(3)
where Br(𝜋𝜁)is the best response policy of the defender, i.e., Br(𝜋𝜁)=
arg min𝜋𝜉𝑈𝜁(𝜋𝜉,𝜋𝜁).
We propose ℜ-MACRL to optimize the objective in the adversar-
ial communication game. ℜ-MACRL is developed based on Policy-
Space Response Oracle (PSRO) [ 19], a deep learning-based exten-
sion of the classic Double Oracle algorithm [ 21]. The workflow of
ℜ-MACRL is depicted in Fig. 3. At each iteration, ℜ-MACRL keeps
a population (set) of policies Π𝑡⊂Π, e.g.,Π𝑡
𝜁=(𝜋11,𝜋12)andΠ𝑡
𝜉=
(𝜋21,𝜋22). We can evaluate the utility table 𝑈(Π𝑡)for the current
iteration and calculate a Nash equilibrium 𝜋𝑡∗=⟨Δ(Π𝑡
𝜉),Δ(Π𝑡
𝜁)⟩
for the game restricted to policies in Π𝑡by, for example, linear pro-
gramming, replicator dynamics, etc. Δdenotes the meta-strategy
which is an arbitrary categorical distribution. Next, we calculate
the best response policy Br(𝜋𝑡
∗,−𝑖)to the NE in this restricted
game for each player 𝑖(player−𝑖denotes the opponent of player 𝑖,
𝑖∈{𝜉,𝜁}), e.g.,𝜋13=Br(𝜋𝑡
∗,𝜉)and𝜋23=Br(𝜋𝑡
∗,𝜁). We extend the
population by adding the best response policies to the policy set:
Algorithm 1: ℜ-MACRL
1Input: initial policy sets for both players Π0, maximum
number of iterations 𝑇;
2Empirically estimate utilities 𝑈(Π0)for each joint policy
𝜋∈Π0;
3Initialize mixed strategies for both players
𝜋0
∗,𝑖=Uniform(Π0
𝑖)for𝑖∈{𝜉,𝜁};
4Initialize number of iterations 𝑡=0;
5while not converge and 𝑡≤𝑇do
6 forplayer𝑖∈{𝜉,𝜁}do
7 Train𝜋𝑡+1
𝑖by letting it exploit 𝜋𝑡
∗,−𝑖;
8 Extend policy set Π𝑡+1
𝑖=Π𝑡
𝑖∪{𝜋𝑡+1
𝑖};
9 Check convergence and 𝑡=𝑡+1;
10 Estimate missing entries in 𝑈(Π𝑡+1);
11 Compute mixed strategies 𝜋𝑡+1∗by solving the matrix
game defined by 𝑈(Π𝑡+1);
12Output: mixed strategies 𝜋∗for both players ;
Π𝑡+1
𝑖=Π𝑡
𝑖∪Br(𝜋𝑡
∗,−𝑖)for𝑖∈{𝜉,𝜁}. After extending the popula-
tion, we complete the utility table 𝑈(Π𝑡+1)and perform the next
iteration. The algorithm converges if the best response policies are
already in the population. Practically, ℜ-MACRL approximates the
utility function 𝑈(·)by having each policy in the population Π𝑡
𝑖
playing with each other policy in Π𝑡
−𝑖and tracking the average util-
ities. The approximation of the best response policies is performed
by maximizingJ𝜉(𝜃𝜇|𝜋𝑡
∗,𝜁)andJ𝜁(𝜃𝑑,𝜃𝑟|𝜋𝑡
∗,𝜉)for the attacker and
the defender, where the full expressions of the two objectives are
described in Eq. 1 and Eq. 2, respectively.
The overall algorithm of ℜ-MACRL is presented in Algorithm 1.
We first initialize the meta-game and the mixed strategies (line 2 and
line 3). Then, at each step, we train the attacker and the defender
alternately by letting them best respond to their corresponding
opponent (line 7). Next, the learned new policy 𝜋𝑡+1
𝑖is added to
the policy set Π𝑡+1
𝑖for the two players respectively (line 8). After
extending the policy set, we can check the convergence (line 9) and
calculate the missing entries in 𝑈(Π𝑡+1)(line 10). Finally, we solve
the new meta-game (line 11) and repeat the iteration.
4 EXPERIMENTS
We conduct extensive experiments on a variety of state-of-the-art
MACRL algorithms to answer: Q1:Are MACRL methods vulnerable
to message attacks and whether the two-stage message filter is able
to consistently recover multi-agent coordination? Q2:Whether ℜ-
MACRL is able to improve the robustness of MACRL algorithms under
message attacks? Q3:Whether our method is able to scale to scenarios
where there are multiple attackers? Q4:Which components contribute
to the performance of the method and how does the proposed method
work? We first categorize existing MACRL algorithms and then
select representative algorithms to perform the evaluation. All ex-
periments are conducted on a server with 8 NVIDIA Tesla V100
GPUs and a 44-core 2.20GHz Intel(R) Xeon(R) E5-2699 v4 CPU.Categories Methods Environments
CD CommNet [32] Predator Prey (PP) [4]
LC TarMAC [6] Traffic Junction (TJ) [32]
CC NDQ [40] StarCraft II (SCII) [28]
Table 1: The chosen algorithms and environments.
4.1 Experimental Setting
MACRL methods are commonly categorized by whether they are
implicit or explicit [ 1,24]. In implicit communication, the actions
of agents influence the observations of the other agents. Whereas
in explicit communication, agents have a separate set of commu-
nication actions and exchange information via communication
actions. In this paper, we focus on explicit communication because
in implicit communication, to carry out an attack, the attacker’s
behaviour is often bizarre, making the attack trivial and easily de-
tectable. We consider the following three realistic types of explicit
communication:
•Communication with delay (CD): Communication in real
world is usually not real-time. We can model this by assum-
ing that it takes one or a few time steps for the messages
being received by the targeted agents [32].
•Local communication (LC): Messages sometimes cannot
be broadcast to all agents due to communication distance
limitations or privacy concerns. Therefore, agents need to
learn to communicate locally, affecting only those agents
within the same communication group [4, 6].
•Communication with cost or limited bandwidth (CC):
Agents should avoid sending redundant messages because
communication in real world is costly and communication
channels often have a limited bandwidth [40, 44].
Following the above taxonomy, we select some representative state-
of-the-art algorithms to perform evaluation2. As in Table 1, we
select CommNet [ 32], TarMAC [ 6] and NDQ [ 40] for CD, LC and
CC respectively. A brief introduction to each of the chosen algo-
rithms is provided in Appendix A. We evaluate in the following
environments, which are similar to those in the paper that first
introduced the algorithms:
Predator Prey (PP). There are 3 predators, trying to catch 6 prey
in a7×7grid. Each predator has local observation of a 3×3sub-
grid centered around it and can move in one of the 4 compass
directions, remain still, or perform catch action. The prey moves
randomly and is caught if at least two nearby predators try to catch
it simultaneously. The predator will obtain a team reward at 10 for
each successful catch and will be punished 𝑝for each losing catch
action. There are two tasks with 𝑝=0and𝑝=−0.5, respectively. A
prey will be removed from the grid after it being caught. An episode
ends after 60 steps or all preys have been caught.
Traffic Junction (TJ). In TJ, there are 𝑁𝑚𝑎𝑥 cars and the aim of
each car is to complete its pre-assigned route. Each car can observe
of a3×3region around it, but is free to communicate with other
cars. The action space for each car at every time step is { gas,brake }.
The reward function is −0.01𝜏+𝑟collision , where𝜏is the number of
time steps since the activation of the car, and 𝑟collision =−10is a
2Sweeping the whole list of algorithms for each category is impossible and unnecessary
since there have been a lot of algorithms proposed and algorithms in each category
usually share common characteristics.
0 100K 200K
Steps203040Test Returnp=0
Attacker
Defender
CommNet
0 100K 200K
Steps102030p=-0.5
Attacker
Defender
CommNetFigure 4: Attack and defence on CommNet.
collision penalty. Cars become available to be sampled and put back
to the environment with new assigned routes once they complete
their routes.
StarCraft II (SCII). We consider SMAC [ 28] combat scenarios
where the enemy units are controlled by StarCraft II built-in AI
(difficulty level is set to hard), and each of the ally units is con-
trolled by a learning agent. The units of the two groups can be
asymmetric. The action space contains no-op ,move[direction] ,
attack[enemy id] , and stop . Agents receive a globally shared
reward which calculates the total damage made to the enemy units
at each time step. We conduct experiments on four SMAC tasks:
3bane_vs_hM, 4bane_vs_hM, 1o_2r_vs_4r and 1o_3r_vs_4r. More
details about the tasks are in Appendix B.
For each of the tasks, we first train the chosen MACRL methods
to obtain the action policy and the message policy for each agent.
After that, we randomly select an agent to be malicious and train its
adversarial policy to examine whether MACRL methods are vulner-
able to message attacks. Then we perform defence by training the
message filter, during which the adversarial policy of the malicious
agent is fixed but keeps working. To show that a single message
filter is brittle and can be easily exploited if the attacker adapts to
it, we freeze the learned message filter and retrain the adversarial
policy. Finally, we integrate the message filter into the framework
ofℜ-MACRL and justify if ℜ-MACRL is helpful to improve the
robustness. All experiments are carried out with five random seeds
and results are shown with a 95% confidence interval.
4.2 Recovering Multi-Agent Coordination
We first evaluate the performance of our attacking method on the
three selected MACRL algorithms. Then we try to recover multi-
agent coordination for the attacked algorithms by applying the
message filter.
CommNet. The experiments for CommNet are conducted on preda-
tor prey (PP) [ 4]. We set the punishment as 𝑝=0and𝑝=−0.5to
create two PP tasks with different difficulties. As in Fig. 4, at the
beginning of the attack, the performance of CommNet does not
have obvious decrease, indicating that injecting random noise into
the message is hard to disrupt agent coordination. As we gradually
train the adversarial policy, there is a significant drop in the test re-
turn, with 40% and 33% decreases in the task of 𝑝=0and𝑝=−0.5,
respectively. Multi-agent cooperation has been severely affected
due to the malicious messages. When the test return decreases to
preset thresholds, i.e., 23 for 𝑝=0and 20 for𝑝=−0.5, we freeze
the adversarial policy network and start to train the message fil-
ter. As shown in the blue curves in Fig 4, with the message filter,Easy Hard
TarMAC 99.9±0.1% 94.9±0.2%
TarMAC w/ 𝜋𝜉87.2±4.68% 88.75±7.29%
TarMAC w/ 𝜋𝜁96.41±1.38%93.23±8.11%
Table 2: Attack and defence on TarMAC.
020K 40K 60K 80K
Steps255075Test Win %3bane_vs_hM
Attacker
Defender
NDQ
0 20K 40K 60K
Steps405060701o_2r_vs_4r
Attacker
Defender
NDQ
Figure 5: Attack and defence on NDQ.
016K32K48K64K80K
Steps2030Test Returnp=0 (CommNet)
016K32K48K64K
Steps2025p=-0.5 (CommNet)
Figure 6: Exploiting the message filter in CommNet.
test return steadily approaches the converged value of CommNet
(red line), indicating that the message filter can effectively recover
multi-agent coordination under attack.
TarMAC. We conduct message attack and defence on the Traffic
Junction (TJ) environment [ 32]. There are two modes in TJ, i.e.,
easy andhard . The easy task has one junction of two one-way
roads on a 7×7grid with𝑁𝑚𝑎𝑥=5. In the hard task, its map has
four junctions of two-way roads on a 18×18grid and𝑁𝑚𝑎𝑥=20.
As shown in Table 2, under attack, the success rate of TarMAC
decreases in both the easy and the hard scenarios, demonstrating
the vulnerability of TarMAC under malicious messages. After that,
we equip TarMAC with the defender, then its performance improves
considerably, demonstrating the merit of the message filter.
NDQ. We further examine the adversarial communication prob-
lem in NQD. We perform evaluation on two StarCraft II Multi-
Agent Challenge (SMAC) [ 28] scenarios, i.e., 3bane _vs_hMand
1o_2r_vs_4r, in which the communication between cooperative
agents is necessary [ 40]. As presented in Fig. 5, under attack, the
test win rate of NDQ decreases dramatically, demonstrating that
NDQ is also vulnerable to message attacks. After applying the mes-
sage filter as the defender, we can find the test win rate quickly
reaches to around 60% in3bane _vs_hMand55% in1o_2r_vs_4r,
demonstrating that, once again, our defence methods succeed in
restoring multi-agent coordination under message attacks.
4.3 Improving Robustness with ℜ-MACRL
We have demonstrated that many state-of-the-art MACRL algo-
rithms are vulnerable to message attacks, and after applying the
Methods Scenarios 𝑢ℜ
𝜁𝑢𝑣𝑛
𝜁
CommNet𝑝=0 41.75±0.00 40.74±0.23
𝑝=−0.5 35.38±1.28 31.91±0.94
TarMACEasy−4.08±0.04−4.24±0.08
Hard−9.33±0.29−9.80±0.08
NDQ3bane_vs_hM 12.36±0.26 11.32±0.15
1o_2r_vs_4r 17.70±0.16 16.33±0.54
Table 3: Expected utilities for the defender trained with ℜ-
MACRL and the vanilla approach.
message filter, multi-agent coordination can be recovered. In this
part, we integrate the message filter into the framework of ℜ-
MACRL and show that the robustness of MACRL algorithms can
be significantly improved with ℜ-MACRL.
Exploiting the message filter. We first show that a single mes-
sage filter is brittle and can be easily exploited if the attacker adapts
to it. We perform experiments on CommNet by freezing the mes-
sage filter and retraining the adversarial policy. As depicted in Fig.
6, the test return of the team gradually decreases as the training
proceeds, with around 30% and 20% decreases in the task of 𝑝=0
and𝑝=−0.5respectively. We also conduct experiments on NDQ
and similar phenomenon is observed (see Appendix D). We con-
clude that even though the designed message filter is able to recover
multi-agent cooperation under message attacks, its performance
can degrade if faced with an adaptive attacker.
Improving robustness. To illustrate the improvement in robust-
ness, we make comparisons between the defender trained by ℜ-
MACRL and the vanilla defending method. For the defender trained
withℜ-MACRL, a population of attack policies are learned together
with the defender, whose policy is also a mixture of sub-policies.
In the vanilla training method, only a single defending policy is
trained for the defender. We use the expected utility value (the ac-
cumulated team return) as the metric to compare the performance
of the defender. Specifically, the larger the expected utility, the bet-
ter the robustness. We denote the expected utility of the defender
trained by ℜ-MACRL as 𝑢ℜ
𝜁and the result for the vanilla one as
𝑢vn
𝜁. As shown in Table 3, ℜ-MACRL consistently outperforms the
vanilla method over all the algorithms and environments. The im-
provement of expected utilities indicates that the defender trained
byℜ-MACRL is more robust. Intuitively, the defender benefits from
exploring a wider range of the policy space with ℜ-MACRL, which
enables the defender to maintain multi-agent coordination when
faced with attacks.
4.4 Scaling to Multiple Attackers
To examine the performance of the method in scenarios where
there is more than one attacker, we conduct experiments on NDQ
in two new challenge tasks: 4bane_vs_hM and 1o_3r_vs_4r. In the
former maps, i.e., 3bane_vs_hM and 1o_2r_vs_4r, there are only
three agents in the team, making multiple attackers unreasonable.
To mitigate this, we create 4bane_vs_hM and 1o_3r_vs_4rbased on
3bane_vs_hM and 1o_2r_vs_4rby increasing one more agent to the
team. We randomly sample two agents to be malicious. According
to our assumptions (agents have no knowledge about who are
0 40K 80K 120K
Steps6080100Test Win %4bane_vs_hM
Attacker
Defender
NDQ
0 40K 80K 120K
Steps204060801o_3r_vs_4r
Attacker
Defender
NDQFigure 7: Multiple attackers: Attack and defence on NDQ.
Methods Scenarios 𝑢ℜ
𝜁𝑢𝑣𝑛
𝜁
NDQ4bane_vs_hM 15.86±0.08 15.50±0.29
1o_3r_vs_4r 18.39±0.80 17.03±0.53
Table 4: Multiple attackers: Expected utilities for the de-
fender trained with ℜ-MACRL and the vanilla approach.
malicious and malicious agents cannot communicate with each
other), we can directly apply the attacking method to each of the
malicious agents. As shown in Fig. 7, the attackers can quickly learn
effective adversarial policies with less learning steps. On the other
hand, the message filter can still successfully recover multi-agent
cooperation under multiple attackers, though it takes much more
learning steps to converge. We further evaluate the effectiveness
ofℜ-MACRL on the two tasks and consistent results are observed:
trained with ℜ-MACRL, the agents can obtain larger expected
utilities. More results on other algorithms and environments are
provided in Appendix D.
4.5 Ablation Study and Analysis
We have shown that by integrating the message filter into the
framework of ℜ-MACRL, the robustness of MACRL algorithms can
be improved. However, the performance of ℜ-MACRL directly is
affected by the message filter. In this part, we will take a deeper
look at how the message filter works.
Which components contribute to the performance? The mes-
sage filter consists of two components: the anomaly detector and
the message reconstructor. Here, we alternatively disable these two
components in the message filter to study how they contribute
to the performance. We run experiments on three scenarios, with
each scenario corresponding to an MACRL algorithm. Following
the former training procedure, we first train the original MACRL
algorithm to obtain the action policy and the message policy; then
we sample an attacker and train its adversarial policy by PPO; next
we freeze the policy of the attacker and train the message filter
(with the anomaly detector or the reconstructor disabled). As in Fig.
8, if we disable the message reconstructor in the message filter and
replace it with a random message generator, after being attacked,
multi-agent coordination is hard to be recovered, demonstrating the
importance of the reconstructor. We further disable the anomaly
detector and randomly choose an agent to reconstruct its message,
as in Fig. 9, the defending performance is also poor. The ablation
illustrates that both the anomaly detector and the reconstructor are
critical in the message filter.
0 40K 80K 120K
Steps2030Test Returnp=-0.5 (CommNet)
0 200K 400K 600K
Steps8090Test Success %Hard (TarMAC)
0 20K 40K 60K 80K
Steps050Test Won %3bane_vs_hM (NDQ)Attacker DefenderFigure 8: Defending by the message filter with the disabled message reconstructor.
0 40K 80K 120K
Steps2030T est Returnp=-0.5 (CommNet)
0 200K 400K 600K
Steps8090T est Success %Hard (T arMAC)
0 20K 40K 60K 80K
Steps050T est Win %3bane_vs_hM (NDQ)Attacker Defender
Figure 9: Defending by the message filter with the disabled anomaly detector.
(a)Qvalues(w/oattack)(b)Qvalues(w/attack)(c)Qvalues(w/defence)1.750.981.750.991.760.97
(d)Messageattack(e)Reconstructedmessages=
⨁
Figure 10: Q values and received messages under attack and defence. (a)-(c): Under attack, the optimal action of the benign
agent 1shifts from 𝑎3to𝑎1. After applying the message filter, its optimal action is recovered. (d)-(e): The attacked messages
become quite different from the original messages, while the reconstructed messages are to similar the original.
How does the proposed method work? Now we take a deeper
look at how the message filter works. We conduct experiments
on NDQ in the 3bane _vs_hMtask. There are three agents in the
team, of which agent 0is the attacker and the others, namely agent
1and agent 2, are benign agents. As shown in Fig. 10 (a)-(c), the
action space of each agent contains eight elements. White cells
in Q values correspond to illegal actions at the current state, and
the action with red Q value is optimal, e.g., 𝑎5of agent 2. When
attacked by the agent 0, the optimal action of agent 1shifts from𝑎3
to𝑎1, leading to sub-optimality. After applying the message filter,
the decision of agent 1is corrected to 𝑎3. We further examine the
messages received by agent 1and agent 2. As in Fig. 10 (d)-(e), the
attacked messages are quite different from the original messages,
while after applying the message filter, the distance between the
original messages and the reconstructed messages becomes closer.5 CONCLUSION
In this paper, we systematically examine the problem of adversarial
communication in MACRL. The problem is of importance but has
been largely neglected before. We first provide the formulation of
adversarial communication. Then we propose an effective method
to model message attacks in MACRL. Following that, we design a
two-stage message filter to defend against message attacks. Finally,
to improve robustness, we formulate the adversarial communication
problem as a two-player zero-sum game and propose ℜ-MACRL to
improve the robustness. Experiments on a wide range of algorithms
and tasks show that many state-of-the-art MACRL methods are
vulnerable to message attacks, while our algorithm can consistently
recover multi-agent cooperation and improve the robustness of
MACRL algorithms under message attacks.
REFERENCES
[1]Sanjeevan Ahilan and Peter Dayan. 2021. Correcting Experience Replay for
Multi-Agent Communication. ICLR (2021).
[2]Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. 2010. The
security of machine learning. Machine Learning 81, 2 (2010), 121–148.
[3]Jan Blumenkamp and Amanda Prorok. 2020. The emergence of adversarial
communication in multi-agent reinforcement learning. CoRL (2020).
[4]Wendelin Böhmer, Vitaly Kurin, and Shimon Whiteson. 2020. Deep coordination
graphs. In ICML . 980–991.
[5]Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of
neural networks. In 2017 IEEE Symposium on Security and Privacy (SP) . 39–57.
[6]Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike
Rabbat, and Joelle Pineau. 2019. Tarmac: Targeted multi-agent communication.
InICML . 1538–1546.
[7]Jakob N Foerster, Yannis M Assael, Nando de Freitas, and Shimon Whiteson.
2016. Learning to communicate with Deep multi-agent reinforcement learning.
InNeurIPS . 2145–2153.
[8]Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart
Russell. 2020. Adversarial Policies: Attacking Deep Reinforcement Learning. In
ICLR .
[9]Ian Goodfellow, Nicolas Papernot, Sandy Huang, Yan Duan, Pieter Abbeel, and
Jack Clark. 2017. Attacking machine learning with adversarial examples. OpenAI.
https://blog. openai. com/adversarial-example-research (2017).
[10] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and
J Doug Tygar. 2011. Adversarial machine learning. In Proceedings of the 4th ACM
Workshop on Security and Artificial Intelligence . 43–58.
[11] Maximilian Hüttenrauch, Adrian Šošić, and Gerhard Neumann. 2017. Guided
deep reinforcement learning for swarm systems. arXiv preprint arXiv:1709.06011
(2017).
[12] Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro
Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. 2019. Social influence
as intrinsic motivation for multi-agent deep reinforcement learning. In ICML .
3040–3049.
[13] Jiechuan Jiang and Zongqing Lu. 2018. Learning attentional communication for
multi-agent cooperation. In NeurIPS . 7254–7264.
[14] Daewoo Kim, Sangwoo Moon, David Hostallero, Wan Ju Kang, Taeyoung Lee,
Kyunghwan Son, and Yung Yi. 2019. Learning to schedule communication in
multi-agent reinforcement learning. ICLR (2019).
[15] Woojun Kim, Jongeui Park, and Youngchul Sung. 2021. Communication in multi-
Agent reinforcement learning: Intention Sharing. ICLR (2021).
[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[17] Hubert Kirrmann. 2015. Fault Tolerant Computing in Industrial Automation .
Switzerland: ABB Research Center.
[18] Landon Kraemer and Bikramjit Banerjee. 2016. Multi-agent reinforcement learn-
ing as a rehearsal for decentralized planning. Neurocomputing 190 (2016), 82–94.
[19] Marc Lanctot, Vinicius Zambaldi, Audr ¯unas Gruslys, Angeliki Lazaridou, Karl
Tuyls, Julien Pérolat, David Silver, and Thore Graepel. 2017. A unified game-
theoretic approach to multiagent reinforcement learning. In NeurIPS . 4193–4206.
[20] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu,
and Min Sun. 2017. Tactics of adversarial attack on deep reinforcement learning
agents. In IJCAI . 3756–3762.
[21] H Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. 2003. Planning in
the presence of cost functions controlled by an adversary. In ICML . 536–543.
[22] Rupert Mitchell, Jan Blumenkamp, and Amanda Prorok. 2020. Gaussian Process
Based Message Filtering for Robust Multi-Agent Cooperation in the Presence of
Adversarial Communication. arXiv preprint arXiv:2012.00508 (2020).
[23] Paul Muller, Shayegan Omidshafiei, Mark Rowland, Karl Tuyls, Julien Perolat,
Siqi Liu, Daniel Hennes, Luke Marris, Marc Lanctot, Edward Hughes, et al .2019.
A generalized training approach for multiagent learning. In ICLR .
[24] Frans A Oliehoek, Christopher Amato, et al .2016. A Concise Introduction to
Decentralized POMDPs . Vol. 1. Springer.
[25] Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. 2008. Optimal and
approximate Q-value functions for decentralized POMDPs. JAIR 32 (2008), 289–
353.
[26] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security . 506–519.
[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-
maison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning
Library. In NeurIPS . 8026–8037.
[28] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Far-
quhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr,Jakob Foerster, and Shimon Whiteson. 2019. The StarCraft multi-agent challenge.
CoRR abs/1902.04043 (2019).
[29] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347
(2017).
[30] Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. 2018. Learning when
to communicate at scale in multiagent cooperative and competitive tasks. ICLR
(2018).
[31] Arambam James Singh, Akshat Kumar, and Hoong Chuin Lau. 2020. Hierarchical
Multiagent Reinforcement Learning for Maritime Traffic Management. In AAMAS .
1278–1286.
[32] Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. 2016. Learning multiagent
communication with backpropagation. In NeurIPS . 2252–2260.
[33] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
InICLR .
[34] Keen Security Lab Tencent. 2019. Experimental Security Research of Tesla
Autopilot. (2019). https://keenlab.tencent.com/en/whitepapers/Experimental_
Security_Research_of_Tesla_Autopilot.pdf
[35] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh,
and Patrick McDaniel. 2018. Ensemble adversarial training: attacks and defenses.
InICLR .
[36] James Tu, Tsunhsuan Wang, Jingkang Wang, Sivabalan Manivasagam, Mengye
Ren, and Raquel Urtasun. 2021. Adversarial attacks on multi-agent communica-
tion. arXiv preprint arXiv:2101.06560 (2021).
[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NeurIPS . 5998–6008.
[38] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, An-
drew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds,
Petko Georgiev, et al. 2019. Grandmaster level in StarCraft II using multi-agent
reinforcement learning. Nature 575, 7782 (2019), 350–354.
[39] Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, and Zinovi Rabinovich.
2020. Learning efficient multi-agent communication: An information bottleneck
approach. In ICML . 9908–9918.
[40] Tonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. 2019. Learn-
ing Nearly Decomposable Value Functions Via Communication Minimization. In
ICLR .
[41] Hang Xu, Rundong Wang, Lev Raizman, and Zinovi Rabinovich. 2021. Transfer-
able environment poisoning: Training-time attack on reinforcement learning. In
AAMAS . 1398–1406.
[42] Huan Zhang, Hongge Chen, Duane S Boning, and Cho-Jui Hsieh. 2021. Robust
reinforcement learning on state observations with learned optimal adversary. In
ICLR .
[43] Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning,
and Cho-Jui Hsieh. 2020. Robust deep reinforcement learning against adversarial
perturbations on state observations. In NeurIPS . 21024–21037.
[44] Sai Qian Zhang, Jieyu Lin, and Qi Zhang. 2020. Succinct and robust multi-agent
communication with temporal message control. arXiv preprint arXiv:2010.14391
(2020), 17271–17282.
A INTRODUCTION TO THE SELECTED
MACRL ALGORITHMS
In this section, we introduce the selected MACRL algorithms for
evaluation:
CommNet [32] is a simple yet effective multi-agent communica-
tive algorithm where incoming messages of each agent are gener-
ated by averaging the messages sent by all agents in the last time
step. We present the architecture of CommNet in Fig. 13.
TarMAC [6] extends CommNet by allowing agents to pay atten-
tion to important parts of the incoming messages via Attention [ 37]
network. Concretely, each agent generates signature and query
vectors when sending message. In the message receiving phase, at-
tention weights of incoming messages are calculated by comparing
the similarity between the query vector and the signature vector of
each incoming message. Then a weighted sum over all incoming
messages is performed to determine the message an agent will re-
ceive for decentralized execution. The architecture of TarMAC is
in Fig. 14.
NDQ [40] achieves nearly decomposable Q-functions via communi-
cation minimization. Specifically, it trains the communication model
by maximizing mutual information between agents’ action selec-
tion and communication messages while minimizing the entropy
of messages between agents. Each agent broadcasts messages to all
other agents. The sent messages are sampled from learned distribu-
tions which are optimized by mutual information. The overview of
NDQ is in Fig. 15.
B THE SMAC MAPS
Figure 11: Snapshots of the StarCraft II scenarios. Left:
3bane_vs_hM. Right: 1o_2r_vs_4r.
For the SCII environments, we conduct experiments on the fol-
lowing four maps to examine the attack and defence on NDQ:
3bane_vs_hM: 3 Banelings try to kill a Hydralisk assisted by
a Medivac. 3 Banelings together can just blow up the Hydralisk.
Therefore, to win the game, 3 Banelings ally units should not give
the Hydralisk changes to rest time when the Medivac can restore
its health.
4bane_vs_hM: Similar to 3bane_vs_hM, the main difference in
3bane_vs_hM is there are 4 Banelings.
1o_2r_vs_4r: Ally units consist of 1 Overseer and 2 Roaches.
The Overseer can find 4 Reapers and then notify its teammates to
kill the invading enemy units, the Reapers, in order to win the game.
At the start of an episode, ally units spawn at a random point on
the map while enemy units are initialized at another random point.
Given that only the Overseer knows the position of the enemy unit,
the ability to learn to deliver this message to its teammates is vital
for effectively winning the combat.1o_3r_vs_4r: Similar to 1o_2r_vs_4r, the main difference in
1o_3r_vs_4r is there are 1 Overseer and 3 Roaches.
C HYPERPARAMETERS
We train all the selected MACRL methods with the same hyperpa-
rameters as in the corresponding papers, to reproduce the perfor-
mances. We use Adam [ 16] optimizer with a learning rate of 0.001
to train the attacker model as well as the anomaly detector and
the message reconstructor. We use the default hyperparameters of
Adam optimizer. 𝛼,𝛽1and𝛽2are all 0.001. The clip ratio in PPO is
0.5. The attacker model contains 3 linear layers and each layer has
64 neurons with ReLU activation. Same deep neural networks are
used in the anomaly detector and the message reconstructor. We
use PyTorch [ 27] to implement all the deep neural network models.
D ADDITIONAL RESULTS
We train an adaptive attacker in NDQ to exploit the message filter.
As in Fig. 12, the test won rate decrease significantly, illustrating
that a single message filter is brittle and can easily be exploited.
0 8K 16K 24K 32K
Steps204060Test Win %3bane_vs_hM (NDQ)
08K16K24K32K40K
Steps40601o_2r_vs_4r (NDQ)
Figure 12: Exploiting the message filter in NDQ.
We also conduct experiments of multi attackers on the original
environments. Two agents are selected randomly to be malicious.
As in Table. 5, ℜoutperforms the vanilla method in most of the
tasks, demonstrating the effectiveness of our method.
Methods Scenarios 𝑢ℜ
𝜁𝑢𝑣𝑛
𝜁
CommNet𝑝=0 41 .64±1.90 41.25±1.21
𝑝=−0.5 36.83±1.71 35.27±0.66
TarMACEasy−0.70±0.00−0.80±0.00
Hard−0.90±0.00−1.00±0.00
NDQ3bane_vs_hM 12.14±0.35 11.46±0.23
1o_2r_vs_4r 17.72±0.84 16.77±0.62
Table 5: Multiple attackers: Expected utilities for the de-
fender trained with ℜ-MACRL and the vanilla approach.
Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.mean2 Problem Formulation 33
We consider the setting where we have Magents, all cooperating to maximize reward Rin some 34
environment. We make the simplifying assumption that each agent receives R, independent of their 35
contribution. In this setting, there is no difference between each agent having its own controller, or 36
viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our 37
controller is a large feed-forward neural network that maps inputs for all agents to their actions, each 38
agent occupying a subset of units. A speciﬁc connectivity structure between layers (a) instantiates the 39
broadcast communication channel between agents and (b) propagates the agent state in the manner of 40
an RNN. 41
Because the agents will receive reward, but not necessarily supervision for each action, reinforcement 42
learning is used to maximize expected future reward. We explore two forms of communication within 43
the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and 44
will be treated as such by the reinforcement learning. In the continuous case, the signals passed 45
between agents are no different than hidden states in a neural network; thus credit assignment for the 46
communication can be performed using standard backpropagation (within the outer RL loop). 47
We use policy gradient [ 33] with a state speciﬁc baseline for delivering a gradient to the model. 48
Denote the states in an episode by s(1),. . . ,s (T), and the actions taken at each of those states 49
asa(1),. . . ,a (T), where Tis the length of the episode. The baseline is a scalar function of the 50
states b(s,✓), computed via an extra head on the model producing the action probabilities. Beside 51
maximizing the expected reward with policy gradient, the models are also trained to minimize the 52
distance between the baseline value and actual reward. Thus, after ﬁnishing an episode, we update 53
the model parameters ✓by 54
 ✓=TX
t=12
4@logp(a(t)|s(t),✓)
@✓ TX
i=tr(i) b(s(t),✓)!
 ↵@
@✓ TX
i=tr(i) b(s(t),✓)!23
5.
Here r(t)is reward given at time t, and the hyperparameter ↵is for balancing the reward and the 55
baseline objectives, set to 0.03 in all experiments. 56
3 Model 57
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(ommiting the time 58
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the 59
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping 60
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent. 61
Note that this single controller  encompasses the individual controllers for each agents, as well as 62
the communication between agents. 63
One obvious choice for  is a fully-connected multi-layer neural network, which could extract 64
features hfrom sand use them to predict good actions with our RL framework. This model would 65
allow agents to communicate with each other and share views of the environment. However, it 66
is inﬂexible with respect to the composition and number of agents it controls; cannot deal well 67
with agents joining and leaving the group and even the order of the agents must be ﬁxed. On the 68
other hand, if no communication is used then we can write a={ (s1),. . . , (sJ)}, where  is a 69
per-agent controller applied independently. This communication-free model satisﬁes the ﬂexibility 70
requirements1, but is not able to coordinate their actions. 71
3.1 Controller Structure 72
We now detail the architecture for  that has the modularity of the communication-free model but 73
still allows communication.  is built from modules fi, which take the form of multilayer neural 74
networks. Here i2{0,. . ,K }, where Kis the number of communication layers in the network. 75
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j, 76
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors 77
1Assuming sjincludes the identity of agent j.
22 Problem Formulation 33
We consider the setting where we have Magents, all cooperating to maximize reward Rin some 34
environment. We make the simplifying assumption that each agent receives R, independent of their 35
contribution. In this setting, there is no difference between each agent having its own controller, or 36
viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our 37
controller is a large feed-forward neural network that maps inputs for all agents to their actions, each 38
agent occupying a subset of units. A speciﬁc connectivity structure between layers (a) instantiates the 39
broadcast communication channel between agents and (b) propagates the agent state in the manner of 40
an RNN. 41
Because the agents will receive reward, but not necessarily supervision for each action, reinforcement 42
learning is used to maximize expected future reward. We explore two forms of communication within 43
the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and 44
will be treated as such by the reinforcement learning. In the continuous case, the signals passed 45
between agents are no different than hidden states in a neural network; thus credit assignment for the 46
communication can be performed using standard backpropagation (within the outer RL loop). 47
We use policy gradient [ 33] with a state speciﬁc baseline for delivering a gradient to the model. 48
Denote the states in an episode by s(1),. . . ,s (T), and the actions taken at each of those states 49
asa(1),. . . ,a (T), where Tis the length of the episode. The baseline is a scalar function of the 50
states b(s,✓), computed via an extra head on the model producing the action probabilities. Beside 51
maximizing the expected reward with policy gradient, the models are also trained to minimize the 52
distance between the baseline value and actual reward. Thus, after ﬁnishing an episode, we update 53
the model parameters ✓by 54
 ✓=TX
t=12
4@logp(a(t)|s(t),✓)
@✓ TX
i=tr(i) b(s(t),✓)!
 ↵@
@✓ TX
i=tr(i) b(s(t),✓)!23
5.
Here r(t)is reward given at time t, and the hyperparameter ↵is for balancing the reward and the 55
baseline objectives, set to 0.03 in all experiments. 56
3 Model 57
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(ommiting the time 58
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the 59
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping 60
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent. 61
Note that this single controller  encompasses the individual controllers for each agents, as well as 62
the communication between agents. 63
One obvious choice for  is a fully-connected multi-layer neural network, which could extract 64
features hfrom sand use them to predict good actions with our RL framework. This model would 65
allow agents to communicate with each other and share views of the environment. However, it 66
is inﬂexible with respect to the composition and number of agents it controls; cannot deal well 67
with agents joining and leaving the group and even the order of the agents must be ﬁxed. On the 68
other hand, if no communication is used then we can write a={ (s1),. . . , (sJ)}, where  is a 69
per-agent controller applied independently. This communication-free model satisﬁes the ﬂexibility 70
requirements1, but is not able to coordinate their actions. 71
3.1 Controller Structure 72
We now detail the architecture for  that has the modularity of the communication-free model but 73
still allows communication.  is built from modules fi, which take the form of multilayer neural 74
networks. Here i2{0,. . ,K }, where Kis the number of communication layers in the network. 75
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j, 76
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors 77
1Assuming sjincludes the identity of agent j.
2
h0=[h0
1,h0
2,. . . ,h0
J], and computes: 78
hi+1
j=fi(hi
j,ci
j) (1)
79
ci+1
j=1
J 1X
j06=jhi+1
j0. (2)
In the case that fiis a single linear layer followed by a nonlinearity  , we have: hi+1
j= (Hihi
j+ 80
Cici
j)and the model can be viewed as a feedforward network with layers hi+1= (Tihi)where hi81
is the concatenation of all hi
jandTtakes the block form: 82
Ti=0
BBBB@HiCiCi... Ci
CiHiCi... Ci
CiCiHi... Ci
...............
CiCiCi... Hi1
CCCCA,Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.multilayer  
NN Avg.	2 Problem Formulation 33
We consider the setting where we have Magents, all cooperating to maximize reward Rin some 34
environment. We make the simplifying assumption that each agent receives R, independent of their 35
contribution. In this setting, there is no difference between each agent having its own controller, or 36
viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our 37
controller is a large feed-forward neural network that maps inputs for all agents to their actions, each 38
agent occupying a subset of units. A speciﬁc connectivity structure between layers (a) instantiates the 39
broadcast communication channel between agents and (b) propagates the agent state in the manner of 40
an RNN. 41
Because the agents will receive reward, but not necessarily supervision for each action, reinforcement 42
learning is used to maximize expected future reward. We explore two forms of communication within 43
the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and 44
will be treated as such by the reinforcement learning. In the continuous case, the signals passed 45
between agents are no different than hidden states in a neural network; thus credit assignment for the 46
communication can be performed using standard backpropagation (within the outer RL loop). 47
We use policy gradient [ 33] with a state speciﬁc baseline for delivering a gradient to the model. 48
Denote the states in an episode by s(1),. . . ,s (T), and the actions taken at each of those states 49
asa(1),. . . ,a (T), where Tis the length of the episode. The baseline is a scalar function of the 50
states b(s,✓), computed via an extra head on the model producing the action probabilities. Beside 51
maximizing the expected reward with policy gradient, the models are also trained to minimize the 52
distance between the baseline value and actual reward. Thus, after ﬁnishing an episode, we update 53
the model parameters ✓by 54
 ✓=TX
t=12
4@logp(a(t)|s(t),✓)
@✓ TX
i=tr(i) b(s(t),✓)!
 ↵@
@✓ TX
i=tr(i) b(s(t),✓)!23
5.
Here r(t)is reward given at time t, and the hyperparameter ↵is for balancing the reward and the 55
baseline objectives, set to 0.03 in all experiments. 56
3 Model 57
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(ommiting the time 58
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the 59
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping 60
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent. 61
Note that this single controller  encompasses the individual controllers for each agents, as well as 62
the communication between agents. 63
One obvious choice for  is a fully-connected multi-layer neural network, which could extract 64
features hfrom sand use them to predict good actions with our RL framework. This model would 65
allow agents to communicate with each other and share views of the environment. However, it 66
is inﬂexible with respect to the composition and number of agents it controls; cannot deal well 67
with agents joining and leaving the group and even the order of the agents must be ﬁxed. On the 68
other hand, if no communication is used then we can write a={ (s1),. . . , (sJ)}, where  is a 69
per-agent controller applied independently. This communication-free model satisﬁes the ﬂexibility 70
requirements1, but is not able to coordinate their actions. 71
3.1 Controller Structure 72
We now detail the architecture for  that has the modularity of the communication-free model but 73
still allows communication.  is built from modules fi, which take the form of multilayer neural 74
networks. Here i2{0,. . ,K }, where Kis the number of communication layers in the network. 75
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j, 76
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors 77
1Assuming sjincludes the identity of agent j.
22 Problem Formulation 33
We consider the setting where we have Magents, all cooperating to maximize reward Rin some 34
environment. We make the simplifying assumption that each agent receives R, independent of their 35
contribution. In this setting, there is no difference between each agent having its own controller, or 36
viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our 37
controller is a large feed-forward neural network that maps inputs for all agents to their actions, each 38
agent occupying a subset of units. A speciﬁc connectivity structure between layers (a) instantiates the 39
broadcast communication channel between agents and (b) propagates the agent state in the manner of 40
an RNN. 41
Because the agents will receive reward, but not necessarily supervision for each action, reinforcement 42
learning is used to maximize expected future reward. We explore two forms of communication within 43
the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and 44
will be treated as such by the reinforcement learning. In the continuous case, the signals passed 45
between agents are no different than hidden states in a neural network; thus credit assignment for the 46
communication can be performed using standard backpropagation (within the outer RL loop). 47
We use policy gradient [ 33] with a state speciﬁc baseline for delivering a gradient to the model. 48
Denote the states in an episode by s(1),. . . ,s (T), and the actions taken at each of those states 49
asa(1),. . . ,a (T), where Tis the length of the episode. The baseline is a scalar function of the 50
states b(s,✓), computed via an extra head on the model producing the action probabilities. Beside 51
maximizing the expected reward with policy gradient, the models are also trained to minimize the 52
distance between the baseline value and actual reward. Thus, after ﬁnishing an episode, we update 53
the model parameters ✓by 54
 ✓=TX
t=12
4@logp(a(t)|s(t),✓)
@✓ TX
i=tr(i) b(s(t),✓)!
 ↵@
@✓ TX
i=tr(i) b(s(t),✓)!23
5.
Here r(t)is reward given at time t, and the hyperparameter ↵is for balancing the reward and the 55
baseline objectives, set to 0.03 in all experiments. 56
3 Model 57
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(ommiting the time 58
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the 59
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping 60
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent. 61
Note that this single controller  encompasses the individual controllers for each agents, as well as 62
the communication between agents. 63
One obvious choice for  is a fully-connected multi-layer neural network, which could extract 64
features hfrom sand use them to predict good actions with our RL framework. This model would 65
allow agents to communicate with each other and share views of the environment. However, it 66
is inﬂexible with respect to the composition and number of agents it controls; cannot deal well 67
with agents joining and leaving the group and even the order of the agents must be ﬁxed. On the 68
other hand, if no communication is used then we can write a={ (s1),. . . , (sJ)}, where  is a 69
per-agent controller applied independently. This communication-free model satisﬁes the ﬂexibility 70
requirements1, but is not able to coordinate their actions. 71
3.1 Controller Structure 72
We now detail the architecture for  that has the modularity of the communication-free model but 73
still allows communication.  is built from modules fi, which take the form of multilayer neural 74
networks. Here i2{0,. . ,K }, where Kis the number of communication layers in the network. 75
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j, 76
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors 77
1Assuming sjincludes the identity of agent j.
2Figure 1: Blah.
The key idea is that Tisdynamically sized . First, the number of agents may vary. This motivates 83
the the normalizing factor J 1in equation (2), which resacles the communication vector by the 84
number of communicating agents. Second, the blocks are applied based on category , rather than by 85
coordinate. In this simple form of the model “category” refers to either “self” or “teammate”; but as 86
we will see below, the communication architecture can be more complicated than “broadcast to all”, 87
and so may require more categories. Note also that Tiis permutation invariant, thus the order of the 88
agents does not matter. 89
At the ﬁrst layer of the model an encoder function h0
j=p(sj) is used. This takes as input state-view 90
sjand outputs feature vector h0
j(inRd0for some d0). The form of the encoder is problem dependent, 91
but for most of our tasks they consist of a lookup-table embedding (or bags of vectors thereof). Unless 92
otherwise noted, c0
j=0for all j. 93
At the output of the model, a decoder function q(hK
j)is used to output a distribution over the space of 94
actions. q(.)takes the form of a single layer network, followed by a softmax. To produce a discrete 95
action, we sample from the this distribution. 96
Thus the entire model, which we call a Communication Neural Net (CommNN), (i) takes the state- 97
view of all agents s, passes it through the encoder h0=p(s), (ii) iterates handcin equations (1) 98
and (2) to obain hK, (iii) samples actions afor all agents, according to q(hK). 99
3.2 Model Extensions 100
Local Connectivity: An alternative to the broadcast framework described above is to allow agents 101
to communicate to others within a certain range. Let N(j)be the set of agents present within 102
communication range of agent j. Then (2) becomes: 103
ci+1
j=1
|N(j)|X
j02N(j)hi+1
j0. (3)
3h0=[h0
1,h0
2,. . . ,h0
J], and computes: 78
hi+1
j=fi(hi
j,ci
j) (1)
79
ci+1
j=1
J 1X
j06=jhi+1
j0. (2)
In the case that fiis a single linear layer followed by a nonlinearity  , we have: hi+1
j= (Hihi
j+ 80
Cici
j)and the model can be viewed as a feedforward network with layers hi+1= (Tihi)where hi81
is the concatenation of all hi
jandTtakes the block form: 82
Ti=0
BBBB@HiCiCi... Ci
CiHiCi... Ci
CiCiHi... Ci
...............
CiCiCi... Hi1
CCCCA,Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.multilayer  
NN Avg.	2 Problem Formulation 33
We consider the setting where we have Magents, all cooperating to maximize reward Rin some 34
environment. We make the simplifying assumption that each agent receives R, independent of their 35
contribution. In this setting, there is no difference between each agent having its own controller, or 36
viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our 37
controller is a large feed-forward neural network that maps inputs for all agents to their actions, each 38
agent occupying a subset of units. A speciﬁc connectivity structure between layers (a) instantiates the 39
broadcast communication channel between agents and (b) propagates the agent state in the manner of 40
an RNN. 41
Because the agents will receive reward, but not necessarily supervision for each action, reinforcement 42
learning is used to maximize expected future reward. We explore two forms of communication within 43
the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and 44
will be treated as such by the reinforcement learning. In the continuous case, the signals passed 45
between agents are no different than hidden states in a neural network; thus credit assignment for the 46
communication can be performed using standard backpropagation (within the outer RL loop). 47
We use policy gradient [ 33] with a state speciﬁc baseline for delivering a gradient to the model. 48
Denote the states in an episode by s(1),. . . ,s (T), and the actions taken at each of those states 49
asa(1),. . . ,a (T), where Tis the length of the episode. The baseline is a scalar function of the 50
states b(s,✓), computed via an extra head on the model producing the action probabilities. Beside 51
maximizing the expected reward with policy gradient, the models are also trained to minimize the 52
distance between the baseline value and actual reward. Thus, after ﬁnishing an episode, we update 53
the model parameters ✓by 54
 ✓=TX
t=12
4@logp(a(t)|s(t),✓)
@✓ TX
i=tr(i) b(s(t),✓)!
 ↵@
@✓ TX
i=tr(i) b(s(t),✓)!23
5.
Here r(t)is reward given at time t, and the hyperparameter ↵is for balancing the reward and the 55
baseline objectives, set to 0.03 in all experiments. 56
3 Model 57
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(ommiting the time 58
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the 59
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping 60
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent. 61
Note that this single controller  encompasses the individual controllers for each agents, as well as 62
the communication between agents. 63
One obvious choice for  is a fully-connected multi-layer neural network, which could extract 64
features hfrom sand use them to predict good actions with our RL framework. This model would 65
allow agents to communicate with each other and share views of the environment. However, it 66
is inﬂexible with respect to the composition and number of agents it controls; cannot deal well 67
with agents joining and leaving the group and even the order of the agents must be ﬁxed. On the 68
other hand, if no communication is used then we can write a={ (s1),. . . , (sJ)}, where  is a 69
per-agent controller applied independently. This communication-free model satisﬁes the ﬂexibility 70
requirements1, but is not able to coordinate their actions. 71
3.1 Controller Structure 72
We now detail the architecture for  that has the modularity of the communication-free model but 73
still allows communication.  is built from modules fi, which take the form of multilayer neural 74
networks. Here i2{0,. . ,K }, where Kis the number of communication layers in the network. 75
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j, 76
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors 77
1Assuming sjincludes the identity of agent j.
22 Problem Formulation 33
We consider the setting where we have Magents, all cooperating to maximize reward Rin some 34
environment. We make the simplifying assumption that each agent receives R, independent of their 35
contribution. In this setting, there is no difference between each agent having its own controller, or 36
viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our 37
controller is a large feed-forward neural network that maps inputs for all agents to their actions, each 38
agent occupying a subset of units. A speciﬁc connectivity structure between layers (a) instantiates the 39
broadcast communication channel between agents and (b) propagates the agent state in the manner of 40
an RNN. 41
Because the agents will receive reward, but not necessarily supervision for each action, reinforcement 42
learning is used to maximize expected future reward. We explore two forms of communication within 43
the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and 44
will be treated as such by the reinforcement learning. In the continuous case, the signals passed 45
between agents are no different than hidden states in a neural network; thus credit assignment for the 46
communication can be performed using standard backpropagation (within the outer RL loop). 47
We use policy gradient [ 33] with a state speciﬁc baseline for delivering a gradient to the model. 48
Denote the states in an episode by s(1),. . . ,s (T), and the actions taken at each of those states 49
asa(1),. . . ,a (T), where Tis the length of the episode. The baseline is a scalar function of the 50
states b(s,✓), computed via an extra head on the model producing the action probabilities. Beside 51
maximizing the expected reward with policy gradient, the models are also trained to minimize the 52
distance between the baseline value and actual reward. Thus, after ﬁnishing an episode, we update 53
the model parameters ✓by 54
 ✓=TX
t=12
4@logp(a(t)|s(t),✓)
@✓ TX
i=tr(i) b(s(t),✓)!
 ↵@
@✓ TX
i=tr(i) b(s(t),✓)!23
5.
Here r(t)is reward given at time t, and the hyperparameter ↵is for balancing the reward and the 55
baseline objectives, set to 0.03 in all experiments. 56
3 Model 57
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(ommiting the time 58
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the 59
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping 60
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent. 61
Note that this single controller  encompasses the individual controllers for each agents, as well as 62
the communication between agents. 63
One obvious choice for  is a fully-connected multi-layer neural network, which could extract 64
features hfrom sand use them to predict good actions with our RL framework. This model would 65
allow agents to communicate with each other and share views of the environment. However, it 66
is inﬂexible with respect to the composition and number of agents it controls; cannot deal well 67
with agents joining and leaving the group and even the order of the agents must be ﬁxed. On the 68
other hand, if no communication is used then we can write a={ (s1),. . . , (sJ)}, where  is a 69
per-agent controller applied independently. This communication-free model satisﬁes the ﬂexibility 70
requirements1, but is not able to coordinate their actions. 71
3.1 Controller Structure 72
We now detail the architecture for  that has the modularity of the communication-free model but 73
still allows communication.  is built from modules fi, which take the form of multilayer neural 74
networks. Here i2{0,. . ,K }, where Kis the number of communication layers in the network. 75
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j, 76
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors 77
1Assuming sjincludes the identity of agent j.
2Figure 1: Blah.
The key idea is that Tisdynamically sized . First, the number of agents may vary. This motivates 83
the the normalizing factor J 1in equation (2), which resacles the communication vector by the 84
number of communicating agents. Second, the blocks are applied based on category , rather than by 85
coordinate. In this simple form of the model “category” refers to either “self” or “teammate”; but as 86
we will see below, the communication architecture can be more complicated than “broadcast to all”, 87
and so may require more categories. Note also that Tiis permutation invariant, thus the order of the 88
agents does not matter. 89
At the ﬁrst layer of the model an encoder function h0
j=p(sj) is used. This takes as input state-view 90
sjand outputs feature vector h0
j(inRd0for some d0). The form of the encoder is problem dependent, 91
but for most of our tasks they consist of a lookup-table embedding (or bags of vectors thereof). Unless 92
otherwise noted, c0
j=0 for all j. 93
At the output of the model, a decoder function q(hK
j)is used to output a distribution over the space of 94
actions. q(.)takes the form of a single layer network, followed by a softmax. To produce a discrete 95
action, we sample from the this distribution. 96
Thus the entire model, which we call a Communication Neural Net (CommNN), (i) takes the state- 97
view of all agents s, passes it through the encoder h0=p(s), (ii) iterates handcin equations (1) 98
and (2) to obain hK, (iii) samples actions afor all agents, according to q(hK). 99
3.2 Model Extensions 100
Local Connectivity: An alternative to the broadcast framework described above is to allow agents 101
to communicate to others within a certain range. Let N(j)be the set of agents present within 102
communication range of agent j. Then (2) becomes: 103
ci+1
j=1
|N(j)|X
j02N(j)hi+1
j0. (3)
3tanhConnecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.CommNet modelthcommunication step Module for agent
Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti=0
BBBB@AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai1
CCCCA.
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.
their actions, each agent occupying a subset of units. A speciﬁc connectivity structure between layers
(a) instantiates the broadcast communication channel between agents and (b) propagates the agent
state.
3 Communication Model
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(omitting the time
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent.
Note that this single controller  encompasses the individual controllers for each agents, as well as
the communication between agents.
3.1 Controller Structure
We now detail our architecture for  that allows communication without losing modularity.  is built
from modules fi, which take the form of multilayer neural networks. Here i2{0,. . ,K }, where K
is the number of communication steps in the network.
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j,
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors
h0=[h0
1,h0
2,. . . ,h0
J], and computes:
hi+1
j =fi(hi
j,ci
j) (1)
ci+1
j =1
J 1X
j0 6=jhi+1
j0. (2)
In the case that fiis a single linear layer followed by a nonlinearity  , we have: hi+1
j= (Hihi
j+
Cici
j)and the model can be viewed as a feedforward network with layers hi+1= (Tihi)where hi
is the concatenation of all hi
jandTitakes the block form (where ¯Ci=Ci/(J 1)):
Ti=0
BBBBB@Hi¯Ci¯Ci...¯Ci
¯CiHi¯Ci...¯Ci
¯Ci¯CiHi...¯Ci
...............
¯Ci¯Ci¯Ci... Hi1
CCCCCA,Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.mean2 Problem Formulation 33
We consider the setting where we have Magents, all cooperating to maximize reward Rin some 34
environment. We make the simplifying assumption that each agent receives R, independent of their 35
contribution. In this setting, there is no difference between each agent having its own controller, or 36
viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our 37
controller is a large feed-forward neural network that maps inputs for all agents to their actions, each 38
agent occupying a subset of units. A speciﬁc connectivity structure between layers (a) instantiates the 39
broadcast communication channel between agents and (b) propagates the agent state in the manner of 40
an RNN. 41
Because the agents will receive reward, but not necessarily supervision for each action, reinforcement 42
learning is used to maximize expected future reward. We explore two forms of communication within 43
the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and 44
will be treated as such by the reinforcement learning. In the continuous case, the signals passed 45
between agents are no different than hidden states in a neural network; thus credit assignment for the 46
communication can be performed using standard backpropagation (within the outer RL loop). 47
We use policy gradient [ 33] with a state speciﬁc baseline for delivering a gradient to the model. 48
Denote the states in an episode by s(1),. . . ,s (T), and the actions taken at each of those states 49
asa(1),. . . ,a (T), where Tis the length of the episode. The baseline is a scalar function of the 50
states b(s,✓), computed via an extra head on the model producing the action probabilities. Beside 51
maximizing the expected reward with policy gradient, the models are also trained to minimize the 52
distance between the baseline value and actual reward. Thus, after ﬁnishing an episode, we update 53
the model parameters ✓by 54
 ✓=TX
t=1 
  logp(a(t)|s(t),✓)
 ✓ TX
i=tr(i) b(s(t),✓) 
   
 ✓ TX
i=tr(i) b(s(t),✓) 2 
 .
Here r(t)is reward given at time t, and the hyperparameter  is for balancing the reward and the 55
baseline objectives, set to 0.03 in all experiments. 56
3 Model 57
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(ommiting the time 58
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the 59
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping 60
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent. 61
Note that this single controller  encompasses the individual controllers for each agents, as well as 62
the communication between agents. 63
One obvious choice for  is a fully-connected multi-layer neural network, which could extract 64
features hfrom sand use them to predict good actions with our RL framework. This model would 65
allow agents to communicate with each other and share views of the environment. However, it 66
is inﬂexible with respect to the composition and number of agents it controls; cannot deal well 67
with agents joining and leaving the group and even the order of the agents must be ﬁxed. On the 68
other hand, if no communication is used then we can write a={ (s1),. . . , (sJ)}, where  is a 69
per-agent controller applied independently. This communication-free model satisﬁes the ﬂexibility 70
requirements1, but is not able to coordinate their actions. 71
3.1 Controller Structure 72
We now detail the architecture for  that has the modularity of the communication-free model but 73
still allows communication.  is built from modules fi, which take the form of multilayer neural 74
networks. Here i2{0,. . ,K }, where Kis the number of communication layers in the network. 75
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j, 76
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors 77
1Assuming sjincludes the identity of agent j.
22 Problem Formulation 33
We consider the setting where we have Magents, all cooperating to maximize reward Rin some 34
environment. We make the simplifying assumption that each agent receives R, independent of their 35
contribution. In this setting, there is no difference between each agent having its own controller, or 36
viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our 37
controller is a large feed-forward neural network that maps inputs for all agents to their actions, each 38
agent occupying a subset of units. A speciﬁc connectivity structure between layers (a) instantiates the 39
broadcast communication channel between agents and (b) propagates the agent state in the manner of 40
an RNN. 41
Because the agents will receive reward, but not necessarily supervision for each action, reinforcement 42
learning is used to maximize expected future reward. We explore two forms of communication within 43
the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and 44
will be treated as such by the reinforcement learning. In the continuous case, the signals passed 45
between agents are no different than hidden states in a neural network; thus credit assignment for the 46
communication can be performed using standard backpropagation (within the outer RL loop). 47
We use policy gradient [ 33] with a state speciﬁc baseline for delivering a gradient to the model. 48
Denote the states in an episode by s(1),. . . ,s (T), and the actions taken at each of those states 49
asa(1),. . . ,a (T), where Tis the length of the episode. The baseline is a scalar function of the 50
states b(s,✓), computed via an extra head on the model producing the action probabilities. Beside 51
maximizing the expected reward with policy gradient, the models are also trained to minimize the 52
distance between the baseline value and actual reward. Thus, after ﬁnishing an episode, we update 53
the model parameters ✓by 54
 ✓=TX
t=1 
  logp(a(t)|s(t),✓)
 ✓ TX
i=tr(i) b(s(t),✓) 
   
 ✓ TX
i=tr(i) b(s(t),✓) 2 
 .
Here r(t)is reward given at time t, and the hyperparameter  is for balancing the reward and the 55
baseline objectives, set to 0.03 in all experiments. 56
3 Model 57
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(ommiting the time 58
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the 59
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping 60
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent. 61
Note that this single controller  encompasses the individual controllers for each agents, as well as 62
the communication between agents. 63
One obvious choice for  is a fully-connected multi-layer neural network, which could extract 64
features hfrom sand use them to predict good actions with our RL framework. This model would 65
allow agents to communicate with each other and share views of the environment. However, it 66
is inﬂexible with respect to the composition and number of agents it controls; cannot deal well 67
with agents joining and leaving the group and even the order of the agents must be ﬁxed. On the 68
other hand, if no communication is used then we can write a={ (s1),. . . , (sJ)}, where  is a 69
per-agent controller applied independently. This communication-free model satisﬁes the ﬂexibility 70
requirements1, but is not able to coordinate their actions. 71
3.1 Controller Structure 72
We now detail the architecture for  that has the modularity of the communication-free model but 73
still allows communication.  is built from modules fi, which take the form of multilayer neural 74
networks. Here i2{0,. . ,K }, where Kis the number of communication layers in the network. 75
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j, 76
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors 77
1Assuming sjincludes the identity of agent j.
2
h0=[h0
1,h0
2,. . . ,h0
J], and computes: 78
hi+1
j=fi(hi
j,ci
j) (1)
79
ci+1
j=1
J 1X
j06=jhi+1
j0. (2)
In the case that fiis a single linear layer followed by a nonlinearity  , we have: hi+1
j= (Hihi
j+ 80
Cici
j)and the model can be viewed as a feedforward network with layers hi+1= (Tihi)where hi81
is the concatenation of all hi
jandTtakes the block form: 82
Ti= 
     HiCiCi... Ci
CiHiCi... Ci
CiCiHi... Ci
...............
CiCiCi... Hi 
     ,Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.multilayer  
NN Avg.	2 Problem Formulation 33
We consider the setting where we have Magents, all cooperating to maximize reward Rin some 34
environment. We make the simplifying assumption that each agent receives R, independent of their 35
contribution. In this setting, there is no difference between each agent having its own controller, or 36
viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our 37
controller is a large feed-forward neural network that maps inputs for all agents to their actions, each 38
agent occupying a subset of units. A speciﬁc connectivity structure between layers (a) instantiates the 39
broadcast communication channel between agents and (b) propagates the agent state in the manner of 40
an RNN. 41
Because the agents will receive reward, but not necessarily supervision for each action, reinforcement 42
learning is used to maximize expected future reward. We explore two forms of communication within 43
the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and 44
will be treated as such by the reinforcement learning. In the continuous case, the signals passed 45
between agents are no different than hidden states in a neural network; thus credit assignment for the 46
communication can be performed using standard backpropagation (within the outer RL loop). 47
We use policy gradient [ 33] with a state speciﬁc baseline for delivering a gradient to the model. 48
Denote the states in an episode by s(1),. . . ,s (T), and the actions taken at each of those states 49
asa(1),. . . ,a (T), where Tis the length of the episode. The baseline is a scalar function of the 50
states b(s,✓), computed via an extra head on the model producing the action probabilities. Beside 51
maximizing the expected reward with policy gradient, the models are also trained to minimize the 52
distance between the baseline value and actual reward. Thus, after ﬁnishing an episode, we update 53
the model parameters ✓by 54
 ✓=TX
t=1 
  logp(a(t)|s(t),✓)
 ✓ TX
i=tr(i) b(s(t),✓) 
   
 ✓ TX
i=tr(i) b(s(t),✓) 2 
 .
Here r(t)is reward given at time t, and the hyperparameter  is for balancing the reward and the 55
baseline objectives, set to 0.03 in all experiments. 56
3 Model 57
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(ommiting the time 58
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the 59
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping 60
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent. 61
Note that this single controller  encompasses the individual controllers for each agents, as well as 62
the communication between agents. 63
One obvious choice for  is a fully-connected multi-layer neural network, which could extract 64
features hfrom sand use them to predict good actions with our RL framework. This model would 65
allow agents to communicate with each other and share views of the environment. However, it 66
is inﬂexible with respect to the composition and number of agents it controls; cannot deal well 67
with agents joining and leaving the group and even the order of the agents must be ﬁxed. On the 68
other hand, if no communication is used then we can write a={ (s1),. . . , (sJ)}, where  is a 69
per-agent controller applied independently. This communication-free model satisﬁes the ﬂexibility 70
requirements1, but is not able to coordinate their actions. 71
3.1 Controller Structure 72
We now detail the architecture for  that has the modularity of the communication-free model but 73
still allows communication.  is built from modules fi, which take the form of multilayer neural 74
networks. Here i2{0,. . ,K }, where Kis the number of communication layers in the network. 75
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j, 76
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors 77
1Assuming sjincludes the identity of agent j.
22 Problem Formulation 33
We consider the setting where we have Magents, all cooperating to maximize reward Rin some 34
environment. We make the simplifying assumption that each agent receives R, independent of their 35
contribution. In this setting, there is no difference between each agent having its own controller, or 36
viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our 37
controller is a large feed-forward neural network that maps inputs for all agents to their actions, each 38
agent occupying a subset of units. A speciﬁc connectivity structure between layers (a) instantiates the 39
broadcast communication channel between agents and (b) propagates the agent state in the manner of 40
an RNN. 41
Because the agents will receive reward, but not necessarily supervision for each action, reinforcement 42
learning is used to maximize expected future reward. We explore two forms of communication within 43
the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and 44
will be treated as such by the reinforcement learning. In the continuous case, the signals passed 45
between agents are no different than hidden states in a neural network; thus credit assignment for the 46
communication can be performed using standard backpropagation (within the outer RL loop). 47
We use policy gradient [ 33] with a state speciﬁc baseline for delivering a gradient to the model. 48
Denote the states in an episode by s(1),. . . ,s (T), and the actions taken at each of those states 49
asa(1),. . . ,a (T), where Tis the length of the episode. The baseline is a scalar function of the 50
states b(s,✓), computed via an extra head on the model producing the action probabilities. Beside 51
maximizing the expected reward with policy gradient, the models are also trained to minimize the 52
distance between the baseline value and actual reward. Thus, after ﬁnishing an episode, we update 53
the model parameters ✓by 54
 ✓=TX
t=1 
  logp(a(t)|s(t),✓)
 ✓ TX
i=tr(i) b(s(t),✓) 
   
 ✓ TX
i=tr(i) b(s(t),✓) 2 
 .
Here r(t)is reward given at time t, and the hyperparameter  is for balancing the reward and the 55
baseline objectives, set to 0.03 in all experiments. 56
3 Model 57
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(ommiting the time 58
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the 59
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping 60
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent. 61
Note that this single controller  encompasses the individual controllers for each agents, as well as 62
the communication between agents. 63
One obvious choice for  is a fully-connected multi-layer neural network, which could extract 64
features hfrom sand use them to predict good actions with our RL framework. This model would 65
allow agents to communicate with each other and share views of the environment. However, it 66
is inﬂexible with respect to the composition and number of agents it controls; cannot deal well 67
with agents joining and leaving the group and even the order of the agents must be ﬁxed. On the 68
other hand, if no communication is used then we can write a={ (s1),. . . , (sJ)}, where  is a 69
per-agent controller applied independently. This communication-free model satisﬁes the ﬂexibility 70
requirements1, but is not able to coordinate their actions. 71
3.1 Controller Structure 72
We now detail the architecture for  that has the modularity of the communication-free model but 73
still allows communication.  is built from modules fi, which take the form of multilayer neural 74
networks. Here i2{0,. . ,K }, where Kis the number of communication layers in the network. 75
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j, 76
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors 77
1Assuming sjincludes the identity of agent j.
2Figure 1: Blah.
The key idea is that Tisdynamically sized . First, the number of agents may vary. This motivates 83
the the normalizing factor J 1in equation (2), which resacles the communication vector by the 84
number of communicating agents. Second, the blocks are applied based on category , rather than by 85
coordinate. In this simple form of the model “category” refers to either “self” or “teammate”; but as 86
we will see below, the communication architecture can be more complicated than “broadcast to all”, 87
and so may require more categories. Note also that Tiis permutation invariant, thus the order of the 88
agents does not matter. 89
At the ﬁrst layer of the model an encoder function h0
j=p(sj) is used. This takes as input state-view 90
sjand outputs feature vector h0
j(inRd0for some d0). The form of the encoder is problem dependent, 91
but for most of our tasks they consist of a lookup-table embedding (or bags of vectors thereof). Unless 92
otherwise noted, c0
j=0 for all j. 93
At the output of the model, a decoder function q(hK
j)is used to output a distribution over the space of 94
actions. q(.)takes the form of a single layer network, followed by a softmax. To produce a discrete 95
action, we sample from the this distribution. 96
Thus the entire model, which we call a Communication Neural Net (CommNN), (i) takes the state- 97
view of all agents s, passes it through the encoder h0=p(s), (ii) iterates handcin equations (1) 98
and (2) to obain hK, (iii) samples actions afor all agents, according to q(hK). 99
3.2 Model Extensions 100
Local Connectivity: An alternative to the broadcast framework described above is to allow agents 101
to communicate to others within a certain range. Let N(j)be the set of agents present within 102
communication range of agent j. Then (2) becomes: 103
ci+1
j=1
|N(j)|X
j02N(j)hi+1
j0. (3)
3h0=[h0
1,h0
2,. . . ,h0
J], and computes: 78
hi+1
j=fi(hi
j,ci
j) (1)
79
ci+1
j=1
J 1X
j06=jhi+1
j0. (2)
In the case that fiis a single linear layer followed by a nonlinearity  , we have: hi+1
j= (Hihi
j+ 80
Cici
j)and the model can be viewed as a feedforward network with layers hi+1= (Tihi)where hi81
is the concatenation of all hi
jandTtakes the block form: 82
Ti= 
     HiCiCi... Ci
CiHiCi... Ci
CiCiHi... Ci
...............
CiCiCi... Hi 
     ,Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.multilayer  
NN Avg.	2 Problem Formulation 33
We consider the setting where we have Magents, all cooperating to maximize reward Rin some 34
environment. We make the simplifying assumption that each agent receives R, independent of their 35
contribution. In this setting, there is no difference between each agent having its own controller, or 36
viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our 37
controller is a large feed-forward neural network that maps inputs for all agents to their actions, each 38
agent occupying a subset of units. A speciﬁc connectivity structure between layers (a) instantiates the 39
broadcast communication channel between agents and (b) propagates the agent state in the manner of 40
an RNN. 41
Because the agents will receive reward, but not necessarily supervision for each action, reinforcement 42
learning is used to maximize expected future reward. We explore two forms of communication within 43
the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and 44
will be treated as such by the reinforcement learning. In the continuous case, the signals passed 45
between agents are no different than hidden states in a neural network; thus credit assignment for the 46
communication can be performed using standard backpropagation (within the outer RL loop). 47
We use policy gradient [ 33] with a state speciﬁc baseline for delivering a gradient to the model. 48
Denote the states in an episode by s(1),. . . ,s (T), and the actions taken at each of those states 49
asa(1),. . . ,a (T), where Tis the length of the episode. The baseline is a scalar function of the 50
states b(s,✓), computed via an extra head on the model producing the action probabilities. Beside 51
maximizing the expected reward with policy gradient, the models are also trained to minimize the 52
distance between the baseline value and actual reward. Thus, after ﬁnishing an episode, we update 53
the model parameters ✓by 54
 ✓=TX
t=1 
  logp(a(t)|s(t),✓)
 ✓ TX
i=tr(i) b(s(t),✓) 
   
 ✓ TX
i=tr(i) b(s(t),✓) 2 
 .
Here r(t)is reward given at time t, and the hyperparameter  is for balancing the reward and the 55
baseline objectives, set to 0.03 in all experiments. 56
3 Model 57
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(ommiting the time 58
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the 59
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping 60
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent. 61
Note that this single controller  encompasses the individual controllers for each agents, as well as 62
the communication between agents. 63
One obvious choice for  is a fully-connected multi-layer neural network, which could extract 64
features hfrom sand use them to predict good actions with our RL framework. This model would 65
allow agents to communicate with each other and share views of the environment. However, it 66
is inﬂexible with respect to the composition and number of agents it controls; cannot deal well 67
with agents joining and leaving the group and even the order of the agents must be ﬁxed. On the 68
other hand, if no communication is used then we can write a={ (s1),. . . , (sJ)}, where  is a 69
per-agent controller applied independently. This communication-free model satisﬁes the ﬂexibility 70
requirements1, but is not able to coordinate their actions. 71
3.1 Controller Structure 72
We now detail the architecture for  that has the modularity of the communication-free model but 73
still allows communication.  is built from modules fi, which take the form of multilayer neural 74
networks. Here i2{0,. . ,K }, where Kis the number of communication layers in the network. 75
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j, 76
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors 77
1Assuming sjincludes the identity of agent j.
22 Problem Formulation 33
We consider the setting where we have Magents, all cooperating to maximize reward Rin some 34
environment. We make the simplifying assumption that each agent receives R, independent of their 35
contribution. In this setting, there is no difference between each agent having its own controller, or 36
viewing them as pieces of a larger model controlling all agents. Taking the latter perspective, our 37
controller is a large feed-forward neural network that maps inputs for all agents to their actions, each 38
agent occupying a subset of units. A speciﬁc connectivity structure between layers (a) instantiates the 39
broadcast communication channel between agents and (b) propagates the agent state in the manner of 40
an RNN. 41
Because the agents will receive reward, but not necessarily supervision for each action, reinforcement 42
learning is used to maximize expected future reward. We explore two forms of communication within 43
the controller: (i) discrete and (ii) continuous. In the former case, communication is an action, and 44
will be treated as such by the reinforcement learning. In the continuous case, the signals passed 45
between agents are no different than hidden states in a neural network; thus credit assignment for the 46
communication can be performed using standard backpropagation (within the outer RL loop). 47
We use policy gradient [ 33] with a state speciﬁc baseline for delivering a gradient to the model. 48
Denote the states in an episode by s(1),. . . ,s (T), and the actions taken at each of those states 49
asa(1),. . . ,a (T), where Tis the length of the episode. The baseline is a scalar function of the 50
states b(s,✓), computed via an extra head on the model producing the action probabilities. Beside 51
maximizing the expected reward with policy gradient, the models are also trained to minimize the 52
distance between the baseline value and actual reward. Thus, after ﬁnishing an episode, we update 53
the model parameters ✓by 54
 ✓=TX
t=1 
  logp(a(t)|s(t),✓)
 ✓ TX
i=tr(i) b(s(t),✓) 
   
 ✓ TX
i=tr(i) b(s(t),✓) 2 
 .
Here r(t)is reward given at time t, and the hyperparameter  is for balancing the reward and the 55
baseline objectives, set to 0.03 in all experiments. 56
3 Model 57
We now describe the model used to compute p(a(t)|s(t),✓)at a given time t(ommiting the time 58
index for brevity). Let sjbe the jth agent’s view of the state of the environment. The input to the 59
controller is the concatenation of all state-views s={s1,. . . ,s J}, and the controller  is a mapping 60
a= (s), where the output ais a concatenation of discrete actions a={a1,. . . ,a J}for each agent. 61
Note that this single controller  encompasses the individual controllers for each agents, as well as 62
the communication between agents. 63
One obvious choice for  is a fully-connected multi-layer neural network, which could extract 64
features hfrom sand use them to predict good actions with our RL framework. This model would 65
allow agents to communicate with each other and share views of the environment. However, it 66
is inﬂexible with respect to the composition and number of agents it controls; cannot deal well 67
with agents joining and leaving the group and even the order of the agents must be ﬁxed. On the 68
other hand, if no communication is used then we can write a={ (s1),. . . , (sJ)}, where  is a 69
per-agent controller applied independently. This communication-free model satisﬁes the ﬂexibility 70
requirements1, but is not able to coordinate their actions. 71
3.1 Controller Structure 72
We now detail the architecture for  that has the modularity of the communication-free model but 73
still allows communication.  is built from modules fi, which take the form of multilayer neural 74
networks. Here i2{0,. . ,K }, where Kis the number of communication layers in the network. 75
Each fitakes two input vectors for each agent j: the hidden state hi
jand the communication ci
j, 76
and outputs a vector hi+1
j. The main body of the model then takes as input the concatenated vectors 77
1Assuming sjincludes the identity of agent j.
2Figure 1: Blah.
The key idea is that Tisdynamically sized . First, the number of agents may vary. This motivates 83
the the normalizing factor J 1in equation (2), which resacles the communication vector by the 84
number of communicating agents. Second, the blocks are applied based on category , rather than by 85
coordinate. In this simple form of the model “category” refers to either “self” or “teammate”; but as 86
we will see below, the communication architecture can be more complicated than “broadcast to all”, 87
and so may require more categories. Note also that Tiis permutation invariant, thus the order of the 88
agents does not matter. 89
At the ﬁrst layer of the model an encoder function h0
j=p(sj) is used. This takes as input state-view 90
sjand outputs feature vector h0
j(inRd0for some d0). The form of the encoder is problem dependent, 91
but for most of our tasks they consist of a lookup-table embedding (or bags of vectors thereof). Unless 92
otherwise noted, c0
j=0 for all j. 93
At the output of the model, a decoder function q(hK
j)is used to output a distribution over the space of 94
actions. q(.)takes the form of a single layer network, followed by a softmax. To produce a discrete 95
action, we sample from the this distribution. 96
Thus the entire model, which we call a Communication Neural Net (CommNN), (i) takes the state- 97
view of all agents s, passes it through the encoder h0=p(s), (ii) iterates handcin equations (1) 98
and (2) to obain hK, (iii) samples actions afor all agents, according to q(hK). 99
3.2 Model Extensions 100
Local Connectivity: An alternative to the broadcast framework described above is to allow agents 101
to communicate to others within a certain range. Let N(j)be the set of agents present within 102
communication range of agent j. Then (2) becomes: 103
ci+1
j=1
|N(j)|X
j02N(j)hi+1
j0. (3)
3tanhConnecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.CommNet modelthcommunication step Module for agent
Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.Connecting Neural Models
Anonymous Author(s)
Afﬁliation
Address
email
Abstract
abstract 1
1 Introduction 2
In this work we make two contributions. First, we simplify and extend the graph neural network 3
architecture of ??. Second, we show how this architecture can be used to control groups of cooperating 4
agents. 5
2 Model 6
The simplest form of the model consists of multilayer neural networks fithat take as input vectors 7
hiandciand output a vector hi+1. The model takes as input a set of vectors {h0
1,h0
2,. . . ,h0
m}, and 8
computes 9
hi+1
j=fi(hi
j,ci
j)
10
ci+1
j=X
j0 6=jhi+1
j0;
We set c0
j=0 for all j, and i2{0,. . ,K }(we will call Kthe number of hops in the network). 11
If desired, we can take the ﬁnal hK
jand output them directly, so that the model outputs a vector 12
corresponding to each input vector, or we can feed them into another network to get a single vector or 13
scalar output. 14
If each fiis a simple linear layer followed by a nonlinearity  : 15
hi+1
j= (Aihi
j+Bici
j),
then the model can be viewed as a feedforward network with layers 16
Hi+1= (TiHi),
where Tis written in block form 17
Ti= 
     AiBiBi... Bi
BiAiBi... Bi
BiBiAi... Bi
...............
BiBiBi... Ai 
     .
The key idea is that Tisdynamically sized , and the matrix can be dynamically sized because the 18
blocks are applied by type, rather than by coordinate. 19
Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.
Figure 1: An overview of our CommNet model. Left: view of module fifor a single agent j. Note
that the parameters are shared across all agents. Middle: a single communication step, where each
agents modules propagate their internal state h, as well as broadcasting a communication vector c
on a common channel (shown in red). Right: full model, showing input states sfor each agent, two
communication steps and the output actions for each agent.
A key point is that Tisdynamically sized since the number of agents may vary. This motivates the
the normalizing factor J 1in equation (2), which rescales the communication vector by the number
of communicating agents. Note also that Tiis permutation invariant, thus the order of the agents
does not matter.
2Figure 13: The Architecture of CommNet from [32]. An overview of our CommNet model. Left: view of module 𝑓𝑖for a single
agent𝑗. Note that the parameters are shared across all agents. Middle: a single communication step, where each agents modules
propagate their internal state ℎ, as well as broadcasting a communication vector 𝑐on a common channel (shown in red). Right:
full model Φ, showing input states 𝑠for each agent, two communication steps and the output actions for each agent.
Figure 14: Overview of TarMAC from [6]. Left: At every timestep, each agent policy gets a local observation 𝜔𝑡
𝑖and aggregated
message𝑐𝑡
𝑖as input, and predicts an environment action 𝑎𝑡
𝑖and a targeted communication message 𝑚𝑡
𝑖. Right: Targeted com-
munication between agents is implemented as a signature-based soft attention mechanism. Each agent broadcasts a message
𝑚𝑡
𝑖consisting of a signature 𝑘𝑡
𝑖, which can be used to encode agent-specific information and a value 𝑣𝑡
𝑖, which contains the
actual message. At the next timestep, each receiving agent gets as input a convex combination of message values, where the
attention weights are obtained by a dot product between sender’s signature 𝑘𝑡
𝑖and a query vector 𝑞𝑡+1
𝑗predicted from the
receiver’s hidden state.
Xmessage𝑚""#𝑜""𝑜#	𝑄""(𝜏"", 𝑎"",𝑚""""+)𝑄#(𝜏#, 𝑎#,𝑚#""+)Mixing Network𝑄-.-
𝑚""""+𝑚#""+X𝑎""𝑎#
message𝑚#""X
𝐺𝑅𝑈""𝐺𝑅𝑈#Messages 𝑚(2#)""from other agentsConcatenateXAgent 𝑖MLPMLPMLPMLPMessages 𝑚2""#from other agentsAgent 𝑗
𝑜#MLPGRUℎ#-ℎ#-67𝜏#𝐼(𝐴"";M#""|Τ"",𝑀2#"")Mutual Information Loss𝐼(𝐴#;M""#|Τ#,𝑀2""#)Mutual Information Loss
Entropy Loss𝐻(𝑀#"")𝐻(𝑀""#)Entropy LossFigure 15: Overview of NDQ from [40]. The message encoder generates an embedding distribution that is sampled and con-
catenated with the current local history to serve as an input to the local action-value function. Local action values are fed into
a mixing network to to get an estimation of the global action value.."
2205.09362v2.pdf,"Sparse Adversarial Attack in Multi-agent
Reinforcement Learning
Yizheng Hu
Academy for Advanced Interdisciplinary Studies
Peking University
No.5 Yiheyuan Road, Haidian District, Beijing 100871
1801111560@pku.edu.cn
Zhihua Zhang
School of Mathematical Sciences
Peking University
No.5 Yiheyuan Road, Haidian District, Beijing 100871
zhzhang@math.pku.edu.cn
Abstract
Cooperative multi-agent reinforcement learning (cMARL) has many real applica-
tions, but the policy trained by existing cMARL algorithms is not robust enough
when deployed. There exist also many methods about adversarial attacks on the
RL system, which implies that the RL system can suffer from adversarial attacks,
but most of them focused on single agent RL. In this paper, we propose a sparse
adversarial attack on cMARL systems. We use (MA)RL with regularization to
train the attack policy. Our experiments show that the policy trained by the current
cMARL algorithm can obtain poor performance when only one or a few agents in
the team (e.g., 1 of 8 or 5 of 25) were attacked at a few timesteps (e.g., attack 3 of
total 40 timesteps).
1 Introduction
Many real-world sequential decision problems involve multiple agents in the same environment,
where all agents work together to accomplish a certain goal. Cooperative multi-agent reinforcement
learning (cMARL) is a key technique for solving these problems. In recent years, cMARL has
been applied to many ﬁelds, such as trafﬁc light control [ 31], autonomous driving [ 25], wireless
networking [15], etc.
However, many prior works about adversarial attacks on single agent RL have already shown that the
policies trained by most current single agent RL algorithms are not robust enough, thus one cannot
use them with conﬁdence in real-world applications. But to the best of our knowledge, there are very
few prior works on attacking MARL policies. Lin et al. [10] and Guo et al. [5]showed that a MARL
policy can perform awful when one agent behaved adversarially against other agents.
Unlike many prior works which attack RL agents at every timesteps, in this paper we attempt to look
further. More speciﬁcally, we want to ﬁnd whether the policy trained by the current MARL algorithm
like QMIX can be signiﬁcantly affected, when only a few agents in the team were attacked at a few
timesteps during the whole episode, i.e., sparse attack . Here “sparse” has two meanings: sparseness
in agent dimension and sparseness in time dimension.
This result is important for judging whether the current MARL policy can be deployed into real-world
scenarios, because in many existing cMARL algorithms the team can only guarantee to get a high
Preprint. Under review.arXiv:2205.09362v2  [cs.AI]  8 Aug 2022
reward when all agents execute their optimal policy accurately. But this may not be true for real-world
scenarios. For a machine team, the machines may malfunction sometimes; for a human team, the
teammates may not be absolutely credible, because the competitors may plant a spy who will sabotage
the team.
But of course, if a machine fails frequently, it will be replaced by a better machine soon; and if
a spy sabotages the team frequently, he/she will be easy to be spotted and kicked out from the
team. Real-world machines may more likely to fail only occasionally, and a spy can only sabotage
occasionally to avoid being kicked out. This is why we focus on sparse attack.
Prior sparse attack work mainly focused on single agent RL. Most of them used some rule-based
method to get the timestep of the attack, which can be shown to be sub-optimal (see 4.1). In this
work, we attempt to use (MA)RL algorithms to learn the optimal sparse attack policy. To the best of
our knowledge, our work is the ﬁrst work about optimal sparse attack in the MARL environment.
Our contributions are summarized as follows:
•We perform adversarial attacks in the MARL environment, and consider both single agent
attack and multi agent attack.
•We propose a (MA)RL based optimal sparse attack method in the MARL environment,
which outperforms existing rule based sparse attack methods.
•Our experiments show that the current MARL algorithms, at least for QMIX, are not robust
enough.
2 Related Work
MARL algorithms Existing cMARL algorithms can be roughly divided into two types: policy-
based and value based. Representative policy-based algorithms include COMA [ 4], MADDPG [ 12],
etc; and representative value-based methods include VDN [ 29] QMIX [ 21], QTRAN [ 26], ROMA
[32], Weighted-QMIX [22], etc.
Adversarial attacks in RL Adversarial attacks in RL can also be roughly divided into two types:
attack agent (e.g., perturb agent’s observation or action) or modify environment. 1)For attacking
agent, early works like [ 3,6] used simple adversarial example algorithm to attack agent’s state. Later
works like [ 30,23] used RL to learn the perturbation of agent’s state that can minimize agent’s reward.
Lee et al. [8]attacked agent’s action directly. Xiao et al. [33], Lin et al. [11], Hussenot et al. [7]tried
different adversarial attack task (same perturb across time; mislead agent to certain state). 2)For
modifying environment, Mankowitz et al. [13,14], Abdullah et al. [1], Zhang et al. [36], Yu et al.
[35] provided different kinds of environment modiﬁcation.
Sparse attack in RL Lin et al. [11], Qu et al. [20], Yang et al. [34] used rule based methods to
decide attack steps (based on the “difference” or “entropy” of the agent’s policy or value function).
Behzadan and Hsu [2], Qiaoben et al. [19] used RL to learn the attack steps and then applied
adversarial example at those steps. Sun et al. [28] used RL to learn both the attack step and the
perturbation. All these works are conducted in single agent RL.
Adversarial attacks and adversarial training in MARL Lin et al. [10], Guo et al. [5]attacked
one agent in the cMARL environment. Pham et al. [17] used model based method to attack cMARL
agents. Li et al. [9], Sun et al. [27], Nisioti et al. [16] did mini-max adversarial training in the cMARL
environment, which assumed some agents may behave adversarially against other agents. All these
works assumed agent(s) can behave adversarially at any timestep.
3 Problem Formulation
A standard cooperative MARL system contains nagents that can be described as a multi-agent
Markov decision processes (multi-agent MDP, MMDP) (S;fAign
i=1;fOign
i=1;R;P ). HereSis the
environment’s global state space. Each agent ihas its own action space Aiand observation space Oi.
At each time step t, agentiﬁrst observes oi;t2Oibased on the current global state st2S. The
2
observation oi;tcan be the same as stif the environment is fully observable, or be different if the
environment is partially observable. After that, agent performs action ai;tbased on its policy i(jhi;t),
hi;tis agenti’s all historical information until t. The team receives reward rt=R(st;a1;t;:::;an;t),
and then the environment transfers to next state st+1P(jst;a1;t;:::;an;t). The team’s goal is to
maximize its expected total reward Es0p0;at;st+1P[P
tR(st;at)].
To simplify the notation, we may use atto refer toa1;t;:::;an;t, use afi;j;kg;tto refer to agent
fi;j;kg’s action, and use a fi;j;kg;tto refer all except agent fi;j;kg’s action.
For partially observable environments, we may assume the global state stis known during MARL’s
training period, but not during its testing period. The MARL algorithm can use global state informa-
tion to help agents learn its policy, but each agent must be able to perform its policy only with its own
observation. This is a common setting in many existing cMARL algorithms.
If the attacker decide to attack m2[1;n]agent(s) in the team (denoted by k=fk1;;kmg,
ki2[1;n]fori2[1;m]), the sparse adversarial attack task can be formulated as a constrained
optimization problem:
min
ak;tEs0p0;st+1P""X
tR(st;ak;t;a
 k;t)#
s.t.mX
i=1X
t1[aki;t6=a
ki;t]N:(1)
whereais the optimal policy without attack, and Nis the maximum number of attack steps. This
optimization problem is a constrained (MA)RL problem.
Moreover, since our goal is to ﬁnd out “whether MARL policy can be signiﬁcantly affected by a little
modiﬁcation”, the maximum number of attack steps, i.e., N, is also a hyper-parameter that needs to
be tuned. Instead of solving the constrained (MA)RL and tune the hyper-parameter N, we can just
relax the hard constraintPm
i=1P
t1[aki;t6=a
ki;t]Ninto a soft regularization:
min
ak;tEs0p0;st+1P""X
tR(st;ak;t;a
 k;t) +mX
i=1X
t1[aki;t6=a
ki;t]#
: (2)
The soft regularized problem is enough for our goal. In the regularized problem, can be used to
control the sparsity of attack indirectly, large leading to less attack.
Most existing works on adversarial attack on RL policy were targeted on attacking state or observation.
Unlike this, we make a stronger assumption: the attacker can directly modify agent’s action. This
assumption is reasonable under sparsity regularization, and it does also reﬂects some real-world
scenarios. A faulty machine may perform sub-optimal actions even when the observation is correct,
and a spy may perform any action that can sabotage the team, as long as he/she doesn’t do this
frequently.
4 Methodology
4.1 Rule based sparse attack is sub-optimal
Some prior work about sparse attack in RL used rule based method to choose the attack step. For
example, one calculates t= maxa(ajst) mina(ajst)[11] ort=Pm
i=0p(ai
t)logp(ai
t)
logm[20]
ﬁrst and then attacks twith hight. For Q learning, both the methods used softmax (Q)to calculate
t. These two methods have a certain intuitive meaning: higher tmeans policy has higher conﬁdence
in its optimal action, which potentially means step tis more important and more worthwhile to be
attacked. But both the methods are sub-optimal, at least for attacking Q learning agent. Here’s an
example:
Example 1 Consider af0;1gaction game with total Ttimesteps. At timestep tthere are 2tstates.
The state transit function is deterministic and different action sequences will lead to different states.
3
Figure 1: Example 1
 Figure 2: Example 2
The game can be described by a binary tree. The agent only receives reward at the last step, with no
discount factor.
Consider a reward function like Figure 1. All other terminal states have a reward in [0;47].
For this game, if the sparse attack budget is not less than 2, the only optimal attack policy is attacking
points B and C, resulting  100reward.
ButQ
B(0) = 50;Q
B(1) = 49;Q
A(0) = 50;Q
A(1) = 48 . TheQfunction at point B has less
difference than point A. Therefore these two methods are both sub-optimal.

Some other existing works used RL to learn the sparse attack timesteps. For example, Behzadan
and Hsu [2]perturbed the action to arg minaQ(st;a)if the attacker decided to attack at timestep t;
Qiaoben et al. [19] used RL to learn the sparse attack timesteps with ﬁxed adversary perturbation
h(s).
These methods could also be sub-optimal, because the learnt Qfunction describes the optimal reward
after having performed certain action, so perturbing action to arg minaQonly minimize the upper
bound of the reward, not the lower bound. Here’s an example:
Example 2 The environment setting is the same as Example 1, except the action space is f0;1;2g.
The game can be described by a ternary tree.
Consider a reward function like Figure 2. All other terminal states have a reward in [0;47].
For this game, if the sparse attack budget is not less than 2, the only optimal attack policy is attacking
point A with action 1 and point B with action 1, resulting  100reward.
ButQ
A(0) = 50;Q
A(1) = 49;Q
A(2) = 48 , the optimal attack policy is not the one with the lowest
Qvalue. Therefore this method is sub-optimal.

The detailed calculation of these two examples can be found in Appendix A.1
4.2 Using (MA)RL algorithm to solve the optimal sparse attack policy
The fundamental reason why rule-based sparse attacks are sub-optimal is that, the Qfunction only
describes optimal reward in certain case and it contains little information about the worst reward. So
in order to obtain the optimal sparse attack policy, one way is to use a (MA)RL algorithm to solve the
regularized optimization problem.
SetR0(st;ak;t) = R(st;ak;t;a
 k;t) Pm
i=11[aki;t6=a
ki;t]andP0(st+1jst;ak;t) =
P(st+1jst;ak;t;a
 k;t). Then the regularized optimization problem (2) becomes a standard (M)MDP
in the original environment, with agent k1;;km, state transition function P0, and reward R0.
Under some regularity assumptions, a optimal policy of the MDP exists [ 18], therefore we call this
optimal policy as “optimal sparse attack policy”.
This (M)MDP can be solved by any existing (MA)RL algorithm.
Form= 1, this is a partially observable single agent RL problem. Most common settings in partially
observable RL assume that the true state is unknown. But in this case, the environment is still the
4
original multi agent environment, in which the global state is known during the training period. We
can take advantage of this feature and use the global state to help training. For example, we can apply
the QMIX algorithm into this single agent case.
Algorithm 1 shows the learning procedure.
Algorithm 1 Optimal sparse attack
Require: Attack agents k1;;km; Optimal Q function Q
1;;Q
n; Hyperparameter 
Ensure: Optimal sparse attack Q function Qadv
k1;;Qadv
km
Initialize replay buffer D=;
foreach epoch do
forsampling loop do
Obtain current standot. Compute optimal actions a
i;t= arg max aQ
i(oi;t;a)fori=
1;;n.
Roll out attacked agents’ action aadv
ki;t, using""-greedy or other method, with Qadv
ki(oki;t;a)
fori= 1;;m
Perform action aadv
k;t;a
 k;tand getrt,st+1andot+1.
Compute attacked agents’ reward r0
t= rt Pm
i=11[aadv
ki;t6=a
ki;t]
Store transition (st;ok;t;aadv
k;t;r0
t;st+1;ok;t+1)intoD
end for
fortraining loop do
Sample a minibatch Mfrom replay buffer D.
ComputeQadv
ki(oki;aki)fori= 1;;m.
ComputeQ(s;ak)using QMIX’s mixer.
Perform an update step.
end for
end for
Comparison with existing works. Except for sub-optimal rule based sparse attack discussed
earlier, Sun et al. [28] used RL to learn the optimal sparse attack either, and Lin et al. [10], Guo et al.
[5], Pham et al. [17] used RL to learn the optimal attack in cMARL system. The main difference
between our work and theirs is:
•Sun et al. [28] only considered attacking single agent RL policy. Our work focuses on
attacking multi agent RL policy, and testing on many different environments. Their work
used hard constrain applied to the environment to control the sparsity, i.e., the attacker will
fail to attack if maximum of attack step reached. Our work uses soft regularization to control
the sparsity.
•Both Lin et al. [10], Guo et al. [5], Pham et al. [17] considered the dense attack, and the
ﬁrst two works only attempted to attack one agent in the team. Our work focuses on sparse
attack, and attempts to attack both single agent and multiple agents.
5 Experiments
5.1 Experiment settings
We evaluate our optimal sparse attack method on SMAC environment [ 24] with different maps. For
maps with only a few agents, we only attempt to attack a single agent; For maps with many agents,
we attempt to attack both single agent and multiple agents.
Environment and experiment settings SMAC [ 24] is an MIT-licensed multi agent partially ob-
servable environment based on the StarCraft II game. Each agent can only observe a part of the
global state within a circular area around it. Please refer to the SMAC paper for more details.
We use QMIX [ 21] to train the base policy to be attacked. We mainly focus on attacking QMIX
policies, but we also try attacking VDN [ 29] and QTRAN [ 26] policies in some maps. The base
policy is trained with the same settings and same parameters as in the QMIX paper. We also use
5
QMIX to train the optimal sparse attack policy. The optimal sparse attack policy is also trained with
the same settings and same parameters as in the QMIX paper, except the number of agents is changed.
Map selection SMAC environment contains many different maps, but only those maps that QMIX
can achieve good performance are suitable for evaluating the performance of the sparse attack policy.
We evaluate our method on different kinds of maps:
•Maps with a few agents: 3m and 3s_vs_4z. In these two maps, we only attack single agent.
Because all the agents are homogeneous, we just choose agent 0 as a representative agent.
•Maps with a medium number of agents: 2s3z, 8m, 1c3s5z, so_many_baneling. In these
maps, we attempt to attack both single agent and multiple agents. For homogeneous map 8m
and so_many_baneling, we just randomly choose the representative agent; for heterogeneous
map, we choose the representative agent based on agent type.
•Maps with a large number of agents: 25m and bane_vs_bane. In these maps, we attempt to
attack both single agent and multiple agents. Since the number of agents is large, we just
randomly choose the representative agent.
Some detailed explanation can be found in Appendix A.2.
Evaluation criteria and baselines We use the winning rate of 1000 episodes and the average
number of attack steps as our evaluation criteria. The attack policy with fewer attack steps and a
lower winning rate has better performance.
To evaluate the stability of the experiment, we run each (MA)RL training 5 times with different
randomly selected random seeds, discard the policy with the highest and lowest winning rate, and
report the result of the rest 3 policies.
We evaluate our result with these baselines:
•Random sparse attack. At each timesteps we randomly attack it with a certain probability,
with either random action, or use the action that have the lowest Q value. We use the
abbreviation Ra-R or Ra-L to represent them.
•Rulebased sparse attack. Following Lin et al. [11], we compute = maxasoftmax (Q) 
minasoftmax (Q)and attack those timesteps with high tand with the action that have the
lowest Q value. We use the abbreviation Ru-B to represent it.
•RLwithfixed attack policy. Following Behzadan and Hsu [2], we use RL to learn the attack
timesteps, and then attack those timesteps with the action that have the lowest Q value. We
use the abbreviation RL-F to represent it. In Behzadan and Hsu [2]they ﬁxed the attack cost
cadvto1per attack step, but in order to make these result comparable, we may adjust cadv
to control the attack sparsity.
•Rule based dense attack. Attack every timesteps with the action that have the lowest Q
value. This baseline is only used in a few case, because it is a much stronger attack. We use
the abbreviation Ru-D to represent it.
We use the abbreviation OPT to represent our method: optimal sparse attack. For hyperparameter ,
we train different attack policies with different , and choose the one that can signiﬁcantly lower the
winning rate. Results below show that larger leads to less attack.
5.2 Maps with a few agents
Table 1 shows the result of the 3m map and the 3s_vs_4z map. For these two maps, the winning
rate without adversarial attack is 97.2% and 90.8% respectively. We can see that, for 3m map, OPT
can lower the winning rate to 0% with only around 18% of timesteps being attacked, and 1.8% with
around 14%. For 3s_vs_4z map, the winning rate cannot be as low as 3m map, but OPT can still
signiﬁcantly lower the winning rate with around 10%-15% of timesteps being attacked.
These results show that OPT outperforms Ra-R/Ra-L and Ru-B. In 3m map, the performance of OPT
and RL-F is similar, but in 3s_vs_4z OPT signiﬁcantly outperforms RL-F. Note that, for Ru-B and
RL-F, we do not need to accurately select a threshold or cadvto let them attack exactly the same
6
Table 1: Maps with a few agents
Map 3m Agent 0 3s_vs_4z Agent 0
Attack
typeParameterWinning
rate (%)Attacked steps /
Total stepsParameterWinning
rate (%)Attacked steps /
Total steps
OPT= 10.0 4.278/23.249
= 0:156.5 10.123/167.604
0.0 4.336/23.067 58.3 15.993/171.329
0.0 4.329/23.356 59.9 9.404/165.055
= 51.8 3.464/24.906
= 0:0146.0 22.085/163.402
1.4 3.640/24.975 49.0 24.480/175.448
1.4 3.460/25.436 46.7 20.370/167.866
Ra-R -85.3 20%-90.5 15%
88.4 15% 91.6 10%
Ra-L -48.6 20%-87.2 15%
60.3 15% 90.5 10%
Ru-BThreshold
0.2526.7 15.657/39.389Threshold
0.0466.2 30.794/159.433
Threshold
0.25841.1 12.475/35.468Threshold
0.0590.4 19.811/136.088
RL-Fcadv= 1 0.0 3.997/40.113 cadv= 0:1 78.6 15.958/150.592
number of timesteps as OPT. If OPT attacks fewer timesteps than Ru-B or RL-F, and achieves a lower
winning rate, then OPT just outperforms them.
5.3 Maps with a medium number of agents
For 8m, 2s3z, 1c3s5z, so_many_baneling map, we attempt to attack both single agent and multiple
agents. The winning rate of these 4 maps without adversarial attack is 87.9%, 97.5%, 99.5%, 95.0%
respectively.
Attacking single agent We pick one agent for each map to report here, and more experimental
results can be found in Appendix A.3.
Table 2 shows the result of these 4 maps. OPT can lower the winning rate in all these 4 maps.
Especially in 1c3s5z, OPT lowers the winning rate to less than 1% in only less than 10% of timesteps
being attacked.
In both maps, OPT outperforms Ra-R, Ra-L and Ru-B. In 8m map, the performance of OPT and
RL-F is similar, and in the other 3 maps, OPT signiﬁcantly outperforms RL-F.
Attacking multiple agents Table 3 shows the result of multiple agent attack, and additional results
can be found in Appendix A.3. In these maps, OPT can signiﬁcantly lower the winning rate, and
outperforms all baselines including RL-F. In 1c3s5z map, OPT outperforms Ru-D. In maps with
heterogeneous agents, different types of agents contribute differently to the team, small agents play
less of a role in the team. Compared to previous result, in 1c3s5z map, “c” is large agent and “s” / “z”
are small agents, so attacking “s” / “z” requires more attack steps than attacking “c”.
5.4 Maps with a large number of agents
For 25m and bane_vs_bane map, we also attempt to attack both single agent and multiple agents.
Table 4 shows the result of the 25m map. The winning rate of the 25m map without adversarial attack
is 98.4%. More results can be found in Appendix A.3.
When attacking only one agent in a team with 25 agents, OPT can lower the winning rate a little bit
with around 80% of timesteps being attacked. This still outperforms Ru-D and RL-F. Ru-B has no
baseline in this case, because we fail to ﬁnd a threshold that can attack more than 80% but not near
100% of timesteps. When attacking 5 agents in the team, OPT can signiﬁcantly lower the winning
7
Table 2: Attacking single agent
Map 8m Agent 4 so_many_baneling Agent 6
Attack
typeParameterWinning
rage (%)Attacked steps /
Total stepsParameterWinning
rage (%)Attacked steps /
Total steps
OPT= 10.2 7.109/26.876
= 0:58.4 4.688/24.229
0.2 7.153/27.393 6.6 4.720/24.259
0.1 5.008/26.373 7.9 4.711/24.054
Ra-R - 71.4 25% - 89.8 25%
Ra-L - 33.7 25% - 87.1 25%
Ru-BThreshold
0.1324.1 8.291/33.548Threshold
0.0378.0 11.440/24.830
RL-Fcadv= 0:25 0.6 6.431/99.718 cadv= 0:1 53.6 7.245/25.194
Map 1c3s5z Agent 0 2s3z Agent 1
Attack
typeParameterWinning
rage (%)Attacked steps /
Total stepsParameterWinning
rage (%)Attacked steps /
Total steps
OPT= 10.0 4.097/44.122
= 0:10.4 11.413/119.696
0.1 4.134/44.389 0.5 10.942/119.577
0.0 4.019/45.706 0.5 11.528/119.454
= 50.3 3.617/46.609
= 17.2 11.116/113.693
0.3 3.458/45.725 2.2 10.062/118.165
0.3 3.452/45.555 2.5 10.193/117.992
Ra-R - 96.8 10% - 97.1 10%
Ra-L - 94.7 10% - 91.1 10%
Ru-BThreshold
0.145.7 5.83/64.444Threshold
0.150.8 12.709/56.991
RL-Fcadv= 0:3 2.5 4.602/48.968 cadv= 0:1 22.1 32.688/88.48
Table 3: Attacking multiple agents
Map so_many_baneling Agent 0 and 6 1c3s5z Agent 1 and 7
Attack
typeParameterWinning
rage (%)Attacked steps /
Total stepsParameterWinning
rage (%)Attacked steps /
Total steps
OPT= 0:53.75.643,1.204
/21.219= 0:148.316.265,13.545
/106.380
2.35.755,5.150
/21.19949.615.507,15.621
/110.040
3.35.988,1.344
/21.27648.215.729,14.188
/112.490
= 16.45.405,1.037
/21.044= 0:0129.157.834,58.535
/79.101
8.15.423,1.015
/21.29929.753.077,56.998
/78.544
8.95.600,1.090
/21.20024.954.735,48.304
/78.564
Ra-R - 90.0 30%, 10% - 97.9 15%, 15%
Ra-L - 84.8 30%, 10% - 98.3 15%, 15%
Ru-D - - - - 37.7 100%, 100%
Ru-BThreshold
0.05, 0.174.411.714,2.460
/29.005Threshold
0.04, 0.0568.823.486,27.912
/73.038
RL-Fcadv= 0:1 17.54.783,6.286
/25.019cadv= 0:01 37.077.475,85.843
/118.839
8
Table 4: 25m map
Map 25m Agent 10
Attack
typeParameterWinning
rate (%)Attacked steps /
Total steps
OPT= 0:0169.3 30.302/46.582
68.1 37.845/48.739
70.8 44.589/56.135
= 0:00167.0 51.887/63.311
73.8 45.496/56.599
65.7 49.720/63.385
Ra-R - 83.7 70%
Ra-L - 73.8 70%
Ru-D - 71.8 100%
RL-F cadv= 0 70.7 42.459/62.132
Map 25m Agent 0,5,10,15,20
Attack
typeParameterWinning
rate (%)Attacked steps /
Total steps
OPT = 116.2 3.700,3.840,3.567,4.051,3.467/58.801
16.2 3.575,3.928,3.651,3.999,3.484/59.101
19.3 3.938,3.715,3.608,3.881,3.465/56.315
Ra-R - 96.5 7%, 7%, 7%, 7%, 7%
Ra-L - 89.4 7%, 7%, 7%, 7%, 7%
Ru-BThreshold
0.16,0.13,0.11,0.13,0.1542.8 6.729,4.721,6.189,6.855,5.341/50.164
RL-F cadv= 0:2 35.9 7.335,36.529,6.959,2.511,2.530/101.708
rate with each agent around 7% of timesteps being attacked. All these results outperform all baselines
including RL-F.
5.5 Attacking VDN and QTRAN policies
We also attack VDN and QTRAN policies in two maps: 1c3s5z and 25m, i.e., one homogeneous
map and one heterogeneous map. Results can be found in Appendix A.3. The result is similar to
QMIX policies. Both VDN and QTRAN policies can perform awful under sparse attack, and OPT
outperforms all baseline methods.
6 Conclusion and Future works
In this work we have been concerned with sparse attack in multi agent reinforcement learning. We
have used (MA)RL algorithms to learn the optimal sparse attack policy and evaluated them in SMAC
environments. In most of our experiments, our method outperformed all baselines, which indicated
that rule based sparse attack method is sub-optimal. Our experiments have shown that the policy
trained by the QMIX / VDN / QTRAN algorithm can obtain poor performance when only one or a
few agents in the team are attacked at a few timesteps.
Our results implied that existing cMARL algorithms, at least QMIX / VDN / QTRAN, may not be
robust enough to deploy to real applications. So one straightforward future work is to ﬁnd some ways
to defend this kind of attack. To the best of our knowledge, this work is the ﬁrst work about sparse
attack in MARL. As a ﬁrst-step work, we ﬁxed the target agent and then learn the optimal sparse
attack policy. A more general attack problem is to take k1;;kminto the min operator, i.e., the
attacker must decide both the agent to be attacked and the attack policy. This problem is a mixed
integer programming problem, and is also a possible future work.
9
References
[1]Mohammed Amin Abdullah, Hang Ren, Haitham Bou Ammar, Vladimir Milenkovic, Rui Luo,
Mingtian Zhang, and Jun Wang. Wasserstein robust reinforcement learning. arXiv preprint
arXiv:1907.13196 , 2019.
[2]Vahid Behzadan and William Hsu. Adversarial exploitation of policy imitation. arXiv preprint
arXiv:1906.01121 , 2019.
[3]Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy
induction attacks. arXiv preprint arXiv:1701.04143 , 2017.
[4]Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence , volume 32, 2018.
[5]Jun Guo, Yonghong Chen, Yihang Hao, Zixin Yin, Yin Yu, and Simin Li. Towards comprehen-
sive testing on the robustness of cooperative multi-agent reinforcement learning. arXiv preprint
arXiv:2204.07932 , 2022.
[6]Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial
attacks on neural network policies. arXiv preprint arXiv:1702.02284 , 2017.
[7]Léonard Hussenot, Matthieu Geist, and Olivier Pietquin. Targeted attacks on deep reinforcement
learning agents through adversarial observations. arXiv preprint arXiv:1905.12282 , 2019.
[8]Xian Yeow Lee, Sambit Ghadai, Kai Liang Tan, Chinmay Hegde, and Soumik Sarkar. Spa-
tiotemporally constrained action space attacks on deep reinforcement learning agents. arXiv
preprint arXiv:1909.02583 , 2019.
[9]Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. Robust multi-agent
reinforcement learning via minimax deep deterministic policy gradient. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , volume 33, pages 4213–4220, 2019.
[10] Jieyu Lin, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, and Nicolas Papernot.
On the robustness of cooperative multi-agent reinforcement learning. In 2020 IEEE Security
and Privacy Workshops (SPW) , pages 62–68. IEEE, 2020.
[11] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min
Sun. Tactics of adversarial attack on deep reinforcement learning agents. arXiv preprint
arXiv:1703.06748 , 2017.
[12] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275 ,
2017.
[13] Daniel J Mankowitz, Nir Levine, Rae Jeong, Yuanyuan Shi, Jackie Kay, Abbas Abdolmaleki,
Jost Tobias Springenberg, Timothy Mann, Todd Hester, and Martin Riedmiller. Robust re-
inforcement learning for continuous control with model misspeciﬁcation. arXiv preprint
arXiv:1906.07516 , 2019.
[14] Daniel J Mankowitz, Dan A Calian, Rae Jeong, Cosmin Paduraru, Nicolas Heess, Sumanth
Dathathri, Martin Riedmiller, and Timothy Mann. Robust constrained reinforcement learning
for continuous control with model misspeciﬁcation. arXiv preprint arXiv:2010.10644 , 2020.
[15] Yasar Sinan Nasir and Dongning Guo. Multi-agent deep reinforcement learning for dynamic
power allocation in wireless networks. IEEE Journal on Selected Areas in Communications , 37
(10):2239–2250, 2019.
[16] Eleni Nisioti, Daan Bloembergen, and Michael Kaisers. Robust multi-agent q-learning in
cooperative games with adversaries. 2021.
[17] Nhan H Pham, Lam M Nguyen, Jie Chen, Hoang Thanh Lam, Subhro Das, and Tsui-Wei
Weng. Evaluating robustness of cooperative marl: A model-based approach. arXiv preprint
arXiv:2202.03558 , 2022.
10
[18] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming .
John Wiley & Sons, 2014.
[19] You Qiaoben, Xinning Zhou, Chengyang Ying, and Jun Zhu. Strategically-timed state-
observation attacks on deep reinforcement learning agents. In ICML 2021 Workshop on
Adversarial Machine Learning , 2021.
[20] Xinghua Qu, Zhu Sun, Pengfei Wei, Yew-Soon Ong, and Abhishek Gupta. Minimalistic attacks:
How little it takes to fool a deep reinforcement learning policy. arXiv preprint arXiv:1911.03849 ,
2019.
[21] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning , pages 4295–4304.
PMLR, 2018.
[22] Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding
monotonic value function factorisation for deep multi-agent reinforcement learning. arXiv
preprint arXiv:2006.10800 , 2020.
[23] Alessio Russo and Alexandre Proutiere. Optimal attacks on reinforcement learning policies.
arXiv preprint arXiv:1907.13548 , 2019.
[24] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nan-
tas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon
Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043 , 2019.
[25] Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295 , 2016.
[26] Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran:
Learning to factorize with transformation for cooperative multi-agent reinforcement learning.
InInternational Conference on Machine Learning , pages 5887–5896. PMLR, 2019.
[27] Chuangchuang Sun, Dong-Ki Kim, and Jonathan P How. Romax: Certiﬁably robust deep
multiagent reinforcement learning via convex relaxation. arXiv preprint arXiv:2109.06795 ,
2021.
[28] Jianwen Sun, Tianwei Zhang, Xiaofei Xie, Lei Ma, Yan Zheng, Kangjie Chen, and Yang Liu.
Stealthy and efﬁcient adversarial attacks against deep reinforcement learning. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , volume 34, pages 5883–5891, 2020.
[29] Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi,
Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-
decomposition networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296 ,
2017.
[30] Edgar Tretschk, Seong Joon Oh, and Mario Fritz. Sequential attacks on agents for long-term
adversarial goals. arXiv preprint arXiv:1805.12487 , 2018.
[31] Tong Wang, Jiahua Cao, and Azhar Hussain. Adaptive trafﬁc signal control for large-scale
scenario with cooperative group-based multi-agent reinforcement learning. Transportation
research part C: emerging technologies , 125:103046, 2021.
[32] Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. Roma: Multi-agent reinforce-
ment learning with emergent roles. arXiv preprint arXiv:2003.08039 , 2020.
[33] Chaowei Xiao, Xinlei Pan, Warren He, Jian Peng, Mingjie Sun, Jinfeng Yi, Mingyan Liu,
Bo Li, and Dawn Song. Characterizing attacks on deep reinforcement learning. arXiv preprint
arXiv:1907.09470 , 2019.
[34] Chao-Han Huck Yang, Jun Qi, Pin-Yu Chen, Yi Ouyang, I-Te Danny Hung, Chin-Hui Lee,
and Xiaoli Ma. Enhanced adversarial strategically-timed attacks against deep reinforcement
learning. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP) , pages 3407–3411. IEEE, 2020.
11
[35] Lebin Yu, Jian Wang, and Xudong Zhang. Robust reinforcement learning under model misspec-
iﬁcation. arXiv preprint arXiv:2103.15370 , 2021.
[36] Kaiqing Zhang, Tao Sun, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer Basar. Robust
multi-agent reinforcement learning with model uncertainty. Advances in Neural Information
Processing Systems , 33, 2020.
A Appendix
A.1 Detailed derivation
Example 1 Since the game can be described by a binary tree, we can just use action sequence to
denote each state, i.e., action sequence a0;a1;;at 1will lead to state sa0;a1;;at 1at timestept.
The reward function at terminal step Tcan be denoted by ra0;a1;;aT 1.
Consider this reward function:
• Choose an action sequence a
0;;a
T 1as optimal policy, and set ra
0;;a
T 1= 50
•Choose some p < t < T 1. Choose another action sequence a0
t+1;;a0
T 2, and set
ra
0;;a
t 1;1 a
t;a0
t+1;;a0
T 2;1= 49 ,ra
0;;a
t 1;1 a
t;a0
t+1;;a0
T 2;0= 100
•Choose another action sequence a00
p+1;;a00
T 1, and setra
1;;a
p 1;1 ap;a00
p+1;;a00
T 1=
48
• For any other terminal state, set its reward to any value between [0;47]
For this game, if the sparse attack budget is not less than 2, the only optimal attack policy is to attack
steptand stepT 1, with reward -100:
•ra
0;;a
t 1;1 a
t;a0
t+1;;a0
T 2;1= 49 , and any other terminal state’s reward is at most 48.
If we ﬁrst attack step tand change its action to 1 a
t, then the rest optimal policy is
a0
t+1;;a0
T 2;1.
•Then we attack step T 1and change its action from 1to0, agent will receive  100reward,
which is the only worst reward of the whole game (Any other terminal state’s reward is at
least 0).
Now let’s calculate some Qfunction:
•At timestep t,Q
t(sa
0;;a
t 1;a
t) = 50 ,Q
t(sa
0;;a
t 1;1 a
t) = 49 (The rest optimal
policy isa0
t+1;;a0
T 2;1, which will receive 49reward)
•At timestep p,Q
p(sa
0;;a
p 1;a
p) = 50 ,Q
p(sa
0;;a
p 1;1 a
p) = 48 (The rest optimal
policy isa00
p+1;;a00
T 1, which will receive 48reward. Any other policy can only get at
most 47reward.)
Now, no matter what rule we choose, pis larger than t, which means that the rule based sparse
attack using is sub-optimal. Since the optimal attack policy requires attack at twhile do not attack
atp, which is not possible.
Example 2 Similar to Example 1, we use action sequence to denote each state and reward function.
Consider this reward function:
• Choose an action sequence a
0;;a
T 1as optimal policy, and set ra
0;;a
T 1= 50
•Choose some p < T 1. Choose another action sequence a0
p+1;;a0
T 2, and
setra
0;;a
p 1;(ap+1) mod3;a0
p+1;;a0
T 2;1= 49 ,ra
0;;a
p 1;(ap+1) mod3;a0
p+1;;a0
T 2;0=
 100
•Choose another action sequence a00
p+1;;a00
T 1, and set
ra
0;;a
p 1;(ap+2) mod3;a00
p+1;;a00
T 1= 48
12
• For any other terminal state, set its reward to any value between [0;47]
For this game, if the sparse attack budget is not less than 2, the only optimal attack policy is to attack
steppand stepT 1, with reward -100:
•ra
0;;a
p 1;(ap+1) mod3;a0
p+1;;a0
T 2;1= 49 , and any other terminal state’s reward is at most
48. If we ﬁrst attack step pand change its action to (a
p+ 1) mod3, then the rest optimal
policy isa0
p+1;;a0
T 2;1
•Then we attack step T 1and change its action to 0, agent will receive  100reward, which
is the only worst reward of the whole game (Any other terminal state’s reward is at least 0)
Now let’s calculate some Qfunction:
•Q
p(sa
0;;a
p 1;a
p) = 50
•Q
p(sa
0;;a
p 1;(a
p+1) mod3) = 49 (The rest optimal policy is a0
p+1;;a0
T 2;1, which
will receive 49reward)
•Q
p(sa
0;;a
p 1;(a
p+ 2) mod3) = 48 (The rest optimal policy is a00
p+1;;a00
T 1, which
will receive 48reward. Any other policy can only get at most 47reward.)
Now, if the agent decide to attack at step pwith no attack before, the optimal attack action (a
p+
1)mod3is not the one that has the lowest Qvalue.
A.2 Map selection
The following factors are considered when choosing maps:
•The performance of the QMIX algorithm. If QMIX itself performs poorly in a map, then
there is little value in performing an adversarial attack on it.
• Both homogeneous maps and heterogeneous maps should be chosen.
•The difﬁculty of the map. Since the more difﬁcult the map itself, the easier for adversarial
attack. Therefore we prefer simple maps than difﬁcult maps.
Also, since we focus on attacking multi agent systems, we do not choose the environment with only
two agents.
The ﬁrst factor excludes 5m_vs_6m, MMM2, 3s5z_vs_3s6z, corridor, 6h_vs_8z. See QMIX paper
for speciﬁc.
For homogeneous maps, we choose 3m, 8m, 25m, 3s_vs_4z, and so_many_baneling. We do not
choose 8m_vs_9m, 10m_vs_11m, or 27m_vs_30m, since they are more difﬁcult than the chosen “m”
maps because the team has fewer agents than the enemy. The 3s_vs_3,4,5z maps are similar, so we
just choose one of them.
All other maps are heterogeneous. 2s3z, 3s5z, and 1c3s5z have some similarity (both contains “s”
and “z” agent), and we choose 2s3z and 1c3s5z. For the rest two maps, we choose bane_vs_bane
since it is the only heterogeneous map with a large number of agents.
A.3 Additional experiment results
Table 5 shows additional experiment results of the 2s3z map. OPT can lower the winning rate to less
than 10% in both cases. All results outperform all baselines.
Table 6 shows additional experiment results of the 1c3s5z map. In this map, “c” is a large agent and
“s”/“z” are small agents, therefore “c” is more important to the team. Compared to previous results,
these results show that attacking “s” (agent 1) or “z” (agent 7) is more difﬁcult than attacking “c”
(agent 0). For agent 7, OPT outperforms all baselines including Ru-D and RL-F. For agent 1, the
result is just so-so, but with = 0:1OPT still outperforms Ru-B and RL-F.
Table 7 shows additional result of the 8m and bane_vs_bane map. The winning rate of the
bane_vs_bane map without attack is 84.0%. For 8m map, OPT outperforms Ra-R/Ra-L and Ru-B, its
13
Table 5: 2s3z map
Map 2s3z Agent 2 2s3z Agent 1 and 2
Attack
typeParameterWinning
rate (%)Attacked steps /
Total stepsParameterWinning
rate (%)Attacked steps /
Total steps
OPT= 0:10.0 21.715/44.057
= 0:10..0 17.782,16.216/43.321
0.0 24.301/46.361 0.0 17.294,19.044/44.120
0.0 22.063/43.904 0.0 17.501,16.134/43.904
= 0:511.1 12.547/48.199
= 0:50.4 9.142,8.593/81.301
1.5 13.875/46.908 0.9 6.161,12.821/48.101
5.0 13.363/49.769 0.1 7.277,12.651/44.000
Ra-R - 91.3 15% - 76.0 10%, 30%
- 41.7 50%
Ra-L - 84.5 15% - 39.5 10%, 30%
- 5.4 50%
Ru-BThreshold
0.077.2 15.052/52.341Threshold
0.087,0.1153.9 10.030,12.258/47.077
RL-Fcadv= 0:2 17.6 16.584/76.338 cadv= 0:1 12.3 19.444,10.877/96.483
Table 6: 1c3s5z map
Map 1c3s5z Agent 7 1c3s5z Agent 1
Attack
typeParameterWinning
rate (%)Attacked steps /
Total stepsParameterWinning
rate (%)Attacked steps /
Total steps
OPT= 0:192.6 10.407/48.971
= 0:183.6 13.408/69.093
92.7 10.907/48.597 83.8 14.504/68.233
92.5 9.013/49.212 84.2 14.731/65.389
= 0:0178.6 42.252/60.875
= 0:0184.2 48.083/60.750
79.7 42.170/55.780 86.4 45.741/59.606
77.1 45.324/58.568 79.8 51.944/67.537
Ra-R - 98.0 20% - 98.1 20%
96.9 80%
Ra-L - 98.5 20% - 98.6 20%
87.8 80%
Ru-D - 89.5 100% - 78.4 100%
Ru-BThreshold
0.0596.9 16.756/48.097Threshold
0.0589.3 15.301/57.828
RL-Fcadv= 0:01 90.1 51.914/61.215 cadv= 0:05 84.3 34.966/63.480
result is similar to RL-F. For bane_vs_bane map, with = 0:1, OPT result is similar to Ru-B, and
with= 0:01OPT outperforms RL-F.
Table 8 shows additional multi agent attack result of 25m and bane_vs_bane map. The winning rate
of these 2 maps without adversarial attack is 98.5% and 84.0% respectively. For 25m map, when
attacking 3 agents, OPT can lower the winning rate to around 45% with each agent less than 10% of
timesteps begin attacked, and it outperforms all baselines including RL-F. For bane_vs_bane map,
when attacking 3 or 5 agents, OPT can lowerer the winning rate, and it outperforms Ra-R/Ra-L and
RL-F. But OPT and Ru-B cannot compare directly, since we cannot ﬁnd a thshold to let Ru-B attack
as many timesteps of agent 0 as OPT does.
Table 9 10 shows additional result of attacking VDN and QTRAN policies. During our experiements,
we found QTRAN performs not very well in the 25m map, so we only attack VDN policies in the
25m map. The result is similar to QMIX policies. In most cases, OPT outperforms all baselines, and
in some cases it outperforms Ru-D.
14
Table 7: 8m and bane_vs_bane map
Map 8m Agent 3 and 6 bane_vs_bane Agent 10
Attack
typeParameterWinning
rate (%)Attacked steps /
Total stepsParameterWinning
rate (%)Attacked steps /
Total steps
OPT= 112.8 2.001,1.464/26.848
= 0:180.1 7.124/74.455
8.9 2.319,1.459/26.121 79.1 9.015/73.818
8.4 2.258,1.467/26.332 81.9 7.618/73.672
= 521.1 1.692,1.428/27.900
= 0:0176.0 21.749/85.297
20.3 1.675,1.401/27.311 78.5 20.925/82.861
21.4 1.721,1.421/27.446 76.7 18.996/78.974
Ra-R - 97.6 15% - 81.8 15%
82.8 25%
Ra-L - 88.6 15% - 84.1 15%
80.8 25%
Ru-BThreshold
0.1847.0 12.180/72.400Threshold
0.0579.2 9.21/76.463
RL-Fcadv= 1 23.0 1.896,1.013/28.429 cadv= 0:01 78.6 35.670/83.136
15
Table 8: 25m and bane_vs_bane map: attacking multiple agents
Map 25m Agent 0, 10, 20
attack
typeparameterWinning
rate (%)Attacked steps /
Total steps
OPT = 143.9 4.924,3.491,3.060/51.127
44.4 4.697,3.511,3.269/49.809
44.8 4.826,3.318,2.982/52.125
Ra-R - 95.4 10%, 10%, 10%
Ra-L - 91.4 10%, 10%, 10%
Ru-BThreshold
0.15, 0.1, 0.156.39.622,8.996,6.702
/52.410
RL-F cadv= 0:23 80.5 10.915,4.781,0.252/51.467
Map bane_vs_bane Agent 0, 10, 20
Attack
typeParameterWinning
rate (%)Attacked steps /
Total steps
OPT = 0:0141.3 104.218,21.053,17.397/143.964
44.9 67.105,20.244,22.769/131.688
27.4 102.376,16.529,17.030/160.459
Ra-R - 90.2 75%, 15%, 15%
Ra-L - 91.8 75%, 15%, 15%
Ru-BThreshold
0.005, 0.01, 0.0179.854.483,9.809,9.178
/78.794
RL-F cadv= 0 71.1 63.694,53.639,50.014/92.802
Map bane_vs_bane Agent 0, 5, 10, 15, 20
Attack
typeParameterWinning
rate (%)Attacked steps /
Total steps
OPT = 0:0123.5119.952,19.067,21.522,15.708,16.029
/164.122
20.287.203,25.645,21.332,22.612,22.143
/169.967
26.788.507,21.880,16.905,10.793,10.950
/163.439
Ra-R - 90.7 70%, 15%, 15%, 15%, 15%
Ra-L - 92.2 70%, 15%, 15%, 15%, 15%
Ru-BThreshold
0.005,0.005,0.005,0.005,0.00578.563.232,9.479,9.048,10.210,11.273
/84.098
RL-F cadv= 0 67.1 37.091,31.063,18.260,27.080,20.191/109.740
16
Table 9: 1c3s5z map VDN / QTRAN
Map 1c3s5z Agent 0 VDN 1c3s5z Agent 0 QTRAN
Attack
typeParameterWinning
rate (%)Attacked steps /
Total stepsParameterWinning
rate (%)Attacked steps /
Total steps
OPT= 53.6 5.038/60.800
= 10.0 4.156/69.583
4.0 5.123/59.584 0.0 4.059/67.123
4.4 5.483/60.106 0.0 4.005/69.318
Ra-R - 95.5 10% 84.2 10%
Ra-L - 89.4 10% 80.0 10%
Ru-BThreshold
0.02547.4 7.652/64.042Threshold
0.04537.5 10.996/86.140
RL-Fcadv= 0:3 3.6 5.233/60.466 cadv= 0:1 1.1 11.249/69.823
Map 1c3s5z Agent 1+7 VDN 1c3s5z Agent 1+7 QTRAN
Attack
typeParameterWinning
rate (%)Attacked steps /
Total stepsParameterWinning
rate (%)Attacked steps /
Total steps
OPT= 0:015.760.854,58.683
/73.878= 0:012.559.435,55.070
/75.038
5.055.652,57.354
/75.5602.059.885,57.792
/73.920
5.458.902,58.434
/72.1092.360.870,58.052,
/72.261
Ra-R - 81.5 80%,80% - 58.8 85%,85%
Ra-L - 39.4 80%,80% - 17.6 85%,85%
Ru-D - 24.5 100%,100% - 6.3 100%,100%
RL-Fcadv= 0 31.079.500,59.505
/119.208cadv= 0 11.9121.312,44.639
/138.440
17
Table 10: 25m map VDN
Map 25m Agent 0 VDN
Attack
typeParameterWinning
rate (%)Attacked steps /
Total steps
OPT = 0:0175.6 34.392/61.564
73.1 40.409/64.335
70.2 39.407/64.272
Ra-R - 95.4 55%
Ra-L - 88.4 55%
Ru-D - 81.4 100%
RL-F cadv= 0 82.3 32.235/55.256
Map 25m Agent 0, 10, 20 VDN
Attack
typeParameterWinning
rate (%)Attacked steps /
Total steps
OPT = 113.0 3.384,3.758,3.199/45.249
14.3 3.749,4.153,2.996/49.728
11.1 3.493,3.648,3.148/45.895
Ra-R - 96.7 10%, 10%, 10%
Ra-L - 93.6 70%, 10%, 10%
Ru-BThreshold
0.02,0.014,0.01682.3 4.825,3.816,4.621/42.664
RL-F cadv= 0:2 79.3 6.462,6.787,1.411/53.820
Map 25m 0, 5, 10, 15, 20 VDN
Attack
typeParameterWinning
rate (%)Attacked steps /
Total steps
OPT = 11.35.145,4.456,3.901,4.382,3.114
/48.018
1.24.837,4.328,3.964,4.392,3.177
/46.169
0.94.835,4.805,4.206,4.103,3.240
/49.859
Ra-R - 95.6 10%, 10%, 10%, 10%, 10%
Ra-L - 86.7 10%, 10%, 10%, 10%, 10%
Ru-BThreshold
0.019,0.015,0.015,0.015,0.015509.677,5.401,6.301,5.445,5.343
/54.064
RL-F cadv= 0:1 9.3 5.370,12.808,18.994,11.546,30.295/116.817
18"
2212.02705v5.pdf,"Published in Transactions on Machine Learning Research (01/2024)
What is the Solution for State-Adversarial
Multi-Agent Reinforcement Learning?
Songyang Han songyang.han@uconn.edu
School of Computing
University of Connecticut
Sony AI
Sanbao Su sanbao.su@uconn.edu
School of Computing
University of Connecticut
Sihong He sihong.he@uconn.edu
School of Computing
University of Connecticut
Shuo Han hanshuo@uic.edu
Department of Electrical and Computer Engineering
University of Illinois Chicago
Haizhao Yang hzyang@umd.edu
Department of Mathematics and Department of Computer Science
University of Maryland College Park
Shaofeng Zou szou3@buffalo.edu
Department of Electrical Engineering and Department of Computer Science and Engineering
University at Buffalo, The State University of New York
Fei Miao fei.miao@uconn.edu
School of Computing
University of Connecticut
Reviewed on OpenReview: https: // openreview. net/ forum? id= HyqSwNhM3x
Abstract
Various methods for Multi-Agent Reinforcement Learning (MARL) have been developed
with the assumption that agents’ policies are based on accurate state information. However,
policies learned through Deep Reinforcement Learning (DRL) are susceptible to adversarial
state perturbation attacks. In this work, we propose a State-Adversarial Markov Game
(SAMG) and make the first attempt to investigate different solution concepts of MARL under
state uncertainties. Our analysis shows that the commonly used solution concepts of optimal
agent policy and robust Nash equilibrium do not always exist in SAMGs. To circumvent this
difficulty, we consider a new solution concept called robust agent policy, where agents aim to
maximize the worst-case expected state value. We prove the existence of robust agent policy
for finite state and finite action SAMGs. Additionally, we propose a Robust Multi-Agent
Adversarial Actor-Critic (RMA3C) algorithm to learn robust policies for MARL agents under
state uncertainties. Our experiments demonstrate that our algorithm outperforms existing
methods when faced with state perturbations and greatly improves the robustness of MARL
policies. Our code is public on https://songyanghan.github.io/what_is_solution/ .
1arXiv:2212.02705v5  [cs.AI]  12 Apr 2024
Published in Transactions on Machine Learning Research (01/2024)
1 Introduction
Multi-Agent Reinforcement Learning (MARL) has been successfully used to solve problems such as multi-robot
coordination (Hüttenrauch & Šošić, 2017), resource management (Pretorius et al., 2020), etc. However, Deep
Reinforcement Learning (DRL) policies are vulnerable to adversarial state perturbation attacks (Behzadan &
Munir, 2017; Pattanaik & Tang, 2017; Huang et al., 2017; Lin et al., 2017; Xiao et al., 2019). Even small
changes to the state can lead to drastically different actions (Huang et al., 2017; Lin et al., 2017). To address
this, it is important to develop robust policies that can handle adversarial state perturbations. An example
of this is shown in Fig. 1 where agents need to cooperate and avoid collisions while occupying landmarks. In
(a) with no adversarial state, the agents are able to target different landmarks, but in (b) with adversarial
state perturbations, agents head in the wrong direction.
Figure 1: The agents’ goal is to occupy and cover all landmarks, requiring cooperation to decide which
landmark to cover. Figure a) illustrates the optimal target landmark for each agent without state perturbation.
However, in figure b), an adversary perturbs the state observation of agents, causing agents to head in the
wrong direction and leaving landmark 1 as uncovered. Our work demonstrates that traditional agent policies
can be easily corrupted by adversarial state perturbations. To counter this, we propose a robust agent policy
that maximizes average performance under worst-case state perturbations.
The adversarial state perturbation problem cannot be fully understood using existing research on the Partially
Observable Markov Decision Process (POMDP) or Decentralized Partially Observable Markov Decision
Process (Dec-POMDP) (Oliehoek et al., 2016; Lerer et al., 2020), as the conditional observation probability
cannot capture the worst-case uncertainty under adversarial attacks. Adversarial perturbations have a greater
impact on an agent’s policy than random noise (Kos & Song, 2017; Pattanaik et al., 2018). However, due to
the complexity of interactions among agents and adversaries, it remains challenging to formally analyze the
existence of optimal or equilibrium solutions under adversarial state perturbations in MARL. Therefore, it is
essential to study the fundamental properties of MARL under adversarial state perturbations.
To the best of our knowledge, we make the first attempt to investigate different solution concepts of robust
MARL under adversarial state perturbations. We formulate a State-Adversarial Markov Game (SAMG) to
study the properties and solution concepts of MARL under adversarial state perturbations. We prove that
a state-robust totally optimal agent policy or robust total Nash equilibrium does not always exist in such
scenarios. Instead, we consider a new solution concept, the robust agent policy, and prove its existence for
finite state and action spaces. We design an algorithm, called Robust Multi-Agent Adversarial Actor-Critic
(RMA3C), to train robust policies for all agents under adversarial state perturbations. The algorithm uses a
Gradient Descent Ascent (GDA) optimizer to update each agent’s and adversary’s policy network. Results
from our experiments show that the proposed RMA3C algorithm improves the robustness of the agents’
policies compared to existing MARL methods.
In summary, the main contributions of this work are:
2
Published in Transactions on Machine Learning Research (01/2024)
•We study the fundamental properties of MARL under adversarial state perturbations and prove that
widely used solution concepts such as optimal agent policy or robust Nash equilibrium do not always
exist.
•We consider a new solution concept, robust agent policy, where each agent aims to maximize the
worst-case expected state value. We prove the existence of a robust agent policy for SAMGs with
finite state and action spaces. We propose a Robust Multi-Agent Adversarial Actor-Critic (RMA3C)
algorithm to solve the challenge of training robust policies under adversarial state perturbations
based on gradient descent ascent algorithm.
•We empirically evaluate our proposed RMA3C algorithm. Our algorithm outperforms baselines
with random or adversarial state perturbations and improves agent policies’ robustness under state
uncertainties.
2 Related Work
Multi-Agent Reinforcement Learning (MARL) The MARL has a long history in the AI field (Littman,
1994; Hu et al., 1998; Busoniu et al., 2008). Recent works have been investigated to encourage the collaboration
of the agents by assigning rewards appropriately, such as a value decomposition network (Sunehag et al.,
2018; Rashid et al., 2020; Su et al., 2021), subtracting a counterfactual baseline (Foerster & Farquhar, 2018),
or an implicit method (Zhou et al., 2020). Multi-Agent Deep Deterministic Policy Gradient (MADDPG)
proposes a centralized Q-function to alleviate the problem caused by the non-stationary environment (Lowe
et al., 2017). The scalability issue of MARL can be alleviated by adding attention to the critic (Iqbal &
Sha, 2019), using neighbor information (Qu et al., 2020), or using V-learning (Jin et al., 2021). The “team
stochastic game” (Muniraj et al., 2018; Phan et al., 2020) splits the MARL agents into two teams to compete.
However, during training, all methods assume that agents get the true state value. None of the recent MARL
advances specifies how to deal with perturbed state values by malicious adversaries.
Robust Reinforcement Learning Most existing robust MARL works focus on uncertainties in reward,
transition dynamics, and training partners’ policies, while our work focuses on uncertainties in the state.
Robust reinforcement learning can be traced back to Morimoto & Doya (2005) in the single-agent setting.
With the advent of deep learning techniques, the robust MARL has been recently studied considering different
types of uncertainties such as reward (Chen & Bowling, 2012; Zhang et al., 2020b), transition dynamics (Zhang
et al., 2020b; Sinha et al., 2020; Hu et al., 2020; Yu et al., 2021; Wang et al., 2023), training partner’s
type (Shen & How, 2021), training partners’ policies (Li et al., 2019; van der Heiden et al., 2020; Sun et al.,
2021; 2022). The work in (Zhang et al., 2020b) considers the robust equilibrium of multi-agents with reward
uncertainties where agents can access true state information at each stage. The work in Shen & How (2021)
considers uncertain training partner’s type (e.g. adversary, neutral, or teammate) to the protagonist in
two-player scenarios. The M3DDPG algorithm extends the MADDPG to get a robust policy for the worst
situation by assuming all the training partners are adversaries (Li et al., 2019). However, none of the above
MARL works consider the state perturbations.
For adversarial state perturbations, there are some works (Mandlekar et al., 2017; Pinto et al., 2017; Pattanaik
et al., 2018; Zhang et al., 2020a; 2021; Liang et al., 2022) considering a robust policy in single-agent
reinforcement learning. Though the work (Lin et al., 2020a) studies state perturbation, only one single agent’s
state observation can be perturbed in their MARL. The work (He et al., 2023) shows Nash equilibrium exists
under a specific condition (bijective mapping for adversary policies). However, in this work, we show the
Nash equilibrium is not a good solution concept as it can be corrupted by state perturbation adversaries. We
also propose a new robust agent policy concept for state-adversarial MARL that is proven to exist.
3 State-Adversarial Markov Game (SAMG)
We formulate a State-Adversarial Markov Game (SAMG) G= (N,S,A,r,Ps,p,γ, Pr(s0))withnagents in
the agent setN={1,...,n}. Each agent iis associated with an action ai∈Ai. The global joint action is
a= (a1,...,an)∈A,A:=A1×···×An. The global joint state is s∈S. The probability distribution of the
3
Published in Transactions on Machine Learning Research (01/2024)
initial state is Pr(s0). All agents share a stage-wise reward function r:S×A→ R. We consider that each
agent is associated with an adversary as shown in Fig. 2. Each adversary decides a perturbed state ρi∈S
for the corresponding agent as the agent’s perturbed knowledge or observation about the global state. We
denote the joint perturbed state as ρ:= [ρi]i∈N. We consider the admissible perturbed state as a task-specific
“neighboring” state of s, e.g. the bounded sensor measurement errors, to model the challenges of getting
accurate states for multi-agent systems like connected and autonomous vehicles and multi-robots systems (Liu
et al., 2021; Kothandaraman et al., 2021). To analyze a realistic problem, the power of the state perturbation
should also be limited (Everett et al., 2021; Zhang et al., 2020a). We define an admissible perturbed state set
Psto restrict the perturbed state only to be within a predefined subset of states such that ρ∈Ps:
Definition 3.1 (Admissible Perturbed State Set ).We consider the set of admissible perturbed state for
agentiat statesasPi
s⊆S. Denote the joint admissible perturbed state set at state sasPs:=P1
s×···×Pn
s.
Note that the true state is included in the admissible perturbed state set, i.e., s∈Pi
sfor anyi∈N. For
example, consider a 2-agent 3-state system with S={s1,s2,s3}. When the current true state is s1for both
agents, adversary 1perturbs agent 1’s state observation within P1
s1={s1,s2}; adversary 2perturbs agent 2’s
state observation within P2
s1={s1,s3}.
Figure 2: Comparison between Dec-POMDP and SAMG. In Dec-POMDP, the observation probability
function is fixed, and it will not change according to the change of the agent policy. However, in SAMG the
adversary policy is not a fixed policy, it may change according to the agents’ policies and always select the
worst-case state perturbation for agents. In SAMG, each agent is associated with an adversary to perturb its
knowledge or observation of the true state. Agents want to find a policy πto maximize their total expected
return while adversaries want to find a policy χto minimize agents’ total expected return.
The state perturbation reflects the state uncertainty from the perspective of each agent, but it does not
change the true state of multi-agent systems. The state transition function is p:S×A→ ∆(S), where ∆(S)
is a probability simplex denoting the set of all possible probability measures on S. The state still transits
from the true state to the next state. Each agent is associated with a policy πi:S→ ∆(Ai)to choose
an actionai∈Aigiven the perturbed state ρi. Note that the input of πiis the perturbed state ρi. The
perturbed state affects each agent’s action. The set ∆(Ai)includes all possible probability measures on Ai.
We useπ= (π1,π2,...,πn)to denote the joint agent policy.
The adversary policy, i.e. the state perturbation policy, associated with agent iisχi(·|s) :S→ ∆(Pi
s), where
the input of χiis the true state s∈S. The power of the adversary is limited by the admissible perturbed
state setPi
s. We denote the joint adversary policy as χ= (χ1,χ2,...,χn). The agents want to find a policy π
to maximize their total expected return while adversaries want to find a policy χto minimize the agents’
total expected return. The total expected return is E[/summationtext∞
t=0γtrt+1(st,at)|s0,at∼π(·|ρt),ρt∼χ(·|st)]where
γis a discount factor.
Our SAMG problem cannot be solved by the existing work for single-agent RL with adversarial state
perturbations (Mandlekar et al., 2017; Pattanaik et al., 2018; Zhang et al., 2020a; 2021; Liang et al., 2022).
Each agent’s action in SAMG is selected based on its own perturbed state observation and the state knowledge
4
Published in Transactions on Machine Learning Research (01/2024)
utMaximize State ValueState-of-the-Art Solution Concepts
Non-existence (Theorem 4.3)Non-existence (Theorem 4.7)Optimal Agent Policy (Deﬁnition 4.2)Robust Nash Equilibrium
(Deﬁnition 4.6)
State Value Function
(Equation 1)Robust State Value Function (Deﬁnition 4.4)Uniqueness in Theorem 4.5;
No Agent Deviates UnilaterallyWorst Case Expected State Value (Deﬁnition 4.8)Existence of Robust Agent Policy (Theorem 4.11)New Solution ConceptsRobust Agent Policy (Deﬁnition 4.9)MaximizeEquivalent to a Maximin Problem (Theorem 4.10)
Figure 3: Solution concepts for the SAMGs. We first examine the widely used concepts (optimal agent policy
and robust Nash Equilibrium) and demonstrate their non-existence under adversarial state perturbations. In
response, we consider a new objective, the worst-case expected state value, and a new solution concept, the
robust agent policy.
of each agent can be different after adversarial perturbations, so the SAMG problem cannot be solved by the
above single-agent RL where the agent has only one state observation at each stage.
Our SAMG problem cannot be solved by the existing work in the Decentralized Partially Observable Markov
Decision Process (Dec-POMDP) (Bernstein et al., 2002; Oliehoek et al., 2016) as shown in Fig. 2. In contrast,
the policy in SAMG needs to be robust under a set of admissible perturbed states. The adversary aims to
find the worst-case state perturbation policy χto minimize the MARL agents’ total expected return, but the
Dec-POMDP cannot characterize the worst-case state perturbations. Moreover, agents usually cannot get the
true statesin Dec-POMDP, while in SAMG, the true state sis known by the adversaries. Adversaries can
take the true state information and use it to select state perturbations for the MARL agents. The following
proposition 3.2 shows that under a fixed adversarial policy, the SAMG problem becomes a Dec-POMDP.
However, in SAMG the adversary policy is not a fixed policy, it may change according to the agents’ policies
(see Theorem 4.1 for detail) and always select the worst-case state perturbation for agents. The proof of
proposition 3.2 is in Appendix A. We also give a two-agent two-state SAMG that cannot be solved by
Dec-POMDP in Appendix A.
Proposition 3.2. When the adversary policy is a fixed policy, the SAMG problem becomes a Dec-
POMDP (Oliehoek et al., 2016).
Proposition 3.3. When the adversary policy is a fixed bijective mapping from StoS, the SAMG problem
becomes a Markov game.
Additionally, our SAMG problem cannot be solved by existing methods for robust Markov games considering
the uncertainties from reward (Chen & Bowling, 2012; Zhang et al., 2020b), transition dynamics (Zhang et al.,
2020b; Hu et al., 2020; Sinha et al., 2020; Yu et al., 2021; Wang et al., 2023), training partner’s policies (Li
et al., 2019; van der Heiden et al., 2020). These methods are not applicable to our problem because the
agents do not have access to the true state information after adversarial perturbations.
4 Solution Concepts
In this section, we delve into the solution concepts of the SAMG. We formally define key concepts such as
an optimal adversary policy, state-robust totally optimal agent policy, and robust total Nash equilibrium.
However, we also demonstrate that under an optimal adversary policy, the existence of a state-robust totally
optimal agent policy or robust total Nash equilibrium is not guaranteed as they can be easily corrupted by
adversaries. Therefore, we introduce a new objective, the worst-case expected state value, and prove that
there exists a robust agent policy to maximize it. A concept diagram of this section is shown in Fig. 3.
We first introduce the widely used state value function concept for our proposed SAMG as follows:
Vπ,χ(s) =Eat∼π(·|ρt),ρt∼χ(·|st)/bracketleftigg∞/summationdisplay
t=0γtrt+1(st,at)|s0=s/bracketrightigg
, (1)
5
Published in Transactions on Machine Learning Research (01/2024)
whereγis the discount factor.
4.1 Optimal Adversary Policy
For a fixed agent policy π, define the worst-case state value function ¯Vπunderπby
¯Vπ(s):= min
χVπ,χ(s) (2)
for alls∈S. An adversary policy χ∗is said to be optimalagainst an agent policy πif
Vπ,χ∗(s) =¯Vπ(s) (3)
for alls∈S. The following proposition shows the existence of an optimal adversary for an SAMG.
Proposition 4.1 (Existence of Optimal Adversary Policy .).Given an SAMG, for any given agent
policy, there exists an optimal adversary policy.
The key process of the proof in Appendix B.4 is constructing an MDP for the adversary where the adversary
gets the negative of the agent reward. Since for an MDP with finite state and finite action spaces, there
always exists an optimal policy [Theorem 6.2.10 in Puterman (2014)], an optimal adversary policy of the
corresponding SAMG always exists as well.
4.2 State-robust Totally Optimal Agent Policy
An optimal adversary policy is very powerful and it can easily corrupt the MARL agents’ policies through
state perturbations. We first define a state-robust totally optimal agent policy as follows:
Definition 4.2 (State-robust Totally Optimal Agent Policy ).An agent policy π∗is a state-robust
totally optimal agent policy if ¯Vπ∗(s)≥¯Vπ(s)for anyπand alls∈S.
In the following theorem, we show that a state-robust totally optimal agent policy π∗does not always exist
for SAMGs under an optimal state perturbation adversary.
Theorem 4.3 (Non-existence of State-robust Totally Optimal Agent Policy ).A state-robust totally
optimal agent policy does not always exist for SAMGs.
The proof in Appendix B.5 is done by constructing a counterexample where there is no optimal policy
for the agents. A state-robust totally optimal agent policy is expected to maximize the state value for all
states. However, under the adversarial state perturbations, sometimes agents have to make trade-offs between
different state values and no agent policy can maximize all the state values.
4.3 Robust Total Nash Equilibrium
Then we look at the widely-used Nash equilibrium concept in MARL for SAMGs. A Nash equilibrium is
used to describe policies where no agent wants to deviate unilaterally. If an agent deviates from a Nash
equilibrium, its total expected return won’t increase. Denote the agent policies and adversary policies of
all other agents and adversaries except agent iand adversary iasπ−iandχ−irespectively. Before giving
the definition of a robust total Nash equilibrium, we first show that there exists a unique robust state value
function for agent igiven anyπ−iandχ−i.
Definition 4.4 (Robust state value function ).A state value function Vi
∗,π−i,∗,χ−i:S→Rfor agenti
givenπ−iandχ−iis called a robust state value function if for all s∈S,
Vi
∗,π−i,∗,χ−i(s) = max
πimin
χi/summationdisplay
ρ∈Psχ(ρ|s)/summationdisplay
a∈Aπ(a|ρ)/parenleftigg
r(s,a) +γ/summationdisplay
s′∈Sp(s′|s,a)Vi
∗,π−i,∗,χ−i(s′)/parenrightigg
.
Theorem 4.5 (Existence of Unique Robust State Value Function ).For an SAMG with finite state
and finite action spaces, for any i∈N, given any π−iandχ−iof other agents and adversaries except agent i
6
Published in Transactions on Machine Learning Research (01/2024)
and adversary i, there exists a unique robust state value function Vi
∗,π−i,∗,χ−i:S→Rfor agentisuch that
for alls∈S,
Vi
∗,π−i,∗,χ−i(s) = max
πimin
χi/summationdisplay
ρ∈Psχ(ρ|s)/summationdisplay
a∈Aπ(a|ρ)/parenleftigg
r(s,a) +γ/summationdisplay
s′∈Sp(s′|s,a)Vi
∗,π−i,∗,χ−i(s′)/parenrightigg
.
The proof is based on the contraction mapping property and Banach’s fixed point theorem in Appendix C.1.
The theorem could also follow from viewing the SAMG as an extensive form game under imperfect recall and
applying the approaches used to analyze those games Chen & Bowling (2012).
Based on the well-defined robust state value function, a robust total Nash equilibrium is defined considering
each agent is associated with an adversary that tries to minimize its total expected return.
Definition 4.6 (Robus Total Nash Equilibrium ).For an SAMG, the policy (π∗,χ∗)is a robust total
Nash equilibrium if for all s∈Sand alli∈Nand allπiandχi, it holds that
Vi
πi,π−i∗,χi∗,χ−i∗(s)≤Vi
πi∗,π−i∗,χi∗,χ−i∗(s)≤Vi
πi∗,π−i∗,χi,χ−i∗(s), (4)
whereπ−iandχ−idenotes the agent policies and adversary policies of all the other agents except agent i,
respectively.
Definition 4.6 shows that π∗is in a robust total Nash equilibrium if each agent’s policy is a robust best
response to the other agents’ policies under adversarial state perturbations. When agent iis calculating its
robust best response, it assumes a worst-case perspective of the state perturbations.
Theorem 4.7 (Non-existence of Robust Total Nash Equilibrium ).For SAMGs with finite state and
finite action spaces, the robust total Nash equilibrium defined in Definition 4.6 does not always exist.
The proof in Appendix C.3 is done by constructing a counterexample. For any state s∈S, there exists
a stage-wise equilibrium among the agents and adversaries (See the proof for a stage-wise equilibrium
in Theorem C.6 in Appendix C.2). However, due to the uncertainty of the true state under adversarial
state perturbations, it is possible that the stage-wise equilibrium in one state conflicts with the stage-wise
equilibrium in another state. As a result, the agents may be required to make trade-offs between different
states, making it impossible to find an equilibrium that holds for all states.
4.4 Robust Agent Policy
The state-robust totally optimal agent policy and robust total Nash equilibrium concepts do not always
exist in our SAMG problem according to the above non-existence analysis. To circumvent this difficulty, we
consider another solution concept in which each agent adopts a policy (hereafter referred to as a robust agent
policy) that maximizes the reward under the worst-case state perturbation. We further show that a robust
policy always exists for all agents. We first introduce a new objective for SAMGs, the worst-case expected
state value:
Definition 4.8 (Worst-case Expected State Value ).The worst-case expected state value under the
optimal state perturbation adversary is: Es0∼Pr(s0)/bracketleftbig¯Vπ(s0)/bracketrightbig
,where Pr(s0)is the probability distribution of
the initial state.
To account for the fact that a policy is not able to maximize all state values in SAMG, we can use the
probability of each state as a measure of its importance, and balance the values of different states. The
worst-case expected state value is calculated by taking a weighted sum of all state values based on their initial
state distribution. The agent policy that aims to maximize this worst-case expected state value is referred to
as a robust agent policy.
Definition 4.9 (Robust Agent Policy ).An agent policy π∗that maximizes the worst-case expected state
value is called a robust agent policy:
π∗∈arg max
πEs0∼Pr(s0)/bracketleftbig¯Vπ(s0)/bracketrightbig
. (5)
7
Published in Transactions on Machine Learning Research (01/2024)
The following theorem shows finding a robust agent policy is equivalent to solving a maximin problem.
Theorem 4.10. Finding an agent policy πto maximize the worst-case expected state value under an optimal
adversary for πis equivalent to the maximin problem: maxπminχ/summationtext
s0Pr(s0)Vπ,χ(s0).
In the following theorem, we show the existence of a robust agent policy for finite state and finite action
spaces.
Theorem 4.11 (Existence of Robust Agent Policy ).For SAMGs with finite state and finite action spaces,
there exists a robust agent policy to maximize the worst-case expected state value defined in Definition 4.8.
The proof in Appendix C.4 is based on the Weierstrass M-test (Rudin et al., 1976), uniform limit theo-
rem (Rudin et al., 1976), and the extreme value theorem. Different from the definitions of the state-robust
totally optimal agent policy and robust total Nash equilibrium, the worst-case expected state value objective
does not require the optimality condition to hold for all states. Agents won’t get stuck in trade-offs between
different states, therefore, we can find a robust agent policy to maximize the worst-case expected state value
for the SAMG problem.
The robust agent policy, while motivated in specific instances of non-existence theorems, is designed to offer
broader applicability. The significance of the new solution concept lies in providing alternative solutions
where traditional methods may falter or be inapplicable. This versatility is crucial in advancing the field,
particularly in complex scenarios where standard solutions are inadequate.
Multi-Agent Adversarial Actor-Critic (RMA3C) Algorithm In general, it is challenging to develop
algorithms that compute optimal or equilibrium policies for MARL under uncertainties (Zhang et al., 2020b;
2021). We design a RMA3C Algorithm based on our theoretical analysis above. Each agent has one critic
network, one actor network πiand one adversary network χi. The critic Qtakes in the true global state
and global action during the training process. It returns a Q-value denoting the total expected return given
sanda. We use Gradient Descent Ascent (GDA) optimizer (Lin et al., 2020b) to update parameters for
each agent’s actor network and adversary network for the maximin problem maxπminχ/summationtext
s0Pr(s0)Vπ,χ(s0)
in Theorem 4.10. A detailed introduction for the RMA3C and pseudocode is included in Appendix D.
Figure 4: Our RMA3C algorithm compared with several baseline algorithms in training. The results show
that our RMA3C algorithm outperforms the baselines, achieving higher mean episode rewards and greater
robustness to state perturbations. The baselines were trained under either random state perturbations or a
well-trained adversary policy χ∗(adversaries that are trained for the maximum training episodes in RMA3C).
Overall, our RMA3C algorithm achieved up to 58.46% higher mean episode rewards than the baselines.
5 Experiments
To demonstrate the effectiveness of our algorithm, we utilize the multi-agent particle environments developed
in Lowe et al. (2017) which consist of multiple agents and landmarks in a 2D world. The host machine
adopted in our experiments is a server configured with AMD Ryzen Threadripper 2990WX 32-core processors
and four Quadro RTX 6000 GPUs. Our experiments are performed on Python 3.5.4, Gym 0.10.5, Numpy
1.14.5, Tensorflow 1.8.0, and CUDA 9.0. In our experiments, we consider the set of admissible perturbed
8
Published in Transactions on Machine Learning Research (01/2024)
Table 1: Mean episode reward of 2000 episodes during testing. Our RMA3C policy achieves up to 46.56%
higher mean episode rewards than the baselines with random state perturbations N.
Environment CN ET KA PD
MA Lowe et al. (2017) −388.59±60.72−45.79±23.50-8.80±5.07 3.03±0.67
M3 Li et al. (2019) -390.94 ±59.83 -39.55±20.53 -8.54±5.04 2.12±1.04
MP Yu et al. (2022) -381.70 ±54.06 -37.62±18.94 - -
MADDPG (MA) w/ N-487.67±72.28 -55.79±26.78 -11.21±6.82 1.24±0.47
M3DDPG (M3) w/ N-478.96±70.27 -54.40±26.64 -11.28±6.71 1.30±0.58
MAPPO (MP) w/ N-523.83±78.45 -86.51±30.86 - -
RMA3C w/N(ours)-390.20±64.82 -46.23±24.76 -9.02±5.87 2.48±1.26
state for agent iat statesas anℓ∞norm ball around s:Pi
s:={ρi∈S:∥ρi−s∥∞≤d}wheredis a radius
denoting the perturbation budget. In implementation, the adversary network takes in the true state sand
learns a state perturbation vector ∆iand we project s+ ∆itoPi
s. The environments used in our experiments
include cooperative navigation (CN), exchange target (ET), keep-away (KA), and physical deception (PD). A
detailed introduction to these environments can be found in Appendix E. All hyperparameters used in our
experiments for RMA3C and the baselines are listed in Appendix E, along with additional implementation
details and experiment results.
5.1 Baselines
In our experiment, we have a total of 9 baselines: MADDPG Lowe et al. (2017), M3DDPG Li et al. (2019),
MAPPO Yu et al. (2022), as well as versions of these algorithms with random and well-trained adversarial
state perturbations. Detailed explanation of these baselines can be found in Appendix E.2. To evaluate
robustness under state uncertainty, we add state noise to MADDPG, M3DDPG, and MAPPO produced
by a truncated normal distribution N(0,λ,u,l )whereλis the uncertainty level, uandlare the upper and
lower bounds to ensure noise’s compactness. This simulates adversaries selecting random state perturbations.
In contrast, our RMA3C algorithm trains agents under adversaries that try to minimize the agents’ total
expected return. We save the well-trained adversaries χ∗for each scenario in RMA3C to represent the optimal
state perturbation adversaries. The well-trained adversaries are the adversary policies trained in the RMA3C
algorithm when the algorithm reaches the maximum training episodes (100k episodes). We then use these
adversaries to perturb the states for MADDPG, M3DDPG, and MAPPO to train and test their robustness
under adversarial state perturbations. Because MAPPO provided in Yu et al. (2022) only works in fully
cooperative tasks, we only report its results in cooperative navigation and exchange target. For both training
and testing, we report statistics that are averaged across 10 runs in each scenario and algorithm.
5.2 Comparison Results
Training Comparison Under different Perturbations We compare our RMA3C algorithm with
baselines during the training process to demonstrate its superiority in terms of mean episode rewards under
different state perturbations as shown in Fig. 4. As RMA3C has a built-in adversary to perturb states, we
do not train it under random state perturbations. In comparison to other baselines with different state
perturbations, RMA3C consistently achieved higher mean episode rewards, demonstrating its robustness under
varying state perturbations. Furthermore, when comparing each baseline with random state perturbations to
the same baseline with the well-trained adversary policy χ∗, we can see that the adversary policy trained by
RMA3C is more effective than random state perturbations. This is because χ∗is designed to intentionally
select state perturbations that minimize the agents’ total expected return. The mean episode rewards of the
last 1000 episodes during training are shown in the table in Appendix E.5. Our RMA3C algorithm achieved
up to 58.46% higher mean episode rewards than the baselines under different state perturbations.
Training Comparison With More Agents Our RMA3C algorithm is compared with baselines in
the cooperative navigation scenario with an increasing number of agents. As shown in Fig.4, the original
cooperative navigation environment has 3 agents and our RMA3C algorithm outperforms the baselines in
terms of mean episode rewards. In Fig.5(a), we present the results of training with 4 agents, where our
9
Published in Transactions on Machine Learning Research (01/2024)
(a)
 (b)
Figure 5: 5(a): Our RMA3C algorithm continues to achieve higher mean episode rewards, even with an
increasing number of agents in the environment. 5(b):Our RMA3C algorithm is trained in the cooperative
navigation environment with different perturbation budgets d. Whendincreases, adversaries get more
advantage, and may further decrease agents’ total expected return.
RMA3C algorithm still surpasses the baselines. We include the training results with 6 agents in Appendix E.6.
Our RMA3C algorithm continues to achieve higher mean episode rewards, even with an increasing number of
agents in the environment.
Table 2: Our RMA3C policy achieves up to 54.02% higher mean episode reward than the baselines with
well-trained χ∗.
Environment CN ET KA PD
MADDPG (MA) w/ χ∗-537.56±72.28 -71.65±42.50 -14.72±5.44 -0.95±1.32
M3DDPG (M3) w/ χ∗-515.85±74.58 -70.68±41.54 -13.51±5.30-0.70±0.96
MAPPO (MP) w/ χ∗-572.39±79.34 -109.26±47.97 - -
RMA3C w/ χ∗(ours)-400.82±62.59 -50.23±26.97 -9.64±5.311.23±0.82
TrainingComparisonWithDifferentPerturbationBudgets Wecompareouralgorithmwithbaselines
in the cooperative navigation scenario with varying levels of perturbation budgets d. We consider the set of
admissible perturbed state for agent iat statesas anℓ∞norm ball around s:Pi
s:={ρi∈S:∥ρi−s∥∞≤d}
wheredis a radius denoting the perturbation budget. As shown in Fig. 5(b), when dincreases, adversaries
have greater freedom to perturb the state within a larger admissible perturbed state set. As dincreases,
adversaries get more powerful and lead to a decrease in agents’ total expected return.
Testing Comparison in different Environments Our RMA3C algorithm is tested in different envi-
ronments to demonstrate its robustness under state perturbations. As shown in Table 1, the mean episode
rewards are averaged across 2000 episodes and 10 test runs in each environment. The results of MADDPG,
M3DDPG, and MAPPO, which are not designed to handle state perturbations, are shown as a reference for
the no state perturbation scenario. These algorithms perform poorly when random state perturbations are
introduced, indicating the need for an algorithm that can handle state perturbations. As seen in Table 2,
the RMA3C policy achieves up to 46.56% higher mean episode rewards than the baselines in environments
with random state perturbations. Additionally, we also test the learned policies using different algorithms in
environments with well-trained adversary policies χ∗to perturb states. The results indicate that the RMA3C
policy achieves up to 54.02% higher mean episode reward than the baselines with well-trained adversarial
state perturbations. Overall, these tests demonstrate that the RMA3C algorithm achieves higher robustness
in different environments with state perturbations.
6 Conclusion
In this work, we propose a State-Adversarial Markov Game (SAMG) and investigate the fundamental
properties of robust MARL under adversarial state perturbations. We prove that the widely used solution
10
Published in Transactions on Machine Learning Research (01/2024)
concepts such as optimal agent policy and robust Nash equilibrium do not always exist for SAMGs. Instead,
we consider a new solution concept (the robust agent policy) to maximize the worst-case expected state value
and prove its existence. This is the primary theoretical contribution of our work. Additionally, we also propose
a RMA3C algorithm to find a robust policy for MARL agents under state perturbations. Our numerical
experiments demonstrate that the RMA3C algorithm improves the robustness of the trained policies against
both random and adversarial state perturbations. Some discussions and future directions are provided in
Appendix F.
Acknowledgments
Songyang Han, Sanbao Su, Sihong He, and Fei Miao are supported by the National Science Foundation under
Grants CNS-1952096, and CNS-2047354 grants. Haizhao Yang was partially supported by the US National
Science Foundation under awards DMS-2244988, DMS-2206333, and the Office of Naval Research Award
N00014-23-1-2007.
Shaofeng Zou is supported by the National Science Foundation under Grants CCF-2106560, and CCF-2007783.
This material is based upon work supported under the AI Research Institutes program by National Science
Foundation and the Institute of Education Sciences, U.S. Department of Education through Award # 2229873
- National AI Institute for Exceptional Education. Any opinions, findings and conclusions or recommendations
expressed in this material are those of the author(s) and do not necessarily reflect the views of the National
Science Foundation, the Institute of Education Sciences, or the U.S. Department of Education.
We extend our thanks to Peter Stone and Dustin Morrill in Sony AI for their assistance in proofreading this
paper and for their insightful suggestions that have significantly enhanced the quality of this work. Their
careful attention to detail and valuable feedback were greatly appreciated.
References
Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction attacks.
InMLDM, pp. 262–275. Springer, 2017.
Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of decentralized
control of markov decision processes. Mathematics of operations research , 27(4):819–840, 2002.
Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcement
learning. IEEE Trans. Syst., Man, Cybern. Syst. , 38(2):156–172, 2008.
Katherine Chen and Michael Bowling. Tractable objectives for robust policy optimization. Advances in
Neural Information Processing Systems , 25, 2012.
Gerard Debreu. A social equilibrium existence theorem. Proceedings of the National Academy of Sciences , 38
(10):886–893, 1952.
Michael Everett, Björn Lütjens, and Jonathan P How. Certifiable robustness to adversarial state uncertainty
in deep reinforcement learning. IEEE Trans. Neural Netw. Learn. Syst. , 2021.
Ky Fan. Fixed-point and minimax theorems in locally convex topological linear spaces. Proceedings of the
National Academy of Sciences of the United States of America , 38(2):121, 1952.
Arlington M Fink. Equilibrium in a stochastic n-person game. Journal of science of the hiroshima university,
series ai (mathematics) , 28(1):89–93, 1964.
Jakob Foerster and Gregory Farquhar. Counterfactual multi-agent policy gradients. In AAAI, 2018.
Irving L Glicksberg. A further generalization of the kakutani fixed point theorem, with application to nash
equilibrium points. Proceedings of the American Mathematical Society , 3(1):170–174, 1952.
Delin Guo, Lan Tang, Xinggan Zhang, and Ying-Chang Liang. Joint optimization of handover control and
power allocation based on multi-agent deep reinforcement learning. IEEE Trans. Veh. Technol. , 69(11):
13124–13138, 2020.
11
Published in Transactions on Machine Learning Research (01/2024)
Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, and Fei Miao. Robust multi-agent
reinforcement learning with state uncertainty. Transactions on Machine Learning Research , 2023.
Junling Hu, Michael P Wellman, et al. Multiagent reinforcement learning: theoretical framework and an
algorithm. In ICML, volume 98, pp. 242–250. Citeseer, 1998.
Yizheng Hu, Kun Shao, Dong Li, HAO Jianye, Wulong Liu, Yaodong Yang, Jun Wang, and Zhanxing Zhu.
Robust multi-agent reinforcement learning driven by correlated equilibrium. 2020.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on
neural network policies. ICLR, 2017.
Maximilian Hüttenrauch and Adrian Šošić. Guided deep reinforcement learning for swarm systems. arXiv
preprint arXiv:1709.06011 , 2017.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In ICML, pp.
2961–2970. PMLR, 2019.
Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research , 30(2):257–280, 2005.
Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave minimax
optimization? In ICML, pp. 4880–4889. PMLR, 2020.
Chi Jin, Qinghua Liu, Yuanhao Wang, and Tiancheng Yu. V-learning–a simple, efficient, decentralized
algorithm for multiagent rl. arXiv preprint arXiv:2110.14555 , 2021.
Erim Kardeş, Fernando Ordóñez, and Randolph W Hall. Discounted robust stochastic games and an
application to queueing control. Operations research , 59(2):365–382, 2011.
Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. ICLR, 2017.
Divya Kothandaraman, Rohan Chandra, and Dinesh Manocha. Ss-sfda: Self-supervised source-free domain
adaptation for road segmentation in hazardous environments. In ICCV, pp. 3049–3059, 2021.
Erwin Kreyszig. Introductory functional analysis with applications , volume 17. John Wiley & Sons, 1991.
Adam Lerer, Hengyuan Hu, Jakob Foerster, and Noam Brown. Improving policies via search in cooperative
partially observable games. In AAAI, volume 34, pp. 7187–7194, 2020.
Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. Robust multi-agent reinforcement
learning via minimax deep deterministic policy gradient. In AAAI, volume 33, pp. 4213–4220, 2019.
Yongyuan Liang, Yanchao Sun, Ruijie Zheng, and Furong Huang. Efficient adversarial training without
attacking: Worst-case-aware robust reinforcement learning. NeurIPS , 2022.
Jieyu Lin, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, and Nicolas Papernot. On the
robustness of cooperative multi-agent reinforcement learning. In 2020 IEEE Security and Privacy Workshops
(SPW), pp. 62–68. IEEE, 2020a.
Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax
problems. In International Conference on Machine Learning , pp. 6083–6093. PMLR, 2020b.
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tactics of
adversarial attack on deep reinforcement learning agents. IJCAI, 2017.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine
learning proceedings 1994 , pp. 157–163. Elsevier, 1994.
Ze Liu, Yingfeng Cai, Hai Wang, Long Chen, Hongbo Gao, Yunyi Jia, and Yicheng Li. Robust target
recognition and tracking of self-driving cars with radar and camera information fusion under severe weather
conditions. IEEE Trans. Intell. Transp. Syst. , 2021.
12
Published in Transactions on Machine Learning Research (01/2024)
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In NeurIPS , pp. 6379–6390, 2017.
Ajay Mandlekar, Yuke Zhu, Animesh Garg, Li Fei-Fei, and Silvio Savarese. Adversarially robust policy
learning: Active construction of physically-plausible perturbations. In IROS, pp. 3932–3939. IEEE, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, et al. Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, 2015.
Jun Morimoto and Kenji Doya. Robust reinforcement learning. Neural computation , 17(2):335–359, 2005.
Devaprakash Muniraj, Kyriakos G Vamvoudakis, and Mazen Farhood. Enforcing signal temporal logic
specifications in multi-agent adversarial environments: A deep q-learning approach. In 2018 IEEE
Conference on Decision and Control (CDC) , pp. 4141–4146. IEEE, 2018.
John Nash. Non-cooperative games. Annals of mathematics , pp. 286–295, 1951.
Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain transition
matrices. Operations Research , 53(5):780–798, 2005.
Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs , volume 1.
Springer, 2016.
Anay Pattanaik and Zhenyi Tang. Robust deep reinforcement learning with adversarial attacks. AAMAS,
2017.
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust deep
reinforcement learning with adversarial attacks. In AAMAS, pp. 2040–2042, 2018.
Thomy Phan, Thomas Gabor, Andreas Sedlmeier, Fabian Ritz, Bernhard Kempter, Cornel Klein, Horst
Sauer, Reiner Schmid, Jan Wieghardt, Marc Zeller, et al. Learning and testing resilience in cooperative
multi-agent systems. In Proceedings of the 19th International Conference on Autonomous Agents and
MultiAgent Systems , pp. 1055–1063, 2020.
Lerrel Pinto, James Davidson, and Rahul Sukthankar. Robust adversarial reinforcement learning. In ICML,
pp. 2817–2826. PMLR, 2017.
Arnu Pretorius, Scott Cameron, et al. A game-theoretic analysis of networked system control for common-pool
resource management using multi-agent reinforcement learning. In NeurIPS , volume 33, pp. 9983–9994,
2020.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley &
Sons, 2014.
Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li. Scalable multi-agent reinforcement learning for
networked systems with average reward. In NeurIPS , volume 33, pp. 2074–2086, 2020.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic
value function factorisation for deep multi-agent reinforcement learning. In NeurIPS , December 2020.
Meisam Razaviyayn, Tianjian Huang, Songtao Lu, Maher Nouiehed, Maziar Sanjabi, and Mingyi Hong.
Nonconvex min-max optimization: Applications, challenges, and recent theoretical advances. IEEE Signal
Process. Mag. , 37(5):55–66, 2020.
Walter Rudin et al. Principles of mathematical analysis , volume 3. McGraw-hill New York, 1976.
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences , 39(10):1095–1100, 1953.
Macheng Shen and Jonathan P How. Robust opponent modeling via adversarial ensemble reinforcement
learning. In ICAPS, volume 31, pp. 578–587, 2021.
13
Published in Transactions on Machine Learning Research (01/2024)
Aman Sinha, Matthew O’Kelly, et al. Formulazero: Distributionally robust online adaptation via offline
population synthesis. In ICML, pp. 8992–9004. PMLR, 2020.
Jianyu Su, Stephen Adams, and Peter Beling. Value-decomposition multi-agent actor-critics. In AAAI,
volume 35, pp. 11352–11360, 2021.
Chuangchuang Sun, Dong-Ki Kim, and Jonathan P How. Romax: Certifiably robust deep multiagent
reinforcement learning via convex relaxation. arXiv preprint arXiv:2109.06795 , 2021.
Yanchao Sun, Ruijie Zheng, Parisa Hassanzadeh, Yongyuan Liang, Soheil Feizi, Sumitra Ganesh, and Furong
Huang. Certifiably robust policy learning against adversarial communication in multi-agent systems. ICLR,
2022.
Peter Sunehag, Guy Lever, et al. Value-decomposition networks for cooperative multi-agent learning based
on team reward. In AAMAS, pp. 2085–2087, 2018.
Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning , volume 135. MIT press
Cambridge, 1998.
Tessa van der Heiden, C Salge, Efstratios Gavves, and H van Hoof. Robust multi-agent reinforcement learning
with social empowerment for coordination and communication. arXiv preprint arXiv:2012.08255 , 2020.
Kaixin Wang, Uri Gadot, Navdeep Kumar, Kfir Levy, and Shie Mannor. Robust reinforcement learning via
adversarial kernel approximation. ar@articlechen2012tractable, title=Tractable objectives for robust policy
optimization, author=Chen, Katherine and Bowling, Michael, journal=Advances in Neural Information
Processing Systems, volume=25, year=2012 Xiv preprint arXiv:2306.05859 , 2023.
Chaowei Xiao, Xinlei Pan, et al. Characterizing attacks on deep reinforcement learning. arXiv preprint
arXiv:1907.09470 , 2019.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness
of ppo in cooperative multi-agent games. NeurIPS , 2022.
Jing Yu, Clement Gehring, Florian Schäfer, and Animashree Anandkumar. Robust reinforcement learning: A
constrained game-theoretic approach. In Learning for Dynamics and Control , pp. 1242–1254. PMLR, 2021.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh. Robust
deep reinforcement learning against adversarial perturbations on state observations. NeurIPS , 33:21024–
21037, 2020a.
Huan Zhang, Hongge Chen, Duane Boning, and Cho-Jui Hsieh. Robust reinforcement learning on state
observations with learned optimal adversary. arXiv preprint arXiv:2101.08452 , 2021.
Kaiqing Zhang, Tao Sun, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer Basar. Robust multi-agent
reinforcement learning with model uncertainty. In NeurIPS , 2020b.
Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning implicit credit assignment for
cooperative multi-agent reinforcement learning. In NeurIPS , volume 33, pp. 11853–11864, 2020.
14
Published in Transactions on Machine Learning Research (01/2024)
A Comparison with Dec-POMDP and Markov Games
A.1 Comparison with Dec-POMDP
Our SAMG problem cannot be solved by the existing work in the Decentralized Partially Observable Markov
Decision Process (Dec-POMDP) (Oliehoek et al., 2016). In contrast, the policy in our problem needs to
be robust under a set of admissible perturbed states. The adversary aims to find the worst-case state
perturbation policy χto minimize the MARL agents’ total expected return. In the following proposition,
we show that under certain additional conditions our proposed SAMG problem becomes a Dec-POMDP
problem.
Figure 6: Comparison between Dec-POMDP and SAMG. In Dec-POMDP, the observation probability
function is fixed, and it will not change according to the change of the agent policy. However, in SAMG the
adversary policy is not a fixed policy, it may change according to the agents’ policies and always select the
worst-case state perturbation for agents.
Proposition 3.2. When the adversary policy is a fixed policy, the SAMG problem becomes a Dec-
POMDP (Oliehoek et al., 2016).
Proof.When the adversary policy χis a fixed policy, an SAMG (N,S,A,r,Ps,p,γ, Pr(s0))becomes a Dec-
POMDP (N,S,A,r,O,O,p,γ, Pr(s0)). The agent setN={1,...,n}. The global joint state is s∈S. Each
agentiis associated with an action ai∈Ai. The global joint action is a= (a1,...,an)∈A,A:=A1×···×An.
All agents share a stage-wise reward function r:S×A→ R. The state transition function is p:S×A→ ∆(S),
where ∆(S)is a probability simplex denoting the set of all possible probability measures on S. The state
transits from the true state to the next state. The discount factor is γ. The joint observation set Ois the same
as the joint state set S. The observation probability function O(o|s) =χ(o|s)for anyo∈PsandO(o|s) = 0
for anyo /∈Ps, whereois the observation given the state s. The Pr(s0)is the probability distribution of the
initial state.
In Dec-POMDP, the observation probability function is fixed, and it will not change according to the change
of the agent policy. However, in SAMG the adversary policy is not a fixed policy, it may change according to
the agents’ policies and always select the worst-case state perturbation for agents. In contrast to Dec-POMDP,
the adversary’s policy χis chosen to minimize the total expected return of the agents in our problem.
Additionally, in Dec-POMDP the agents do not have access to the true state s, whereas in our problem, the
adversaries are aware of the true state and can use it to select perturbed states.
15
Published in Transactions on Machine Learning Research (01/2024)
A.2 SAMG cannot be solved by Dec-POMDP: Two-Agent Two-State Game Example
We use a two-agent two-state game to show the difference between Dec-POMDP and SAMG. Consider a
game with two agents N={1,2}and two statesS={s1,s2}as shown in Fig. 7. Each agent has two actions
A1=A2={a1,a2}. The transition probabilities are defined below.
p(s′=s1|s=s1,a1̸=a2) = 1,
p(s′=s2|s=s1,a1=a2) = 1,
p(s′=s2|s=s2,a1̸=a2) = 1,
p(s′=s1|s=s2,a1=a2) = 1. (6)
Specifically, a1=a2includes two cases: a1=a2=a1ora1=a2=a2. Similarly, a1̸=a2includes two cases:
a1=a1,a2=a2ora1=a2,a2=a1.
𝑠!𝑠""𝑎!=𝑎""𝑟=1𝑎!=𝑎""𝑟=0𝑎!≠𝑎""𝑟=1𝑎!≠𝑎""𝑟=0
Figure 7: A two-agent two-state game example. Agents get reward 1 at state s1if they choose the same
action. Agents get reward 1 at state s2if they choose different actions.
Two agents share the same reward function:
r(s,a1,a2) =

1, a1=a2,ands=s1,
0, a1̸=a2,ands=s1,
0, a1=a2,ands=s2,
1, a1̸=a2,ands=s2.(7)
In a SAMG, each agent is associated with an adversary to perturb its knowledge or observation of the true
state. For the power of the adversary, we allow the adversary to perturb any state to the other state:
P1
s=P2
s={s1,s2}. (8)
We useγ= 0.99as the discount factor. Agents want to find a policy πto maximize their total expected
return while adversaries want to find a policy χto minimize agents’ total expected return.
This problem cannot be formulated as a Dec-POMDP Consider one agent policy where both
agents select the same action in s1and select different actions in s2:π1(a1|s1) =π1(a1|s2) =π2(a1|s1) =
π2(a2|s2) = 1. When there is no adversary, agents keep receiving rewards. The values for each state are
˜V(s1) =˜V(s2) =1
1−γ= 100. Because agents share the same reward function, they also share the same
values for each state. However, this policy receives V(s1) =V(s2) = 0when agents are facing the worst-case
adversaries χi(s1|s2) =χi(s2|s1) = 1fori= 1,2and always taking the wrong actions with 0 reward.
If the adversary policy is fixed at χi(s1|s2) =χi(s2|s1) = 1fori= 1,2, this problem becomes a Dec-POMDP
with the observation space O={o1=s1,o2=s2}. The observation function is oi(o1|s2) =oi(o2|s1) = 1for
i= 1,2. The agent policy is π1(a1|o1) =π1(a1|o2) =π2(a1|o1) =π2(a2|o2) = 1.
However, when we consider a different agent policy where both agents select the same action in s2and
select different actions in s1:π1(a1|s2) =π1(a1|s1) =π2(a1|s2) =π2(a2|s1) = 1, agents keep receiving 0
rewards even when the adversary does nothing. For the new agent policy, the worst-case adversary policy is
χi(s1|s1) =χi(s2|s2) = 1fori= 1,2. The corresponding observation function for the new adversary policy is
oi(o1|s1) =oi(o2|s2) = 1fori= 1,2, which is completely different from the previous observation functions.
Because the observation function in Dec-POMDP won’t change according to agents’ policies, therefore, the
SAMG problem cannot be formulated by Dec-POMDP when adversary policy is not fixed.
16
Published in Transactions on Machine Learning Research (01/2024)
Under different observation functions, Dec-POMDP can lead to contradictory agent policies.
Besides the analysis of why this problem cannot be formulated as a Dec-POMDP, we also demonstrate that
Dec-POMDPs fail to solve this problem from a different perspective.
Let’s consider a Dec-POMDP with the observation space O={o1=s1,o2=s2}. The observation function
is defined as oi(o1|s2) =oi(o2|s1) = 1fori= 1,2. In this scenario, the optimal agent policy is to select the
same action in response to o2and choose different actions for o1. Agents keep receiving rewards based on
this policy.
Now let’s consider another Dec-POMDP with the observation space O={o1=s1,o2=s2}. The observation
function is defined as oi(o1|s1) =oi(o2|s2) = 1fori= 1,2. In this case, the optimal agent policy is to select
the same action in response to o1and choose different actions for o2. Agents keep receiving rewards based on
this policy. However, the new optimal agent policy contradicts the previous one.
By comparing these two Dec-POMDPs with different observation functions, we observe that Dec-POMDPs
can yield different agent policies based on different observation functions. This implies that Dec-POMDPs
do not address the problem of selecting an agent policy when the observation function is determined by an
adversary.
Furthermore, we will reanalyze this problem and demonstrate how a SAMG can solve this two-agent two-state
game in Appendix B and C. The SAMG formulation addresses this problem by selecting the agent policy
against the worst-case observation function.
A.3 Comparison with Markov Games
Under a specific condition, when the adversary policy χis a bijective mapping from StoS, the SAMG
problem is equivalent to a Markov game, as demonstrated in the following proposition. This proposition
illustrates the relationship between a SAMG and a Markov game with a particular form of state perturbation.
Whenχis a bijective mapping from StoS, the adversary policy follows χ(ρ|s) = 1selecting the perturbed
stateρfor the true state swith probability 1. Let us use the notation χ(s) =ρfor this special case.
Proposition 3.3. When the adversary policy is a fixed bijective mapping from StoS, the SAMG problem
becomes a Markov game.
Proof.When the adversary policy χis a fixed bijective mapping from StoS, an SAMG problem
(N,S,A,r,Ps,p,γ, Pr(s0))becomes a Markov game (Nnew,Snew,Anew,ri
new,pnew,γ,Pr(snew, 0))that is con-
structed as follows:
Takingsnew=ρ=χ(s)as the new state, the new global joint state set is Snew:=S. The global joint action
setAnew=A=A1×···×Anand the agent set Nnew=Nstay the same.
We can construct a new reward function ri
new:Snew×Anew→Rfor each agent ias
ri
new(snew=χ(s),anew=a) =r(s,a), (9)
and a new state transition function pnew:Snew×Anew→∆(Snew)defined as
pnew(ρ′=χ(s′)|ρ=χ(s),a) =p(s′|s,a). (10)
The new probability of the initial state is
Pr(snew, 0=χ(s0)) = Pr(s0). (11)
Each agent uses a policy πi
new:Snew→∆(Ai)to choose an action based on the new state. Hence, the
SAMG problem becomes a Markov game.
If the adversary’s policy χis a fixed bijective mapping from StoS, the new global joint state set Snewis a
perturbation ofSand each state is assigned a new ""label"" by the adversary. Under this condition, the SAMG
is equivalent to a Markov game.
17
Published in Transactions on Machine Learning Research (01/2024)
B Optimal Adversary Policy and Optimal Agent Policy
In this section, we analyze the existence of the optimal adversary policy and the optimal agent policy. We
will utilize the two-agent two-state game introduced in Appendix A. For completeness, let us revisit this
game with two agents N={1,2}and two statesS={s1,s2}as shown in Fig. 8. Each agent has two actions
A1=A2={a1,a2}. The transition probabilities are defined below.
p(s′=s1|s=s1,a1̸=a2) = 1,
p(s′=s2|s=s1,a1=a2) = 1,
p(s′=s2|s=s2,a1̸=a2) = 1,
p(s′=s1|s=s2,a1=a2) = 1. (12)
Specifically, a1=a2includes two cases: a1=a2=a1ora1=a2=a2. Similarly, a1̸=a2includes two cases:
a1=a1,a2=a2ora1=a2,a2=a1.
𝑠!𝑠""𝑎!=𝑎""𝑟=1𝑎!=𝑎""𝑟=0𝑎!≠𝑎""𝑟=1𝑎!≠𝑎""𝑟=0
Figure 8: A two-agent two-state game example. Agents get reward 1 at state s1if they choose the same
action. Agents get reward 1 at state s2if they choose different actions. This example was used in Appendix A
to show the difference between Dec-POMDP and SAMG. We will revisit this game in Appendix B to discuss
optimal adversary policy and optimal agent policy.
Two agents share the same reward function:
r(s,a1,a2) =

1, a1=a2,ands=s1,
0, a1̸=a2,ands=s1,
0, a1=a2,ands=s2,
1, a1̸=a2,ands=s2.(13)
In a SAMG, each agent is associated with an adversary to perturb its knowledge or observation of the true
state. For the power of the adversary, we allow the adversary to perturb any state to the other state:
P1
s=P2
s={s1,s2}. (14)
We useγ= 0.99as the discount factor. Agents want to find a policy πto maximize their total expected
return while adversaries want to find a policy χto minimize agents’ total expected return.
B.1 Optimal Agent Policy Without Adversaries
When there is no adversary, the optimal policy for agents is to choose the same action in s1and choose
different actions in s2. One example is π1(a1|s1) =π1(a1|s2) =π2(a1|s1) =π2(a2|s2) = 1. The agents
keep receiving rewards. The values for each state are ˜V(s1) =˜V(s2) =1
1−γ= 100. Because agents share
the same reward function, they also share the same values for each state. However, this policy receives
V(s1) =V(s2) = 0when agents are facing adversaries χi(s1|s2) =χi(s2|s1) = 1fori= 1,2and always taking
the wrong actions with 0 reward.
B.2 A Stochastic Policy With Adversaries
We consider a stochastic policy π1(a1|s1) =π1(a1|s2) =π2(a1|s1) =π2(a2|s2) = 0.5. Under this policy, the
probabilities of taking the same or different actions are the same for each state Pr(a1=a2|s1) =Pr(a1̸=
18
Published in Transactions on Machine Learning Research (01/2024)
a2|s1) =Pr(a1=a2|s2) =Pr(a1̸=a2|s2) = 0.5. Agents randomly stay or transit in each state and
receive a positive reward with a 50% probability. The adversary has no power under this policy because πis
the same for both states. The values for each state are V(s1) =V(s2) =˜V(s1) =˜V(s2) =0.5
1−γ= 50.
B.3 Deterministic Policies With Adversaries
Since each agent has two actions for each state, there are in total 24= 16possible deterministic policies for
the two-agent two-state game example. All possible deterministic policies can be classified into three cases:
(1) If agents select the same action in one state siand select different actions in the other state sj, then
we always have V(s1) =V(s2) = 0. This is because adversaries can always use χk(s1|sj) =χk(s2|si) = 1
fork= 1,2such that agents always receive a 0 reward. (2) If agents always select different actions
in both states, then V(s1) = 0,V(s2) = 100. This is because agents never transit to the other state
and keep receiving the same reward. (3) If agents always select the same action in both states, then
V(s1) =1
1−γ2≈50.25,V(s2) =γ
1−γ2≈49.75. This is because agents circulate through both states and
adversaries have no power to change it.
B.4 Optimal Adversary Policy
In this section, we examine optimal policies for both the adversary and the agent in a State-Adversarial
Markov Game (SAMG). The following proposition demonstrates the existence of an optimal adversary in an
SAMG.
Proposition 4.1 (Existence of Optimal Adversary Policy). Given an SAMG, for any given agent
policy, there exists an optimal adversary policy.
Proof.We prove this by constructing an MDP M= (S,ˆA,ˆr,ˆp,γ)such that an optimal policy of Mis an
optimal adversary policy χ∗for the SAMG given the fixed π. In the MDP M, we take all adversaries as a
joint adversary agent. The joint adversary learns a policy χto find a joint perturbed state given the current
true state. The action space ˆA=S×S×···×S . Note that the joint admissible perturbed state set in
Definition 3.1Ps⊆ˆA.
The reward function ˆris defined as:
ˆr(s,ˆa) =−/summationdisplay
a∈Aπ(a|ˆa)r(s,a)forˆa∈Ps. (15)
The transition probability ˆpis defined as
ˆp(s′|s,ˆa) =/summationdisplay
a∈Aπ(a|ˆa)p(s′|s,a)forˆa∈Ps. (16)
The reward function is defined based on the intuition that when the agent receives rgivens,a, the
reward of the adversary is the negative of the agent reward, that is to say, ˆr=−r. Considering that
r(s,a) =E[R|s,a] =−E[ˆR|s,a],
ˆr(s,ˆa) =E[ˆR|s,ˆa]
=/summationdisplay
ˆRˆR/summationdisplay
a∈APr[ˆR|s,a]π(a|ˆa)
=/summationdisplay
a∈A
/summationdisplay
ˆRˆRPr[ˆR|s,a]
π(a|ˆa)
=/summationdisplay
a∈AE[ˆR|s,a]π(a|ˆa)
=−/summationdisplay
a∈AE[R|s,a]π(a|ˆa)
19
Published in Transactions on Machine Learning Research (01/2024)
=−/summationdisplay
a∈Ar(s,a)π(a|ˆa). (17)
Based on the properties of MDP (Sutton et al., 1998; Puterman, 2014), we know that the MDP Mhas an
optimal policy χ∗that satisfies ˆVπ,χ∗(s)≥ˆVπ,χ(s)for allsand allχ, where ˆVπ,χis the state value function
of the MDP M.
The Bellman equation for the MDP Mis
ˆVπ,χ(s) =/summationdisplay
ˆa∈Psχ(ˆa|s)/parenleftigg
ˆr+γ/summationdisplay
s′∈Sˆp(s′|s,ˆa)ˆVπ,χ(s′)/parenrightigg
=/summationdisplay
ˆa∈Psχ(ˆa|s)/summationdisplay
a∈Aπ(a|ˆa)/parenleftigg
−r+γ/summationdisplay
s′∈Sp(s′|s,a)ˆVπ,χ(s′)/parenrightigg
. (18)
By multiplying−1on both sides, we have
(−ˆVπ,χ(s)) =/summationdisplay
ˆa∈Psχ(ˆa|s)/summationdisplay
a∈Aπ(a|ˆa)
/bracketleftigg
r+γ/summationdisplay
s′∈Sp(s′|s,a)(−ˆVπ,χ(s′))/bracketrightigg
. (19)
On the other side, for the SAMG, we have the Bellman equation for any fixed policies πandχas
Vπ,χ(s) =/summationdisplay
ρ∈Psχ(ρ|s)/summationdisplay
a∈Aπ(a|ρ)
/parenleftigg
r+γ/summationdisplay
s′∈Sp(s′|s,a)Vπ,χ(s′)/parenrightigg
. (20)
Whenπandχare fixed, they can be taken together as a single policy, and the existing results from
Dec-POMDP can be directly applied. Comparing Eq. (20) and (19), we know that Vπ,χ(s) = (−ˆVπ,χ(s)).
An optimal adversary policy χ∗for the MDP Msatisfies ˆVπ,χ∗(s)≥ˆVπ,χ(s)for anysand anyχ. Therefore,
χ∗also satisfies Vπ,χ∗(s)≤Vπ,χ(s)for anysand anyχ, and an optimal policy of the MDP Mis an optimal
adversary policy for the SAMG given the fixed π.
B.5 Optimal Agent Policy With Adversaries
We have established the existence of an optimal adversary in SAMGs. Next, we consider a state-robust
totally optimal agent policy under this optimal adversary. The following proposition demonstrates that a
deterministic agent policy is not always superior to a stochastic policy in SAMGs.
Proposition B.1. There exists an SAMG and some stochastic policy πsuch that we cannot find a better
deterministic policy π′satisfying ¯Vπ′(s)≥¯Vπ(s)for alls∈S.
Proof.We prove this theorem by giving a counter-example where no deterministic policy is better than a
stochastic policy. As shown in the two-agent two-state game example in Fig. 8, all 16 deterministic policies
are no better than the stochastic policy π1(a1|s1) =π1(a1|s2) =π2(a1|s1) =π2(a2|s2) = 0.5.
Finally, we show a state-robust totally optimal agent policy π∗does not always exist such that ¯Vπ∗(s)≥¯Vπ(s)
for anyπand alls∈Sin SAMGs in the following theorem.
Theorem 4.3 (Non-existence of State-robust Totally Optimal Agent Policy). A state-robust totally
optimal agent policy does not always exist for SAMGs.
20
Published in Transactions on Machine Learning Research (01/2024)
Proof.We prove this theorem by showing that the two-agent two-state game in Fig. 8 does not have an
optimal policy. We first show that the policy π1:π1(a1|s1) =π1(a1|s2) =π2(a2|s1) =π2(a2|s2) = 1is not an
optimal policy. Because agents always select different actions in both states, agents always stay in the same
state and adversaries have no power to change it. The values for each state are ¯Vπ1(s1) = 0,¯Vπ1(s2) = 100.
Now we consider the stochastic policy π2:π1(a1|s1) =π1(a1|s2) =π2(a1|s1) =π2(a2|s2) = 0.5. The values
for each state are ¯Vπ2(s1) =¯Vπ2(s2) = 50. Because ¯Vπ2(s1)>¯Vπ1(s1), the policy π1is not an optimal policy
for agents.
If there exists an optimal policy π∗, then it must be better than π1and have ¯Vπ∗(s1)>0,¯Vπ∗(s2) = 100. In
order to have ¯Vπ∗(s2) = 100, agents must select different actions in s2and keep receiving the positive rewards
from each step. In order to have ¯Vπ∗(s1)>0, agents must have a chance to select the same action in s1, i.e.,
Pr(a1=a2|s1)>0. However, if Pr(a1=a2|s1)>0, then adversaries can have χi(s1|s2)>0fori= 1,2to
perturb the state s2tos1and reduce ¯Vπ∗(s2). Therefore, no policy can do better than π1and sinceπ1is not
an optimal policy, there is no optimal policy for agents.
In the comparison of π1andπ2in the above proof, it is apparent that it is not always possible to maximize
the state value of all states and that trade-offs may need to be made among different states. Using the
traditional definition of an optimal policy, it is not possible to determine which policy, π1orπ2, is better.
However, if we use the worst-case expected state value concept from Definition 4.8 and assume that the
initial state is always s2, then we can conclude that π1is an optimal agent policy, as it gives the maximum
worst-case expected state value of 100 in this case.
C Stage-wise Equilibrium, Robust Total Nash Equilibrium, and Robust Agent Policy
In Theorem 4.3, it has been proven that a state-robust totally optimal agent policy does not always exist for
SAMGs. This section explores alternative solution concepts for the agent policy in SAMGs. We begin by
demonstrating the existence of a unique robust state value function for each agent in C.1. Building on this
property, we establish the existence of a stage-wise equilibrium for each state in C.2. However, we show in C.3
that the robust total Nash equilibrium may not always exist. As an alternative, we propose the concept of a
robust agent policy and demonstrate its existence in C.4.
We first give a review of the Nash equilibrium used in the literature. The Nash equilibrium is a widely
used solution concept in game theory, first proposed by Nash in Nash (1951) for general-sum finite one-shot
games. It states that each player selects the best response strategy to the others’ strategies and no player
would want to deviate from the equilibrium, as doing so would result in a worse utility. This concept was
later extended to infinite games by Debreu (Debreu, 1952), Glicksberg (Glicksberg, 1952), and Fan (Fan,
1952). Markov games, which involve a sequential decision process in a two-player zero-sum setting, were first
defined by Shapley in Shapley (1953). Fink extended the Nash equilibrium concept to Markov games in Fink
(1964) and proved that an equilibrium point exists in n-player general-sum discounted Markov games. The
uncertainty in transition dynamics of a Markov game was considered in Nilim & El Ghaoui (2005); Iyengar
(2005) using a robust optimization approach, with independent proofs for the existence of the equilibrium
point. Additionally, uncertainty in utility (or ""reward"" in reinforcement learning) was also taken into account
in Kardeş et al. (2011) for n-player finite state/action discounted Markov games, with a proof for the existence
of the equilibrium point.
Despite the extensive study of the Nash equilibrium in game theory, the uncertainty in the state has not yet
been explored in the context of Markov games. To the best of our knowledge, we are the first to formulate the
problem of n-player finite state/action discounted Markov games with state uncertainty and to demonstrate
the existence of a stage-wise equilibrium, as well as the non-existence of a robust total Nash equilibrium.
We use the following Assumption C.1 throughout this section.
Assumption C.1. The global state set Sand the global action set Aare finite sets.
21
Published in Transactions on Machine Learning Research (01/2024)
C.1 Unique Robust State Value Function
Denote the agent policies and adversary policies of all other agents and adversaries except agent iand
adversaryiasπ−iandχ−irespectively. We show that there exists a unique robust state value function for
agentigiven anyπ−iandχ−i.
Definition 4.4 (Robust state value function). A state value function Vi
∗,π−i,∗,χ−i:S→Rfor agenti
givenπ−iandχ−iis called a robust state value function if for all s∈S,
Vi
∗,π−i,∗,χ−i(s) = max
πimin
χi/summationdisplay
ρ∈Psχ(ρ|s)/summationdisplay
a∈Aπ(a|ρ)
/parenleftigg
r(s,a) +γ/summationdisplay
s′∈Sp(s′|s,a)Vi
∗,π−i,∗,χ−i(s′)/parenrightigg
. (21)
Note that we use π(a|ρ) = Πn
i=1πi(ai|ρi)to denote the joint agent policy. We use χ(ρ|s) = Πn
i=1χi(ρi|s)to
denote the joint adversary policy.
Before proving the existence of the unique robust state value function, we first introduce some notations
for this proof. For a given state value function Vi
∗,π−i,∗,χ−i:S→Rdefined on a finite state set S, we can
construct a state value vector vi=vec(Vi
∗,π−i,∗,χ−i) = [Vi
∗,π−i,∗,χ−i(s)]s∈S∈V:=R|S|by traversing all states,
where vec(·)is a vectorization function. The infinity norm on Vis∥vi∥∞=maxs∈S|Vi(s)|. Define the total
expected return in state sforπiandχias
fi
s(vi,πi,π−i,χi,χ−i) =/summationdisplay
ρ∈Psχ(ρ|s)/summationdisplay
a∈Aπ(a|ρ)
/parenleftigg
r(s,a) +γ/summationdisplay
s′∈Sp(s′|s,a)[vec−1(vi)](s′)/parenrightigg
, (22)
whereπ−iandχ−idenotes the agent policies and the adversary policies of all other agents except agent i.
Define the robust state value in state sgivenπ−iandχ−ias a function ψi
s:V→R,
ψi
s(vi,π−i,χ−i) = max
πimin
χifi
s(vi,πi,π−i,χi,χ−i). (23)
Note thatψi
sgives a real number that denotes the total expected return in state sgivenπ−iandχ−i. We can
construct a mapping Ψi
π,χ:V→Vfrom any state value vector vito a robust state value vector [Ψi
π,χ(vi)]s∈S
by traversing all s, that is to say, [Ψi
π,χ(vi)]s∈S=ψi
s(vi,π−i,χ−i).
Lemma C.2. For anyi∈N, the function Ψi
π,χ:V→Vis a contraction mapping given any π−iandχ−iof
other agents and adversaries except agent iand adversary i.
Proof.Let us consider two vectors vi,zi∈V. For anyi∈N, given any π−iandχ−i, for alls∈S, we have
ψi
s(vi,π−i,χ−i) = max
πimin
χifi
s(vi,πi,π−i,χi,χ−i)
=fi
s(vi,πi∗,π−i,χi∗,χ−i), (24)
whereπi∗is the corresponding maximizer, and χi∗is the corresponding optimizer for πi∗. Similarly, with the
optimizersωi∗andφi∗
1for the following maximin optimization problem, we have
ψi
s(zi,π−i,χ−i) = max
ωimin
φifi
s(zi,ωi,π−i,φi,χ−i)
=fi
s(zi,ωi∗,π−i,φi∗
1,χ−i)
≥fi
s(zi,πi∗,π−i,φi∗
2,χ−i), (25)
22
Published in Transactions on Machine Learning Research (01/2024)
where
φi∗
2= arg min
φifi
s(zi,πi∗,π−i,φi,χ−i). (26)
Then, for any i∈N, given any π−iandχ−i, for alls∈S, it holds that
ψi
s(vi,π−i,χ−i)−ψi
s(zi,π−i,χ−i)
=fi
s(vi,πi∗,π−i,χi∗,χ−i)−fi
s(zi,ωi∗,π−i,φi∗
1,χ−i)
≤fi
s(vi,πi∗,π−i,χi∗,χ−i)−fi
s(zi,πi∗,π−i,φi∗
2,χ−i)
≤fi
s(vi,πi∗,π−i,φi∗
2,χ−i)−fi
s(zi,πi∗,π−i,φi∗
2,χ−i)
=/summationdisplay
ρ∈Psφi∗
2(ρi|s)/productdisplay
j̸=iχj(ρj|s)/summationdisplay
a∈Aπi∗(ai|ρi)×
/productdisplay
k̸=iπk(ak|ρk)/parenleftigg
r+γ/summationdisplay
s′∈Sp(s′|s,a)[vec−1(vi)](s′)/parenrightigg
−/summationdisplay
ρ∈Psφi∗
2(ρi|s)/productdisplay
j̸=iχj(ρj|s)/summationdisplay
a∈Aπi∗(ai|ρi)×
/productdisplay
k̸=iπk(ak|ρk)/parenleftigg
r+γ/summationdisplay
s′∈Sp(s′|s,a)[vec−1(zi)](s′)/parenrightigg
=/summationdisplay
ρ∈Psφi∗
2(ρi|s)/productdisplay
j̸=iχj(ρj|s)/summationdisplay
a∈Aπi∗(ai|ρi)×
/productdisplay
k̸=iπk(ak|ρk)γ/summationdisplay
s′∈Sp(s′|s,a)×
/braceleftbig
[vec−1(vi)](s′)−[vec−1(zi)](s′)/bracerightbig
≤/summationdisplay
ρ∈Psφi∗
2(ρi|s)/productdisplay
j̸=iχj(ρj|s)/summationdisplay
a∈Aπi∗(ai|ρi)×
/productdisplay
k̸=iπk(ak|ρk)γ/summationdisplay
s′∈Sp(s′|s,a)∥vi−zi∥∞
=γ∥vi−zi∥∞. (27)
The second inequality in Eq. (27) follows
χi∗= arg min
χifi
s(vi,πi∗,π−i,χi,χ−i). (28)
Because for any i∈N, given any π−iandχ−i, for alls∈S
ψi
s(vi,π−i,χ−i)−ψi
s(zi,π−i,χ−i)≤γ∥vi−zi∥∞, (29)
Based on symmetry, we have
ψi
s(zi,π−i,χ−i)−ψi
s(vi,π−i,χ−i)≤γ∥zi−vi∥∞
=γ∥vi−zi∥∞. (30)
Thus, it holds that for any i∈N, given any π−iandχ−i
∥Ψi
π,χ(vi)−Ψi
π,χ(zi)∥∞≤γ∥vi−zi∥∞, (31)
that is to say, the function Ψi
π,χis a contraction mapping.
23
Published in Transactions on Machine Learning Research (01/2024)
Theorem 4.5 (Existence of Unique Robust State Value Function). For an SAMG with finite state
and finite action spaces, for any i∈N, given any π−iandχ−iof other agents and adversaries except agent i
and adversary i, there exists a unique robust state value function Vi
∗,π−i,∗,χ−i:S→Rfor agentisuch that
for alls∈S,
Vi
∗,π−i,∗,χ−i(s) = max
πimin
χi/summationdisplay
ρ∈Psχ(ρ|s)/summationdisplay
a∈Aπ(a|ρ)
/parenleftigg
r(s,a) +γ/summationdisplay
s′∈Sp(s′|s,a)Vi
∗,π−i,∗,χ−i(s′)/parenrightigg
. (32)
Proof.For anyi∈N, there exists a state value function Vi
∗,π−i,∗,χ−isatisfying (32) if and only if vi=
vec(Vi
∗,π−i,∗,χ−i)is a fixed point of Ψi
π,χ:V→V, where [Ψi
π,χ(vi)]s∈S=ψi
s(vi,π−i,χ−i)andψi
s(vi,π−i,χ−i)
is defined in (23). We use Banach’s fixed point theorem to prove this as follows.
Because any finite-dimensional normed vector space is complete (Kreyszig, 1991), the (V,∥·∥∞)is a complete
Banach space. Also, for any i∈N, given any π−iandχ−i, the function Ψi
π,χis a contraction mapping
according to Lemma C.2. Therefore, by Banach’s fixed point theorem, there is a unique fixed point visuch
that Ψi
π,χ(vi) =vi. In other words, for any i∈N, given any π−iandχ−i, there exists a unique Vi
∗,π−i,∗,χ−i
such that
Vi
∗,π−i,∗,χ−i(s) = max
πimin
χifi
s(vi,πi,π−i,χi,χ−i). (33)
Denote the state value function for agent igiven anyπ−iandχ−iof other agents and adversaries except
agentiand adversary ias
Vi
πi,π−i,χi,χ−i(s) =fi
s(vi,πi,π−i,χi,χ−i), (34)
wherevi= vec(Vi
∗,π−i,∗,χ−i). Then we have the following corollary for Theorem C.1.
Corollary C.3. For an SAMG with finite state and finite action spaces, let Vi
∗,π−i,∗,χ−ibe the unique robust
state value function for agent igiven anyπ−iandχ−isuch that for all s∈S,
Vi
∗,π−i,∗,χ−i(s) = max
πimin
χifi
s(vi,πi,π−i,χi,χ−i)
=fi
s(vi,πi∗,π−i,χi∗,χ−i), (35)
wherevi=vec(Vi
∗,π−i,∗,χ−i),πi∗is the corresponding maximizer at state s, andχi∗is the corresponding
optimizer for πi∗at states, then for state sit holds that Vi
πi∗,π−i,χi∗,χ−i(s)≥Vi
πi,π−i,χi∗,χ−i(s)for anyπi,
andVi
πi∗,π−i,χi∗,χ−i(s)≤Vi
πi∗,π−i,χi,χ−i(s)for anyχi.
C.2 Existence of the Stage-wise Equilibrium
Before we show the existence of the robust total Nash equilibrium, we first show a concept of the stage-wise
equilibrium.
Definition C.4 (Stage-wise Equilibrium ).For an SAMG, the policy (π∗,χ∗)is a stage-wise equilibrium
for statesif for alli∈Nand allπiandχi, it holds that
Vi
πi,π−i∗,χi∗,χ−i∗(s)≤Vi
πi∗,π−i∗,χi∗,χ−i∗(s)
≤Vi
πi∗,π−i∗,χi,χ−i∗(s), (36)
whereπ−iandχ−idenotes the agent policies and adversary policies of all the other agents except agent i,
respectively.
24
Published in Transactions on Machine Learning Research (01/2024)
The Nash equilibrium was originally proposed by Nash for finite one-shot games, in which the state transition
of the environment is not considered. When the concept of Nash equilibrium is extended to Markov games,
the existence of the equilibrium is shown through the existence of a state-wise equilibrium for each state. A
policy that is a stage-wise equilibrium for all states is considered a Nash equilibrium for the Markov game.
This idea brings the following proposition to show the relationship between the robust total Nash equilibrium
and the stage-wise equilibrium for SAMGs.
Proposition C.5. The policy (π∗,χ∗)is a robust total Nash equilibrium for an SAMG if the policy (π∗,χ∗)
is a stage-wise equilibrium for all s∈S.
Proof.It is a natural result according to the Definition 4.6 and the Definition C.4.
We show the existence of the stage-wise equilibrium defined in Definition C.4 in the following theorem.
Theorem C.6 (Existence of Stage-wise equilibrium ).For SAMGs with finite state and finite action
spaces, the stage-wise equilibrium defined in Definition C.4 exists for any s∈S.
Proof.Let us construct a 2nplayer game for any s∈S. We havenagents and nadversaries in the player
set. We introduce uniform notations for the agents and adversaries to describe a 2nplayer game at state s.
The player setI={1,...,n,n + 1,...,2n}. The first half of the player set {1,...,n}represents agents, while
the second half{n+ 1,...,2n}represents adversaries. The set of available actions for player iis
Ai
s=

Ai×Ai···×Ai
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
total number:|Pis|, i= 1,...,n ;
Pi−n
s, i =n+ 1,...,2n.(37)
Each adversary’s action set includes all possible perturbed states in the admissible perturbed state set at
states. Each agent’s action set includes all possible joint actions given every possible perturbed state.
Take the two-agent two-state game in Fig. 8 as an example, the player set I={1,2,3,4}. Player 3 is the
adversary for agent player 1. Player 4 is the adversary for agent player 2. If the current true state is s1, then
A1
s1=A2
s1={(a1,a1),(a1,a2),(a2,a2),(a2,a1)}are the action sets for two agent players. In A1
s1for agent
1, the joint action (a1,a2)means selecting a1if the perturbed state for agent 1 is s1and selecting a2if the
perturbed state for agent 1 is s2. For two adversary players, A3
s1=A4
s1={s1,s2}, as adversaries can perturb
the true state s1tos2.
We consider the mixed strategy σi
s∈∆(Ai
s)for playeri. Note that the mixed strategy for each adversary
gives us the probability distribution of all possible perturbed states for state s, i.e.χi−n(ρi−n|s) =σi
s(ρi−n)
fori=n+ 1,...,2n. Then we show how we can get each agent’s policy πi(ai|ρi)based on its mixed strategy
σi
sby calculating the marginal probabilities. Denote the total number of possible perturbed state for agent
iat statesasPsuch thatP=|Pi
s|. Here we drop the subscript sinPsfor a concise representation.
The perturbed state set for agent iis represented as {ρi
1,ρi
2,...,ρi
P}. Denote the joint action of agent ias
bi= (bi
1,bi
2,...,bi
P)wherebi
kis the action selected for the perturbed state ρi
k∈Pi
s. Then the mixed strategy
σi
s(bi
1,bi
2,...,bi
P)gives us the joint probability of selecting bi
kforρi
kfor allk= 1,2,...,P. We can get the
marginal probability of selecting action aigiven the perturbed state ρi
k∈Pi
sas
πi(ai|ρi
k) =/summationdisplay
{bi∈Ais|bi
k=ai}σi
s(bi
1,bi
2,...,bi
P). (38)
The marginal probability of selecting action aigiven the perturbed state ρi
kis calculated by summing up
the joint probability over all joint actions in which agent iselectsaigiven the perturbed state ρi
k. Take the
two-agent two-state game in Fig. 8 as an example, if the current perturbed state for agent 1 is ρ1=s1, then
agent 1’s policy is
π1(a1|ρ1=s1) =σ1(a1,a1) +σ1(a1,a2)
π1(a2|ρ1=s1) =σ1(a2,a1) +σ1(a2,a2). (39)
25
Published in Transactions on Machine Learning Research (01/2024)
Note that the mixed strategy σi
s∈∆(Ai
s)only gives part of the agent and adversary policies. For example,
the mixed strategy for the adversaries only gives a distribution of the perturbed states for st=s. We
construct the complete agent and adversary policies as follows: For i= 1,...,n, the agent i’s policy is
πi(ai|ρi) =

/summationtext
{bi∈Ais|bi
k=ai}σi
s(bi
1,bi
2,...,bi
P),
forρi=ρi
k∈Pi
s;
U(Ai),
forρi/∈Pi
s,(40)
whereU(Ai)represents a uniform distribution on Ai. Fori= 1,...,n, the adversary i’s policy is
χi(ρi|st) =/braceleftigg
σi+n
s(ρi),forst=s;
U(Pi
s),forst̸=s,(41)
whereU(Pi
s)represents a uniform distribution on Pi
s.
The utility function for player iis
ui
s(σi
s,σ−i
s) =

fi
s(vi∗,πi,π−i,χi,χ−i),
fori= 1,...,n ;
−fi−n
s(v(i−n)∗,πi−n,π−(i−n),
χi−n,χ−(i−n)),
fori=n+ 1,...,2n.(42)
whereσ−i
sdenotes the strategies of all other players except player i,vi∗=vec(Vi
∗,π−i,∗,χ−i), andVi
∗,π−i,∗,χ−i
is the unique robust state value function of agent iwhen the policies of other agents and adversaries are
given byπ−iandχ−i. Thevi∗satisfies
[vec−1(vi∗)](s) = max
πimin
χifi
s(vi∗,πi,π−i,χi,χ−i), (43)
wherefi
sis defined for player iin (22) as
fi
s(vi,πi,π−i,χi,χ−i) =/summationdisplay
ρ∈Psχ(ρ|s)/summationdisplay
a∈Aπ(a|ρ)
/parenleftigg
r(s,a) +γ/summationdisplay
s′∈Sp(s′|s,a)[vec−1(vi)](s′)/parenrightigg
.
Note thatσ−i
sincludes both π−iandχ−ifor anyi∈I, and the existence of Vi
∗,π−i,∗,χ−iis guaranteed by
Theorem C.1. Thus, the utility function is well-defined.
Since the state set Sis finite,Pi
s⊆Sis a finite set for all i∈N. Also,Aiis a finite set for all i∈N. Therefore,
∆(Ai
s)is compact and convex for all i∈I. Moreover, for all i∈I,ui
s(σi
s,·)is linear in σi
sand therefore
continuous and concave in σi
s. According to the theorem (Debreu Debreu (1952), Glicksberg Glicksberg (1952),
Fan Fan (1952)), the conditions for the existence of a Nash Equilibrium are satisfied, hence, there exists a
Nash equilibrium σ∗
sfor this 2nplayer game for any s∈Ssuch that for any i∈I,ui
s(σi∗
s,σ−i∗
s)≥ui
s(σi
s,σ−i∗
s)
for anyσi
s.
Denote the agent and adversary policies as (π∗,χ∗)that are constructed following Eq. (40) and Eq. (41) by
plugging in the Nash equilibrium (σi∗
s,σ−i∗
s). Substituting the (π∗,χ∗)intoui
s(σi∗
s,σ−i∗
s)≥ui
s(σi
s,σ−i∗
s)and
plugging in the definition of the utility functions, for any i= 1,2,...,n, it holds that
fi
s(vi∗,πi∗,π−i∗,χi∗,χ−i∗)≥fi
s(vi∗,πi,π−i∗,χi∗,χ−i∗), (44)
for anyπi. Also, for any i= 1,2,...,n, it holds that
fi
s(vi∗,πi∗,π−i∗,χi∗,χ−i∗)≤fi
s(vi∗,πi∗,π−i∗,χi,χ−i∗), (45)
26
Published in Transactions on Machine Learning Research (01/2024)
for anyχi. Therefore,
max
πimin
χifi
s(vi∗,πi,π−i∗,χi,χ−i∗)
=fi
s(vi∗,πi∗,π−i∗,χi∗,χ−i∗). (46)
According to Corollary C.3, for any πi, it holds that
Vi
πi∗,π−i∗,χi∗,χ−i∗(s)≥Vi
πi,π−i∗,χi∗,χ−i∗(s), (47)
Also, for any χi, it holds that
Vi
πi∗,π−i∗,χi∗,χ−i∗(s)≤Vi
πi∗,π−i∗,χi,χ−i∗(s). (48)
Thus, the stage-wise equilibrium defined in Definition C.4 exists for any s∈S.
C.3 Non-existence of Robust Total Nash Equilibrium
Theorem C.6 demonstrates the existence of a stage-wise equilibrium for any state s∈S. In classic Markov
games (Fink, 1964) and Markov games with reward/transition uncertainties (Kardeş et al., 2011; Nilim &
El Ghaoui, 2005; Iyengar, 2005), this result naturally extends to the existence of a Nash equilibrium policy,
as all agents’ and adversaries’ policies are based on the current true state. If a stage-wise equilibrium exists
for any state s∈S, then a Nash equilibrium can be constructed by taking the policies for each state sfrom
their corresponding stage-wise equilibrium for state s(Fink, 1964; Kardeş et al., 2011; Nilim & El Ghaoui,
2005; Iyengar, 2005). However, this natural extension cannot be used for our SAMG problem because the
agent’s policy is based on the perturbed state instead of the true state. The problem is that the agent’s
stage-wise equilibrium in one state may not be consistent with its stage-wise equilibrium in a different state.
We illustrate this idea in the following theorem to show that the robust total Nash equilibrium does not
always exist for SAMGs.
Theorem 4.7 (Non-existence of Robust Total Nash Equilibrium). For SAMGs with finite state and
finite action spaces, the robust total Nash equilibrium defined in Definition 4.6 does not always exist.
Proof.We prove this theorem by showing that the following two-agent two-state game in Fig. 9 does not
have a robust total Nash equilibrium. The two-agent two-state game in Fig. 9 is basically the same as the
𝑠!𝑠""𝑎!=𝑎""𝑟=1𝑎!=𝑎""𝑟=0𝑎!≠𝑎""𝑟=1𝑎!≠𝑎""𝑟=0
Figure 9: A new two-agent two-state game example. Agents get reward 1 at state s1if they choose the same
action. Agents get reward 1 at state s2if they choose different actions.
two-agent two-state game in Fig. 8. The only difference is we changed the state transition for the state s1.
The new state transition functions for the state s1are
p(s′=s2|s=s1,a1̸=a2) = 1,
p(s′=s1|s=s1,a1=a2) = 1. (49)
We first consider the stage-wise equilibriums for each state.
For states1, the stage-wise equilibrium requires Pr(a1
t=a2
t) = 1for allt. One example of the agent policy is
π1(a1|s1) =π1(a1|s2) =π2(a1|s1) =π2(a1|s2) = 1. Note that the agent should have a policy for both s1and
s2even when considering the state-wise equilibrium for the state s1(This means the current true state is s1).
27
Published in Transactions on Machine Learning Research (01/2024)
This is because the adversary can perturb each agent’s state observation to be s2. There is no requirement
for the adversary policy in the state-wise equilibrium because when Pr(a1
t=a2
t) = 1, the true state never
transits. The state value for s1isV(s1) = 100.
Similarly, for state s2, the stage-wise equilibrium requires Pr(a1
t̸=a2
t) = 1for allt. One example of the agent
policy isπ1(a1|s1) =π1(a1|s2) =π2(a2|s1) =π2(a2|s2) = 1. There is no requirement for the adversary policy
in the state-wise equilibrium of s2. The state value for s2isV(s2) = 100.
Since the stage-wise equilibriums have conflict requirements for the agent policy in s1ands2, there is no
agent policy satisfying the requirements of the stage-wise equilibriums in both s1ands2at the same time.
Therefore, there is no robust total Nash equilibrium for agents in this two-agent two-state game.
In the proof of Theorem 4.7, we intended to present a straightforward example for ease of understanding.
However, more counter-examples can be more illustrative in demonstrating the prevalence of non-existence
scenarios. As long as the two stage-wise equilibriums have different requirements (not necessarily contrary to
each other), there is no Nash equilibrium.
To elaborate, consider a 2-state 2-action game. If, for state s1, the stage-wise equilibrium necessitates choosing
actiona1with probability 0.2 and a2with probability 0.8, and for state s2, the stage-wise equilibrium requires
choosinga1with probability 0.6 and a2with probability 0.4, then it’s clear that no Nash equilibrium can
simultaneously satisfy these requirements. This example illustrates the absence of a robust Nash equilibrium
in such a 2-state 2-action game scenario.
Our conclusion is similar to that of Theorem 4.3, in that it is not always possible to find a policy that is a
stage-wise equilibrium for all states. When facing adversarial state perturbations, trade-offs must be made
among different states. As a result, the traditional solution concepts of an optimal agent policy and the
robust total Nash equilibrium cannot be applied to SAMGs.
C.4 Existence of Robust Agent Policy
We need to consider a new objective that is not state-dependent. Therefore, we propose a new objective, the
worst-case expected state value, in Definition 4.8 as
Es0∼Pr(s0)/bracketleftbig¯Vπ(s0)/bracketrightbig
,
where Pr(s0)is the probability distribution of the initial state.
The new objective of ""worst-case expected state value"" is designed specifically for the state perturbation
problem present in SAMGs. It is proposed as a response to our analysis of the non-existence of widely-used
concepts. We demonstrate that these concepts can be easily corrupted by adversaries, requiring agents to
make trade-offs between different states. This is the reason for introducing the new objective. The agent
policy that aims to maximize this worst-case expected state value is referred to as a robust agent policy.
In this section, we show the existence of a robust agent policy to maximize the worst-case expected state
value. We first introduce lemmas for this proof.
Denotepπ,χ,s 0(st)as the probability of reaching state stgiven the agent policy π, adversary policy χ, and
initial state s0. Letpπ,χ,s 0(s0) = 1. The connection between pπ,χ,s 0(st+1)andpπ,χ,s 0(st)is:
pπ,χ,s 0(st+1) =
/summationdisplay
st∈S/summationdisplay
at∈A/summationdisplay
ρt∈Pp(st+1|st,at)π(at|ρt)χ(ρt|st)pπ,χ,s 0(st). (50)
For a concise representation, we omit the subscript stofPstin this section. Consider the function
gs0
t(π,χ) =/summationdisplay
st∈S/summationdisplay
at∈A/summationdisplay
ρt∈Ppπ,χ,s 0(st)×
π(at|ρt)χ(ρt|st)γtrt+1(st,at). (51)
28
Published in Transactions on Machine Learning Research (01/2024)
Lemma C.7. The function gs0
tis continuous on ∆(A)×∆(P)for anyt= 0,1,2,...,nwheren∈N+.
Proof.To prove the continuity, we construct some equivalent vectors as follows. We define a vector ⃗ π∈R|A||P|
and⃗ π(a,ρ) =π(a|ρ)fora∈A,ρ∈P, and a vector ⃗ χ∈R|P||S|where⃗ χ(ρ,s) =χ(ρ|s)forρ∈P,s∈S. And
a vector constant ⃗ r∈R|S||A|where⃗ r(s,a) =r(s,a).
⃗ π⊤= [π(a1|ρ1),···,π(a|A||ρ1),π(a2|ρ1),···,π(a|A||ρ|P|)]
⃗ χ⊤= [χ(ρ1|s1),···,χ(ρ|P||s1),χ(ρ2|s1),···,χ(ρ|P||s|S|)]
⃗ pt= [pπ,χ,s 0(st=s1),···,pπ,χ,s 0(st=s|S|)] (52)
Note that when ρ /∈Ps, then the entry χ(ρ|s) = 0.⃗ pt∈R|S|can be expressed as a linear combination of
⃗ pt−1,⃗ πand⃗ χaccording to (50). Let’s first consider the case t= 0,
gs0
0(π,χ) =/summationdisplay
a0∈A/summationdisplay
ρ0∈Pπ(a0|ρ0)χ(ρ0|s0)r(s0,a0) (53)
Functiongs0
0can be expressed as a linear combination of ⃗ r,⃗ πand⃗ χ. We consider the general case
gs0
t(π,χ) =/summationdisplay
st∈S/summationdisplay
at∈A/summationdisplay
ρt∈Ppπ,χ,s 0(st)×
π(at|ρt)χ(ρt|st)γtrt+1(st,at). (54)
Functiongs0
tcan be expressed as a linear combination of ⃗ r,⃗ pt,⃗ πand⃗ χ. Therefore, gs0
tis continuous on
∆(A)×∆(P)for anyt= 0,1,2,...,nwheren∈N+.
Lemma C.8. For anys0∈S, the series{/summationtextn
t=0gs0
t(π,χ)},n= 1,2,...,converges uniformly on ∆(A)×∆(P).
Proof.ConsiderMs0
t(π,χ) =γtRmax, whereRmaxis the largest absolute value of the rewards. We can check
that|gs0
t(π,χ)|≤Ms0
t(π,χ)fort≥0as follows.
|gs0
t(π,χ)|
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
st∈S/summationdisplay
at∈A/summationdisplay
ρt∈Ppπ,χ,s 0(st)π(at|ρt)χ(ρt|st)γtrt+1(st,at)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
st∈S/summationdisplay
at∈A/summationdisplay
ρt∈Ppπ,χ,s 0(st)π(at|ρt)χ(ρt|st)γtRmax/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=γtRmax×/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
st∈S/summationdisplay
at∈A/summationdisplay
ρt∈Ppπ,χ,s 0(st)π(at|ρt)χ(ρt|st)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=γtRmax×/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
st∈S/summationdisplay
at∈A/summationdisplay
ρt∈PPr(st,at,ρt|s0,π,χ )/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=γtRmax×1 =Ms0
t(π,χ). (55)
Meanwhile,
∞/summationdisplay
t=0Ms0
t(π,χ) =∞/summationdisplay
t=0γtRmax=Rmax
1−γ, (56)
29
Published in Transactions on Machine Learning Research (01/2024)
so/summationtextgs0
tconverges uniformly on ∆(A)×∆(P)according to the Weierstrass M-test in Theorem 7.10 of Rudin
et al. (1976).
Lemma C.8 shows the series {/summationtextn
t=0gs0
t(π,χ)},n= 1,2,...,converges uniformly on ∆(A)×∆(P)for any
s0∈S. In the following lemma, we show/summationtext∞
t=0gs0
t(π,χ)is continuous on ∆(A)×∆(P)for anys0∈S.
Denotehs0(π,χ) =/summationtext∞
t=0gs0
t(π,χ).
Lemma C.9. The function hs0is continuous on ∆(A)×∆(P)for anys0∈S.
Proof.Considerhs0n(π,χ) =/summationtextn
t=0gs0
t(π,χ)forn∈N+. Sincehs0nis a linear combination of {gs0
t}t=0,1,2,···,n
andgs0
tis continuous on ∆(A)×∆(P)for anyt= 0,1,2,···,naccording to Lemma C.7, the sequence {hs0n}
is a sequence of continuous functions on ∆(A)×∆(P). Meanwhile, hs0n→hs0uniformly on ∆(A)×∆(P)for
anys0∈Saccording to Lemma C.8, therefore hs0is continuous on ∆(A)×∆(P)for anys0∈Saccording
to the uniform limit theorem in Theorem 7.12 of Rudin et al. (1976).
The following theorem shows finding a robust agent policy is equivalent to solving a maximin problem.
Theorem 4.10. Finding an agent policy πto maximize the worst-case expected state value under an optimal
adversary for πis equivalent to the maximin problem: maxπminχ/summationtext
s0Pr(s0)Vπ,χ(s0).
Proof.According to the Proposition 4.1, for any fixed agent policy π, there exists an optimal adversary policy
χ∗such that ¯Vπ(s0) = minχVπ,χ(s0)for anys0∈S. Thus,
max
πEs0∼Pr(s0)/bracketleftbig¯Vπ(s0)/bracketrightbig
= max
πEs0∼Pr(s0)/bracketleftbigg
min
χVπ,χ(s0)/bracketrightbigg
(Eq. (2) )
= max
π/summationdisplay
s0Pr(s0) min
χVπ,χ(s0) (Definition of Expectation )
= max
πmin
χ/summationdisplay
s0Pr(s0)Vπ,χ(s0),(Proposition 4.1 ) (57)
Finally, we show the existence of the robust agent policy to maximize the worst-case expected state value in
the following theorem.
Theorem 4.11(Existence of Robust Agent Policy). For SAMGs with finite state and finite action spaces,
there exists a robust agent policy to maximize the worst-case expected state value defined in Definition 4.8.
Proof.According to Theorem 4.10, finding an agent policy πto maximize the worst-case expected state value
under an optimal adversary for πis equivalent to the following maximin problem:
max
πF(π)
:= max
πEs0∼Pr(s0)/bracketleftbig¯Vπ(s0)/bracketrightbig
= max
πmin
χ/summationdisplay
s0Pr(s0)Vπ,χ(s0)
= max
πmin
χJ(π,χ), (58)
where the objective function in (58) can be expanded as follows:
J(π,χ)
=Es0∼Pr(s0)[Vπ,χ(s0)]
30
Published in Transactions on Machine Learning Research (01/2024)
=/summationdisplay
s0Pr(s0)Vπ,χ(s0)
=/summationdisplay
s0Pr(s0)Eat∼π,ρt∼χ/bracketleftigg∞/summationdisplay
t=0γtrt+1(st,at)|s0/bracketrightigg
=/summationdisplay
s0Pr(s0)∞/summationdisplay
t=0Eat∼π,ρt∼χ/bracketleftbig
γtrt+1(st,at)|s0/bracketrightbig
(linearity of the expectation )
=/summationdisplay
s0Pr(s0)∞/summationdisplay
t=0/summationdisplay
st∈S/summationdisplay
at∈A/summationdisplay
ρt∈Ppπ,χ,s 0(st)×
π(at|ρt)χ(ρt|st)γtrt+1(st,at)
=/summationdisplay
s0Pr(s0)∞/summationdisplay
t=0gs0
t(π,χ)
=/summationdisplay
s0Pr(s0)hs0. (59)
BecauseJ(π,χ)is a linear combination of {hs0}s0∈S,Sis finite, and hs0is continuous on ∆(A)×∆(P)
for anys0∈Saccording to Lemma C.9, the objective function J(π,χ) =/summationtext
s0Pr(s0)hs0is continuous on
∆(A)×∆(P). Consider the function F(π) =minχJ(π,χ). Since the adversary policy space ∆(P)is compact,
the function Fis continuous in π. Meanwhile, the agent policy space ∆(A)is closed. Therefore, there exists
an agent policy πto maximize Faccording to the extreme value theorem.
Theorem 4.11 shows the existence of a robust agent policy. Different from the definitions of the state-robust
totally optimal agent policy and robust total Nash equilibrium, the worst-case expected state value objective
does not require the optimality condition to hold for all states. Agents won’t get stuck in trade-offs between
different states, therefore, we can find a robust agent policy to maximize the worst-case expected state value
for the SAMG problem.
Now look back at the two-agent two-state game in Fig. 8. If we use the worst-case expected state value
concept from Definition 4.8 and assume that the initial state is always s2, then we can conclude that π1:
π1(a1|s1) =π1(a1|s2) =π2(a2|s1) =π2(a2|s2) = 1is a robust agent policy, as it gives the maximum
worst-case expected state value of 100 for this game.
D Robust Multi-Agent Adversarial Actor-Critic (RMA3C) Algorithm
In general, it is challenging to develop algorithms that compute optimal or equilibrium policies for MARL
under uncertainties (Zhang et al., 2020b; 2021). Our algorithm adopts centralized training and decentralized
execution paradigm following the popular framework in Lowe et al. (2017). During training, there is a
centralized critic Q(s,a)that records the total expected return given the global state sand global action a.
The connection between Q(s,a)andV(s)is that for any i∈N,s∈S,a∈A,
Q(s,a) =r(s,a) +γ/summationdisplay
s′∈Sp(s′|s,a)V(s′). (60)
Each agent’s state input for the actor is perturbed by an adversary χi(·|s) :S→ ∆(Pi
s). During execution,
each agent iselects action aibased on the perturbed state ρi∈Susing a trained policy πi:S→ ∆(Ai).
We want to find a policy πifor each agent to maximize the worst-case expected state value in Definition 4.8
under adversarial state perturbations.
As shown in Alg. 1, our algorithm has a centralized critic network Qfor training. Each agent has one actor
networkπiand one adversary network χi. The critic Qtakes in the true global state and global action
31
Published in Transactions on Machine Learning Research (01/2024)
Algorithm 1: Robust Multi-Agent Adversarial Actor-Critic (RMA3C) Algorithm
1Randomly initialize the critic network Q, the actor network πi, and the adversary network χifor each
agent;
2Initialize target networks Q′,πi′,χi′;
3foreach episode do
4The initial state s0←sample from Pr(s0);
5Initialize a random process Xfor action exploration;
6foreach time step do
7fori=1 to ndo
8 ρi←sample from χi(·|s);
9 ai←sample from πi(·|ρi) +X;
10 end
11 Execute actions a= (a1,...,an);
12 Obtain the reward rand the next state s′;
13D←D∪ (s,a,r,s′);
14s←s′;
15Q←MGD_Optimizer (Q,D,Q′,π′,χ′);
16 /* Mini-batch gradient descent optimizer for critic. */
17π,χ←GDA_Optimizer (Q,π,χ );
18 /* Gradient descent ascent optimizer for policies. */
19 Update all target networks: θi′←τθi+ (1−τ)θi′.
20end
21end
during the training process. It returns a Q-value denoting the total expected return given sanda. The state
transition experience is represented by (s,a,r,s′)wheres′is the next state. It is stored in a replay buffer
Dfor the critic network’s training. We apply ""replay buffer"" and ""target network"" techniques (Mnih et al.,
2015). The critic network is trained with a mini-batch gradient descent optimizer in line 15. In line 16, we
use Gradient Descent Ascent (GDA) optimizer (Lin et al., 2020b) to update parameters for each agent’s actor
network and adversary network for the maximin problem maxπminχ/summationtext
s0Pr(s0)Vπ,χ(s0)in Theorem 4.10. A
detailed introduction for the GDA optimizer is included in Appendix E.4.
We have added an adversarial network that inputs the true state and outputs a perturbed state in RMA3C.
This is in contrast to MADDPG and MAPPO, which do not include such a network. Compared to M3DDPG,
which has a target policy network for each agent with outputs for the action space, our adversarial network’s
output pertains to the state space, indicating a different computational load.
E Implementation Detail
All hyperparameters used in experiments are listed in table 3.
E.1 Environments
We have tested our algorithm in environments provided by Lowe et al. (2017) as shown in Fig. 10.
E.1.1 Cooperative navigation (CN)
This is a cooperative task. There are 3 agents and 3 landmarks. Agents want to occupy/cover all the
landmarks. They need to cooperate through physical actions about their preferred landmark to cover. Also,
they will be penalized when collisions happen.
32
Published in Transactions on Machine Learning Research (01/2024)
Table 3: Hyperparameters for our RMA3C algorithm and the baselines.
Parameter RMA3C M3DDPG MADDPG MAPPO
optimizer for the critic network Adam Adam Adam Adam
learning rate for agent policy π 0.01 0.01 0.01 0.0007
learning rate for adversary policy χ0.001 / / /
discount factor 0.95 0.95 0.95 0.99
replay buffer size 106106106/
activation function Relu Relu Relu Relu
number of hidden layers 2 2 2 1
number of hidden units per layer 64 64 64 64
number of samples per minibatch 1024 1024 1024 1
target network update coefficient τ0.01 0.01 0.01 /
GDA optimizer steps 20 / / /
radiusd 1.0 / / /
uncertainty level λ 0.5 0.5 0.5 0.5
upper boundary u 1.0 1.0 1.0 1.0
lower boundary l -1.0 -1.0 -1.0 -1.0
episodes in training 10k 10k 10k 10k
time steps in one episode 25 25 25 25
Figure 10: Some environments to test our algorithm, including a) Cooperative navigation (CN) b) Exchange
target (ET) c) Keep-away (KA) d) Physical deception (PD).
E.1.2 Exchange target (ET)
This is a cooperative task. There are 2 agents and 3 landmarks. Each agent needs to get to its target
landmark, which is known only by another agent. They have to learn communication and get to landmarks.
Besides, both of them are generous agents that pay more attention to helping others, i.e. rewarded more if
the other agent gets closer to the target landmark.
33
Published in Transactions on Machine Learning Research (01/2024)
E.1.3 Keep-away (KA)
This is a competitive task. There is 1 agent, 1 adversary, and 1 landmark. The agent knows the position of
the target landmark and wants to reach it. The adversary does not know the target landmark and wants to
prevent the agent from reaching the target by pushing them away or occupying the target temporarily.
E.1.4 Physical deception (PD)
This is a mixed cooperative and competitive task. There are 2 collaborative agents, 2 landmarks including
a target, and 1 adversary. Both the collaborative agents and the adversary want to reach the target, but
only collaborative agents know the correct target. The collaborative agents should learn a policy to cover all
landmarks so that the adversary does not know which one is the true target.
E.2 Baselines
We compare the performance of our algorithm with MADDPG (Lowe et al., 2017), M3DDPG (Li et al., 2019),
and MAPPO (Yu et al., 2022) and follow their open-source implementation. We have a brief introduction of
these methods in the following sections. There is no robustness considered in MADDPG and MAPPO. The
M3DDPG considers the robustness of training partner’s policies, but it does not consider state uncertainty.
The MAPPO is the multi-agent version of the Proximal Policy Optimization (PPO), a popular policy gradient
algorithm. Because MAPPO only works in fully cooperative tasks, we only report its results in cooperative
navigation and exchange target. Note that MAPPO is also used in Guo et al. (2020) but they do not provide
an open-source implementation. Therefore, we select the latest implementation in Yu et al. (2022) with the
open-source code.
E.3 Multi-Agent Deep Deterministic Policy Gradient (MADDPG)
It is difficult to apply single-agent RL algorithms directly to the multi-agent case because the environment’s
state transition is also influenced by the policy of other agents and it is non-stationary from a single agent’s
view. To alleviate this problem and stabilize training, the MADDPG algorithm is proposed using a centralized
Qfunction that has global state and global action information (Lowe et al., 2017). It assumes all agents are
self-interested and every agent’s objective is to maximize its own total expected return. The objective for
agentiisJ(θi) =E[Ri]and its gradient is
∇θiJ(θi) = (61)
Ex,a∼D/bracketleftbig
∇θiµi(oi)∇aiQi(x,a1,...,an)|ai=µi(oi)/bracketrightbig
,
whereQi(x,a1,...,an)is a centralized action-value function, x= (o1,...,on), andoirepresents agent
i’s observation. The experience replay buffer Dcontains transition experience x,a1,...,an,x′,r1,...,rnto
decorrelate data. The centralized Qican be trained using the Bellman loss:
L(θi) =Ex,a,r,x′∼D[y−Qi(x,a1,...,an)]2,
y=ri+γQi′(x′,a1′,...,an′)|aj′=µj′(oj), (62)
whereQi′is the target network whose parameters are copied from Qwith a delay to stabilize the moving
target. Note that this algorithm adopts a centralized training and decentralized execution paradigm. When
testing, each agent can only access its local observation to select actions.
In M3DDPG (Li et al., 2019), the uncertainty from the training partner’s policies is considered: all other
partners are considered as adversaries that select actions to minimize the total expected return of the
training agent. In other words, when updating both actor and critic, they select training partner’s actions by
aj̸=i= arg min aj̸=iQi(x,a1,...,an).
34
Published in Transactions on Machine Learning Research (01/2024)
E.4 Gradient Descent Ascent (GDA)
Gradient Descent Ascent (GDA) (Lin et al., 2020b) is currently one widely-used algorithm for solving the
following minimax optimization problem:
min
xmax
yf(x,y). (63)
GDA simultaneously performs gradient descent update on the variable xand gradient ascent update on the
variableyaccording to (64) with step sizes ηxandηy.
xt+1=xt−ηx∇xf(xt,yt),
yt+1=yt+ηy∇yf(xt,yt). (64)
It has a variety of variants to accommodate different types of geometries of the minimax problem, such as
convex-concave geometry, nonconvex-concave geometry, nonconvex-nonconcave geometry, etc.
Figure 11: Our RMA3C algorithm compared with several baseline algorithms during the training process. The
results showed that our RMA3C algorithm outperforms the baselines, achieving higher mean episode rewards
and displaying greater robustness to state perturbations. The baselines were trained under either random
state perturbations or a well-trained adversary policy χ∗(adversaries that are trained for the maximum
training episodes in RMA3C). Overall, our RMA3C algorithm achieved up to 58.46% higher mean episode
rewards than the baselines.
E.5 Training Comparison Under different Perturbations
We first compare our algorithm with baselines during the training process to show that our RMA3C algorithm
can outperform baselines to get higher mean episode rewards under different state perturbations. Note
that our RMA3C algorithm has a built-in adversary to perturb states, so we do not train it under random
state perturbations. Comparing RMA3C to other baselines with different state perturbations, the RMA3C
gets higher mean episode rewards. It shows our RMA3C algorithm is more robust under different state
perturbations. Comparing each baseline with random state perturbations to the same baseline with the
well-trained adversary policy χ∗, we can see the adversary trained by the RMA3C is more powerful than
the random state perturbations. Because the adversary policy χ∗intentionally selects state perturbations to
minimize agents’ total expected return. The mean episode reward of the last 1000 episodes during training
is shown in Table 4. Our RMA3C algorithm achieves up to 58.46% higher mean episode rewards than the
baselines under different state perturbations.
E.6 Cooperative Navigation With 6 Agents
We compare our RMA3C algorithm with baselines in the cooperative navigation scenario with more agents
added. The original cooperative navigation environment has 3 agents and the training results are shown in
Fig. 4. We show the training results with 6 agents in Fig. 12. After increasing the total number of agents
35
Published in Transactions on Machine Learning Research (01/2024)
Table 4: Mean episode reward of the last 1000 episodes during the training. Our RMA3C algorithm achieves
up to 58.46% higher mean episode rewards than the baselines. The corresponding figure is 11, and it is also
included in the main content.
CN ET KA
RMA3C (ours) -401.7 -47.02 -8.93
MADDPG w/N-506.48 -63.76 -13.76
M3DDPG w/N-506.54 -61.71 -13.45
MAPPO w/N-569.07 -94.28 -
MADDPG w/ χ∗-548.80 -77.01 -16.30
M3DDPG w/ χ∗-547.99 -75.87 -16.26
MAPPO w/ χ∗-585.83 -113.19 -
Figure 12: Our RMA3C algorithm compared with baselines during the training process in the cooperative
navigation scenario with 6 agents added. Our algorithm gets higher mean episode rewards in the environment
with an increased agent number.
in the environment, our RMA3C algorithm still gets higher mean episode rewards than baselines under
adversarial state perturbations.
We also test the learned policies in the 6-agent Cooperative Navigation (CN) environment to show our
RMA3C policy is more robust under adversarial state perturbations. During testing, the mean episode
rewards are averaged across 2000 episodes and 10 test runs for each algorithm. We put all the well-trained
agents using different algorithms into the 6-agent CN environment with well-trained adversary policies χ∗
to perturb states. The result is shown in Table 5. Our RMA3C policy achieves up to 9.57% higher mean
episode reward than the baselines with well-trained adversarial state perturbations. The result shows that our
RMA3C algorithm achieves higher robustness for a multi-agent system under adversarial state perturbations.
F Discussions and Future Work
In this section, we add several discussions of our work as a first attempt to study different solution concepts
of the SAMG problem. We also point out several future directions for the SAMG problem.
F.1 GDA Convergence
In our RMA3C algorithm, we use Gradient Descent Ascent (GDA) optimizer (Lin et al., 2020b) to update
parameters for each agent’s actor network and the adversary network. Each agent updates the actor network to
maximize the worst-case expected state value in Definition 4.8, while the corresponding adversary updates the
adversary network to minimize the worst-case expected state value. How to solve a non-convex non-concave
minimax problem is a very challenging and not yet well-solved problem. To the best of our knowledge, the
GDA optimizer is currently one of the most widely used and accepted optimizers for this type of problem,
36
Published in Transactions on Machine Learning Research (01/2024)
Table 5: Mean episode rewards of 2000 episodes during testing under well-trained adversarial state perturba-
tions in the cooperative navigation environment with 6 agents. Our RMA3C policy achieves up to 9.57%
higher mean episode reward than the baselines with well-trained χ∗.
Environment CN with 6 agents
MADDPG w/ χ∗-3405.274±66.18
M3DDPG w/ χ∗-3452.22±80.16
MAPPO w/ χ∗-3121.90±18.49
RMA3C w/ χ∗-3079.37±16.16
though it is not guaranteed to always converge (Jin et al., 2020; Razaviyayn et al., 2020; Lin et al., 2020b).
Our RMA3C algorithm with GDA optimizer shows performance improvement in terms of policy robustness
in our experiments. Note that we only use the GDA optimizer as a tool in our algorithm by leveraging
the existing literature on solving non-convex non-concave minimax problems. Future advances of numerical
algorithms and solvers for this kind of minimax problem will also benefit our algorithm by replacing the GDA
optimizer with new advances.
F.2 Non-Markovian Policy
In this work, we give the first attempt to focus on the Markovian policy under adversarial state perturbations.
Dealing with the non-Markovian policy will significantly complicate the problem. We are aware of the
suboptimality of Markovian policies, however, considering the computational cost of the non-Markovian
policy of MARL, we decide to focus on Markovian policies in this work for computational tractability.
Moreover, as shown in Proposition 3.2, our SAMG problem is different from a Dec-POMDP. Considering
a non-Markovian policy based on the observation-action history may not give an advantage to the agents.
For example, for the two-agent two-state game in Fig. 8, if the adversary randomly perturbs the state with
χi(s1|s2) = 0.5fori= 1,2, then the agents still only have a 50%chance to guess the true state even with
observation-action history. Considering another example for the two-agent two-state game in Fig. 8, if the
adversary perturbs all states to state s1withχi(s1|s2) = 1andχi(s1|s1) = 1fori= 1,2, then the agents
cannot get extra information for the true state even with observation-action history. We leave the formal
analysis of non-Markovian, non-stationary policy as future work.
F.3 Non-collaborative Game
In the problem formulation, we consider a collaborative game, where all agents share one stage-wise reward
function. The new objective for the SAMG, the worst-case expected state value under state perturbations, is
well-defined as proved in Theorem 4.11. For non-collaborative games, if each agent has its own reward function,
and adversary iwants to minimize the total expected return of agent i, then for a fixed agent policy π, then
adversaries are playing a Markov game. In this case, only the Nash equilibrium exists among nadversaries,
but optimal adversary policy may not exist. Therefore, for non-collaborative games, the worst-case expected
state value is not well-defined. Even though the worst-case expected state value is not well-defined for
non-collaborative games, the experiment results of the competitive games and mixed-cooperative-competitive
game environments in Table 1 also show that our RMA3C algorithm can get larger mean episode rewards in
non-collaborative games under adversarial state perturbations. Hence, our RMA3C algorithm can increase
the robustness of policies of non-collaborative games in empirical experiments. We leave the formal analysis
of the non-collaborative games as future work.
37"
2302.03322v3.pdf,"Graphical Abstract
Attacking Cooperative Multi-Agent Reinforcement Learning by
Adversarial Minority Influence
Simin Lia, e, Jun Guoa, Jingqiao Xiua, Yuwei Zhenga, Pu Fenga, Xin Yua,
Jiakai Wangb, Aishan Liua, Yaodong Yangd, Bo Ane, Wenjun Wua, Xianglong
Liua, b, c ➀
➀Corresponding author: Xianglong Liu. Email address: xlliu@buaa.edu.cn.arXiv:2302.03322v3  [cs.LG]  30 Jul 2024
Highlights
Attacking Cooperative Multi-Agent Reinforcement Learning by
Adversarial Minority Influence
Simin Lia, e, Jun Guoa, Jingqiao Xiua, Yuwei Zhenga, Pu Fenga, Xin Yua,
Jiakai Wangb, Aishan Liua, Yaodong Yangd, Bo Ane, Wenjun Wua, Xianglong
Liua, b, c ➀
•We develop AMI, a strong and practical attack towards c-MARL, which
leverages the intricate influence among agents and the cooperative na-
ture of victims in c-MARL.
•We introduce the unilateral influence filter andtargeted adversarial or-
acleto optimize the deviation of victim policies and deceive victims into
suboptimal cooperation, thereby ensuring a potent attack capability.
•AMI achieves the first successful attack against real world robot swarms
and effectively fool agents in simulation environments into collectively
worst-case scenarios, including StarCraft II and Multi-agent Mujoco.
➀Corresponding author: Xianglong Liu. Email address: xlliu@buaa.edu.cn.
Attacking Cooperative Multi-Agent Reinforcement
Learning by Adversarial Minority Influence
Simin Lia, e, Jun Guoa, Jingqiao Xiua, Yuwei Zhenga, Pu Fenga, Xin Yua,
Jiakai Wangb, Aishan Liua, Yaodong Yangd, Bo Ane, Wenjun Wua,
Xianglong Liua, b, c ➀
aState Key Lab of Software Development Environment, Beihang
University, Beijing, China
bZhongguancun Laboratory, Beijing, China
cInstitute of Data Space, Hefei Comprehensive National Science Center, Hefei, China
dInstitute of Artificial Intelligence, Peking University, Beijing, China
eNanyang Technological University, Singapore
Abstract
This study probes the vulnerabilities of cooperative multi-agent reinforce-
ment learning (c-MARL) under adversarial attacks, a critical determinant
of c-MARL’s worst-case performance prior to real-world implementation.
Current observation-based attacks, constrained by white-box assumptions,
overlook c-MARL’s complex multi-agent interactions and cooperative objec-
tives, resulting in impractical and limited attack capabilities. To address
these shortcomes, we propose Adversarial Minority Influence (AMI), a prac-
tical and strong for c-MARL. AMI is a practical black-box attack and can
be launched without knowing victim parameters. AMI is also strong by
considering the complex multi-agent interaction and the cooperative goal of
agents, enabling a single adversarial agent to unilaterally misleads major-
ity victims to form targeted worst-case cooperation. This mirrors minor-
ity influence phenomena in social psychology. To achieve maximum devia-
tion in victim policies under complex agent-wise interactions, our unilateral
attack aims to characterize and maximize the impact of the adversary on
the victims. This is achieved by adapting a unilateral agent-wise relation
metric derived from mutual information, thereby mitigating the adverse ef-
fects of victim influence on the adversary. To lead the victims into a jointly
➀Corresponding author: Xianglong Liu. Email address: xlliu@buaa.edu.cn.
Preprint submitted to Neural Networks July 31, 2024
detrimental scenario, our targeted attack deceives victims into a long-term,
cooperatively harmful situation by guiding each victim towards a specific
target, determined through a trial-and-error process executed by a reinforce-
ment learning agent. Through AMI, we achieve the first successful attack
against real-world robot swarms and effectively fool agents in simulated en-
vironments into collectively worst-case scenarios, including Starcraft II and
Multi-agent Mujoco. The source code and demonstrations can be found at:
https://github.com/DIG-Beihang/AMI .
Keywords: Multi-agent reinforcement learning, trustworthy reinforcement
learning, adversarial attack, algorithmic testing
1. Introduction
Cooperative multi-agent reinforcement learning (c-MARL) involves the
coordination of multiple agents to maximize their shared objective in an
environment [1, 2, 3, 4, 5, 6, 7]. Applications of c-MARL includes cooperative
gaming [8], traffic signal management [9], distributed resource allocation [10],
and cooperative swarm control [11, 12, 13, 3, 14, 15].
While c-MARL has achieved notable success, research has exposed the
vulnerability of c-MARL agents to observation-based adversarial attacks
[16, 17], wherein adversaries introduce perturbations to an agent’s observa-
tion, causing it to execute suboptimal actions. Given the interrelated nature
of victim actions for cooperation, other c-MARL agents may become disori-
ented and being non-robust (see Fig. 1). As c-MARL algorithms frequently
feature in security-sensitive applications, assessing their worst-case perfor-
mance against potential adversarial interference is crucial before real world
deployment. However, observation-based attacks against c-MARL depend
on white-box access to victim and complete control of agent observations,
rendering them highly impractical. For instance, in autonomous driving sce-
narios, it can be prohibitively hard for attackers to access the architectures,
weights, and gradients utilized by a vehicle or to introduce arbitrary pixel-
wise manipulations to camera input at each timestep [16, 18, 19].
In this paper, we take a practical and black-box alternative by conducting
a policy-based adversarial attack ( i.e., adversarial policy) [20, 21, 22] to assess
the robustness of c-MARL. Unlike direct observation manipulation, policy-
based attacks perturb observations in a natural manner by incorporating an
adversarial agent within the c-MARL environment—a feasible approach in
2
Figure 1: While observation-based attack requires white-box assess to victim and manip-
ulates agent observation directly, our adversarial minority influence attack is a black-box,
policy-based attack that leverages one minority attacker to unilaterally influence majority
victims towards a jointly worst target.
numerous c-MARL applications. For instance, adversaries can legitimately
participate in distributed resource allocation clusters as individual workers
[23, 10] or control their own vehicles while influencing other vehicles in au-
tonomous driving [24, 25]. By interacting with the environment, the adver-
sary learns an adversarial policy that directs it to execute physically plausible
actions, which in turn adversely influence the observations of victim agents.
Although policy-based attacks have been investigated in two-agent com-
petitive games [20, 21, 22], these studies have neglected two crucial challenges
in c-MARL, resulting in diminished attack efficacy. Ideally, the adversary
should influence victims toward a cooperatively inferior policy. This gives rise
to two issues: (1) The influence problem, wherein all agents mutually influ-
ence one another in c-MARL; consequently, attacking a single victim impacts
the policies of other victims as well. In the face of such intricate agent-wise
influence, it becomes difficult for the adversary to maximally deviate victim
policies and identify the optimal attack strategy. (2) The cooperation prob-
lem, which arises as merely perturbing victim actions arbitrarily or toward
locally suboptimal cases is insufficient to indicate the failure of cooperative
victims. The adversary faces the challenge of exploring and deceiving victims
into a long-term, jointly detrimental failure scenario.
To address these challenges, we introduce Adversarial Minority Influence
(AMI) , a black-box policy-based attack for c-MARL. Shown in Fig. 1, AMI
differs from other attacks by deceiving victims in an unilateral andtargeted
3
manner. The term minority influence originates from social psychology [26],
wherein minorities (adversary) can unilaterally sway majorities (victims) to
adopt its own targeted belief. Technically, to maximally deviate victim poli-
cies amidst complex agent interactions, we quantify and maximize the ad-
versarial impact from the adversary to each victim. Our unilateral influence
filter adapts mutual information, a bilateral agent-wise relation metric, into
a unilateral relation metric that represents the influence from the adversary
to the victim. This is achieved by decomposing mutual information and fil-
tering out the harmful influence from victims to the adversary. To deceive
victims into a jointly worst-case failure, we learn a long-term, worst-case
target for each victim, which leads to collective failure. The target is de-
termined by the targeted adversarial oracle , a reinforcement learning agent
that generates worst-case target actions for each victim by co-adapting with
our adversarial policy. Ultimately, the attacker modifies its policy to direct
victims toward these adversarial target actions through unilateral influence.
Ourcontributions are listed as follows:
•We develop AMI, a strong and practical attack towards c-MARL, which
leverages the intricate influence among agents and the cooperative na-
ture of victims in c-MARL.
•We introduce the unilateral influence filter andtargeted adversarial or-
acleto optimize the deviation of victim policies and deceive victims into
suboptimal cooperation, thereby ensuring a potent attack capability.
•AMI achieves the first successful attack against real world robot swarms
and effectively fool agents in simulation environments into collectively
worst-case scenarios, including StarCraft II and Multi-agent Mujoco.
2. Related Work
2.1. Overview of Adversarial Attacks
Initially proposed in the field of computer vision, adversarial attacks con-
sist of carefully crafted perturbations that, while imperceptible to humans,
can deceive deep neural networks (DNNs) into making incorrect predictions
[27, 28, 29]. Given a DNN Fθ, a clean image x, a perturbed image xadv,
and the ground truth label y, an adversarial example can be formulated as
follows:
Fθ(xadv)̸=y s.t. ∥x−xadv∥ ≤ϵ. (1)
4
In this formulation, ∥·∥represents a distance metric used to constrain the dis-
tance between xandxadvbyϵ. Subsequently, it was demonstrated that rein-
forcement learning (RL) is also susceptible to adversarial attacks [18, 30, 31].
Owing to the sequential decision-making nature of RL, adversarial attacks
in this context aim to generate a perturbation policy παthat minimizes the
victim’s cumulative rewardP
tγtrt, which can be expressed as:
min
παX
tγtrt. (2)
Adversarial attacks are important to distinguish as test-time attacks, where
the adversary targets a specific victim without participating in the training
process. As another line of research, training-time attacks interfere with vic-
tim training, resulting in trained victims either failing to perform well (poi-
soning attack) [32, 33] or executing adversary-specified actions when specific
triggers are present (backdoor attack) [34, 35, 36]. Note that our method is
a test-time attack, and is not related to training-time attacks.
2.2. RL Attacks by Observation Perturbation
Test-time perturbation of RL observations can deceive the policy of RL
agents, causing them to execute suboptimal actions and fail to achieve their
goals. For single-agent RL attacks, early research employed heuristics such
as preventing victims from selecting the best action [18] or choosing actions
with the lowest value at critical time steps [30, 31]. Later work framed the
adversary and victim within an MDP [19], enabling the optimal observation
perturbation to be learned as an action within the current state using an RL
agent [37, 38]. For c-MARL attacks, Lin et al. [16] proposed a two-step attack
that first learns a worst-case attack policy and then employs a gradient-based
attack [39] to execute it. [17] generated attacks on one victim and transfer it
to the rest of the victims. However, they assume attacker can modify victim
observations and has white-box access to victim parameters, which can be
impractical in real world Another line of research, termed adversarial com-
munication, targets communicative c-MARL by sending messages to victim
agents that cause failure upon receiving the adversarial message. Adversarial
messages can be added to representations [40] or learned by the adversary
[41, 42, 43]. However, these methods are inapplicable when victim agents
do not communicate, a common assumption in many mainstream c-MARL
algorithms [1, 3, 2].
5
2.3. RL Attacks by Adversarial Policy
Distinct from observation-based attacks, adversarial policy attacks do not
necessitate access to victim observations or parameters (black-box). Rather,
they introduce an adversarial agent whose actions are designed to deceive
victim agents, causing them to take counterintuitive actions and ultimately
fail to achieve their goals. In this paper, the terms “policy-based attack”
and “adversarial policy” are used interchangeably. Gleave et al. [20] were
the first to introduce adversarial policy in two-agent zero-sum games. This
approach was latter applied to multi-agent consensus game, where agents
have similar, but non-identical objectives [44]. Subsequent research has en-
hanced adversarial policy by exploiting victim agent vulnerabilities. Wu et
al. [21] induced larger deviations in victim actions by perturbing the most
sensitive feature in victim observations, identified through a saliency-based
approach. However, larger deviations in victim actions do not necessarily cor-
respond to strategically worse policies. Guo et al. [22] extended adversarial
policies to general-sum games by simultaneously maximizing the adversary’s
reward and minimizing the victim’s rewards. Yet, none of these studies
have considered adversarial policies in c-MARL settings. To better evaluate
the performance of our attack in multi-agent adversarial policy scenario, we
adapt some observation-based attack in MARL [17] as baselines.
3. Problem Formulation
We conceptualize adversarial attacks targeting c-MARL agents within the
framework of a partially observable Stochastic game (POSG) [45], which can
be characterized by a tuple:
G=⟨N,S,O, O,A,T, R, γ⟩. (3)
Specifically, N={Nα,Nν}={1, ..., N}is the set containing Nagents.
Throughout this paper, we use αfor adversaries and νfor victims. Sis the
global state space, O=×i∈NOiis the observation space, Ois the observation
emission function, A=×i∈NAiis the action space, T:S × A → ∆(S) is
the state transition probability. The reward Rα:S × A → Ris shared for
all adversaries and Rν:S ×A → Ris shared for all victims. γ∈[0,1) is the
discount factor.
At each timestep, each agent iobserves ot,i=O(st, i) and add it to his-
toryht,i= [o0,i, a0,i, ..., o t,i] to alleviate partial observability [46]. For victims,
6
we assume its joint policies πν(aν|h) =Q
i∈Nνπν(aν
i|hi) are fixed during de-
ployment [47] and take joint actions according to joint policies aν∼πν(·|h).
For adversary, it takes actions by an adversarial policy aα∼πα(·|hi). Next,
the game environment proceeds to the next state via transition probability
T(st+1|st, aα,aν) and receive the reward for adversary, rα
t=Rα(st, aα
t,aν
t).
The objective of the adversary is to learn an adversarial policy παto maximize
its expected discounted cumulative reward, i.e.,J(πα) =Es,aα,aν[P
tγtrα
t].
Since defender policies are fixed, the attacker must solve a reinforcement
learning problem, but can exploit the weakness in policies of other agents by
maximally deviating victims into a jointly worst-case situation.
Assumptions. To keep our attack practical, we assume attacker cannot
manipulate victim observations or choose which agent to control. Addition-
ally, attacker does not have the models ( i.e., architectures, weights, gradients)
and rewards of victims. Following the centralized training, decentralized ex-
ecution (CTDE) paradigm for c-MARL attack [16, 40, 42, 43], we posit that
adversaries have access to the state and reward only during training. How-
ever, during actual attack deployment, they must rely solely on their local
observation histories. To implement our AMI attack in physical environment,
we adopt a Sim2Real paradigm [47], such that the adversary first trains its
adversarial policy within a simulated environment, then freezes the policy’s
parameters and deploys the attack to real-world robots.
Applications. Our adversary can be launched as a malicious participa-
tor in a multi-agent distributed system [23, 10]. Adversaries can also hijack
an agent directly [48, 49, 50] and utilize the controlled agent to take an ad-
versarial policy. Apart from malicious use, our AMI attack also functions
as a testing algorithm to evaluate the worst-case robustness of c-MARL
algorithms, which helps managing algorithmic risks before deploying it in
risk-sensitive applications.
4. Method
As previously mentioned, adversarial policy towards c-MARL presents
two challenges: the influence challenge, which necessitates the adversary
to maximize victim policy deviations under intricate agent-wise interactions;
and the cooperation challenge, which requires the adversary to deceive agents
into jointly worst failures. In this paper, we address these challenges through
ouradversarial minority influence (AMI) framework. As depicted in Fig.
2, AMI achieves potent attack capabilities by unilaterally guiding victims
7
Figure 2: Framework of AMI. Unilateral influence filter decompose mutual information
into minority influence and majority influence terms, while keeping latter for asymmetric
influence. Targeted adversarial oracle is an RL agent that generates a worst-case target
for each victim. Attacking victims towards this target results in jointly worst cooperation.
towards a targeted worst-case scenario. To maximize policy deviation of
victims, we propose the unilateral influence filter that characterizes the ad-
versarial impact from the adversary to victims by decomposing the bilateral
mutual information metric and eliminating the detrimental influence from
victims to the adversary. To steer victims towards jointly worst actions, the
targeted adversarial oracle is a reinforcement learning agent that co-adapts
with the adversary and generates cooperatively worst-case target actions for
each victim at every timestep.
4.1. Unilateral Influence Filter
In a c-MARL attack, due to the intercorrelated nature of agent policies,
targeting one victim inevitably impacts the policies of other victims as well.
In light of such intricate relationships, maximizing policy deviations for vic-
tims necessitates that the adversary first characterizes the influence of its
actions on each victim before maximizing that influence. Consequently, to
delineate the unilateral influence from the adversary to the victim, we draw
inspiration from the minority influence theory in social psychology [26].
Using mutual information as a starting point, we emphasize that the poli-
cies of the adversary and the victims are interdependent. However, mutual
information fails to fully reflect the deviation in victim policy as it includes
the reverse influence from the victims to the adversary, which is counterpro-
ductive for the attack. By focusing on the one-way influence from adversary
to victims only, our approach aligns with the principles of minority influence,
where a small, focused group can effectively influence the behavior of a larger
group.
We begin by examining mutual information, a commonly employed bi-
lateral influence metric in the c-MARL literature [51, 52, 53, 54], which
captures the relationship between agents. For instance, considering social
8
influence [51], the mutual information between the adversary action aα
tand
the influenced victim action aν
t+1,ican be expressed as I(aα
t;aν
t+1,i|st,aν
t),
where aα
tdenotes the action taken by adversary αat time t, and aν
t+1,i
represents the action taken by the ithvictim νat time t+ 1. Since vic-
tim observation and parameter is unknown, the action probability of aν
t+1,i
can be approximated by network pϕin a supervised learning objective, i.e.,
max ϕPNν
i=1PT−1
t=0logpϕ(ˆaν
t+1,i|st,at), where Tdenotes the total timesteps of
an episode. In the remainder of our paper, we use ˆ ato signify that the action
is predicted, rather than ground truth.
Although mutual information is capable of characterizing agent-wise in-
fluence in c-MARL, it is important to note that attacks towards c-MARL
differ from cooperation due to the fixed nature of victim parameters , which
renders victim actions more challenging to modify compared to those of the
adversary. To demonstrate the effects of fixed victim policy, we decompose
the mutual information between the adversary action aα
tand a victim agent
aν
t+1,ias follows:
I(aα
t; ˆaν
t+1,i|st,aν
t) =−H(ˆaν
t+1,i|st, aα
t,aν
t)| {z }
majority
+H(ˆaν
t+1,i|st,aν
t)|{z }
minority.(4)
In the context of a c-MARL attack, we refer to the first term as majority in-
fluence , that is, the extent to which the minority (attacker) adapts its policy
to conform with the policies of the majority (victims). In order to maxi-
mize mutual information, H(ˆaν
t+1,i|st, aα
t,aν
t) should be minimized , such that
having knowledge of aα
treduces the uncertainty surrounding the victim pol-
icy. In policy-based attacks within c-MARL, the majority influence term in
mutual information causes attackers to comply with victim policies, yielding
high mutual information but weak attack capability. To elucidate this, con-
sider that the parameters of victims are fixed, whereas the adversary policy
is learned. We assume that it is significantly more straightforward to modify
the adversary’s policy than the victim’s policy. Consequently, in order to
minimize majority influence, the most effective approach for an attacker is
to adjust its action aα
tto render it more predictive of victim actions, without
actually altering it.
Simultaneously, the second term, H(ˆaν
t+1,i|st,aν
t), can be interpreted as
a form of minority influence devoid of victim impact, reflecting solely the
9
adversary’s effect on victims. This term accounts for the entropy of the
victim policy without conditioning on aα
t, thereby establishing an unilateral
metric. Given that the influence of the adversary action aα
tis marginalized,
the adversary is unable to modify its action to cater to the policy of victims.
Building upon the aforementioned discourse, we examine and generalize
minority influence within mutual information to enhance attack capabilities.
In particular, maximizing minority influence requires current adversary pol-
icy to have large uncertainty in victim policy:
H(ˆaν
t+1,i|st,aν
t) =−DKL 
pϕ(ˆaν
t+1,i|st,aν
t)|| U)
+c
=−DKL 
E˜aα
t∼πα
t
pϕ(ˆaν
t+1,i|st,˜aα
t,aν
t)
|| U)
+c,(5)
see Appendix. Appendix A for detailed derivation. In the equation, Urep-
resents a uniform distribution for victim actions applicable to both discrete
and continuous action spaces, DKLis the Kullback-Leibler divergence, cis a
constant, ˜ aα
trefers to the counterfactual action sampled from the adversarial
policy πα.
Equation A.1 shows the second term is equivalent to minimizing the KL
divergence between the victim policy and the uniform distribution. For a
more general relation metric between adversary and victim, we can release
the constraints of Eqn. A.1 and replace Uby a worst-case target distribu-
tionDand generalize DKLto any distance metric d(·,·). Consequently, the
unilateral influence can be expressed as:
I=d 
E˜aα
t∼πα
pϕ(ˆaν
t+1,i|st,˜aα
t,aν
t)
,D)
. (6)
In this manner, influencing victims unilaterally is tantamount to minimizing
the distance between the expected victim policy under the adversary policy
and a target distribution.
4.2. Targeted Adversarial Oracle
Upon elucidating the maximization of victim policy deviations through
unilateral influence, it remains crucial to ensure that victims are guided to-
ward an optimal target. Merely deviating the victim policy arbitrarily or in
the direction of a locally inferior case does not guarantee a globally worst-case
failure for c-MARL. To enhance attack capability, it is necessary to ascer-
tain globally worst target actions for victims and subsequently steer each
victim towards its target D(Eqn. 6) using the proposed unilateral influence
approach. Consequently, we introduce a reinforcement learning agent that
10
learns these jointly worst target actions by co-adapting its policy with the
attacker in a trial-and-error process.
To accomplish the targeted attack objective, we introduce the Targeted
Adversarial Oracle (TAO) , a reinforcement learning agent πτthat guides the
attacker to influence each victim toward their globally worst-case direction.
To achieve this, TAO use global state as input, and is used to guide the
attacker only in training. During execution, the attacker acts on its own
without the guidance of TAO. As an RL agent, TAO adjusts to the current
perturbation budget of the adversary through a trial-and-error process: if
the adversary can significantly impact victim policies, TAO can generate the
target more aggressively, directing victims to undertake riskier actions, thus
achieving greater attack capability; conversely, if the adversary has limited
influence on victims, TAO strives to introduce minor yet effective pertur-
bations to victims, which is feasible under the current perturbation budget.
Analogous to the attacker, TAO’s objective is to maximize the adversary’s
goal. We define the following Bellman operator Bτto update the value func-
tion of TAO:
(BτQτ) (s, aτ, aα) =rα
t+γX
s′T(s′|s, aα,aν)X
a′τ∈A
π(a′τ|s′)X
(a′ν,a′α)∈Aπα(a′α|h′
i)πν(a′α|h′)Qτ(s′, a′τ, a′α).(7)
To avoid non-stationarity, Qτalso depends on aα, the adversary’s action.
While aτdoes not directly interact with the environment, the attacker’s
policy is implicitly influenced by aτthrough the unilateral influence term
added to attacker’s reward. Assuming the space of state, action of TAO and
adversary are finite, we can proof Bτis a contraction operator. Consequently,
updating Qτ(s, aτ, aα) byBτwill converge to optimal value Qτ,∗(s, aτ, aα).
See the proof in Appendix. Appendix B.
In practice, we use proximal policy optimization (PPO) [55] to optimize
the policy of TAO. The value function used by PPO is an advantage function
Aτ
t, with the policy of TAO πτupdated by PPO objective:
max
πτEπτ[min (clip ( ρt,1−ϵ,1 +ϵ)Aτ
t, ρtAτ
t)],
where ρt=1
Nν|Nν|X
i=1πτ(aτ
t+1,i|st,aν
t, aα
t)
πτ
old(aτ
t+1,i|st,aν
t, aα
t),(8)
11
where πτ
oldandπτrepresent the previous and updated policies for TAO,
respectively. Aτ
tis the advantage function determined by the generalized
advantage estimation (GAE) [56]. The function clip( ρt,1−ϵ,1+ϵ) constrains
the input ρtwithin the limits of 1 −ϵand 1+ ϵ.|Nν|is the number of victims.
In this case, the policy πτ(aτ
t+1,i|st,aν
t, aα
t), as determined by TAO, functions
as the desired target distribution Din Eqn. 6 for victim iat time t. Notably,
as PPO is an on-policy algorithm, we derive Dby sampling a target action
aτ
t+1,ifrom πτinstead.
4.3. Overall Training
In summary, the influence metric Iα
tused by AMI at time tcombines the
optimal target action aτ
t+1,igenerated by TAO as the target distribution D
for unilateral influence in Eqn. 6:
Iα
t=NνX
i=1
d
E˜aα
t∼παh
pϕ(ˆaν
t+1,i|st,˜aα
t,aν
t)i
, aτ
t+1,i∼πτ(·|st, aα
t,aν
t)
,(9)
In the case of d(·,·) as the distance function for AMI, the calculation differs
depending on the control setting. For discrete control , the distance function
is computed as d(p(ˆaν
i|s,a), aτ
i) =−||p(ˆaν
i|s,a)−1A(aτ
i)||1, where p(ˆaν
i|s,a) is
a shorthand for E˜aα
t∼πα
pϕ(ˆaν
t+1,i|st,˜aα
t,aν
t)
andaτ
idenotes aτ
t+1,i.1A(aτ
i) =
[aτ
i∈ Aτ
i] is the indicator function with action space Aτ
i. A negative sign is
added to minimize the distance between the one-hot target action 1A(aτ
i) and
the victim action probability. In the case of continuous control , the distance
function is calculated as d(p(ˆaν
i|s,a), aτ
i) =p(aτ
i|s,a), such that the target
action aτ
t+1,ihas a high probability in the estimated victim action probability
distribution. As verified in ablations, while we tune the distance functions
for best performance, our method is not sensitive to the choice of distance
functions.
In the final step, Iα
tserves as an auxiliary reward that is optimized by
the policy of the adversarial agent πα(aα
t|hα
t). The reward rAMI
t for the
adversarial agent πθto optimize can be expressed as:
rAMI
t=rα
t+λ·Iα
t, (10)
In this case, λis a hyperparameter that balances the trade-off between the
adversary reward rα
tand maximizing the influence Iα
ton the victim agents.
12
We define the following Bellman operator Bαto update the value function of
adversary’s policy:
(BαQα) (s, aτ, aα) =rα+λ·Iα+γX
s′T(s′|s, aα,aν)
X
a′τ∈Aπ(a′τ|s′)X
(a′ν,a′α)∈Aπα(a′α|h′
i)πν(a′α|h′)Qα(s′, a′τ, a′α).
(11)
Here, the dependency on aτis added to avoid non-stationarity. Using similar
techniques for proofing the convergence of Bτ, we can proof Bαis a contrac-
tion operator, thus updating Qα(s, aτ, aα) viaBαconverge to the optimal
value Qα,∗(s, aτ, aα). See the proof in Appendix. Appendix C.
In practice, we compute the advantage function Aα
tusing rAMI
tvia GAE
[56] and train the adversary using PPO [55]:
max
παEπα[min (clip ( ρt,1−ϵ,1 +ϵ)Aα
t, ρtAα
t)],
where ρt=πα(aα
t|hα
t)
πα
old(aα
t|hα
t).(12)
The complete training procedure is outlined in Algorithm 1.
5. Experiments
In this section, we perform comprehensive experiments in both simulated
and real-world environments to assess the effectiveness of our AMI approach
in terms of attack capability.
5.1. Experimental Setup
5.1.1. Environments
We assess the effectiveness of AMI in three distinct environments: (1) A
real-world multi-robot rendezvous environment, in which robot swarms learn
to gather together, known as rendezvous [12]. (2) StarCraft Multi-Agent
Challenge (SMAC) [8], involving discrete control across six tasks, where the
objective is to control a group of agents in a StarCraft game to defeat an
opposing group; (3) Multi-Agent Mujoco (MAMujoco) [11] for continuous
control, comprising six tasks that require controlling robotic joints to op-
timize speed in a specific direction. All victim policies were trained using
MAPPO [3]. During the attack, the first agent is selected as the adversary.
13
Algorithm 1 Adversarial Minority Influence Algorithm.
Input: Policy of victims πν, adversary παand targeted adversarial oracle
(TAO) πτ. Value function of adversary Vαand TAO Vτ. Opponent
modelling network pϕ.
Output: Trained policy network of adversary agent πθ.
1:fork = 0, 1, 2, ... K do
2: Perform rollout using current adversarial policy network παand TAO
agent πτ. Collect a set of trajetories Dk=τi, where i = 1, 2, ..., |Dk|.
3: Update opponent modelling model pϕ.
4: Calculate advantage function of TAO Aτ
tby GAE, using value function
Vαand adversary reward rα; Update value function network Vτof
TAO.
5: Update the policy network πτof TAO using Eqn. 8;
6: Calculate reward rAMI
tfor adversary by Eqn. 10.
7: Calculate advantage function Aα
tby GAE, using value function Vαand
reward rAMI; Update value function network Vαof adversary.
8: Update policy network παof adversary by Eqn. 12.
9:end for
5.1.2. Compared methods and evaluation metrics
We benchmark AMI against state-of-the-art adversarial policy methods,
including single-agent adversarial policy, Gleave et al. [20], Wu et al. [21],
Guo et al. [22] and multi-agent attack GMA [17]. We adapt adversarial
policy for single-agent case to multi-agent by substituting a single RL vic-
tim with multiple RL victims. As for multi-agent observation-based GMA
(a) E-puck2 robot
 (b) Playground
Figure 3: Illustration of the robot and playground for our real-world multi-robot ren-
dezvous environment.
14
attack, we allow their attack to manipulate victim action arbitrarily, same
as our setting. To ensure a fair comparison, AMI and all baselines employ
the same codebase, network structure, and hyperparameters. For method-
specific hyperparameters, we tune their values for optimal performance. The
adversary’s goal is to maximize the adversarial reward rα, defined as (1) max-
imizing the loss of allies and minimizing the loss of enemies for SMAC. (2)
minimizing the speed of agents in +x direction for MAMujoco. (3) maximiz-
ing the euclidean distance between all agents for rendezvous. All experiments
were conducted with five random seeds, and results are presented with a 95%
confidence interval. The hyperparameters used for all experiments are listed
in Appendix. Appendix D.
5.2. AMI Attack in Real World
To highlight the effectiveness of AMI, we evaluate AMI in real-world
multi-robot environments. To the best of our knowledge, this is the first
evaluation of adversarial policy in real world. As shown in Fig. 3, we create
an environment with 10 e-puck2 robots (Fig. 3a) [57] in an indoor playground
(Fig. 3b). The task is called rendezvous , where robots are randomly dispersed
in the arena and must gather together.
We train these robots using the widely adopted Sim2Real paradigm [47]
in the RL community, in which agents first learn their policy in a simulated
environment before being deployed in the real world with fixed parameters.
To evaluate attack performance, we present results from both simulated and
real-world scenarios in Fig. 4. Each method was tested 10 times in the real
world, leading to several key findings:
(1) AMI outperforms all baselines in both simulated and real-world envi-
ronments. The improvement of AMI in the real world is statistically signifi-
0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 1e7303540455055 Adversary Reward
AMI (ours)Guo et, al.Wu et, al.
GMAGleave et, al.
(a) Simulation Results
Gleaves et, al. Wu et, al. Guo et, al. GMA AMI (ours)
Method354045505560Adversary Reward (b) Real World Results
Figure 4: Comparisons of AMI against baselines in simulation and real-world experiments.
15
(a)Gleaves et al.
 (b)Wu et al.
 (c)Guo et al.
 (d)GMA
 (e) AMI (ours)
Figure 5: Behaviors of robot swarms under our AMI attack, adversary indicated by red
square. Our adversary is the only one to fool away an agent to group with our adversary.
0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 1e7−125−75−25050Adversary RewardMAp5: Walker 2x30.0 0.2 0.4 0.6 0.8 1.0
Timesteps 1e7−1000−50005001000Adversary RewardMap1: HalfCheetah 2x3
0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 1e7−600−400−2000Adversary RewardMap2: HalfCheetah 6x1
0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 1e7−300−250−200−150−100−50050Adversary RewardMap4: ManyagentSwimmer 6x2
0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 1e7−300−200−1000Adversary RewardMap3: ManyagentSwimmer 4x2
0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 1e7−1010−1000−990−980−970Adversary RewardMap6: Ant 4x2
75
25
−100−500 1 2 3 4 5
Timesteps 1e60246Adversary RewardMap3: 3s5z
0 1 2 3 4 5
Timesteps 1e6024681012Adversary RewardMap1: MMM
0 1 2 3 4 5
Timesteps 1e60246810Adversary RewardMap2: 3s5z vs 3s6z
0 1 2 3 4 5
Timesteps 1e63
246Adversary RewardMap4: 6h vs 8z
0 1 2 3 4 5
Timesteps 1e605101520Adversary RewardMap5: 3m
0 1 2 3 4 5
Timesteps 1e624681012Adversary RewardMap6: 10m vs 11mGleave et, al. Wu et, al. Guo et, al. AMI
-2
-41
0
-15GMA
(a) SMAC (b) MAMujoco
Figure 6: Learning curves for AMI and baseline attack methods in six SMAC and six
MAMujoco environments. The solid curves correspond to the mean, and the shaped
region represents the 95% confidence interval over 5 random seeds.
cant ( p < . 05) compared to all baselines under a paired samples t-test and,
on average, 5.43 higher than the best-performing baseline in the real world.
(2) The superiority of AMI can be further demonstrated by agent behav-
iors. As shown in the final state photographed in Fig. 5, under baseline
attacks, victims gather together as usual without being influenced, with
adversaries moving away (Fig. 5a-5d). However, with our AMI attack,
one victim gets influenced by the adversary, and are fooled to gather with
the adversary, instead of majority victims (Fig. 5e). Notably, our AMI
is the only method to achieve this. See video demonstrations in https:
// github. com/ DIG-Beihang/ AMI .
16
(a) (b) (c) (d) (e)
 (c)Still
Moving
Encounter
EnemiesFigure 7: Understanding the behavior of AMI. Attacker and victims in red, enemies in
blue. (a) attackers entice victims to get back. (b) victims were influenced into a bad
position. (c) some victims encounter enemies, while others are still moving. (d) first-
arrived victims died, and enemies focused fire on the rest. (e) attacker moves near to get
killed.
5.3. AMI Attack in Simulation Environment
Apart from real world results, we further evaluate the effectiveness of
AMI in 12 tasks in simulated environments for completeness, including six
discrete control tasks (SMAC) and six continuous control tasks (MAMu-
joco), demonstrating its superior performance. For simplicity, we assume the
attacker controls the first agent in all environments. As shown in Fig. 6,
by strategically influencing victims toward a jointly worst target, our AMI
outperforms the competing methods in 10 out of 12 environments across
both continuous and discrete control, highlighting the effectiveness of our
approach.
5.4. Analyzation of AMI policies
02468101214161820222426
0.00.10.20.30.40.5
TimestepAction ProbabilityIdleMove Attack enemiesMV N
MV S
MV E
MV W
ATK 1
ATK 2
ATK 3
ATK 4
ATK 5
ATK 6
ATK 7
ATK 8
ATK 9
ATK10
 ATK11
Delayed Arrival
Unfocused Fire
(a) TAO, Agent 3 (High AMI)
0246810121416182022240123456789101112131415
 0.000.050.100.150.200.250.300.350.40
TimestepAction Probability (b) TAO, Agent 7 (Low AMI)
024681012141618202224260123456789101112131415
 0.00.10.20.30.40.5
TimestepAction Probability (c) TAO, Agent 9 (High AMI)
024681012141618202224260123456789101112131415
 0.00.20.40.60.8
TimestepAction Probability (d) Joint Victim Actions
Figure 8: Actions suggested by TAO and taken by victims, evaluated in 10m vs 11m .
Red and blue bracket indicates delayed arrival and unfocused fire behavior of AMI. MV
N means move north and ATK X means attack enemy ID X.
To investigate the attack behavior of AMI, we visualize the behavior of
victims subjected to AMI attacks in 10m vs 11m environment of SMAC,
shown in Fig. 7 and 8.
Victim behavior under AMI. Victims under AMI attack exhibit two
critical behaviors that contributes to collective failure: (1) delayed arrival .
As depicted in Fig. 7, victims are influenced by the attacker and divided
into two groups. These groups encounter enemies at different timesteps:
17
while some victims confront the full force of their adversaries, others are
still approaching and do not have enemies within their firing range. (2)
unfocused fire . In SMAC, focused fire involves allies collaboratively attacking
and defeating enemies one at a time. However, in the presence of the attacker,
allies fire in an unfocused manner. As illustrated in Fig. 8d, victims’ shots
at enemies (Action ID 5-15) are randomly distributed, lacking a coordinated
target. Consequently, victims fail to eliminate enemy units and face stronger
enemy fire.
Influence through the lens of TAO. The behavior of victims under
AMI can be explained by the target actions generated by TAO. As demon-
strated in Fig. 8, for understanding delayed arrival , at the game’s onset,
victims 3 and 7 are encouraged to move north (Action ID 1), arriving later
compared to agents moving directly east; for victim 9, which is moving east,
it is encouraged to move west or stay idle: had it followed this target, it
would have arrived slightly later than agents moving east directly ( i.e., not
being influenced), but still earlier than agents moving north, also resulting
in delayed arrival. To comprehend unfocused fire , agents 3, 7, and 9 are
encouraged to attack different enemies at the current timestep, limiting the
number of victims attacking each enemy and suppressing focused fire.
Adaptive target generation. Furthermore, we discover that TAO can
generate adaptive policies for victims with varying susceptibility, as illus-
trated in Fig. 8. By calculating AMI, we find that agents 3 and 9 are more
influenced, and their target policies learned by TAO are more determinis-
tic. In this manner, TAO generates a targeted goal for susceptible agents,
as they have a higher probability of being influenced toward the collectively
worst target policy. Conversely, since agent 7 is less influenced, the target
policy learned by TAO is less deterministic. In this case, TAO generates an
untargeted goal for insusceptible agents: as their policies are difficult to influ-
ence, the attacker achieves the best results by preventing them from playing
the optimal policy at the current timestep. In this way, TAO automatically
learns different policies for susceptible and insusceptible victims under the
current attacks.
5.5. Ablations
In this section, we verify the effectiveness of each component in our model.
We conduct all experiments on 3s5z vs 3s6z for SMAC and HalfCheetah 6x1
for MAMujoco.
18
5.5.1. Ablations on unilateral and targeted properties
0 1 2 3 4 5
Timesteps 1e6345678910Adversary Reward
AMI (ours)
AMI w/o unilateral
AMI w/o target
Gleave et, al.Map: 3s5z vs 3s6z
(a) SMAC environment
0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 1e7−700−600−500−400−300−200−1000Adversary Reward
Map: HalfCheetah 6x1 (b) MAMujoco environment
Figure 9: Ablation of unilateral and targeted properties of AMI. Both properties improve
attack performance.
In this ablation study, we seek to assess the individual contributions of
AMI’s unilateral and targeted attack properties. For a version of AMI with-
out unilateral attack capabilities, we integrate the majority influence term
from Eqn. 4 into Eqn. 9, resulting in bilateral influence. In contrast, for
AMI without a targeted attack, we employ Eqn. A.1 as the influence metric.
We also provide a comparison with the results of Gleave et al. (i.e., absent
of unilateral and targeted attacks). As illustrated in Fig. 9, the effectiveness
of AMI relies on both enhancements. Notably, we observe that AMI without
unilateral attack capabilities underperforms Gleave et al. in the HalfCheetah
6x1environment. This finding implies that majority influence can strongly
diminish attack capability by encouraging the attacker to modify its actions
to overfit to victim policies.
5.5.2. Ablations on distance metric
Next, we investigate the performance of AMI when utilizing various dis-
tance metrics. We present alternative distance metrics for both discrete
and continuous environments in Table 1, where ˆ µν
i(s,a) represents the mean
predicted by the opponent modeling for continuous control, under the as-
sumption that actions adhere to a Gaussian distribution. The outcomes are
illustrated in Fig. 10, which reveals that the attack capability of AMI re-
mains largely unaffected by the choice of distance metrics for both continuous
and discrete control scenarios. Notably, the metrics employed in our AMI ( ℓ1
for discrete control and Prob for continuous control) yield the most favorable
results.
19
Table 1: In this section, we address the distance metrics employed for both discrete and
continuous environments. AMI is determined using these metrics and subsequently maxi-
mized by the attacker under the optimal hyperparameter λ.
Name Equation Environment
ℓ1 −||p(ˆaν
i|s,a)−1A(aτ
i)||1 Discrete
ℓ2 −||p(ˆaν
i|s,a)−1A(aτ
i)||2 Discrete
ℓ∞ −||p(ˆaν
i|s,a)−1A(aτ
i)||∞ Discrete
CE log( p(aτ
i|s,a)) Discrete
Prob p(aτ
i|s,a) Discrete
ℓ1 −||ˆµν
i(s,a)−aτ
i||1 Continuous
CE log( p(aτ
i|s,a)) Continuous
Prob p(aτ
i|s,a) Continuous
0 1 2 3 4 5
Timesteps 1e6678910Adversary Reward
CE
Prob
Map: 3s5z vs 3s6z
(a) Distance - Discrete
0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 1e7−600−500−400−300−200−1000Adversary Reward
 CE
Prob
Map: HalfCheetah 6x1 (b) Distance - Continuous
Figure 10: Ablation on distance metric. The performance of AMI is stable under different
distance metrics.
5.5.3. Comparing with mutual information
Since unilateral influence is derived from mutual information, we also
take mutual information [51] that depicts the bilateral influence between vic-
tims and adversary as a baseline. Specifically, we evaluate the result for four
different λvalues that modulate the magnitude of mutual information. As
illustrated in Fig. 11, utilizing mutual information often results in inferior
performance compared to Gleave et al. (i.e., not employing mutual infor-
mation). Moreover, the outcome worsens as λincreases. These observations
serve as motivation for the development of AMI, a unilateral and targeted
influence designed for c-MARL attacks, which yields superior performance.
5.5.4. Ablations on hyperparameter
Lastly, we evaluate AMI under varying hyperparameters λ, which modu-
late the contribution of the AMI reward Iα
tto the total reward. Specifically,
20
0 1 2 3 4 5
Timesteps 1e6−5.0−2.50.02.55.07.510.0Adversary Reward
λ=1
λ=10
λ=100
λ=1000
AMI (ours)
Gleave et, al.Map: 3s5z vs 3s6z(a) SMAC environment
0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 1e7−1200−1000−800−600−400−2000200Adversary Reward
Map: HalfCheetah 6x1 (b) MAMujoco environment
Figure 11: Using mutual information proposed in [51] as influence metric. Since mutual
information is bidirectional, such metric was ineffective for attack.
0 1 2 3 4 5
Timesteps 1e6345678910Adversary Reward
Map: 3s5z vs 3s6z
λ=0
λ=0.01
λ=0.1
λ=1
λ=10
(a) SMAC environment
0.0 0.2 0.4 0.6 0.8 1.0
Timesteps 1e7−600−400−2000Adversary Reward
Map: HalfCheetah 6x1 (b) MAMujoco environment
Figure 12: Ablation on hyperparameter λ.
we assess our AMI attack for λ={0,0.01,0.1,1,10}, where λ= 0 reduces
to the method of Gleave et al . As depicted in Fig. 12, incorporating AMI
enhances the overall attack performance compared to Gleave et al , thereby
demonstrating the effectiveness of our approach. Moreover, selecting an ap-
propriate hyperparameter leads to optimal AMI performance ( λ= 0.1 in3s5z
vs 3s6z andλ= 1 in HalfCheetah 6x1 ). We also observe that performance
does not improve monotonically with increasing λvalues. This can be at-
tributed to the following factors: (1) a large λgenerates high rewards, which
may introduce instability when training the critic network, and (2) a sub-
stantial λcauses errors in the opponent modeling module pϕto accumulate,
resulting in excessive emphasis on the attack capability of the subsequent
timestep.
21
6. Conclusion
In this paper, we present AMI as a strong and practical black-box attack
to assess the robustness of c-MARL, in which the attacker unilaterally influ-
ences victims to establish a worst-case collaboration. Firstly, we adapt agent-
wise bilateral mutual information to a unilateral adversary-victim influence
for policy-based attacks by decomposing mutual information and filtering out
the influence from victims to the adversary. Secondly, we employ a reinforce-
ment learning agent to generate the jointly worst-case target for the attacker
to influence in order to maximize team reward. Through AMI, we pioneers
the successful execution of adversarial attacks on real-world robotic swarms,
and demonstrate its effectiveness in compelling agents towards collectively
unfavorable outcomes in simulated environments. Our findings not only offer
valuable insights into system vulnerabilities but also open opportunities to
strengthen the resilience of cooperative multi-agent systems.
Acknowledgements
This work was supported by the National Key Research and Development
Plan of China (2022ZD0116405), the National Natural Science Foundation
of China (62022009, 62206009, 62306025), and the State Key Laboratory of
Software Development Environment.
Appendix A. Detailed Derivation of Eqn. 5
Here we present the detailed derivation of Eqn. 5 in our main paper.
22
H(ˆaν
t+1,i|st,aν
t)
=−X
ˆaν
t+1,i∈Aipϕ(ˆaν
t+1,i|st,aν
t) log 
pϕ(ˆaν
t+1,i|st,aν
t)
=−X
ˆat+1,i∈Aipϕ(ˆaν
t+1,i|st,aν
t) log 
pϕ(ˆaν
t+1,i|st,aν
t
−pϕ(ˆaν
t+1,i|st,aν
t) log(U) +c,
=−DKL 
pϕ(ˆaν
t+1,i|st,aν
t)|| U)
+c
=−DKL(
X
˜aα
tpϕ(ˆaν
t+1,i|st,˜aα
t,aν
t)·p(˜aα
t|st,aν
t)
|| U)) +c,
=−DKL(
X
˜aα
tpϕ(ˆaν
t+1,i|st,˜aα
t,aν
t)·πα(˜aα
t|st)
|| U)) +c,
=−DKL 
E˜aα
t∼πα
pϕ(ˆaν
t+1,i|st,˜aα
t,aν
t)
|| U)
+c,
(A.1)
Appendix B. Convergence of Value Function Qτ(s, aτ, aα)
Here we present the full proof of convergence of Qτ(s, aτ, aα). We first
show that updating Qτby Bellman operator Bτis a contraction on Banach
space, with Bτdefined as:
(BτQτ) (s, aτ, aα) =rα
t+γX
s′T(s′|s, aα,aν)X
a′τ∈A
π(a′τ|s′)X
(a′ν,a′α)∈Aπα(a′α|h′
i)πν(a′α|h′)Qτ(s′, a′τ, a′α).(B.1)
Define two Q functions Qτ
1(s, aτ, aα) and Qτ
2(s, aτ, aα), we need to show
the Bellman operator Bτis a contraction in sup-norm:
23
||BτQτ
1− BτQτ
2||∞
= max
s,aτ,aα|(BτQτ
1)(s, aτ, aα)−(BτQτ
2)(s, aτ, aα)|
= max
s,aτ,aαrα
t+γT(s′|s, aα,aν)X
a′τ∈Aπ(a′τ|s′)X
(a′ν,a′α)∈A
πα(a′α|h′
i)πν(a′α|h′)Qτ
1(s′, a′τ, a′α)−rα
t−γT(s′|s, aα,aν)
X
a′τ∈Aπ(a′τ|s′)X
(a′ν,a′α)∈Aπα(a′α|h′
i)πν(a′α|h′)Qτ
2(s′, a′τ, a′α)
= max
s,aτ,aαγT(s′|s, aα,aν)X
a′τ∈Aπ(a′τ|s′)X
(a′ν,a′α)∈Aπα(a′α|h′
i)
πν(a′α|h′)(Qτ
1(s′, a′τ, a′α)−Qτ
2(s′, a′τ, a′α))
≤max
s,aτ,aαγT(s′|s, aα,aν)X
a′τ∈Aπ(a′τ|s′)X
(a′ν,a′α)∈Aπα(a′α|h′
i)
πν(a′α|h′)|Qτ
1(s′, a′τ, a′α)−Qτ
2(s′, a′τ, a′α)|
≤γ||Qτ
1(s′, a′τ, a′α)−Qτ
2(s′, a′τ, a′α)||∞(B.2)
Thus, Bτis a contraction operator. Finally, by Banach’s fixed point the-
orem, with finite joint action space A, state space S, and assume each state-
action pair is visited infinitesimally often, updating Qτ(s, aτ, aα) by Bellman
operator Bτwill converge to the optimal value function Qτ,∗(s, aτ, aα). Note
that the guaranteed convergence happens in tabular case. This motivates us
to use PPO algorithm as a practical solver of this problem.
Appendix C. Convergence of Value Function Qα(s, aτ, aα)
The convergence of Qα(s, aα, aα) follows the same technique with the
convergence proof of Qτ(s, aτ, aα). We state the proof here again for com-
pleteness.
Again, we first show that updating Qαby Bellman operator Bαis a
24
contraction on Banach space, with Bαdefined as:
(BαQα) (s, aτ, aα) =rα+λ·Iα+γX
s′T(s′|s, aα,aν)
X
a′τ∈Aπ(a′τ|s′)X
(a′ν,a′α)∈Aπα(a′α|h′
i)πν(a′α|h′)Qα(s′, a′τ, a′α).
(C.1)
Define two Q functions Qα
1(s, aτ, aα) and Qα
2(s, aτ, aα), we need to show
the Bellman operator Bαis a contraction in sup-norm:
||BαQα
1− BαQα
2||∞
= max
s,aτ,aα|(BαQα
1)(s, aτ, aα)−(BαQα
2)(s, aτ, aα)|
= max
s,aτ,aαrα
t+λ·Iα+γT(s′|s, aα,aν)X
a′τ∈Aπ(a′τ|s′)
X
(a′ν,a′α)∈Aπα(a′α|h′
i)πν(a′α|h′)Qα
1(s′, a′τ, a′α)−rα
t
−λ·Iα−γT(s′|s, aα,aν)X
a′τ∈Aπ(a′τ|s′)
X
(a′ν,a′α)∈Aπα(a′α|h′
i)πν(a′α|h′)Qα
2(s′, a′τ, a′α)
= max
s,aτ,aαγT(s′|s, aα,aν)X
a′τ∈Aπ(a′τ|s′)X
(a′ν,a′α)∈Aπα(a′α|h′
i)
πν(a′α|h′)(Qα
1(s′, a′τ, a′α)−Qα
2(s′, a′τ, a′α))
≤max
s,aτ,aαγT(s′|s, aα,aν)X
a′τ∈Aπ(a′τ|s′)X
(a′ν,a′α)∈Aπα(a′α|h′
i)
πν(a′α|h′)|Qα
1(s′, a′τ, a′α)−Qα
2(s′, a′τ, a′α)|
≤γ||Qα
1(s′, a′τ, a′α)−Qα
2(s′, a′τ, a′α)||∞(C.2)
Thus, Bαis a contraction operator. Finally, by Banach’s fixed point the-
orem, with finite joint action space A, state space S, and assume each state-
action pair is visited infinitesimally often, updating Qα(s, aτ, aα) by Bell-
man operator Bαwill converge to the optimal value function Qα,∗(s, aτ, aα).
25
Again, the guaranteed convergence happens in tabular case. We thus use
PPO algorithm as a practical solver of this problem.
Appendix D. Experiment Hyperparameters
In this section, we describe the hyperparameters of AMI and baselines.
ForSMAC environment , Table. D.2 describes the shared parameters used
by all methods in SMAC experiments. Table. D.3 denotes the hyperparam-
eters used for each individual methods.
Table D.2: Shared hyperparameters for SMAC, used in AMI and all baselines.
Hyperparameter Value Hyperparameter Value
lr 1e-4 mini-batch num 1
parallel envs 32 max grad norm 10
gamma 0.99 max episode len 150
actor network mlp actor lr =lr
hidden dim 64 critic lr =lr
hidden layer 1 PPO epoch 4
activation ReLU PPO clip 0.2
optimizer Adam entropy coef 0.01
GAE lambda 0.95 eval episode 20
Table D.3: Method-specific parameters for Wu et al. ,Guo et al. and our AMI in SMAC
environment.
Hyperparameters for Wu et al.
Hyperparameter Value Hyperparameter Value
epsilon state 0.1 epsilon action 0.1
lrstate =actor lr lraction =actor lr
Hyperparameters for Guo et al.
Hyperparameter Value Hyperparameter Value
victim critic lr =lr
Hyperparameters for AMI
Hyperparameter Value Hyperparameter Value
pϕlr =lr TAO lr =lr
TAO critic lr =lr AMI lambda [.03, .05, .1]
26
ForMAMujoco environment , Table. D.4 describes the parameters
used for all experiments. Table. D.5 denotes the hyperparameters used for
each individual methods.
Table D.4: Shared hyperparameters for MAMujoco, used in AMI and all baselines in
MAMujoco environment.
Hyperparameter Value Hyperparameter Value
parallel envs 32 mini-batch num 40
gamma 0.99 max grad norm 10
gain 0.01 max episode len 1000
actor network mlp actor lr [5e-6, 5e-4]
std y coef 0.5 critic lr 5e-3
std x coef 1 Huber loss True
hidden dim 64 Huber delta 10
hidden layer 1 PPO epoch 5
activation ReLU PPO clip 0.2
optimizer Adam entropy coef 0.01
GAE lambda 0.95 eval episode 32
Forrendezvous environment , the hyperparameters and implementa-
tions follows MAMujoco environment, with small variations to achieve best
performance in this scenario. Table. D.6 describes the parameters used for
the rendezvous environment. Table. D.7 denotes the hyperparameters used
for each individual methods.
References
[1] T. Rashid, M. Samvelyan, C. Schroeder, G. Farquhar, J. Foerster,
S. Whiteson, Qmix: Monotonic value function factorisation for deep
multi-agent reinforcement learning, in: International conference on ma-
chine learning, PMLR, 2018, pp. 4295–4304.
[2] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, I. Mor-
datch, Multi-agent actor-critic for mixed cooperative-competitive envi-
ronments, Advances in neural information processing systems 30 (2017).
[3] C. Yu, A. Velu, E. Vinitsky, Y. Wang, A. Bayen, Y. Wu, The surprising
effectiveness of ppo in cooperative, multi-agent games, arXiv preprint
arXiv:2103.01955 (2021).
27
Table D.5: Method-specific parameters for Wu et al. ,Guo et al. and our AMI in MAMu-
joco environment.
Hyperparameters for Wu et al.
Hyperparameter Value Hyperparameter Value
epsilon state 0.1 epsilon action 0.1
lrstate =actor lr lraction =actor lr
Hyperparameters for Guo et al.
Hyperparameter Value Hyperparameter Value
victim critic lr =lr
iteration 30
Hyperparameters for AMI
Hyperparameter Value Hyperparameter Value
pϕlr =lr TAO lr =lr
TAO critic lr =lr AMI lambda [.01, .1, .3, 1]
Table D.6: Shared hyperparameters for rendezvous, used in AMI and all baselines.
Hyperparameter Value Hyperparameter Value
parallel envs 32 mini-batch num 40
gamma 0.99 max grad norm 10
gain 0.01 max episode len 1000
actor network mlp actor lr 5e-5
std y coef 0.5 critic lr 5e-3
std x coef 1 Huber loss True
hidden dim 64 Huber delta 10
hidden layer 1 PPO epoch 5
activation ReLU PPO clip 0.2
optimizer Adam entropy coef 0.01
GAE lambda 0.95 eval episode 32
28
Table D.7: Method-specific parameters for Wu et al. ,Guo et al. and our AMI in ren-
dezvous environment.
Hyperparameters for Wu et al.
Hyperparameter Value Hyperparameter Value
epsilon state 0.1 epsilon action 0.1
lrstate =actor lr lraction =actor lr
Hyperparameters for Guo et al.
Hyperparameter Value Hyperparameter Value
victim critic lr =lr
Hyperparameters for AMI
Hyperparameter Value Hyperparameter Value
pϕlr =lr TAO lr =lr
TAO critic lr =lr AMI lambda .003
[4] J. Ji, T. Qiu, B. Chen, B. Zhang, H. Lou, K. Wang, Y. Duan, Z. He,
J. Zhou, Z. Zhang, et al., Ai alignment: A comprehensive survey, arXiv
preprint arXiv:2310.19852 (2023).
[5] J. Ji, B. Chen, H. Lou, D. Hong, B. Zhang, X. Pan, J. Dai, Y. Yang,
Aligner: Achieving efficient alignment through weak-to-strong correc-
tion, arXiv preprint arXiv:2402.02416 (2024).
[6] J. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian, B. Chen, R. Sun,
Y. Wang, Y. Yang, Beavertails: Towards improved safety alignment of
llm via a human-preference dataset, Advances in Neural Information
Processing Systems 36 (2024).
[7] J. Ji, K. Wang, T. Qiu, B. Chen, J. Zhou, C. Li, H. Lou, Y. Yang, Lan-
guage models resist alignment, arXiv preprint arXiv:2406.06144 (2024).
[8] M. Samvelyan, T. Rashid, C. S. De Witt, G. Farquhar, N. Nardelli,
T. G. Rudner, C.-M. Hung, P. H. Torr, J. Foerster, S. Whiteson, The
starcraft multi-agent challenge, arXiv preprint arXiv:1902.04043 (2019).
[9] T. Chu, J. Wang, L. Codec` a, Z. Li, Multi-agent deep reinforcement
learning for large-scale traffic signal control, IEEE transactions on intel-
ligent transportation systems 21 (3) (2019) 1086–1095.
29
[10] C. Zhang, V. Lesser, P. Shenoy, A multi-agent learning approach to on-
line distributed resource allocation, in: Twenty-first international joint
conference on artificial intelligence, 2009.
[11] B. Peng, T. Rashid, C. Schroeder de Witt, P.-A. Kamienny, P. Torr,
W. B¨ ohmer, S. Whiteson, Facmac: Factored multi-agent centralised
policy gradients, Advances in Neural Information Processing Systems
34 (2021) 12208–12221.
[12] M. H¨ uttenrauch, S. Adrian, G. Neumann, et al., Deep reinforcement
learning for swarm systems, Journal of Machine Learning Research
20 (54) (2019) 1–31.
[13] W. J. Yun, S. Park, J. Kim, M. Shin, S. Jung, D. A. Mohaisen, J.-H.
Kim, Cooperative multiagent deep reinforcement learning for reliable
surveillance via autonomous multi-uav control, IEEE Transactions on
Industrial Informatics 18 (10) (2022) 7086–7096.
[14] X. Yu, R. Shi, P. Feng, Y. Tian, J. Luo, W. Wu, Esp: Exploiting sym-
metry prior for multi-agent reinforcement learning, in: ECAI 2023, IOS
Press, 2023, pp. 2946–2953.
[15] X. Yu, W. Wu, P. Feng, Y. Tian, Swarm inverse reinforcement learn-
ing for biological systems, in: 2021 IEEE International Conference on
Bioinformatics and Biomedicine (BIBM), IEEE, 2021, pp. 274–279.
[16] J. Lin, K. Dzeparoska, S. Q. Zhang, A. Leon-Garcia, N. Papernot, On
the robustness of cooperative multi-agent reinforcement learning, in:
2020 IEEE Security and Privacy Workshops (SPW), IEEE, 2020, pp.
62–68.
[17] L. Zan, X. Zhu, Z.-L. Hu, Adversarial attacks on cooperative multi-
agent deep reinforcement learning: a dynamic group-based adversarial
example transferability method, Complex & Intelligent Systems (2023)
1–12.
[18] S. Huang, N. Papernot, I. Goodfellow, Y. Duan, P. Abbeel, Adversar-
ial attacks on neural network policies, arXiv preprint arXiv:1702.02284
(2017).
30
[19] H. Zhang, H. Chen, C. Xiao, B. Li, M. Liu, D. Boning, C.-J. Hsieh,
Robust deep reinforcement learning against adversarial perturbations on
state observations, Advances in Neural Information Processing Systems
33 (2020) 21024–21037.
[20] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, S. Russell, Adver-
sarial policies: Attacking deep reinforcement learning, arXiv preprint
arXiv:1905.10615 (2019).
[21] X. Wu, W. Guo, H. Wei, X. Xing, Adversarial policy training against
deep reinforcement learning, in: 30th USENIX Security Symposium
(USENIX Security 21), 2021, pp. 1883–1900.
[22] W. Guo, X. Wu, S. Huang, X. Xing, Adversarial policy learning in
two-player competitive games, in: International Conference on Machine
Learning, PMLR, 2021, pp. 3910–3919.
[23] J. Wu, X. Xu, Decentralised grid scheduling approach based on multi-
agent reinforcement learning and gossip mechanism, CAAI Transactions
on Intelligence Technology 3 (1) (2018) 8–17.
[24] S. Shalev-Shwartz, S. Shammah, A. Shashua, Safe, multi-agent,
reinforcement learning for autonomous driving, arXiv preprint
arXiv:1610.03295 (2016).
[25] S. Bhalla, S. Ganapathi Subramanian, M. Crowley, Deep multi agent
reinforcement learning for autonomous driving, in: Canadian Conference
on Artificial Intelligence, Springer, 2020, pp. 67–78.
[26] W. D. Crano, V. Seyranian, Majority and minority influence, Social and
Personality Psychology Compass 1 (1) (2007) 572–589.
[27] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfel-
low, R. Fergus, Intriguing properties of neural networks, arXiv preprint
arXiv:1312.6199 (2013).
[28] I. J. Goodfellow, J. Shlens, C. Szegedy, Explaining and harnessing ad-
versarial examples, arXiv preprint arXiv:1412.6572 (2014).
[29] N. Carlini, D. Wagner, Towards evaluating the robustness of neural
networks, in: 2017 ieee symposium on security and privacy (sp), IEEE,
2017, pp. 39–57.
31
[30] J. Kos, D. Song, Delving into adversarial attacks on deep policies, arXiv
preprint arXiv:1705.06452 (2017).
[31] Y.-C. Lin, Z.-W. Hong, Y.-H. Liao, M.-L. Shih, M.-Y. Liu, M. Sun, Tac-
tics of adversarial attack on deep reinforcement learning agents, arXiv
preprint arXiv:1703.06748 (2017).
[32] Y. Huang, Q. Zhu, Deceptive reinforcement learning under adversarial
manipulations on cost signals, in: International Conference on Decision
and Game Theory for Security, Springer, 2019, pp. 217–237.
[33] F. Wu, L. Li, C. Xu, H. Zhang, B. Kailkhura, K. Kenthapadi, D. Zhao,
B. Li, Copa: Certifying robust policies for offline reinforcement learning
against poisoning attacks, arXiv preprint arXiv:2203.08398 (2022).
[34] V. Behzadan, W. Hsu, Sequential triggers for watermarking of deep
reinforcement learning policies, arXiv preprint arXiv:1906.01126 (2019).
[35] P. Kiourti, K. Wardega, S. Jha, W. Li, Trojdrl: Trojan attacks on deep
reinforcement learning agents, arXiv preprint arXiv:1903.06638 (2019).
[36] L. Wang, Z. Javed, X. Wu, W. Guo, X. Xing, D. Song, Backdoorl: Back-
door attack against competitive reinforcement learning, arXiv preprint
arXiv:2105.00579 (2021).
[37] H. Zhang, H. Chen, D. Boning, C.-J. Hsieh, Robust reinforcement learn-
ing on state observations with learned optimal adversary, arXiv preprint
arXiv:2101.08452 (2021).
[38] Y. Sun, R. Zheng, Y. Liang, F. Huang, Who is the strongest enemy?
towards optimal and efficient evasion attacks in deep rl, arXiv preprint
arXiv:2106.05087 (2021).
[39] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, A. Swami,
The limitations of deep learning in adversarial settings, in: 2016 IEEE
European symposium on security and privacy (EuroS&P), IEEE, 2016,
pp. 372–387.
[40] J. Tu, T. Wang, J. Wang, S. Manivasagam, M. Ren, R. Urtasun, Ad-
versarial attacks on multi-agent communication, in: Proceedings of the
IEEE/CVF International Conference on Computer Vision, 2021, pp.
7768–7777.
32
[41] W. Xue, W. Qiu, B. An, Z. Rabinovich, S. Obraztsova, C. K. Yeo, Mis-
spoke or mis-lead: Achieving robustness in multi-agent communicative
reinforcement learning, arXiv preprint arXiv:2108.03803 (2021).
[42] J. Blumenkamp, A. Prorok, The emergence of adversarial communica-
tion in multi-agent reinforcement learning, in: Conference on Robot
Learning, PMLR, 2021, pp. 1394–1414.
[43] R. Mitchell, J. Blumenkamp, A. Prorok, Gaussian process based message
filtering for robust multi-agent cooperation in the presence of adversarial
communication, arXiv preprint arXiv:2012.00508 (2020).
[44] M. Figura, K. C. Kosaraju, V. Gupta, Adversarial attacks in consensus-
based multi-agent reinforcement learning, in: 2021 American Control
Conference (ACC), IEEE, 2021, pp. 3050–3055.
[45] E. A. Hansen, D. S. Bernstein, S. Zilberstein, Dynamic programming
for partially observable stochastic games, in: AAAI, Vol. 4, 2004, pp.
709–715.
[46] F. A. Oliehoek, C. Amato, A concise introduction to decentralized
POMDPs, Springer, 2016.
[47] S. H¨ ofer, K. Bekris, A. Handa, J. C. Gamboa, M. Mozifian, F. Golemo,
C. Atkeson, D. Fox, K. Goldberg, J. Leonard, et al., Sim2real in robotics
and automation: Applications and challenges, IEEE transactions on
automation science and engineering 18 (2) (2021) 398–400.
[48] S. M. Giray, Anatomy of unmanned aerial vehicle hijacking with signal
spoofing, in: 2013 6th International Conference on Recent Advances in
Space Technologies (RAST), IEEE, 2013, pp. 795–800.
[49] B. Ly, R. Ly, Cybersecurity in unmanned aerial vehicles (uavs), Journal
of Cyber Security Technology 5 (2) (2021) 120–137.
[50] N. M. Rodday, R. d. O. Schmidt, A. Pras, Exploring security vulnerabil-
ities of unmanned aerial vehicles, in: NOMS 2016-2016 IEEE/IFIP Net-
work Operations and Management Symposium, IEEE, 2016, pp. 993–
994.
33
[51] N. Jaques, A. Lazaridou, E. Hughes, C. Gulcehre, P. Ortega, D. Strouse,
J. Z. Leibo, N. De Freitas, Social influence as intrinsic motivation for
multi-agent deep reinforcement learning, in: International conference on
machine learning, PMLR, 2019, pp. 3040–3049.
[52] T. Wang, J. Wang, Y. Wu, C. Zhang, Influence-based multi-agent ex-
ploration, arXiv preprint arXiv:1910.05512 (2019).
[53] A. Fayad, M. Ibrahim, Influence-based reinforcement learning for
intrinsically-motivated agents, arXiv preprint arXiv:2108.12581 (2021).
[54] P. Li, H. Tang, T. Yang, X. Hao, T. Sang, Y. Zheng, J. Hao, M. E.
Taylor, Z. Wang, Pmic: Improving multi-agent reinforcement learn-
ing with progressive mutual information collaboration, arXiv preprint
arXiv:2203.08553 (2022).
[55] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal
policy optimization algorithms, arXiv preprint arXiv:1707.06347 (2017).
[56] J. Schulman, P. Moritz, S. Levine, M. Jordan, P. Abbeel, High-
dimensional continuous control using generalized advantage estimation,
arXiv preprint arXiv:1506.02438 (2015).
[57] F. Mondada, M. Bonani, X. Raemy, J. Pugh, C. Cianci, A. Klaptocz,
S. Magnenat, J.-C. Zufferey, D. Floreano, A. Martinoli, The e-puck,
a robot designed for education in engineering, in: Proceedings of the
9th conference on autonomous robot systems and competitions, Vol. 1,
IPCB: Instituto Polit´ ecnico de Castelo Branco, 2009, pp. 59–65.
34"
2305.12872v3.pdf,"Published as a conference paper at ICLR 2024
BYZANTINE ROBUST COOPERATIVE MULTI -AGENT RE-
INFORCEMENT LEARNING AS A BAYESIAN GAME
Simin Li1, Jun Guo1, Jingqiao Xiu1, Ruixiao Xu1, Xin Yu1, Jiakai Wang2,
Aishan Liu1, 4, Yaodong Yang3∗, Xianglong Liu1, 2, 4∗
1SKLSDE Lab, Beihang University, China2Zhongguancun Laboratory, China
3Institute of Artificial Intelligence, Peking University & BigAI, China
4Institute of data space, Hefei Comprehensive National Science Center, China
ABSTRACT
In this study, we explore the robustness of cooperative multi-agent reinforcement
learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary,
worst-case actions due to malfunction or adversarial attack. To address the uncer-
tainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust
Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries
as nature-dictated types, represented by a separate transition. This allows agents to
learn policies grounded on their posterior beliefs about the type of other agents,
fostering collaboration with identified allies and minimizing vulnerability to adver-
sarial manipulation. We define the optimal solution to the BARDec-POMDP as an
ex interim robust Markov perfect Bayesian equilibrium, which we proof to exist
and the corresponding policy weakly dominates previous approaches as time goes
to infinity. To realize this equilibrium, we put forward a two-timescale actor-critic
algorithm with almost sure convergence under specific conditions. Experiments
on matrix game, Level-based Foraging and StarCraft II indicate that, our method
successfully acquires intricate micromanagement skills and adaptively aligns with
allies under worst-case perturbations, showing resilience against non-oblivious
adversaries, random allies, observation-based attacks, and transfer-based attacks.
1 I NTRODUCTION
Cooperative multi-agent reinforcement learning (c-MARL) (Rashid et al., 2018; Yu et al., 2021; Kuba
et al., 2021) has shown remarkable efficacy in managing groups of agents with aligned interests in
complex tasks (Vinyals et al., 2019; Berner et al., 2019). Nevertheless, real-world applications often
deviates from the presumption of full cooperation. In robot swarm control (Hüttenrauch et al., 2019),
individual robots may act unpredictably due to hardware or software malfunctions, or even display
worst-case adversarial actions if compromised by a non-oblivious adversary (Gleave et al., 2019; Lin
et al., 2020; Dinh et al., 2023; Liu et al., 2019; 2020a;b; 2023; Wang et al., 2021). Such uncertainty
of allies undermine the cooperative premise of c-MARL, rendering the learned policy non-robust.
In single-agent reinforcement learning (RL), robustness under uncertainty is addressed through a
maximin optimization between an uncertainty set and a robust agent within the framework of robust
Markov Decision Processes (MDPs) (Nilim & El Ghaoui, 2005; Iyengar, 2005; Wiesemann et al.,
2013; Pinto et al., 2017; Tessler et al., 2019; Zhang et al., 2020a). However, ensuring robustness in
c-MARL when dealing with uncertain allies presents a greater challenge. This is largely due to the
potential for Byzantine failure (Yin et al., 2018; Xue et al., 2021), situations where defenders are left
in the dark regarding which ally may be compromised and what their resulting actions might be.
To address Byzantine failures, we employ a Bayesian game approach, which treats Byzantine
adversaries as types assigned by nature, with each agent operating unaware of others’ type. We
formalize robust c-MARL as a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP),
where existing robust MARL researches (Li et al., 2019; Sun et al., 2022; Phan et al., 2020; 2021)
can be reinterpreted as pursuing an ex ante equilibrium (Shoham & Leyton-Brown, 2008), viewing
∗Corresponding Authors. E-mails: yaodong.yang@pku.edu.cn, xlliu@buaa.edu.cn.
1arXiv:2305.12872v3  [cs.GT]  17 Jun 2024
Published as a conference paper at ICLR 2024
all other agents as potential adversaries. However, these methods might not yield optimal outcomes
as they can mask the trade-offs between the equilibria that cooperative and robustness-focused agents
aim for. Moreover, this approach can result in overly conservative strategies (Li et al., 2019; Sun
et al., 2022), given the low likelihood of adversaries taking control of all agents.
Instead, we seek an ex interim mixed-strategy robust Markov perfect Bayesian equilibrium, which
weakly dominates the policy of ex ante equilibrium in previous robust MARL studies as time goes to
infinity. Agents in our ex interim equilibrium makes decisions based on its inferred posterior belief
over other agents, enhancing cooperation with allies and defense against adversaries concurrently. To
realize this equilibrium, we derive a robust Harsanyi-Bellman equation for value function update and
introduce a two-timescale actor-critic algorithm, with almost sure convergence under certain assump-
tions. Experiments in matrix game, Level-based Foraging and StarCraft II shows our defense exhibits
intricate micromanagement skills and adaptively aligns with allies under worst-case perturbations.
Consequently, our defense outperforms existing baselines under non-oblivious adversaries, random
allies, observation-based attacks and transfer-based attacks by large margins.
Contribution. Our contributions are two-fold: first, we theoretically formulate Byzantine adver-
saries in c-MARL as a BARDec-POMDP, and concurrently pursues robustness and cooperation by
targeting an ex interim equilibrium. Secondly, to achieve this equilibrium, we devise an actor-critic
algorithm that ensures almost sure convergence under certain conditions. Empirically, our method
exhibits greater resilience against a broad spectrum of adversaries on three c-MARL environments.
Related Work. Our research belongs to the field of robust RL, theoretically framed as robust
MDPs (Nilim & El Ghaoui, 2005; Iyengar, 2005; Tamar et al., 2013; Wiesemann et al., 2013). This
framework trains a defender to counteract a worst-case adversary amid uncertainty, which can stem
from environment transitions (Pinto et al., 2017; Mankowitz et al., 2019), actions (Tessler et al.,
2019), states (Zhang et al., 2020a; 2021) and rewards (Wang et al., 2020). In robust MARL, action
uncertainty has been a central focus. M3DDPG (Li et al., 2019) enhances robustness in MARL
through agents taking jointly worst-case actions under a small perturbation budget. Evaluation
was done via one agent consistently introducing worst-case perturbations. This is later known as
adversarial policy (Gleave et al., 2019) or non-oblivious adversary (Dinh et al., 2023), a practical
and detrimental form of attack. Follow-up works either enhanced M3DDPG (Sun et al., 2022) or
defended against uncertain adversaries by presupposing each agent as potentially adversarial (Nisioti
et al., 2021; Phan et al., 2020; 2021), which our BARDec-POMDP formulation interprets as seeking
a conservative ex ante equilibrium. Another approach by Kalogiannis et al. (2022) studies a special
case that the adversary is known. Besides action perturbation, studies have also explored robust
MARL under uncertainties in reward (Zhang et al., 2020c), environmental dynamics (Zhao et al.,
2020), and observations (Han et al., 2022; He et al., 2023; Zhou & Liu, 2023).
Bayesian games and their MARL applications represents another relevant field. With roots in
Harsanyi’s pioneering work (Harsanyi, 1967), Bayesian games have been used to analyze games with
incomplete information by transforming them into complete information games featuring chance
moves made by nature. Within MARL, Bayesian games have been utilized to coordinate varying
agent types, a concept known as ad hoc coordination (Albrecht & Ramamoorthy, 2015; Albrecht
et al., 2016; Stone et al., 2010; Barrett et al., 2017). This problem was theoretically framed as a
stochastic Bayesian game and solved using the Harsanyi-Bellman ad hoc coordination algorithm
(Albrecht & Ramamoorthy, 2015). Subsequent research has concentrated on agents with varying
types (Ravula, 2019), open ad hoc teamwork (Rahman et al., 2022), and human coordination (Tylkin
et al., 2021; Strouse et al., 2021). Our work differs from these works by assuming a worst-case,
non-oblivious adversary with conflicting goals, whereas in ad hoc coordination, agents have common
goals and non-conflicting secondary objectives (Grosz & Kraus, 1999; Mirsky et al., 2022).
2 P ROBLEM FORMULATION
2.1 C OOPERATIVE MARL AND ITS FORMULATION
The problem of c-MARL can be formulated as a Decentralized Partially Observable Markov Decision
Process (Dec-POMDP) (Oliehoek & Amato, 2016), defined as a tuple:
G:=⟨N,S,O, O,A,P, R, γ⟩,
2
Published as a conference paper at ICLR 2024
where N={1, ..., N}is the set of Nagents, Sis the global state space, O=×i∈NOiis the
observation space, with Othe observation emission function. A=×i∈NAiis the joint action space,
P:S × A → ∆(S)is the state transition probability, mapping from current state and joint actions
to a probability distribution over the state space. R:S × A → Ris the shared reward function for
cooperative agents and γ∈[0,1)is the discount factor.
At time tand global state st∈ S, each agent iadds current observation oi
tto its history and gets
Hi
t= [oi
0, ai
0, ...oi
t]. Then, each agent iselects its action ai
t∈ Aiusing its policy πi(·|Hi
t), which
maps current history to its action space. The global state then transitions to st+1according to
transition probability P(st+1|st,at), with at={a1
t, ..., aN
t}the joint actions. Each agent receives
ashared reward rt=R(st,at). The goal of all agents is to learn a joint policy π=Q
i∈Nπithat
maximize the long-term return J(π) =E[P∞
t=0γtrt|s0,at∼π(·|Ht)].
2.2 B AYESIAN ADVERSARIAL ROBUST DEC-POMDP
In numerous real-world situations, some allies may experience Byzantine failure (Yin et al., 2018;
Xue et al., 2021) and thus, not perform cooperative actions as expected. This includes random actions
due to hardware/software error and adversarial actions if being controlled by an adversary, which
violates the fully cooperative assumption in Dec-POMDP. We propose Bayesian Adversarial Robust
Dec-POMDP (BARDec-POMDP) to cope with uncertainties in agent actions, defined as follows:
ˆG:=⟨N,S,Θ,O, O,A,Pα,P, R, γ⟩,
where N,S,O,O,A,γrepresent the number of agents, global state space, observation space,
observation emission function, joint action space and discount factor, following Dec-POMDP.
Environment
Agents Action Perturbation
Figure 1: Framework of c-MARL with Byzantine
adversaries. The action taken by agents with θi=
1are replaced by the adversary policy ˆπi.As depicted in Fig. 1, BARDec-POMDP views
the Byzantine adversary as an uncertain tran-
sition characterized by type θand adversarial
policy ˆπ. At the start of each episode, a typeθ
is selected from the type space Θ =×i∈NΘi,
withθi={0,1}.θi= 0 indicates the agent
is cooperative and Θi= 1signifies adversaries.
At time t, if agent iis assigned θi= 1, the ac-
tionai
ttaken by cooperative agent iwith policy
πi(·|Hi
t)is replaced by action ˆai
tsampled from
an adversary with policy ˆπi(·|Hi
t, θ). The attack
process is characterized by action perturbation
probability Pα(at|at,ˆπ, θ) =Q
i∈Nˆπi(·|Hi
t, θ)·θi+δ(ai
t−ai
t)·(1−θi)that maps joint actions at
to joint actions with perturbations at, where δ(·)is the Dirac delta function. Note that the actions of
cooperative agents and adversaries are taken simultaneously. Finally, the state transition probability
P(st+1|st,at)takes the perturbed actions and output the state of next timestep. The shared reward
rt=R(st,at)for (cooperative) agents is defined over perturbed actions. Given type θ, the value
function can be defined as Vθ(s) =E[P∞
t=0γtrt|s0=s,at∼π(·|Ht),ˆat∼ˆπ(·|Ht, θ)]. We leave
the goal of adversary and robust agents to Section. 2.3 and 2.4 below.
Our BARDec-POMDP formulation is flexible and draws close connection with current literature.
Regarding type space, Dec-POMDP can be viewed as a BARDec-POMDP with Θ =0N. Robust
MARL approaches, such as M3DDPG (Li et al., 2019) and ROMAX (Sun et al., 2022) assumes agents
are entirely adversarial, which refers to type space Θ =1N. Subsequent robust MARL researchs
(Phan et al., 2020; 2021; Nisioti et al., 2021), though not explicitly defining the type space, can be
integrated in our BARDec-POMDP. Our formulation also draws inspiration from state-adversarial
MDP (Zhang et al., 2020a) which considers adversary as a part of decision making process, and
probabilistic action robust MDP (Tessler et al., 2019) by their formulation of action perturbation.
2.3 T HREAT MODEL
The robustness towards action perturbations in both single and multi-agent RL has gained prominence
since the pioneering works of (Tessler et al., 2019; Li et al., 2019). Action uncertainties, formulated
as a type of adversarial attack known as adversarial policy (Gleave et al., 2019; Wu et al., 2021; Guo
et al., 2021), or non-oblivious adversary (Dinh et al., 2023), represent a pragmatic and destructive
3
Published as a conference paper at ICLR 2024
form of attack that is challenging to counter. In line with these works, we propose a practical threat
model with certain assumptions on attackers and defenders.
Assumption 2.1 (Attacker’s capability and limitations) .At the onset of an episode, the attacker can
select θandarbitrarily manipulate the actions of agents with type θi= 1. Within an episode, the
type cannot be altered and we assume there is only one attacker.
Our main focus in this paper is to model action uncertainties of unknown agents in c-MARL as types
shaped by nature and to advance corresponding solution concept. In line with (Li et al., 2019), we
assume one agent is vulnerable to action perturbations in each episode. In real-world, the type space
can be more complicated, potentially involving adversaries controlling multiple agents (Nisioti et al.,
2021), perturbing actions intermittently (Lin et al., 2017), or featuring a non-binary type space (Xie
et al., 2022). These variations can be viewed as straightforward extensions of our work.
Proposition 2.1 (Existence of worst-case adversary) .For any robust c-MARL with fixed agent policy,
a worst-case ( i.e., most harmful) adversary exists.
Proof sketch. Since defender policies are fixed, they can be considered part of the environment
transitions for attackers. Thus, the attackers solve an RL problem. See full proof in Appendix. A.1.
Assumption 2.2 (Defender’s capability and limitations) .The defender can use all available informa-
tion during training stage, including global state and information of other agents. However, during
testing, the defender relies solely on partial observations and is agnostic of the type of other agents.
The defender’s policy is fixed during an attack and must resist the worst-case adversary ˆπ∗.
2.4 S OLUTION CONCEPT
(a)ex ante
Expectation
Attack
(b)ex interim
Adaptive 
Adjustment
Attack
Cooperative
Equilibrium
Robust
Equilibrium
Adversarial
Attack
Uncertainty
Set
Figure 2: ex ante RMPBE obscures differences between each
type by taking expectation, while our ex interim RMPBE
adapts to current type.In this section, we first introduce
the non-optimal solution concept seek
by existing robust c-MARL methods,
then pose our optimal solution con-
cept for BARDec-POMDP. Specifi-
cally, existing robust c-MARL meth-
ods blindly maximize reward without
considering the type of others. This
is akin to an ex ante equilibrium in
Bayesian game (Shoham & Leyton-
Brown, 2008), where agents make de-
cisions based on the prior belief about
the types of other agents.
Definition 2.1 (ex ante robustness) .A joint cooperative policy πEA
∗= (πEA,i
∗)i∈Nand adversarial
policy ˆπEA
∗= (ˆπEA,i
∗)i∈Nforms an ex ante robust Markov perfect Bayesian equilibrium (RMPBE),
if for all p(θ), s∈ S, H∈(O × A )∗,
(πEA
∗(·|H),ˆπEA
∗(·|H, θ))∈arg max
π(·|H)Ep(θ)h
min
ˆπ(·|H,θ)Vθ(s)i
, (1)
withVθ(s) =P
a∈APα(a|a,ˆπ, θ)Q
i∈Nπi(ai|Hi)(R(s,a) +γP
s′∈SP(s′|s,a)Vθ(s′)).
By maximizing the expected value under prior p(θ)over types, as illustrated in Fig. 2, the policy
might struggle to balance the different equilibrium corresponding to cooperation and robustness
against different agents as (Byzantine) adversaries. In contrast, we propose a more refined ex interim
robustness concept, such that under current history, each agent make optimal decisions to maximize
their expected value from their posterior belief of current type, with following assumptions:
Assumption 2.3. Assume belief and policy are updated under following conditions: (1) Consistency .
At each timestep t, each agent updates its belief bi
t=p(θ|Hi
t)of the current type by Bayes’ rule. (2)
Sequential rationality . Each policy maximizes the expected value function under belief bi.
Here we use πi(·|Hi, bi)to denote the explicit dependence on belief bi. The two conditions are
common in dynamic games with incomplete information (Shoham & Leyton-Brown, 2008).
4
Published as a conference paper at ICLR 2024
Definition 2.2 (ex interim robustness) .Under Assumption 2.3 and let b= (bi)i∈N, a joint cooperative
policy πEI
∗= (πEI,i
∗)i∈Nand adversarial policy ˆπEI
∗= (ˆπEI,i
∗)i∈Nforms an ex interim robust
Markov perfect Bayesian equilibrium, if ∀s∈ S, H∈(O × A )∗,
(πEI
∗(·|H, b),ˆπEI
∗(·|H, θ))∈arg max
π(·|H,b)Ep(θ|H)h
min
ˆπ(·|H,θ)Vθ(s))i
. (2)
Note that both ex ante andex interim RMPBE requires optimality of each individual agent i. To
reduce notation complexity, we use joint policy and belief instead.
Proposition 2.2 (Existence of RMPBE) .Assume a BARDec-POMDP of finite agents, finite set of
state, observation and action space, agents use stationary policies, the type space Θis a compact set,
then ex ante andex interim mixed strategy robust Markov perfect Bayesian equilibrium exists.
Proof sketch. The proof is done by first showing the policy and its corresponding value function,
with uncertainties of the current type and presence of the adversaries, satisfy the requirements of
Kakutani’s fixed point theorem. Next, by Kakutani’s fixed point theorem, there always exists an
optimal fixed point corresponding to a mixed strategy RMPBE. See full proof in Appendix. A.2.
Unlike c-MARL with optimal deterministic policies (Oliehoek et al., 2008), in robust c-MARL,
a pure-strategy equilibrium is not guaranteed to exist. This is intuitive since zero-sum games do
not always have a pure-strategy equilibrium. The finding suggests the optimal policies for robust
c-MARL are stochastic. Next, we show the relation between ex ante andex interim equilibrium.
Proposition 2.3. Under Assumption 2.3, given finite type space and the prior of each type is not zero,
ast→ ∞ ,πEI
∗(·|Ht, bt)weakly dominates πEA
∗(·|Ht)under the worst-case adversary.
Proof sketch. Ast→ ∞ , by the consistency of Bayes’ rule, the belief converges to the true type.
Thus, ex interim policies that maximize the value function under the true type are guaranteed to
weakly dominate ( i.e., have value higher or equal to) ex ante policies. See full proof in Appendix A.3.
3 A LGORITHM
In this section, we explain how to find the optimal solution in Definition 2.2. We start by defining the
robust Harsanyi-Bellman equation, an update rule of value function which converges to a fixed point.
Then, we develop a two-timescale actor-critic algorithm that considers belief of others’ type, which
ensures almost sure convergence under assumptions in stochastic approximation theory.
3.1 R OBUST HARSANYI -BELLMAN EQUATION
We first define the Bellman-type update of value functions for our ex interim equilibrium. Considering
the Q function before and after action perturbation, we can formulate the Q function via cumulative
reward, with posterior belief bi=p(θ|Hi)over type:
Qi(s,a, bi) =Ep(θ|Hi)""
E""∞X
t=0γtrts0=s,a0=a,at∼π(·|Ht, bt),ˆat∼ˆπ(·|Ht, θ)##
,(3)
Qi(s,a, bi) =Ep(θ|Hi)""
E""∞X
t=0γtrts0=s,a0=a,at∼π(·|Ht, bt),ˆat∼ˆπ(·|Ht, θ)##
,(4)
The two Q functions are defined with different purpose. Qi(s,a, bi)is the expected Q function before
action perturbation, suitable for decision making of defenders, such as fictitious self-play (Heinrich &
Silver, 2016) and soft actor-critic (Haarnoja et al., 2018). In this way, the action perturbation can be
viewed as part of the environment transition, resulting in P(s′|s,a,ˆπ, θ) =P(s′|s,a)·Pα(a|a,ˆπ, θ).
On the other hand, Qi(s,a, bi)is the Q function with actions taken by the adversary, suitable for
policy gradients (Sutton & Barto, 2018) and decision making of the adversary. For Qi(s,a, Hi),
the action perturbation is integrated into the policy of robust agents, resulting in a mixed policy
π(ˆa|H, b, θ ) =Pα(a|a,ˆπ, θ)·π(a|H, b) = (1 −θ)·π(a|H, b) +θ·ˆπ(ˆa|H, θ). The relationship
between the two Q functions is as follows:
Qi(s,a, bi) =X
θ∈Θp(θ|Hi)X
ˆa∈APα(at|at,ˆπ, θ)Qi(s,a, bi). (5)
5
Published as a conference paper at ICLR 2024
Next, we formulate the Bellman-type equation for the two Q functions, which we call the robust
Harsanyi-Bellman equation. This update differs from the conventional approach by considering the
posterior belief over other agents and the worst-case adversary.
Definition 3.1. We define the robust Harsanyi-Bellman equation for Q function as:
Qi
∗(s,a, bi) = max
π(·|H,b)min
ˆπ(·|H,θ)X
θ∈Θp(θ|Hi)X
s′∈SX
ˆa∈AP(s′|s,a,ˆπ, θ)h
R(s,a) +γX
a′∈Aπ(a′|H′, b′)Qi
∗(s′,a′, b′i)i
,(6)
Qi
∗(s,a, bi) = max
π(·|H,b)min
ˆπ(·|H,θ)R(s,a) +γX
s′∈SP(s′|s,a)X
θ∈Θp(θ|H′i)
X
a′∈Aπ(a′|H′, b′, θ)Qi
∗(s′,a′, b′i).(7)
This Q function can be estimated via Temporal Difference (TD) loss.
Proposition 3.1 (Convergence) .Assume the belief is updated via Bayes’ rule, the space of state,
actions and belief are finite, updating value functions by robust Harsanyi-Bellman equation converge
to the optimal value Qi
∗(s,a, bi)andQi
∗(s,a, bi).
Proof sketch. The proof is done by combining the standard convergence proof of Q function with
adversaries and Bayesian belief update, and showing our Q function forms a contraction mapping.
Next, applying Banach’s fixed point theorem completes the proof. See full proof in Appendix. A.4.
3.2 A CTOR -CRITIC ALGORITHM FOR ex interim ROBUST EQUILIBRIUM
Armed with the robust Harsanyi-Bellman equation, we propose an actor-critic algorithm to achieve
our proposed ex interim equilibrium with almost sure convergence under certain assumptions. We
first derive the policy gradient theorem for robust c-MARL. Assume policies of robust agents and
adversaries are parameterized by πϕ:= (πi
ϕi)i∈Nandˆπˆϕ:= (ˆπi
ˆϕi)i∈Nrespectively, forming a mixed
policy πϕ,ˆϕ= (1−θ)·πϕ+θ·ˆπˆϕ. Define the performance for a robust agent iin the episodic case
asJi(ϕ) =Es∼ρπ(s)[Vi(s, bi)]and (zero-sum) adversary as Ji(ˆϕ) =Es∼ρπ(s)[−Vi(s, bi)], where
ρπ(s)is the state visitation frequency. The policy gradients for πϕandˆπˆϕare then defined as:
Theorem 3.1. The policy gradient theorem for robust agent and adversary iis:
∇ϕiJi(ϕi) =Es∼ρπ(s),a∼πϕ,ˆϕ(a|H,b,θ )
(1−θi)∇logπϕi(ai|Hi, bi)Qi(s,a, bi)
, (8)
∇ˆϕiJi(ˆϕi) =Es∼ρπ(s),a∼πϕ,ˆϕ(a|H,b,θ )h
−θi∇log ˆπˆϕi(ˆai|Hi, θ)Qi(s,a, bi)i
. (9)
The policy gradient naturally depends on Qi(s,a, bi), which is related to policy gradient and decision
of adversaries. Specifically, θcuts off the gradient of robust agents πϕiwithθi= 1and cut off the
gradient of adversary ˆπˆϕiwithθi= 0. The detailed derivation is deferred to Appendix. A.5.
Convergence. In zero-sum Markov games, achieving convergence through policy gradients remains
challenging, with current theoretical results being dependent on specific conditions (Daskalakis
et al., 2020; Zhang et al., 2020b; Kalogiannis et al., 2022). In this paper, we prove that, under
certain assumptions in stochastic approximation theory (Borkar, 1997; Borkar & Meyn, 2000; Borkar,
2009), applying a two-timescale update for both adversaries and defenders, as stated in Theorem
3.1, guarantees almost sure convergence ( i.e., converge with probability 1) to an ex interim RMPBE.
A detailed proof is provided in Appendix A.6 as an application of stochastic approximation. With
this two-timescale update, the adversary’s policy is updated on a faster timescale and is essentially
equilibrated, while the defender’s policy is updated on a slower timescale and remains quasi-static.
Despite these advances, establishing finite sample, global convergence guarantees without restrictive
assumptions remains an open problem, warranting future research.
Finally, we suggest update rules for critic and belief networks. Assuming the critic Qi
ψ(s,a, bi)is
parameterized by ψ. As for belief bi, the calculation of bivia Bayes’ rule requires assess to the policy
6
Published as a conference paper at ICLR 2024
of other agents, which is not possible during deployment. To remedy this, we approximate the belief
bi= max ξpξ(θ|Hi)using a neural network parameterized by ξ. The objectives to update critic and
belief network are:
min
ψ 
R(s,a)−γQi
ψ(s′,a′, b′i) +Qi
ψ(s,a, bi)2, (10)
min
ξ−θlog 
pξ(θ|Hi)
−(1−θ) log 
1−pξ(θ|Hi)
, (11)
with critic trained via TD loss and belief network trained by binary cross entropy loss. See the
pseudo-code for our algorithm in Appendix. B.
4 E XPERIMENTS
(a) Toy
 (b) LBF
 (c) SMAC
Figure 3: Environments used in our experiments. The toy iterative matrix game is proposed by Han
et al. (2022). We use map 12x12-4p-3f-c for LBF and map 4m vs 3m for SMAC.
Environments. To validate the efficacy of our proposed approach, we conducted experiments on
three benchmark cooperative MARL environments, as shown in Fig. 7. Environments include a toy
iterative matrix game proposed by (Han et al., 2022), rewarding XNOR or XOR actions at different
state, 12x12-4p-3f-c of Level-Based Foraging (LBF) (Papoudakis et al., 2020) and 4m vs 3m of
the StarCraft Multi-agent Challenge (SMAC) (Samvelyan et al., 2019), which reduce to 3min the
presence of an adversary.
Baselines. Our comparative study includes MADDPG (Lowe et al., 2017), M3DDPG (Li et al.,
2019), MAPPO (Yu et al., 2021), RMAAC He et al. (2023), ex ante robust MAPPO (EAR-MAPPO),
a MAPPO variant of (Phan et al., 2020; Zhang et al., 2020c) that considers ex ante equilibrium,
which is also an ablation of our approach without belief. We dubbed our method ex interim robust
MAPPO (EIR-MAPPO) and add an ideal case which grants access to true type, labelled “True
Type”. It’s worth noting that we couldn’t directly adapt M3DDPG onto the MAPPO framework
due to its reliance on Q(s,a), a component not compatible with MAPPO’s use of V(s)as a critic.
More experiment details are given in Appendix. C. For fair comparison, all methods use the same
codebase, network structure and hyperparameters. Code and demo videos available at https:
//github.com/DIG-Beihang/EIR-MAPPO .
Evaluation protocol. In each environment with Ncooperative agents, the robust policy was trained
using five random seeds. Attack results were compiled by launching attacks on each of the Nagents
using the same five seeds, yielding a total of 5×Nattacks per environment. We plot all results with
95% confidence interval.
Evaluated attacks. We consider four types of threats. (1) Non-oblivious adversaries (Gleave et al.,
2019): we fix the trained policy and deployed a zero-sum, worst-case adversarial policy to attack each
agents separately. (2) Random agents: an agent perform random actions from a uniform distribution,
possibly via hardware or software failure (labelled as “random”). (3) Noisy observations: we add
ℓ∞bounded adversarial noise (Lin et al., 2020) with perturbation budgets ϵ∈ {0.2,0.5,1.0}to the
observation of an agent (denoted as “ ϵ=”). (4) Transferred adversaries: attackers initially train a
policy on a surrogate algorithm, then directly transfer the attack to target other algorithms.
4.1 R OBUSTNESS OVER NON-OBLIVIOUS ATTACKS
We first evaluate our performance under the most arduous non-oblivious attack, where an adversary
can manipulate any agent in cooperative tasks and execute arbitrary learned worst-case policy.
The cooperation and attack performance on three environments are given in Fig. 4. Across all
environments, our EIR-MAPPO consistently delivers robust performance close to the maximum
7
Published as a conference paper at ICLR 2024
MADDPG MAPPO True Type EIR-MAPPO M3DDPG EAR-MAPPO
0.00.51.01.52.02.53.03.54.0
Timesteps 1e75060708090100 RewardCooperative, Toy
05101520 RewardCooperative, SMAC 4m vs 3m Cooperative, LBF 12x12-4p-3f-c
0.00.51.01.52.02.53.03.54.0
Timesteps 1e7
0 1 2 3 4 5
1e6Adversary, SMAC 4m vs 3m
Timesteps05101520 Reward
01020304050
0 1 2 3 4 5
1e6 TimestepsAdversary, ToyRewardAdversarial, LBF 12x12-4p-3f-c
0.00.20.40.60.81.0
0 1 2 3 4 5
1e6 TimestepsReward0.00.51.01.52.02.53.03.54.0
Timesteps 1e70.00.20.40.60.81.0RewardRMAAC
Figure 4: Cooperative and robust performance on three c-MARL environments. EIR-MAPPO
achieves higher robust performance against non-oblivious adversaries and have cooperative perfor-
mance on par with baselines. Reported on 5 seeds for cooperation and 5×Nattacks.
reward achievable in each environment under attack (50 for Toy, 1.0 for LBF, 20 for SMAC),
outperforming baselines by large margins and displays robustness equalling the ideal True Type
defense. Concurrently, EIR-MAPPO maintains cooperative performance on par with MAPPO.
Swayed
Allies
(a) MADDPG/M3DDPG
Unfocused
Fire (b) MAPPO/RMAAC
Unfocused
Fire
Bad Kiting (c) EAR-MAPPO
Focus
FireKiting (d) EIR-MAPPO
Figure 5: Agent behaviors under attack. Red square indicates the adversary agent. Existing methods
are either swayed, having unfocused fire or perform bad kiting. In contrast, our EIR-MAPPO learns
kiting and focused fire simultaneously, under the presence of a worst-case adversary.
As illustrated in Fig. 5, a detailed examination of each method’s behaviour under attack enriches
our comprehension of robustness, with adversaries marked by a red square. First, MADDPG and
M3DDPG can be easily swayed by adversaries. In Fig. 5a, a downward-moving adversary easily
diverts two victims from the battle, resulting in a single victim facing three opponents. As for MAPPO
and RMAAC, agents fail to master useful micromanagement strategies, such as kiting or focused fire
during combat1. Consequently, the agent do not exihibit any cooperation skills under attack and are
not skillful enough to win the game. As for EAR-MAPPO, agents occasionally demonstrate kiting
but fall short in executing focused fire. They spread fire over two enemies instead of concentrating
fire on one. Furthermore, even successful kiting can be compromised. In Fig. 5c, the adversary
advances, causing two half-health agents to mistakenly believe that an ally is coming to its aid, and
thus retreats to kite the enemy. This, however, leaves another low-health ally vulnerable to enemy fire
and immediately being eliminated. Finally, we find that both EIR-MAPPO and True Type demonstrate
focused fire and kiting, proving resistant to adversarial agents. Illustrated in Fig. 5d, two low-health
agents retreat to avoid being eliminated, while an agent with high health advances to shield its allies,
showcasing classic kiting behaviour. Moreover, allies coordinate to eliminate enemies, leaving one
enemy nearly unscathed and another at half health.
4.2 R OBUSTNESS OVER VARIOUS TYPE OF ATTACKS
Apart from the worst-case oblivious adversary, c-MARL can encounter various uncertainties in real
world, ranging from allies taking random actions, having uncertainties in observations, or a transferred
1Micromanagements are granular control strategies for agents to win in StarCraft II (Samvelyan et al., 2019).
Kiting enables agents to evade enemy fire by stepping outside the enemy’s attack range, thereby compelling the
enemy to give chase rather than attack; focused fire requires taking enemies down one after another.
8
Published as a conference paper at ICLR 2024
adversary trained on alternate algorithms. We examine the robust performance of all methods under
such diverse uncertainties in Fig. 6. The rows signify uncertainties while the columns represent the
evaluated methods. Diagonal entries ( i.e., blocks with the same uncertainty and evaluated method)
denote non-oblivious attacks. For each uncertainty (column), the method of highest reward was
marked by a red square. Furthermore, since the True Type represents an ideal scenario, if it secures
the highest reward, we mark the method that gained the second-highest reward as well.
UncertaintiesLBF 12x12-4p-3f-c SMAC 4m vs 3m
=0.2
=0.5
=1.0
MADDPG M3DDPGMAPPO RMAAC
EAR-MAPPO EIR-MAPPOTrue Type
MADDPG M3DDPGMAPPO RMAAC
EAR-MAPPO EIR-MAPPOTrue TypeMADDPG
M3DDPG
MAPPO
RMAAC
EAR-MAPPO
EIR-MAPPO
True Type
Random
AverageToy
4.8 6.02 10.63 10.7 16.01 19.38 18.52
8.01 3.13 11.25 11.93 17.17 18.78 19.58
6.48 4.91 7.93 10.72 16.44 19.49 18.94
6.0 5.42 9.67 9.43 16.35 19.54 18.24
6.89 5.73 9.6 10.32 11.76 18.89 18.38
7.22 4.73 10.81 10.71 15.69 16.82 18.52
7.115.74 9.39 10.98 14.86 19.23 16.9
8.84 6.63 13.29 12.92 17.04 19.53 19.4
16.18 12.03 11.96 19.48 14.87 19.63 18.91
14.39 11.33 9.28 18.78 12.23 17.87 17.71
11.62 10.86 9.19 19.22 12.23 17.4 17.52
8.87 6.96 10.27 13.2 14.97 18.78 18.42
5 10 15
MADDPG M3DDPGMAPPO RMAAC
EAR-MAPPO EIR-MAPPOTrue Type
0.01 0.12 0.79 0.51 0.84 0.88 0.84
0.06 0.03 0.78 0.65 0.88 0.89 0.84
0.09 0.13 0.53 0.54 0.88 0.91 0.85
0.09 0.12 0.68 0.31 0.88 0.91 0.81
0.1 0.12 0.79 0.61 0.64 0.87 0.82
0.04 0.110.73 0.61 0.81 0.82 0.82
0.07 0.13 0.77 0.6 0.83 0.87 0.82
0.14 0.17 0.87 0.74 0.88 0.89 0.88
0.81 0.84 0.97 0.95 0.98 0.99 0.99
0.64 0.55 0.75 0.83 0.9 0.96 0.94
0.36 0.26 0.74 0.78 0.82 0.93 0.88
0.22 0.23 0.77 0.65 0.85 0.9 0.86
0.2 0.4 0.6 0.8
0.0 8.38 72.76 69.41 64.33 55.06 51.54
5.58 0.01 72.13 69.32 69.45 50.15 52.68
76.51 74.85 4.75 39.67 62.29 85.91 49.18
49.91 49.91 42.0 7.69 75.46 76.64 50.85
45.77 45.6 43.82 64.32 17.16 65.18 46.22
56.21 55.3 58.89 57.1159.31 44.14 52.27
52.23 52.01 37.99 51.66 47.07 55.93 42.23
50.03 50.27 49.18 49.72 49.86 50.4 50.3
99.7 87.18 59.44 87.04 73.94 80.01 46.66
50.42 33.88 14.82 61.1 28.71 47.03 43.61
50.04 34.22 5.88 61.59 21.58 44.18 43.6
48.76 44.69 41.97 56.24 51.74 59.51 48.1
0 20 40 60 80
Figure 6: Evaluating robustness over diverse attacks, row indicates uncertainties and column indicates
evaluated methods. EIR-MAPPO achieves improved robustness on almost all uncertainties in LBF
and SMAC, while having higher robustness on average in Toy. Results averaged over 5×Nattacks.
We first emphasize the effectiveness of our EIR-MAPPO approach. Considering the average reward
gained under a broad range of uncertainties, our EIR-MAPPO surpasses baselines by 5.81% on Toy,
5.88% on LBF, and 25.45% on SMAC. Notably, EIR-MAPPO and True Type yields highest reward
in almost all LBF and SMAC environments. In toy environment, owing to the existence of two
pure-strategy equilibria, algorithms and attacks deploying deterministic strategies can occasionally
yield higher rewards by chance. However, given its superior worst-case robustness, our EIR-MAPPO
consistently delivers commendable results under all uncertainties.
Our second observation focuses on the relationship between action and observation perturbations. As
an algorithm designed to counteract observation uncertainties, RMAAC is robust against observation
perturbations, but fails to counter unseen action perturbations. In contrast, our EIR-MAPPO maintains
its robustness against observation perturbations, even though it has not encountered the attack before.
This resilience is due to the fact that observational attacks ultimately affect agents’ choices of actions ,
which reduces observation uncertainty to a form of action uncertainty.
5 C ONCLUSIONS
In this paper, we study robust c-MARL against Byzantine threat, where agents in this system to
fail, or being compromised by an adversary. We frame the problem as a Dec-POMDP and define
its solution as an ex interim RMPBE, such that the equilibrated policy weakly dominates ex ante
solutions in prior research, when time goes to infinity. To actualize this equilibrium, we introduce
Harsanyi-Bellman equation for value function updates, and an actor-critic algorithm with almost sure
convergence under specific conditions. Experimental results show that under worst-case adversarial
perturbation, our method can learn intricate and adaptive cooperation skills, and can withstand
non-oblivious, random, observation-based, and transferred adversaries. As future work, we plan to
apply our method to c-MARL applications, including robot swarm control (Hüttenrauch et al., 2019),
traffic light management (Chu et al., 2019), and power grid maintenance (Xi et al., 2018).
9
Published as a conference paper at ICLR 2024
6 A CKNOWLEDGEMENTS
This work was supported by the National Key Research and Development Plan of China
(2022ZD0116405) and National Natural Science Foundation of China (62306025, 62022009,
62132010 and 62206009).
REFERENCES
Stefano V Albrecht and Subramanian Ramamoorthy. A game-theoretic model and best-response
learning method for ad hoc coordination in multiagent systems. arXiv preprint arXiv:1506.01170 ,
2015.
Stefano V Albrecht, Jacob W Crandall, and Subramanian Ramamoorthy. Belief and truth in hypothe-
sised behaviours. Artificial Intelligence , 235:63–94, 2016.
Karl J Astrom et al. Optimal control of markov processes with incomplete state information. Journal
of mathematical analysis and applications , 10(1):174–205, 1965.
Samuel Barrett, Avi Rosenfeld, Sarit Kraus, and Peter Stone. Making friends on the fly: Cooperating
with new teammates. Artificial Intelligence , 242:132–171, 2017.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ ebiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale
deep reinforcement learning. arXiv preprint arXiv:1912.06680 , 2019.
Daniel S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of
decentralized control of markov decision processes. Mathematics of operations research , 27(4):
819–840, 2002.
Vivek S Borkar. Stochastic approximation with two time scales. Systems & Control Letters , 29(5):
291–294, 1997.
Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint , volume 48. Springer,
2009.
Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation and
reinforcement learning. SIAM Journal on Control and Optimization , 38(2):447–469, 2000.
Tianshu Chu, Jie Wang, Lara Codecà, and Zhaojian Li. Multi-agent deep reinforcement learning for
large-scale traffic signal control. IEEE Transactions on Intelligent Transportation Systems , 21(3):
1086–1095, 2019.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 , 2014.
Constantinos Daskalakis, Dylan J Foster, and Noah Golowich. Independent policy gradient methods
for competitive reinforcement learning. Advances in neural information processing systems , 33:
5527–5540, 2020.
Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge? arXiv preprint arXiv:2011.09533 , 2020.
Persi Diaconis and David Freedman. On the consistency of bayes estimates. The Annals of Statistics ,
pp. 1–26, 1986.
Le Cong Dinh, David Henry Mguni, Long Tran-Thanh, Jun Wang, and Yaodong Yang. Online markov
decision processes with non-oblivious strategic adversary. Autonomous Agents and Multi-Agent
Systems , 37(1):15, 2023.
Arlington M Fink. Equilibrium in a stochastic n-person game. Journal of science of the hiroshima
university, series ai (mathematics) , 28(1):89–93, 1964.
10
Published as a conference paper at ICLR 2024
Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adversarial
policies: Attacking deep reinforcement learning. arXiv preprint arXiv:1905.10615 , 2019.
Barbara J Grosz and Sarit Kraus. The evolution of sharedplans. Foundations of rational agency , pp.
227–262, 1999.
Wenbo Guo, Xian Wu, Sui Huang, and Xinyu Xing. Adversarial policy learning in two-player
competitive games. In International Conference on Machine Learning , pp. 3910–3919. PMLR,
2021.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International conference
on machine learning , pp. 1861–1870. PMLR, 2018.
Songyang Han, Sanbao Su, Sihong He, Shuo Han, Haizhao Yang, and Fei Miao. What is the solution
for state adversarial multi-agent reinforcement learning? arXiv preprint arXiv:2212.02705 , 2022.
John C Harsanyi. Games with incomplete information played by “bayesian” players, i–iii part i. the
basic model. Management science , 14(3):159–182, 1967.
Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, and Fei Miao. Robust multi-agent
reinforcement learning with state uncertainty. Transactions on Machine Learning Research , 2023.
Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-
information games. arXiv preprint arXiv:1603.01121 , 2016.
Maximilian Hüttenrauch, Sosic Adrian, Gerhard Neumann, et al. Deep reinforcement learning for
swarm systems. Journal of Machine Learning Research , 20(54):1–31, 2019.
Garud N Iyengar. Robust dynamic programming. Mathematics of Operations Research , 30(2):
257–280, 2005.
Fivos Kalogiannis, Ioannis Anagnostides, Ioannis Panageas, Emmanouil-Vasileios Vlatakis-
Gkaragkounis, Vaggos Chatziafratis, and Stelios Stavroulakis. Efficiently computing nash equilibria
in adversarial team markov games. arXiv preprint arXiv:2208.02204 , 2022.
Erim Karde¸ s, Fernando Ordóñez, and Randolph W Hall. Discounted robust stochastic games and an
application to queueing control. Operations research , 59(2):365–382, 2011.
Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong
Yang. Trust region policy optimisation in multi-agent reinforcement learning. arXiv preprint
arXiv:2109.11251 , 2021.
Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. Robust multi-agent
reinforcement learning via minimax deep deterministic policy gradient. In Proceedings of the
AAAI conference on artificial intelligence , volume 33, pp. 4213–4220, 2019.
Jieyu Lin, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, and Nicolas Papernot. On the
robustness of cooperative multi-agent reinforcement learning. In 2020 IEEE Security and Privacy
Workshops (SPW) , pp. 62–68. IEEE, 2020.
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tactics
of adversarial attack on deep reinforcement learning agents. arXiv preprint arXiv:1703.06748 ,
2017.
Aishan Liu, Xianglong Liu, Jiaxin Fan, Yuqing Ma, Anlan Zhang, Huiyuan Xie, and Dacheng Tao.
Perceptual-sensitive gan for generating adversarial patches. In AAAI , 2019.
Aishan Liu, Tairan Huang, Xianglong Liu, Yitao Xu, Yuqing Ma, Xinyun Chen, Stephen J Maybank,
and Dacheng Tao. Spatiotemporal attacks for embodied agents. In ECCV , 2020a.
Aishan Liu, Jiakai Wang, Xianglong Liu, Bowen Cao, Chongzhi Zhang, and Hang Yu. Bias-based
universal adversarial patch attack for automatic check-out. In ECCV , 2020b.
11
Published as a conference paper at ICLR 2024
Aishan Liu, Jun Guo, Jiakai Wang, Siyuan Liang, Renshuai Tao, Wenbo Zhou, Cong Liu, Xianglong
Liu, and Dacheng Tao. X-adv: Physical adversarial object attacks against x-ray prohibited item
detection. In USENIX Security Symposium , 2023.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. Advances in neural information
processing systems , 30, 2017.
Daniel J Mankowitz, Nir Levine, Rae Jeong, Yuanyuan Shi, Jackie Kay, Abbas Abdolmaleki, Jost To-
bias Springenberg, Timothy Mann, Todd Hester, and Martin Riedmiller. Robust reinforcement
learning for continuous control with model misspecification. arXiv preprint arXiv:1906.07516 ,
2019.
Reuth Mirsky, Ignacio Carlucho, Arrasy Rahman, Elliot Fosong, William Macke, Mohan Sridharan,
Peter Stone, and Stefano V Albrecht. A survey of ad hoc teamwork research. In Multi-Agent
Systems: 19th European Conference, EUMAS 2022, Düsseldorf, Germany, September 14–16, 2022,
Proceedings , pp. 275–293. Springer, 2022.
Arnab Nilim and Laurent El Ghaoui. Robust control of markov decision processes with uncertain
transition matrices. Operations Research , 53(5):780–798, 2005.
Eleni Nisioti, Daan Bloembergen, and Michael Kaisers. Robust multi-agent q-learning in cooperative
games with adversaries. In Proceedings of the AAAI Conference on Artificial Intelligence , 2021.
Frans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs . Springer,
2016.
Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value functions
for decentralized pomdps. Journal of Artificial Intelligence Research , 32:289–353, 2008.
Georgios Papoudakis, Filippos Christianos, Lukas Schäfer, and Stefano V Albrecht. Benchmark-
ing multi-agent deep reinforcement learning algorithms in cooperative tasks. arXiv preprint
arXiv:2006.07869 , 2020.
Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr,
Wendelin Böhmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy
gradients. Advances in Neural Information Processing Systems , 34:12208–12221, 2021.
Thomy Phan, Thomas Gabor, Andreas Sedlmeier, Fabian Ritz, Bernhard Kempter, Cornel Klein,
Horst Sauer, Reiner Schmid, Jan Wieghardt, Marc Zeller, et al. Learning and testing resilience
in cooperative multi-agent systems. In Proceedings of the 19th International Conference on
Autonomous Agents and MultiAgent Systems , pp. 1055–1063, 2020.
Thomy Phan, Lenz Belzner, Thomas Gabor, Andreas Sedlmeier, Fabian Ritz, and Claudia Linnhoff-
Popien. Resilient multi-agent reinforcement learning with adversarial value decomposition. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pp. 11308–11316, 2021.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial rein-
forcement learning. In International Conference on Machine Learning , pp. 2817–2826. PMLR,
2017.
Arrasy Rahman, Ignacio Carlucho, Niklas Höpner, and Stefano V Albrecht. A general learn-
ing framework for open ad hoc teamwork using graph-based policy learning. arXiv preprint
arXiv:2210.05448 , 2022.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shi-
mon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement
learning. In International conference on machine learning , pp. 4295–4304. PMLR, 2018.
Manish Chandra Reddy Ravula. Ad-hoc teamwork with behavior-switching agents . PhD thesis, 2019.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043 , 2019.
12
Published as a conference paper at ICLR 2024
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438 ,
2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.
Yoav Shoham and Kevin Leyton-Brown. Multiagent systems: Algorithmic, game-theoretic, and
logical foundations . Cambridge University Press, 2008.
Peter Stone, Gal Kaminka, Sarit Kraus, and Jeffrey Rosenschein. Ad hoc autonomous agent teams:
Collaboration without pre-coordination. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 24, pp. 1504–1509, 2010.
DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett. Collaborating
with humans without human data. Advances in Neural Information Processing Systems , 34:
14502–14515, 2021.
Chuangchuang Sun, Dong-Ki Kim, and Jonathan P How. Romax: Certifiably robust deep multiagent
reinforcement learning via convex relaxation. In 2022 International Conference on Robotics and
Automation (ICRA) , pp. 5503–5510. IEEE, 2022.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
Aviv Tamar, Huan Xu, and Shie Mannor. Scaling up robust mdps by reinforcement learning. arXiv
preprint arXiv:1306.6189 , 2013.
Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applica-
tions in continuous control. In International Conference on Machine Learning , pp. 6215–6224.
PMLR, 2019.
Paul Tylkin, Goran Radanovic, and David C Parkes. Learning robust helpful behaviors in two-
player cooperative atari environments. In Proceedings of the 20th International Conference on
Autonomous Agents and MultiAgent Systems , pp. 1686–1688, 2021.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
starcraft ii using multi-agent reinforcement learning. Nature , 575(7782):350–354, 2019.
Jiakai Wang, Aishan Liu, Zixin Yin, Shunchang Liu, Shiyu Tang, and Xianglong Liu. Dual attention
suppression attack: Generate adversarial camouflage in physical world. In CVPR , 2021.
Jingkang Wang, Yang Liu, and Bo Li. Reinforcement learning with perturbed rewards. In Proceedings
of the AAAI conference on artificial intelligence , volume 34, pp. 6202–6209, 2020.
Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong Yang. Multi-
agent reinforcement learning is a sequence modeling problem. Advances in Neural Information
Processing Systems , 35:16509–16521, 2022.
Wolfram Wiesemann, Daniel Kuhn, and Berç Rustem. Robust markov decision processes. Mathe-
matics of Operations Research , 38(1):153–183, 2013.
Xian Wu, Wenbo Guo, Hua Wei, and Xinyu Xing. Adversarial policy training against deep rein-
forcement learning. In 30th USENIX Security Symposium (USENIX Security 21) , pp. 1883–1900,
2021.
Lei Xi, Jianfeng Chen, Yuehua Huang, Yanchun Xu, Lang Liu, Yimin Zhou, and Yudan Li. Smart
generation control based on multi-agent reinforcement learning with the idea of the time tunnel.
Energy , 153:977–987, 2018.
Annie Xie, Shagun Sodhani, Chelsea Finn, Joelle Pineau, and Amy Zhang. Robust policy learning
over multiple uncertainty sets. In International Conference on Machine Learning , pp. 24414–24429.
PMLR, 2022.
13
Published as a conference paper at ICLR 2024
Wanqi Xue, Wei Qiu, Bo An, Zinovi Rabinovich, Svetlana Obraztsova, and Chai Kiat Yeo. Mis-spoke
or mis-lead: Achieving robustness in multi-agent communicative reinforcement learning. arXiv
preprint arXiv:2108.03803 , 2021.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In International Conference on Machine Learning , pp.
5650–5659. PMLR, 2018.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955 , 2021.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui
Hsieh. Robust deep reinforcement learning against adversarial perturbations on state observations.
Advances in Neural Information Processing Systems , 33:21024–21037, 2020a.
Huan Zhang, Hongge Chen, Duane Boning, and Cho-Jui Hsieh. Robust reinforcement learning on
state observations with learned optimal adversary. arXiv preprint arXiv:2101.08452 , 2021.
Kaiqing Zhang, Bin Hu, and Tamer Basar. On the stability and convergence of robust adversarial
reinforcement learning: A case study on linear quadratic systems. Advances in Neural Information
Processing Systems , 33:22056–22068, 2020b.
Kaiqing Zhang, Tao Sun, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer Basar. Robust multi-
agent reinforcement learning with model uncertainty. Advances in neural information processing
systems , 33:10571–10583, 2020c.
Eric Zhao, Alexander R Trott, Caiming Xiong, and Stephan Zheng. Ermas: Learning policies robust
to reality gaps in multi-agent simulations. 2020.
Yifan Zhong, Jakub Grudzien Kuba, Siyi Hu, Jiaming Ji, and Yaodong Yang. Heterogeneous-agent
reinforcement learning. arXiv preprint arXiv:2304.09870 , 2023.
Ziyuan Zhou and Guanjun Liu. Robustness testing for multi-agent reinforcement learning: State
perturbations on critical agents. arXiv preprint arXiv:2306.06136 , 2023.
14
Published as a conference paper at ICLR 2024
APPENDIX FOR ""BYZANTINE ROBUST COOPERATIVE
MULTI -AGENT REINFORCEMENT LEARNING AS A
BAYESIAN GAME ""
A P ROOFS AND DERIVATIONS
A.1 P ROOF OF PROPOSITION 2.1
In this section, we proof a worst-case adversary always exist for any BARDec-POMDP with fixed
defenders. Considering different numbers of adversaries ( i.e.,P
i∈N), with fixed defenders, the
attackers can be seen as solving a POMDP or Dec-POMDP, where optimal solution exists.
First, ifP
i∈Nθi= 0, no adversary exists and the environment is reduced to a fully cooperative
setting. Since adversary do not even exist, the problem becomes vacuous, and any ˆπachieves the
same (optimal) result.
Second, ifP
i∈Nθi= 1, the problem is reduced to a POMDP Gα:=⟨S,O, O,A,ˆP,Pα, Rα, γ⟩for
adversary, where Sthe global state space, Othe observation space for adversary, Ois the observation
emission function, Ais the action space of adversary, Rα:S × A → Ris the reward function for
adversary. Actions taken by adversary iare sampled via adversarial policy ˆai
t∼ˆπi(·|Hi
t, θ). The
environment transition for adversary is defined as ˆP(st+1|st,ˆai
t) =P(st+1|st,at)·Pα(at|π, θt,ˆai
t),
where atis the mixed joint actions after perturbation in BARDec-POMDP. Here, P(st+1|st,at)is the
environment transition in BARDec-POMDP, which combines with the transition Pα(at|π, θ,ˆai
t) =Q
i∈Nδ(ai
t−ˆai
t)·θi+πi(·|Hi
t)·(1−θi)represents the decision of π, which is fixed and treated as
a part of environment transition. Following the proof of Astrom et al. (1965), there always exists an
optimal policy for POMDP. Thus, there exists an optimal adversary.
Third, withP
i∈Nθi>1, the problem is reduced to a Dec-POMDP ⟨N,S,Oα, O,A,ˆP, Rα, γ⟩,
withSthe global state space, Oα=×i∈{θi=1}Oiis the observation space of adversaries, Ois the
observation emission function, A=×i∈{θi=1}Aiaction space of adversaries, Rα:S × Aα→
Ris the reward function for adversary. Actions taken by adversary are sampled via adversarial
policy ˆat∼ˆπ(·|Ht, θ). The environment transition for adversary is defined as ˆP(st+1|st,ˆat) =
P(st+1|st,at)· Pα(at|π, θ,ˆat). Here, P(st+1|st,ˆat)is the environment transition for BARDec-
POMDP, with the effect of the policy of πmerged in transition Pα(at|π, θ,ˆat) =Q
i∈Nδ(ai
t−ˆai
t)·
θi+πi(·|Hi
t)·(1−θi), which is fixed and treated as a part of transition function. Following the
proof of Bernstein et al. (2002), there always exists an optimal policy for Dec-POMDP. Thus, there
exists an optimal adversary.
A.2 P ROOF OF PROPOSITION 2.2
We proof this by showing our BARDec-POMDP satisfies the requirement of Kakutani’s fixed point
theorem. Our proof is an extension of Karde¸ s et al. (2011), which shows equilibrium exists in robust
stochastic games. In the following proof, we will first state existing results, then shows the existence
of our ex interim RMPBE as a generalization of the proof of (Karde¸ s et al., 2011) considering
Bayesian update and transforming action uncertainty as a kind of environment uncertainty. The
existence proof of ex ante RMPBE rise as a corollary.
A.2.1 P RELIMINARIES
We first introduce existing results.
Theorem A.1 (Kakutani’s fixed point theorem) .IfXis a closed, bounded, and convex set in a
Euclidean space, and ϕis an upper semicontinuous correspondence mapping Xinto the family of
closed, convex subsets of X, then∃x∈X,s.t. x∈ϕ(x).
Next, we brief the definition of robust stochastic game and its equilibrium concept before we introduce
the main result of Karde¸ s et al. (2011).
15
Published as a conference paper at ICLR 2024
In a stochastic game G=<N,S,A,P, R, γ > with finite state and actions, where Nis the indicator
of agents, Sis the state space, A=×i∈NAiis the joint action space, P:S × A → ∆(S)is
the state transition probability, R:S × A × N → RNis the reward function that maps state and
actions to reward of each agent, γis the discount factor. Let s∈S,ai∈ Ai, withathe joint actions,
π:S → ∆(A)is the policy for agent iwithπi(ai|s),ri=R(s, a, i )is the reward for agent i.
Arobust stochastic game is a stochastic game with perturbed reward ˆrt∈ Rαand environment
transition ˆP(s′|s,a)∈ Pα, withRαandPαare the bounded perturbation range.
Define the expected value function Vi
π,ˆr,ˆP(s)under perturbed reward and environment transition
recursively through the following Bellman equation:
Vi
π,ˆr,ˆP(s) =X
a∈Aπi(ai|s)Y
j̸=iπj(aj|s)[ˆri+X
s′∈SˆP(s′|s,a)Vi
π(s′)],
the worst-case value function Vi
π,ˆr∗,ˆP∗(s)is defined as Vi
π,ˆr,ˆP(s) = minˆr,ˆPVi
π,ˆr∗,ˆP∗(s). The equi-
librium policy, if exists, is thus ∀i∈ N, s∈ S, a∈ A ,πi
∗(·|s)∈arg maxπi(·|s)Vi
π(s), with
corresponding value as Vi
{πi∗,π−i},ˆr∗,ˆP∗(s) = max πiVi
π,ˆr∗,ˆP∗(s).
Next, let π(·|s) =Q
i∈Nπi(·|s)be the joint policy, and the set of value functions for each agent
asVπ,ˆr,ˆP(s) = ( V1
π,ˆr,ˆP(s), V2
π,ˆr,ˆP(s), ..., VN
π,ˆr,ˆP(s)). Define mappings βandϕbyβ(π−i) =
{Vi|Vi= max πiminˆr,ˆPVi
{πi,π−i},ˆr,ˆP(s)}andϕ(π−i) ={πi|β(π−i) =Vi
{πi∗,π−i},ˆr∗,ˆP∗(s)}to be
the set of value functions and optimal policies.
Theorem A.2 (Karde¸ s et al. (2011)) .Assume uncertainties in transition probabilities and payoffs
belongs to compact sets, all agents use stationary strategies, then the set of functions {Vπ,ˆr,ˆP(s),ˆr∈
Rα,ˆP ∈ Pα}is equicontinuous and Vπ,ˆr∗,ˆP∗(s)continuous on all its variables. ϕ(π−i)is an upper
semicontinuous correspondence mapping ∆(A)in a convex and closed subsets of ∆(A), which
satisfies the assumptions of Kakutani’s fixed point theorem.
We are now ready to construct our proof.
A.2.2 E XISTENCE OF EX INTERIM RMPBE
We first state the existence of ex interim RMPBE. The proof is conducted by transforming BARDec-
POMDP to a Dec-POMDP with environmental uncertainties, and applying Bayesian update to value
functions. Note that we include some results of Karde¸ s et al. (2011) for a self-contained proof.
First, by definition of our BARDec-POMDP, it can be transformed to a robust Dec-POMDP
with adversary in environmental dynamics. This is done by combining environment transition
P(st+1|st,a)with action perturbation probability Pα(ˆa|a,ˆπ, θ), resulting in P(st+1|st,a,ˆπ, θ) =
P(st+1|st,ˆa)· Pα(ˆa|a,ˆπ, θ). Thus, the robust Dec-POMDP can be seen as a particular case of
stochastic game with shared reward, partial observations and uncertainties in perturbations. Thus,
some results of Karde¸ s et al. (2011) can be taken for our proof. Before that, let us redefine some
notations for clarity of our proof.
Let us redefine the expected value function Vθ(s)using the following Bellman equation:
Vi
π,ˆπ,θ(s) =X
a∈Aπi(ai|Hi, bi)Y
j̸=iπj(aj|Hj, bj)[ri+X
s′∈SX
ˆa∈AP(s′|s,a,ˆπ, θ)Vπ,ˆπ,θ(s′)].
Note that Vi
π,ˆπ,θ(s)assumes the type θis known, thus there is no uncertainty over θand the
function is not updated by robust Harsanyi-Bellman equation. The expected value function with
belief, Vi
π,ˆπ,bi(s), is defined by Vi
π,ˆπ,bi(s) =Ep(θ|H)[Vi
π,ˆπ,θ(s)]. The worst-case value func-
tionVi
π,ˆπ∗,θ(s)andVi
π,ˆπ∗,bi(s)are defined as Vi
π,ˆπ∗,θ(s) = min ˆπVi
π,ˆπ,θ(s)andVi
π,ˆπ∗,bi(s) =
minˆπVi
π,ˆπ,bi(s). With optimal (equilibrium) policy defined as π∗, the corresponding value is
Vi
{πi∗,π−i},ˆπ∗,bi(s) = max πiVi
π,ˆπ∗,bi(s). The mappings βandϕare similarly defined β(π−i) =
{Vi|Vi= max πiminˆπVi
{πi,π−i},ˆπ,bi(s)}andϕ(π−i) ={πi|β(π−i) =Vi
{πi∗,π−i},ˆπ∗,bi(s)}to be
the set of value functions and optimal policies. By Proposition. 3.1, the optimal value at each s∈ S
is unique, which we denote it as vi
s.
16
Published as a conference paper at ICLR 2024
Lemma A.1 (Equicontinuity of {Vi
π,ˆπ,bi(s),ˆπ∈∆(A)}).For every ϵ > 0,∃δ > 0, for any
(π1, Vπ1,ˆπ,bi
1(s′
1), bi
2)and(π2, Vπ2,ˆπ,bi
2(s′
2), bi
2),|π1−π2|+|Vπ1,ˆπ,bi(s′
1)−Vπ2,ˆπ,bi(s′
2)|+|b1−b2|<
δ, then, ∀ˆπ∈∆(A),|Vπ1,ˆπ,bi(s1)−Vπ2,ˆπ,bi(s2)|< ϵ.
Proof . By Lemma 1 of Karde¸ s et al. (2011), the equicontinuity of {Vi
π,ˆπ,θ(s),ˆπ∈∆(A)}holds, if
we consider ˆπas uncertainties in P(s′|s,a,ˆπ, θ). All we need now is to extend the proof considering
belief bi.
|Vπ1,ˆπ,bi(s1)−Vπ2,ˆπ,bi(s2)|
=|Ep(θ|Hi
1)[Vπ1,ˆπ,θ(s1)]−Ep(θ|Hi
2)[Vπ2,ˆπ,θ(s2)]|
=X
θ∈Θ[p(θ|Hi
1)·Vπ1,ˆπ,θ(s1)−p(θ|Hi
2)·Vπ2,ˆπ,θ(s2)]
=X
θ∈Θ[p(θ|Hi
1)·(Vπ1,ˆπ,θ(s1)−Vπ2,ˆπ,θ(s2)) + ( p(θ|Hi
1)−p(θ|Hi
2))·Vπ2,ˆπ,θ(s2)
≤X
θ∈Θ
|p(θ|Hi
1)·(Vπ1,ˆπ,θ(s1)−Vπ2,ˆπ,θ(s2))|+|(p(θ|Hi
1)−p(θ|Hi
2))·Vπ2,ˆπ,θ(s2)|
≤X
θ∈Θ[|p(θ|Hi
1)| · |Vπ1,ˆπ,θ(s1)−Vπ2,ˆπ,θ(s2)|+|(p(θ|Hi
1)−p(θ|Hi
2)| · |Vπ2,ˆπ,θ(s2)|]
Since p(θ|Hi
1)is a probability function, we have p(θ|Hi
1)≤1. Since reward is finite,
and|Vπi,ˆπ,θ(si)|, i∈1,2is defined by discount factor γ,|Vπi,ˆπ,θ(si)|is also bounded, i.e.,
|Vπi,ˆπ,θ(si)| ≤K.
Now, let
|Vπ1,ˆπ,θ(s1)−Vπ2,ˆπ,θ(s2)|< δ1=min{ϵ,1}
2· |Θ|,
|(p(θ|Hi
1)−p(θ|Hi
2)|< δ2=min{ϵ,1}
2·K,
and let δ= min {δ1, δ2}, we have:
|Vπ1,ˆπ,bi(s1)−Vπ2,ˆπ,bi(s2)|
≤X
θ∈Θ[|p(θ|Hi
1)| · |Vπ1,ˆπ,θ(s1)−Vπ2,ˆπ,θ(s2)|+|(p(θ|Hi
1)−p(θ|Hi
2)| · |Vπ2,ˆπ,θ(s2)|]
=X
θ∈Θ[|p(θ|Hi
1)| ·δ1] +X
θ∈Θ[δ2· |Vπ2,ˆπ,θ(s2)|]
≤ϵ/2 +ϵ/2 =ϵ.
Thus, the set of functions {Vi
π,ˆπ,bi(s),ˆπ∈∆(A)}is equicontinuous.
Lemma A.2. ϕ(π−i)is a convex set.
Proof. The proof follows Theorem 4 of Karde¸ s et al. (2011). By Lemma 2 of Karde¸ s et al. (2011), the
pointwise minimum of an equicontinuous set of function is continuous, Vi
π,ˆπ∗,bi(s) = min ˆπVi
π,ˆπ,bi(s)
is continuous on all its variables. Besides, Vi
π,ˆπ∗,bi(s)is defined by a discounted factor and is bounded.
Thus, the maxima of Vi
π,ˆπ∗,bi(s)exists.
Second, in Proposition 3.1, we have proof that updating Q function by robust Harsanyi-Bellman
equation yields an optimal robust Q value. It rise as a simple corollary that the optimal
V value, Vi
{πi∗,π−i},ˆπ∗,bi(s)exists. By equality Vi
{πi∗,π−i},ˆπ∗,bi(s) = max πi(ai|Hi,bi)minˆπP
a∈Aπi(ai|Hi, bi)Q
j̸=iπj(aj|Hj, bj) [ri+P
s′∈SP
ˆa∈AP(s′|s,a,ˆπ, θ)Vi
{πi
∗,π−i},ˆπ∗,bi(s′),
ϕ(π−i)̸=∅.
17
Published as a conference paper at ICLR 2024
Third, by Lemma 3 of Karde¸ s et al. (2011), the value function considering worst-
case adversary Vi
π,ˆπ∗,bi(s) = min ˆπP
a∈Aπi(ai|Hi, bj)Q
j̸=iπj(aj|Hj, bj)[ri+P
s′∈SP
ˆa∈AP
θ∈Θp(θ|Hi)P(s′|s,a,ˆπ, θ)Vπ,ˆπ,bi(s′)]is concave in πiwith fixed π−iand
Vi
π,ˆπ∗,bi(s′)2.
Finally, we show the convexity of ϕ(π−i). Let πi
1,∗, πi
2,∗∈ϕ(π−i), with fixed π−i. By definition
ofVi
{πi∗,π−i},ˆπ∗,bi(s),∀π, s∈ S, b∈∆(Θ) , i∈ N ,Vi
{πi
1,∗,π−i},ˆπ∗,bi(s) =Vi
{πi
2,∗,π−i},ˆπ∗,bi(s)≥
Vi
π,ˆπ∗,bi(s). Thus, ∀λ∈[0,1], we also have λVi
{πi
1,∗,π−i},ˆπ∗,bi(s) + (1 −λ)Vi
{π2,∗,π−i},ˆπ∗,bi(s)≥
Vi
π,ˆπ∗,bi(s). By concavity of Vi
π,ˆπ∗,bi(s),Vi
{πi∗,π−i},ˆπ∗,bi(s) = λVi
{πi
1,∗,π−i},ˆπ∗,bi(s) + (1 −
λ)Vi
{πi
2,∗,π−i},ˆπ∗,bi(s)≤Vi
{λπi
1,∗+(1−λ)πi
2,∗,π−i},ˆπ∗,bi(s). Since by Proposition 3.1, Vi
π,ˆπ∗,bi(s)is
optimal and unique. Thus, λπi
1,∗+ (1−λ)πi
2,∗∈ϕ(π−i),ϕ(π−i)is a convex set.
Next, we first introduce several lemmas, then show ϕ(x)is an upper semicontinuous correspondence.
Lemma A.3. LetTbe the robust Harsanyi-Bellman operator defined in Appendix. A.6.
TVi
{πi∗,π−i},ˆπ∗,bi(s)is continuous in π−i. The set {TV{πi∗,π−i},ˆπ,bi(s)|Vi
{πi∗,π−i},ˆπ∗,bi(s)
is bounded }is equicontinuous.
Proof. By Lemma 4 of Karde¸ s et al. (2011), TVi
{πi∗π−i},ˆπ∗,θ(s)is continuous and the set
{TV{πi∗,π−i},ˆπ,bi(s)|Vi
{πi∗,π−i},ˆπ∗,θ(s)is bounded }is equicontinuous. Since TVi
{πi∗,π−i},ˆπ∗,bi(s) =
TEp(θ|H))[Vi
{πi∗,π−i},ˆπ∗,θ(s)], the expectation of a continuous function is still continuous, and the
expectation over a equicontinuous set is still equicontinuous. This completes the proof.
Lemma A.4. Define the optimal value as vi={vi
1, vi
2, ...vi
S}. Ifπi
n→πi,π−i
n→π−i,β(π−i
n)→
viandπi
n∈ϕ(π−i
n), then πi∈ϕ(π−i),i.e.,ϕ(π−i)is an upper semicontinuous correspondence.
Proof. The proof is by Lemma 5 of Fink (1964). We re-write it here using our notation
to make the proof self-contained. Specifically, ∀s∈ S, H∈(O × A )∗, b∈∆(Θ) . De-
fine function f(·)asf(Vπ,ˆπ∗,bi(s)) = min ˆπ(ˆa|H,θ)P
a∈Aπi(ai|Hi, bi)Q
j̸=iπj(aj|Hj, bj)[ri+P
s′∈SP
ˆa∈AP
θ∈Θp(θ|Hi)P(s′|s,a,ˆπ, bi)Vπ,ˆπ∗,bi(s′)]. Recall vi
sis the fixed point. Let πi
∗∈
ϕ(π−i),|f(vi
s)−vi
s| ≤ |f(vi
s)−f(β(π−i
n|s))|+|f(β(π−i
n|s))−vi
s|=|f(vi
s)−f(β(π−i
n|s))|+
|β(π−i
n|s)−vi
s| →0asn→ ∞ .
Now we need to show when π−i
n→π−iandβ(π−i
n)→vi,β(π−i|s) =vi
s. We have |vi
s− Tvi
s| ≤
|vi
s−β(π−i
n|s)|+|β(π−i
n|s)−Tβ(π−i
n|s)|+|Tβ(π−i
n|s)−Tvi
s|. By our assumption, β(π−i
n)→vi
andπi
n∈ϕ(π−i
n)asn→ ∞ ,|vi
s−β(π−i
n|s)| →0,|Tβ(π−i
n|s)− Tvi
s| →0. By Lemma A.3,
|β(π−i
n|s)− Tβ(π−i
n|s)| → 0. Thus, |vi
s− Tvi
s| → 0asn→ ∞ . As β(π−i) ={Vi|Vi=
max πiminˆπVi
{πi,π−i},ˆπ,bi(s) =vi
s},β(π−i|s) =vi
s.
As we have β(π−i|s) =vi
s, we have vi
s=Tvi
sand is a fixed point. Thus, vi
s=f(vi
s) =β(π−i|s) =
minπiVi
{πi,π−i},ˆπ∗,bi(s). As a consequence, πi∈ϕ(π−i),ϕ(π−i)is an upper semicontinuous
correspondence.
Now, we have proofed ϕis an upper semicontinuous correspondence (Lemma. A.4), mapping ∆(A)
into the family of convex subsets of ∆(A)(Lemma. A.2). Since ϕ(π−i)is an upper semicontinuous
correspondence, it is also a closed set for any π. Thus, the result satisfies the requirement of
Kakutani’s fixed point theorem, with equilibrium policy ϕ(π−i).
A.2.3 E XISTENCE OF EX INTERIM RMPBE
The existence of ex ante RMPBE follows the proof of ex interim RMPBE, but having a prior belief
p(θ)that is never updated. Since the expectation over p(θ)is a linear combination and p(θ)is
bounded, the addition of p(θ)do not violate the convergence and continuity of value functions. The
proof thus follows the result of ex interim RMPBE.
2Note that Lemma 3 of Karde¸ s et al. (2011) considers cost, which is the negative of reward . While in their
proof, the function considering cost is convex. Taking the negative of a convex function is thus concave.
18
Published as a conference paper at ICLR 2024
A.3 P ROOF OF PROPOSITION 3.2
For convenience of notations, Let us redefine the value function under worst-case adversary and type
θat time t, but additionally adding π,ˆπinto the notation of Vθ(s)for clarity, resulting in
Vπ,ˆπ∗
θ(s) = min
ˆπ(·|H,θ)X
a∈APα(a|a,ˆπ, θ)Y
i∈Nπi(ai|Hi)(R(s,a) +γX
s′∈SP(s′|s,a)Vπ,ˆπ∗
θ(s′)).
We can then redefine the value function for ex ante RMPBE as Vπ,ˆπ∗p(θ)(s) =Ep(θ)[Vπ,ˆπ∗
θ(s)]and
the value function for ex interim RMPBE as Vπ,ˆπ∗
b(s) =Ep(θ|H)[Vπ,ˆπ∗
θ(s)].
Next, for each θ∈Θ, with time t→ ∞ , we have bt=p(θ|Ht)→θby consistency of Bayes’ rule
Diaconis & Freedman (1986), if ∀θ∈Θ, p(θ)̸= 0and with finite type space Θ. The resulting value
function at t→ ∞ is thus:
VπEI
∗,ˆπEI
∗
bt→∞(s) =VπEI
∗,ˆπEI
∗
θ(s)≥Vπ,ˆπ∗
θ(s),
where ˆπ∗is always the optimal adversarial policy for current π. The rest follows. As for the expected
return of ex ante robustness for θ, we get:
VπEA
∗,ˆπEA
∗
p(θ)(s) =Ep(θ)h
VπEA
∗,ˆπEA
∗
θ(s))i
.
During evaluation, ∀p(θ), att→ ∞ , since current type θis known and do not change, the return
conditions on θ, instead of expectations on θ. As a consequence, the value function of ex ante and
ex interim equilibrium is VπEA
∗,ˆπEA
∗
θ(s)andVπEI
∗,ˆπEI
∗
θ(s), respectively. At t→ ∞ and∀θ∈Θ, we
havebt=p(θ|Ht)→θ, and following inequality holds:
VπEI
∗,ˆπEI
∗
θ(s)≥VπEA
∗,ˆπEA
∗
θ(s),
which use the fact VπEI
∗,ˆπEI
∗
θ(s)≥Vπ,ˆπ∗
θ(s). Considering to the gap between Vπ,ˆπ∗
θ(s)andVπ,ˆπ∗
p(θ)(s),
a sufficient condition for this equality to hold is when the type space Θcontains one type only. Note
that even at t→ ∞ , the relation between expected value function VπEA
∗,ˆπEA
∗,θ
p(θ)(s)ofex ante equilibrium
andVπEI
∗,ˆπEI
∗
θ(s)ofex interim equilibrium is still not known. This is because the ex ante equilibrium
can get high expected values by simply “believing” it in some prior p(θ)that yields high value.
However, since the belief is not correct, the resulting policy is non-optimal in any type at t→ ∞ .
A.4 P ROOF OF PROPOSITION 3.3
Overview. We first proof the contraction mapping of Q(s,a, bi)by combining standard proof of the
contraction mapping of Q function and Bayesian belief update. Afterwards, applying Banach’s fixed
point theorem completes the proof. The convergence of Q(s,a, bi)follows the same vein.
We first show the proof for Q-function for Qi
∗(s,a, bi). The proof incorporates our robust Harsanyi-
Bellman equation in the contraction mapping proof of robust MDP (Iyengar, 2005). Based on
Bellman equation in Definition 3.1, let ∆(Θ) be a probability over Θ, the optimal Q-function of a
contraction operator T, defined from a generic function Q:S × A × ∆(Θ) →Rcan be defined as:
(TQi)(s,a, bi) = max
π(·|o,b)min
ˆπ(·|o,θ)R(s,a) +γ""X
s′∈SP(s′|s,a)X
θ∈Θp(θ|Hi)
X
ˆa′∈Aπ(a′|H′, b′, θ)Qi
∗(s′,a′, b′i)#
.
Next, we show Tforms a contraction operator, such that for any two Q function Qi
1andQi
2, assuming
TQi
1(s,a, bi)≥ TQi
2(s,a, bi), the following holds:
19
Published as a conference paper at ICLR 2024
||TQi
1− TQi
2||∞≤γ||Qi
1−Qi
2||∞,
Specifically, for ϵ >0and with fixed a∈ A,s∈ S,b∈∆(Θ) ,H∈(O × A )∗forQ1andQ2,
min
ˆπ(·|H,θ)R(s,ˆa) +γ""X
s′∈SP(s′|s,a)X
θ∈Θp(θ|Hi)X
a′∈Aπ(ˆa′|H′, b′, θ)Qi
1(s′,a′, b′i)#
≥ TQi
1−ϵ.
Note that updating belief by Bayes’ rule is required for consistency of b′i. We require fixed Hi
as well since the calculation of belief and Q function depends on Hi. Now we can also choose a
conditional policy measure of the adversary ˆπs, such that:
Eˆπs""
R(s,ˆa) +γ""X
s′∈SP(s′|s,ˆa)X
θ∈Θp(θ|Hi)X
ˆa′∈Aπ(ˆa′|H′, b′, θ)Qi
2(s′,ˆa′, b′i)##
≤
min
ˆπ(·|o,θ)R(s,ˆa) +γ""X
s′∈SP(s′|s,ˆa)X
θ∈Θp(θ|Hi)X
ˆa′∈Aπ(ˆa′|H′, b′, θ)Qi
2(s′,ˆa′, b′i)#
+ϵ.
Then,
0≤ TQi
1− TQi
2
≤ 
min
ˆπ(·|H,θ)R(s,ˆa) +γ""X
s′∈SP(s′|s,a)X
θ∈Θp(θ|Hi)X
ˆa′∈Aπ(ˆa′|H′, b′, θ)Qi
1(s′,ˆa′, b′i)#
+ϵ!
−
 
min
ˆπ(·|H,θ)R(s,a) +γ""X
s′∈SP(s′|s,a)X
θ∈Θp(θ|Hi)X
a′∈Aπ(a′|H′, b′, θ)Qi
2(s′,a′, b′i)#!
≤ 
Eˆπs""
R(s,a) +γ""X
s′∈SP(s′|s,a)X
θ∈Θp(θ|Hi)X
a′∈Aπ(a′|H′, b′, θ)Qi
1(s′,a′, b′i)##
+ϵ!
−
 
Eˆπs""
R(s,a) +γ""X
s′∈SP(s′|s,a)X
θ∈Θp(θ|Hi)X
a′∈Aπ(a′|H′, b′, θ)Qi
2(s′,a′, b′i)##
−ϵ!
=γEˆπs
Qi
1−Qi
2
+ 2ϵ≤γEˆπsQi
1−Qi
2+ 2ϵ≤γEˆπs||Qi
1−Qi
2||∞+ 2ϵ.
Thus, we have:
||TQi
1− TQi
2||∞≤γ||Qi
1−Qi
2||∞+ 2ϵ,
and since by definition, ϵis arbitrary, then we have ||TQi
1− TQi
2||∞≤γ||Qi
1−Qi
2||∞.
Finally, since Tis a contraction operator on a Banach space, by Banach’s fixed point theorem,
updating Qi(s,a, bi)by Bellman operator Twill converge to the optimal value function Qi
∗(s,a, bi).
In the same way, the convergence of Qi(s,a, bi)is again done by robust Harsanyi-Bellman
equation, Qi
∗(s,a, bi) = max
π(·|H,b)min
ˆπ(·|H,θ)P
θ∈Θp(θ|Hi)P
s′∈SP
ˆa∈AP(s′|s,a,ˆπ, θ)[R(s,a) +
γP
a′∈Aπ(a′|H′, b′)Qi
∗(s′,a′, b′i)]. Expanding the function in the same way above completes
the proof.
20
Published as a conference paper at ICLR 2024
A.5 P ROOF OF THEOREM 4.1
We first discuss the policy gradient with πϕi(ai|Hi, bi):
∇ϕiVi(s, bi) =∇ϕi""X
a∈Aπϕ,ˆϕ(a|H, b, θ )Qi(s,a, bi)#
=∇ϕi""X
a∈A
(1−θ)·πϕ(a|H, b) +θ·ˆπˆϕ(ˆa|H, θ)
Qi(s,a, bi)#
=X
a∈Ah
(1−θi)∇ϕiπϕi(ai|Hi, bi)·Qi(s,a, bi) +πϕ,ˆϕ(a|H, b, θ )∇ϕiQi(s,a, bi)i
=X
a∈A""
(1−θi)∇ϕiπϕi(ai|Hi, bi)·Qi(s,a, bi) +πϕ,ˆϕ(ˆa|H, b, θ )∇ϕih
R(s,a)
+γX
s′∈SX
a′∈AP(s′|s,a)X
θ∈Θp(θ|H)πϕ,ˆϕ(a′|H′, b′, θ)Qi(s′,a′, b′i)i
,
=X
a∈A""
(1−θi)∇ϕiπϕi(ai|oi, bi)·Qi(s,a, bi) +πϕ,ˆϕ(a|H, b, θ )h
γ(1−θi)∇ϕi
πϕi(a′|H′, b′) +γX
s′∈SX
a′∈AP(s′|s,a)πϕ,ˆϕ(a′|H′, b′, θ)X
θ∈Θp(θ|H)∇ϕiQi(s′,a′, b′i)i
,
=X
s′∈S∞X
t=0Pr(s→s′, t,π)X
ˆa∈A(1−θi)∇ϕiπϕi(ai|Hi, bi)·Qi(s,a, bi).
Considering ∇ϕiJi(ϕi), we have
∇ϕiJi(ϕi) =∇ϕiVi(s0, bi)
=X
s∈S∞X
t=0Pr(s0→s, t,π)X
a∈A(1−θi)∇ϕiπϕi(ai|Hi, bi)·Qi(s,a, bi)
=X
s∈Sη(s)X
a∈A(1−θi)∇ϕiπϕi(ai|Hi, bi)·Qi(s,a, bi)
=X
s′∈Sη(s′)X
s∈Sη(s)P
s′∈Sη(s′)X
a∈A(1−θi)∇ϕiπϕi(ai|Hi, bi)·Qi(s,a, bi)
=X
s′∈Sη(s′)X
s∈Sρπ(s)X
a∈A(1−θi)πϕi(ai|Hi, bi)·Qi(s,a, bi)
∝X
s∈Sρπ(s)X
a∈A(1−θi)∇ϕiπϕi(ai|Hi, bi)·Qi(s,a, bi).
Using the log-derivative trick, we have:
∇ϕiJi(ϕi)∝X
s∈Sρπ(s)X
a∈A(1−θi)∇ϕiπϕi(ai|Hi, bi)·Qi(s,a, bi)
=X
s∈Sρπ(s)X
a∈A(1−θi)πϕi(ai|Hi, bi)·Qi(s,a, bi)∇ϕiπϕi(ai|Hi, bi)
πϕi(ai|Hi, bi)
=Es∼ρπ(s),a∼π(·|H,b,θ )[(1−θi)∇ϕilnπϕi(ai|Hi, bi)·Qi(s,a, bi)].
21
Published as a conference paper at ICLR 2024
The proof of πˆϕi(ˆai|oi, θ)basically follows the same method.
∇ˆϕiVi(s, bi) =∇ˆϕi""X
a∈Aπϕ,ˆϕ(a|H, b, θ )Qi(s,a, bi)#
=∇ˆϕi""X
a∈A
(1−θ)·πϕ(a|H, b) +θ·ˆπˆϕ(ˆa|H, θ)
Qi(s,a, bi)#
=X
a∈Ah
θi· ∇ˆϕiˆπˆϕi(ˆai|Hi, θ)·Qi(s,a, bi) +πϕ,ˆϕ(ˆa|H, b, θ )∇ˆϕiQi(s,a, bi)i
=X
a∈A""
θi· ∇ˆϕiˆπˆϕi(ˆai|Hi, θ)·Qi(s,a, bi) +πϕ,ˆϕ(ˆa|H, b, θ )∇ˆϕih
R(s,a)
+γX
s′∈SX
a′∈AP(s′|s,a)X
θ∈Θp(θ|H)πϕ,ˆϕ(a′|H′, b′, θ)Qi(s′,ˆa′, b′i)i
,
=X
a∈A""
θi· ∇ˆϕiˆπˆϕi(ˆai|Hi, θ)·Qi(s,a, bi) +πϕ,ˆϕ(a|H, b, θ )h
γ·θi· ∇ˆϕi
ˆπˆϕi(ˆa′i|H′i, θ) +γX
s′∈SX
a′∈AP(s′|s,a)X
θ∈Θp(θ|H)πϕ,ˆϕ(ˆa′|H′, b′, θ)∇ϕiQi(s′,a′, b′i)i
,
=X
s′∈S∞X
t=0Pr(s→s′, t,π)X
a∈Aθi· ∇ˆϕiˆπˆϕi(ˆai|Hi, θ)·Qi(s,a, bi).
Considering ∇ϕiJi(ϕi)and use the log-derivative trick same as above, we get:
∇ˆϕiJi(ˆϕi)∝X
s∈Sρπ(s)X
a∈Aθi· ∇ˆϕiˆπˆϕi(ˆai|Hi, θ)·Qi(s,a, bi)
=X
s∈Sρπ(s)X
a∈Aθiˆπˆϕ(ˆai|Hi, θ)·Qi(s,ˆa, bi)∇ˆϕiˆπˆϕi(ˆai|Hi, θ)
ˆπˆϕi(ˆai|Hi, θ)
=Es∼ρπ(s),a∼π(·|H,b,θ )[θi∇ˆϕiln ˆπˆϕi(ˆai|Hi, θ)·Qi(s,a, bi)].
This completes the proof.
A.6 C ONVERGENCE PROOF OF THEOREM 3.1
We proof this via stochastic approximation theory of Borkar (Borkar, 1997; Borkar & Meyn, 2000;
Borkar, 2009), where the robust agent is quasi-static and the adversary is essentially equilibrated
(Borkar & Meyn, 2000). One notable difference is, since the type in our work was sampled from
prior distribution θ∼p(θ), we take the expectation with respect to p(θ)to follow the notation of
stochastic approximation theory.
Proposition A.1 (Convergence) .Theorem 3.1 in main paper converge to robust Bayesian Markov
Perfect equilibrium a.s.if the following assumption holds.
Assumption A.1. Given step n, learning rate of πandˆπasα(n)andβ(n)withα(n), β(n)∈
(0,1), denote the probability of having an adversary as pθi=p(θi= 1) , such that ∀i∈ N ,
P
tα(n)(1−pθi) =P
nβ(n)pθi=∞,P
t(α(n)(1−pθi))2+(β(n)pθi)2<∞,α(n)(1−pθi)
β(n)pθi)→0.
Note that the assumption slightly differs from standard stochastic approximation, since adversaries
and robust agents are not uniformly explored.
Assumption A.2. ∀i∈ N, θ∈Θ,Qi(s,a, θ)is Lipshitz continuous. As a corollary, Qi(s,a, bi) =
Ep(θ|Hi)[Qi(s,a, θ)]is Lipshitz continuous.
Assumption A.3. Letν(s, a, θ )denote the number of visit to state sand action aunder θi.
∀s, a, θ, ν (s, a, θ )→ ∞ .
22
Published as a conference paper at ICLR 2024
Assumption A.4. The error in stochastic approximation ( i.e., inaccuracy in critic value, environment
noise, belief etc.) constitutes martingale difference sequences with respect to the increasing σ-fields.
Assumption A.5. ∀θ∈Θ, a global asymptotically stable ex interim equilibrium (πEI
∗,ˆπEI
∗)exists.
Assumption A.6. ∀θ∈Θ,supt(||πEI
n||+||ˆπEI
n||)<∞.
proof. We can write the update rule of πandˆπin their Ordinary Differential Equation (ODE) form:
Eθi∼p(θi)[πi
n+1] =Eθi∼p(θi)[πi
n] +α(ν(s, a))(1−pθi)Eθi∼p(θi)[∇logπi
n(R(s,a)
+X
s′∈SP(s′|s,a)X
θ∈Θp(θ|H′i)X
a∈Aπn(a′|H′, b′, θ)Qi
n(s′,a′, b′i))],
Eθi∼p(θi)[ˆπi
n+1] =Eθi∼p(θi)[ˆπi
n] +β(ν(s, a))pθiEθi∼p(θi)[∇log ˆπi
n(R(s,a)
+X
s′∈SP(s′|s,a)X
θ∈Θp(θ|H′i)X
ˆa∈Aπn(a′|H′, b′, θ)Qi
n(s′,a′, b′i))].
where we assume Qi
n(s′,a′, b′i)is the learned critic, updated at a faster timescale than α(n)and
β(n), following Assumption 1.1-1.5, such that Qiis essentially equilibrated. The error terms are
embedded in Qi
n(s′,a′, H′i). Thus, the update rule of πiandˆπifollows the general update rule of
stochastic optimization (Borkar, 2009):
xn+1=xn+α(n)[h(xn, yn) +M(1)
n+1],
yn+1=yn+α(n)[g(xn, yn) +M(2)
n+1].
Thus, by Theorem 4.1 in (Borkar & Meyn, 2000) or Theorem 2 in (Borkar, 2009), πnandˆπnconverge
to equilibrium.
B A DDITIONAL DETAILS ON ALGORITHM
We implement our algorithm on top of MAPPO (Yu et al., 2021). MAPPO is an widely used
multi-agent extension of PPO and consistently achieves strong performance on many benchmark
environments. Note that our method do not include algorithm-specific structures, which means it can
easily be applied to other actor-critic based algorithms, such as IPPO (de Witt et al., 2020), HATRPO
(Kuba et al., 2021), MAT (Wen et al., 2022) etc. easily. However, while technically possible, we
do not suggest a MADDPG implementation (Lowe et al., 2017) of our algorithm. This is because
MADDPG provide deterministic output, but pure-strategy robust Markov perfect Perfect Bayesian
equilibrium is not guaranteed to exist, as we have shown in Appedix. ??by a counterexample.
One important thing to notice is that we empirically find using larger learning rate for adversaries
during training of EIR-MAPPO do not always work well in all environments. For example, the
learning dynamics of adversary in our toy environment can be unstable even with learning rate 5e−4,
and get worse if the learning rate further increase, which is much smaller than the convergence rate
suggested by previous papers (Daskalakis et al., 2020). We empirically find adversaries using slightly
higher learning rates than robust agents works well, but requires extensive tuning. To tackle this
problem, we maintain the central assumption of two-timescale updates ( i.e., updating the adversary
faster and the robust agent slower), but instead update the adversary for more rollouts (denoted by
interval in our algorithm), while update the victim for less rollouts. This do not violate our proof
in Appendix A.6, since taking expectations to policy update brings the same result.
We closely follow the implementation details of MAPPO and PPO (Schulman et al., 2017), including
parameter sharing, generalized advantage estimation (GAE) (Schulman et al., 2015), and other
tricks in the codebase of MAPPO, available at https://github.com/marlbenchmark/
on-policy . Note that we use fixed learning rate for both adversary and robust agents. pξ(θ|Hi)
is a GRU (Chung et al., 2014) with input pξ(bi|oi, hi), where hiis the hidden state that summarize
observations in previous timesteps and oiis observation of current timestep.
23
Published as a conference paper at ICLR 2024
Algorithm 1 ex interim robust c-MARL (EIR-MAPPO).
Input: Policy network of robust agents πϕ, adversary ˆπˆϕi, value function Vψ, belief network pξ.
Output: Trained policy network of robust agents πϕ.
1:fork = 0, 1, 2, ... K do
2: Sample θ∼p(θ). Initialize τ= [].
3: fort = 0, 1, 2, ... T do
4: ∀i∈ N , perform rollout under bi
t=pξ(θ|Hi
t),at∼πϕ(·|Ht, bt),ˆat∼ˆπˆϕi(·|Ht, θ)and
st+1∼P(st+1|st,at, θ,ˆπ), receive Hi
t+1andrt.
5: τ←τ∪(bt,at,ˆat, st+1, Ht+1, rt, Ht, θ).
6: end for
7: fori = 0, 1, 2, ... Ndo
8: ifk %interval == 0 then
9: Using τ, calculate Ai
ψ(s,a, bi)by GAE; calculate bibypξ.
10: ϕ←ϕi+αϕ(1−θi)∇logπϕ(ai|Hi, bi)Ai
ψ(s,a, bi).// Shared parameters
11: ˆϕ←ϕi−αˆϕθi∇log ˆπˆϕ(ai|Hi, θ)Ai
ψ(s,a, bi).
12: ψ←ψ+αψ∇ψ(r−γQi
ψ(s′,a′, b′i) +Qi
ψ(s,a, bi))2
13: ξ←ξ−αξ∇ξ(θlog 
pξ(θ|Hi)
+ (1−θ) log(1 −pξ(θ|Hi))
14: else
15: ˆϕ←ϕi−αˆϕθi∇log ˆπˆϕ(ai|Hi, θ)Ai
ψ(s,a, bi).// Update adversaries for
more rollouts, with others fixed. Empirically stabilize
training.
16: end if
17: end for
18:end for
C A DDITIONAL DETAILS ON EXPERIMENTS
C.1 E NVIRONMENT DETAILS
(a) Toy
 (b) LBF
 (c) SMAC
Figure 7: Environments used in our experiments. The toy game is proposed by (Han et al., 2022).
We use map 3min SMAC (Samvelyan et al., 2019) and map 12x12-4p-4f in LBF (Papoudakis et al.,
2020).
In this section, we introduce more details on environment. Again, we add the figure of environments
in Fig. 7. Next, we introduce the tasks, actions and reward of each environments as follows.
Toy environment. The toy environment was first proposed by (Han et al., 2022) to study the effect
of state-based attacks on c-MARL. In this game, two agents play simultaneously for 100 iterations to
achieve maximum coordination. Specifically, in state s1, two agents seeks same actions (XNOR gate),
while state s2seeks two agents seeks different actions (XOR gate). During attack, the adversary can
take over each agent, and perform actions to maximally attack another agent. The attack requires the
robust agent to simultaneously identify other agents as adversary or allies, while taking cooperative
actions if other agent is an ally, and take randomized action if other agent is an adversary.
Level-Based Foraging environment. Level-Based Foraging environment (Papoudakis et al., 2020)
aims at a set of agents to cooperatively achieve maximum reward in food collection process. Each
24
Published as a conference paper at ICLR 2024
agents are assigned different “levels”, while the food can only be collected via agents with level
higher than the food. Note that we use the cooperative setting in LBF, which majority agents (except
the adversary) have to collaborate jointly to achieve the goal. While there do not exist a commonly
used testbed in LBF, we use 12x12-4p-4f in our experiment.
We also need to notice that, in LBF environment, an adversary is capable of physically interfere with
other agents, such as blocking the way of others or intentionally colliding with other agents, resulting
in a deadlock ever since. While Gleave et al. (2019) cited an important aspect of adversarial policy as
“not to physically interfere with others”, we turn the collision in LBF off to represent this.
StarCraft Multi-Agent Challenge environment. StarCraft Multi-Agent Challenge environment
(Samvelyan et al., 2019) is the most commly used testbed for c-MARL, which agents control a team
of red agents and seeks to win a team of blue agents with built-in AIs. We adapt the map 3mproposed
in SMAC testbed to 4m vs 3m , with one agent as adversary. Thus, the map can still be viewed as 3m,
albeit with one adversary agents trying to fool its teammates. Note that the adversary cannot attack
its allies by design of SMAC environment.
C.2 I MPLEMENTATION DETAILS
The implementation of MAPPO, RMAAC, EAR-MAPPO and EIR-MAPPO are based on the orig-
inal codebase of MAPPO ( https://github.com/marlbenchmark/on-policy ). The
implementation of MADDPG and M3DDPG resembles the code of FACMAC (Peng et al., 2021)
(https://github.com/oxwhirl/facmac ) and Heterogeneous-Agent Reinforcement Learn-
ing (HARL) codebase ( https://github.com/PKU-MARL/HARL ). Our code are available in
supplementary files, and will be open sourced after this paper is accepted.
For all environments, we set p(θ=0N) = 0 .5andp(θ=1i) = 0 .5/N, where 1idenotes the
one-hot vector with θi= 1 and others 0. This probability of selecting p(θ)remains fixed throughout
training process. During training, we store the model of robust agents with improved robustness
without decreasing cooperation reward, evaluated on the adversary during training. While testing, we
held all parameters in robust agents fixed, including policy and belief network. Then, a black-box
adversary was trained following the approach of adversarial policy . The adversary also follows a
CTDE approach, assuming assess to state, reward and local observation during training, and use local
observation only in testing. For fair comparison, we attack all baselines by PPO (Schulman et al.,
2017).
As for M3DDPG, note that the original version of M3DDPG (Li et al., 2019) are designed for
continuous control only, where actions are continuous and can be perturbed by a small value, while
in discrete control, one will have to completely change the action, or not changing the action at all.
To solve that, we add the noise perturbation to the action probability of MADDPG and send it to
Q function instead. We also find using large ϵfor M3DDPG will make the policy impossible to
converge in fully cooperative settings: since M3DDPG add perturbations directly to each agents,
resulting in an overly challenging setting. As such, we select the largest ϵwhich enables maximum
cooperation result in each setting.
As for RMAAC, the perturbation in training is set to ϵ= 0.5following their original paper, except
forϵ= 0.05in toy environment since otherwise the policy will not converge in normal training.
Next, we present all hyperparameters of each environment in the table below. These hyparameters
follows the default in previous papers, including MAPPO (Yu et al., 2021), HARL (Zhong et al.,
2023) and FACMAC (Peng et al., 2021).
25
Published as a conference paper at ICLR 2024
Table 1: Hyperparameters for MAPPO, RMAAC, EAR-MAPPO, EIR-MAPPO in toy environment.
Hyperparameter Value Hyperparameter Value Hyperparameter Value
rollouts 10 mini-batch num 1 PPO epoch 5
gamma 0.99 max grad norm 10 PPO clip 0.05
gain 0.01 max episode len 200 entropy coef 0.01
actor network MLP actor lr 5e-5 eval episode 32
hidden dim 128 critic lr 5e-5 optimizer Adam
belief network GRU adversary lr 5e-5 Huber loss True
use PopArt True belief lr 5e-5 Huber delta 10
adversary interval 10 GAE lambda 0.95 RMAAC ϵ 0.05
Table 2: Hyperparameters for MADDPG and M3DDPG in toy environment.
Hyperparameter Value Hyperparameter Value Hyperparameter Value
rollouts 10 mini-batch num 1 gamma 0.99
actor network MLP actor lr 5e-5 eval episode 32
hidden dim 256 critic lr 5e-5 optimizer Adam
buffer size 1000000 batch size 1000 epsilon 0.1
Table 3: Hyperparameters for the PPO adversary in toy environment.
Hyperparameter Value Hyperparameter Value Hyperparameter Value
rollouts 10 mini-batch num 1 PPO epoch 5
gamma 0.99 max grad norm 10 PPO clip 0.05
gain 0.01 max episode len 200 entropy coef 0.01
actor network MLP adversary lr 5e-5 eval episode 32
hidden dim 128 critic lr 5e-5 optimizer Adam
use PopArt True Huber loss True Huber delta 10
GAE lambda 0.95
Table 4: Hyperparameters for MAPPO, RMAAC, EAR-MAPPO, EIR-MAPPO in SMAC environ-
ment.
Hyperparameter Value Hyperparameter Value Hyperparameter Value
rollouts 20 mini-batch num 1 PPO epoch 5
gamma 0.95 max grad norm 10 PPO clip 0.05
gain 0.01 max episode len 200 entropy coef 0.01
actor network MLP actor lr 5e-4 eval episode 32
hidden dim 128 critic lr 5e-4 optimizer Adam
belief network GRU adversary lr 5e-4 Huber loss True
use PopArt True belief lr 5e-4 Huber delta 10
adversary interval 5 GAE lambda 0.95 RMAAC ϵ 0.05
Table 5: Hyperparameters for MADDPG and M3DDPG in SMAC environment.
Hyperparameter Value Hyperparameter Value Hyperparameter Value
rollouts 20 mini-batch num 1 gamma 0.99
actor network MLP actor lr 5e-4 eval episode 32
hidden dim 256 critic lr 5e-4 optimizer Adam
buffer size 1000000 batch size 1000 epsilon 0.01
26
Published as a conference paper at ICLR 2024
Table 6: Hyperparameters for the PPO adversary in SMAC environment.
Hyperparameter Value Hyperparameter Value Hyperparameter Value
rollouts 20 mini-batch num 1 PPO epoch 5
gamma 0.99 max grad norm 10 PPO clip 0.05
gain 0.01 max episode len 200 entropy coef 0.01
actor network MLP adversary lr 5e-4 eval episode 32
hidden dim 128 critic lr 5e-4 optimizer Adam
use PopArt True Huber loss True Huber delta 10
GAE lambda 0.95
Table 7: Hyperparameters for MAPPO, RMAAC, EAR-MAPPO, EIR-MAPPO in LBF environment.
Hyperparameter Value Hyperparameter Value Hyperparameter Value
rollouts 20 mini-batch num 1 PPO epoch 5
gamma 0.99 max grad norm 10 PPO clip 0.05
gain 0.01 max episode len 200 entropy coef 0.01
actor network MLP actor lr 5e-4 eval episode 32
hidden dim 128 critic lr 5e-4 optimizer Adam
belief network GRU adversary lr 5e-4 Huber loss True
use PopArt True belief lr 5e-4 Huber delta 10
adversary interval 5 GAE lambda 0.95 RMAAC ϵ 0.05
Table 8: Hyperparameters for MADDPG and M3DDPG in LBF environment.
Hyperparameter Value Hyperparameter Value Hyperparameter Value
rollouts 20 mini-batch num 1 gamma 0.99
actor network MLP actor lr 5e-5 eval episode 32
hidden dim 256 critic lr 5e-5 optimizer Adam
buffer size 1000000 batch size 1000 epsilon 0.1
Table 9: Hyperparameters for the PPO adversary in LBF environment.
Hyperparameter Value Hyperparameter Value Hyperparameter Value
rollouts 20 mini-batch num 1 PPO epoch 5
gamma 0.99 max grad norm 10 PPO clip 0.05
gain 0.01 max episode len 200 entropy coef 0.01
actor network MLP adversary lr 5e-4 eval episode 32
hidden dim 128 critic lr 5e-4 optimizer Adam
use PopArt True Huber loss True Huber delta 10
GAE lambda 0.95
27"
2307.16212v1.pdf,"Published in Transactions on Machine Learning Research (06/2023)
Robust Multi-Agent Reinforcement Learning with State Un-
certainty
Sihong He sihong.he@uconn.edu
Department of Computer Science and Engineering
University of Connecticut
Songyang Han songyang.han@uconn.edu
Department of Computer Science and Engineering
University of Connecticut
Sanbao Su sanbao.su@uconn.edu
Department of Computer Science and Engineering
University of Connecticut
Shuo Han hanshuo@uic.edu
Department of Electrical and Computer Engineering
University of Illinois, Chicago
Shaofeng Zou szou3@buffalo.edu
Department of Electrical Engineering
University at Buffalo, The State University of New York
Fei Miao fei.miao@uconn.edu
Department of Computer Science and Engineering
University of Connecticut
Reviewed on OpenReview: https: // openreview. net/ forum? id= CqTkapZ6H9
Abstract
In real-world multi-agent reinforcement learning (MARL) applications, agents may not have
perfect state information (e.g., due to inaccurate measurement or malicious attacks), which
challenges the robustness of agents’ policies. Though robustness is getting important in
MARL deployment, little prior work has studied state uncertainties in MARL, neither in
problem formulation nor algorithm design. Motivated by this robustness issue and the
lack of corresponding studies, we study the problem of MARL with state uncertainty in
this work. We provide the first attempt to the theoretical and empirical analysis of this
challenging problem. We first model the problem as a Markov Game with state perturbation
adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov
Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA.
We conduct a fundamental analysis regarding MG-SPA such as giving conditions under
which such a robust equilibrium exists. Then we propose a robust multi-agent Q-learning
(RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle
high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC)
algorithm based on an analytical expression of the policy gradient derived in the paper.
Our experiments show that the proposed RMAQ algorithm converges to the optimal value
function; our RMAAC algorithm outperforms several MARL and robust MARL methods in
multiple multi-agent environments when state uncertainty is present. The source code is
public on https://github.com/sihongho/robust_marl_with_state_uncertainty .
1arXiv:2307.16212v1  [cs.LG]  30 Jul 2023
Published in Transactions on Machine Learning Research (06/2023)
1 Introduction
Figure 1: Motivation of considering state uncertainty in
single-agent reinforcement learning.Reinforcement Learning (RL) recently has
achieved remarkable success in many decision-
making problems, such as robotics, autonomous
driving, trafficcontrol, andgameplaying(Espeholt
etal.,2018;Silveretal.,2017;Mnihetal.,2015;He
et al., 2022). However, in real-world applications,
the agent may face state uncertainty in which ac-
curate information about the state is unavailable.
This uncertainty may be caused by unavoidable
sensor measurement errors, noise, missing informa-
tion, communication issues, and/or malicious at-
tacks. A policy not robust to state uncertainty can
result in unsafe behaviors and even catastrophic
outcomes. Forinstance, considerthepathplanning
problem shown in Figure 1, where the agent (green
ball) observes the position of an obstacle (red ball)
through sensors and plans a safe (no collision) and
shortest path to the goal (black cross). In Fig-
ure 1-(a), the agent can observe the true state s
(red ball) and choose an optimal and collision-free
curvea∗(in red) tangent to the obstacle. In com-
parison, when the agent can only observe the perturbed state ˜s(yellow ball) caused by inaccurate sensing or
state perturbation adversaries (Figure 1-(b)), it will choose a straight line ˜a(in blue) as the shortest and
collision-free path tangent to ˜s. However, by following ˜a, the agent actually crashes into the obstacle. To
avoid collision in the worst case, one can construct a state uncertainty set that contains the true state based
on the observed state. Then the robustly optimal path under state uncertainty becomes the yellow curve ˜a∗
tangent to the uncertainty set, as shown in Figure 1-(c).
In single-agent RL, imperfect information about the state has been studied in the literature of partially
observable Markov decision process (POMDP) (Kaelbling et al., 1998). However, as pointed out in recent
literature (Huang et al., 2017; Kos & Song, 2017; Yu et al., 2021b; Zhang et al., 2020a), the conditional
observation probabilities in POMDP cannot capture the worst-case (or adversarial) scenario, and the learned
policy without considering state uncertainties may fail to achieve the agent’s goal. Dealing with state
uncertainty becomes even more challenging for Multi-Agent Reinforcement Learning (MARL), where each
agent aims to maximize its own total return during the interaction with other agents and the environment
(Yang & Wang, 2020b). Even if one agent receives misleading state information, its action affects both its
own return and the other agents’ returns (Zhang et al., 2020b) and may result in catastrophic failure. The
existing literature of decentralized partially observable Markov decision process (Dec-POMDP) (Oliehoek
et al., 2016) does not provide theoretical analysis or algorithmic tools for MARL under worst-case state
uncertainties either.
To better illustrate the effect of state uncertainty in MARL, the path planning problem in Figure 1 is modified
such that two agents are trying to reach their individual goals without collision (a penalty or negative reward
applied). When the blue agent knows the true position sg
0(the subscript denotes time, which starts from 0)
of the green agent, it will get around the green agent to quickly reach its goal without collision. However, in
Figure 2-(a), when the blue agent can only observe the perturbed position ˜sg
0(yellow circle) of the green
agent, it would choose a straight line that it thought safe (Figure 2-(a1)), which eventually leads to a crash
(Figure 2-(a2)). In Figure 2-(b), the blue agent adopts a robust trajectory by considering a state uncertainty
set based on its observation. As shown in Figure 2-(b1), there is no overlap between (sb
0,˜sg
0)or(sb
T,˜sg
T).
Since the uncertainty sets centered at ˜sg
0and˜sg
T(the dotted circles) include the true state of the green agent,
this robust trajectory also ensures no collision between (sb
0,sg
0)or(sb
T,sg
T). The blue agent considers the
interactions with the green agent to ensure no collisions at any time. Therefore, it is necessary to consider
state uncertainty in a multi-agent setting where the dynamics of other agents should be considered.
2
Published in Transactions on Machine Learning Research (06/2023)
Figure 2: Motivation of considering state uncertainty in MARL.In this work, we develop a robust
MARL framework that accounts for
state uncertainty. Specifically, we
model the problem of MARL with state
uncertainty as a Markov game with
state perturbation adversaries (MG-
SPA), in which each agent is associ-
ated with a state perturbation adver-
sary. One state perturbation adver-
saryalwaysplaysagainstitscorrespond-
ing agent by preventing the agent from
knowing the true state accurately. We
analyze the MARL problem with adver-
sarial or worst-case state perturbations.
Compared to single-agent RL, MARL
is more challenging due to the interac-
tions among agents and the necessity
of studying equilibrium policies (Nash,
1951; McKelvey & McLennan, 1996;
Slantchev, 2008; Daskalakis et al., 2009;
Etessami & Yannakakis, 2010). The
contributions of this work are summa-
rized as follows.
Contributions: To the best of our knowledge, this work is the first attempt to systematically characterize
state uncertainties in MARL and provide both theoretical and empirical analysis. First, we formulate the
MARL problem with state uncertainty as a Markov game with state perturbation adversaries (MG-SPA). We
define the solution concept of the game as a robust equilibrium (RE), where all players including the agents
and the adversaries use policies from which no one has an incentive to deviate. In an MG-SPA, each agent not
only aims to maximize its return when considering other agents’ actions but also needs to act against all state
perturbation adversaries. Therefore, a robust equilibrium policy of one agent is robust to state uncertainties.
Second, we study its fundamental properties and prove the existence of a robust equilibrium under certain
conditions. We develop a robust multi-agent Q-learning (RMAQ) algorithm with a convergence guarantee
and a robust multi-agent actor-critic (RMAAC) algorithm for handling high-dimensional state-action space.
Finally, we conduct experiments in a two-player game to validate the convergence of the proposed Q-learning
method RMAQ. We test our RMAAC algorithm in several benchmark multi-agent environments. We show
that our RMAQ and RMAAC algorithms can learn robust policies that outperform baselines under state
perturbations in multi-agent environments.
Organization: The rest of the paper is organized as follows. The related work is presented in Section 2.
In Section 3, we introduce some preliminary concepts in RL and MARL. The proposed methodology and
corresponding analysis are in Section 4. The proposed algorithms are in Section 5 and experiments results
are in Section 6. We discuss some future work in Section 7. In Section 8 we conclude.
2 Related work
Robust Reinforcement Learning: Recent robust reinforcement learning studied different types of
uncertainties, such as action uncertainties (Tessler et al., 2019) and transition kernel uncertainties (Sinha
et al., 2020; Yu et al., 2021b; Hu et al., 2020; Wang & Zou, 2021; Lim & Autef, 2019; Nisioti et al., 2021;
He et al., 2022). Some recent attempts at adversarial state perturbations for single-agent validated the
importance of considering state uncertainty and improving the robustness of the learned policy in Deep
RL (Huang et al., 2017; Lin et al., 2017; Zhang et al., 2020a; 2021; Everett et al., 2021). The works of
Zhang et al. (2020a; 2021) formulate the state perturbation in single-agent RL as a modified Markov decision
process, then study the robustness of single-agent RL policies. The works of Huang et al. (2017) and Lin
et al. (2017) show that adversarial state perturbation undermines the performance of neural network policies
3
Published in Transactions on Machine Learning Research (06/2023)
in single-agent reinforcement learning and proposes different single-agent attack strategies. In this work, we
consider the more challenging problem of adversarial state perturbation for MARL, when the environment of
an individual agent is non-stationary with other agents’ changing policies during the training process.
Robust Multi-Agent Reinforcement Learning: There is very limited literature on the solution concept
or theoretical analysis when considering adversarial state perturbations in MARL. Other types of uncertainties
have been investigated in the literature, such as uncertainties about training partner’s type (Shen & How,
2021), the other agents’ policies (Li et al., 2019; Sun et al., 2021; van der Heiden et al., 2020), and reward
uncertainties (Zhang et al., 2020b). However, the policy considered in these papers relies on the true state
information. Hence, the robust MARL considered in this work is fundamentally different since the agents do
not know the true state information. Dec-POMDP enables a team of agents to optimize policies with the
partial observable states (Oliehoek et al., 2016; Chen et al., 2022). The work of Lin et al. (2020) studies state
perturbation in identical-interest MARL, and proposes an attack method to attack the state of one single
agent in order to decrease the team reward. In contrast, we consider the worst-case scenario that the state of
every agent can be perturbed by an adversary and focus on the theoretical analysis of robust MARL including
the existence of optimal value function and robust equilibrium (RE). Our work provides formal definitions of
the state uncertainty challenge in MARL, and derives both theoretical analysis and practical algorithms.
Game Theory and MARL: MARL shares theoretical foundations with the game theory research field
and a literature review has been provided to understand MARL from a game theoretical perspective (Yang
& Wang, 2020a). A Markov game, sometimes called a stochastic game models the interaction between
multiple agents (Owen, 2013; Littman, 1994). Algorithms to compute the Nash equilibrium (NE) in Dec-
POMDP (Oliehoek et al., 2016), POSG (partially observable stochastic game) and analysis assuming that
NE exists (Chades et al., 2002; Hansen et al., 2004; Nair et al., 2002) have been developed in the literature
without proving the conditions for the existence of NE. The main theoretical contributions of this work include
proving conditions under which the proposed MG-SPA has robust equilibrium solutions, and convergence
analysis of our proposed robust multi-agent Q-learning algorithm. This is the first attempt to analyze the
fundamental properties of MARL under adversarial state uncertainties.
3 Preliminary
Q-learning is a model-free single-agent reinforcement learning algorithm (Sutton et al., 1998). The core of
this method is a Bellman equation that q∗(s,a) =r(s,a) +γ/summationtext
s′maxa′∈Aq∗(s′,a′). The Bellman equation
encourages a simple value iteration update which uses the weighted average of old Q-value and the new one.
Q-learning learns the optimal action-value function q∗(s,a)by value iteration: qnew(s,a) = (1−α)qold(s,a) +
α[r(s,a) +γ/summationtext
s′p(s′|s,a) maxa′∈Aqold(s′,a′)], and the optimal action a∗(s) =arg maxa∈Aq∗(s,a). Deep
Q-Networks (DQN) use a neural network with parameter θto approximate Q-value (Mnih et al., 2015).
This allows the algorithm to handle larger state spaces. DQN minimizes the loss function defined in (1),
whereq′is a target network that copies the parameter θoccasionally to make training more stable. Dis an
experience replay buffer. DQN uses experience replay, which involves storing and randomly sampling previous
experiences to train the neural network, to improve the stability and efficiency of the learning process. The
target network helps to prevent the algorithm from oscillating or diverging during training.
L(θ) =Eτ∼D[y−q(s,a|θ)]2, y =r(s,a) +γmax
a′∈Aq′(s′,a′). (1)
Actor-Critic (AC) is a single-agent reinforcement learning algorithm with two parts: an actor decides
which action should be taken and a critic evaluates how well the actor performs (Sutton et al., 1998). The
actor is parameterized by πθ(·|s)and iteratively updates the parameter θto maximize the objective function
J(θ) =Eτ∼p,a∼πθ[/summationtext∞
t=1γt−1rt(st,at)], whereτdenotes a trajectory and pis the state transition probability
distribution. The critic is parameterized by qϕ(s,a)and evaluates actions chosen by the actor by computing
the Q-value i.e. action-value function. The critic can update itself by using (1). The actor updates its
parameter by using the gradient: ∇θJ(θ) =Es∼p,a∼πθ[qπ(s,a)∇θlogπθ(a|s)].
4
Published in Transactions on Machine Learning Research (06/2023)
Markov game (MG) is used to model the interaction between multiple agents (Littman, 1994).
A Markov game, sometimes is called a stochastic game (Owen, 2013) defined as a tuple G:=
(N,S,{Ai}i∈N,{ri}i∈N,p,γ), whereSis the state space, Nis a set ofNagents,Aiis the action space of agent
i, respectively (Littman, 1994; Owen, 2013) .γ∈[0,1)is the discounting factor. We define A=A1×···×AN
as the joint action space. The state transition p:S×A→∆(S)is controlled by the current state and joint
action, where ∆(S)represents the set of all probability distributions over the joint state space S. Each
agent has a reward function, ri:S×A→R. At timet, agentichooses its action ai
taccording to a policy
πi:S→∆(Ai). For each agent i, it attempts to maximize its expected sum of discounted rewards, i.e. its
objective function Ji(s,π) =E/bracketleftbig/summationtext∞
t=1γt−1ri
t(st,at)|s1=s,at∼π(·|st)/bracketrightbig
.
4 Methodology
In this section, to solve the robust multi-agent reinforcement learning problem with state uncertainty, we
first introduce the framework of the Markov game with state perturbation adversaries. We then provide
characterization results for the proposed framework: Markov and history-dependent policies, the definition of
a solution concept called robust equilibrium based on value functions, derivation of Bellman equations, as
well as certain conditions for the existence of a robust equilibrium and the optimal value function.
4.1 Markov Game with State Perturbation Adversaries
We use a tuple ˜G:= (N,M,S,{Ai}i∈N,{B˜i}˜i∈M,{ri}i∈N,p,f,γ )to denote a Markov game with state
perturbation adversaries (MG-SPA). In an MG-SPA, we introduce an additional set of adversaries
M={˜1,···,˜N}to a Markov game (MG) with an agent set N. Each agent iis associated with an adversary
˜iand can observe the true state s∈Sif without adversarial perturbation. Each adversary ˜iis associated
with an action b˜i∈B˜iand the same state s∈Sthat agent ihas. We define the adversaries’ joint action
asb= (b˜1,...,b˜N)∈B,B=B˜1×···×B˜N. At time t, adversary ˜ican manipulate the corresponding
agenti’s state information. Once adversary ˜igets statest, it chooses an action b˜i
taccording to a policy
ρ˜i:S→∆(B˜i). According to a perturbation function f, adversary ˜iperturbs state stto˜si
t=f(st,b˜i
t)∈S.
We use ˜st= (˜s1
t,···,˜sN
t)to denote a joint perturbed state and use the notation f(st,bt) =˜st. We denote
the adversaries’ joint policy as ρ(b|s) =/producttext
˜i∈Mρ˜i(b˜i|s). The definitions of agent action and agents’ joint
action are the same as their definitions in an MG. Agent ichooses its action ai
twith ˜si
taccording to a policy
πi(ai
t|˜si
t),πi:S→∆(Ai). We denote the agents’ joint policy as π(a|˜s) =/producttext
i∈Nπ(ai|˜si). Agents execute
the agents’ joint action at, then at time t+ 1, the joint state stturns to the next state st+1according to a
transition probability function p:S×A×B→∆(S). Each agent igets a reward according to a state-wise
reward function ri
t:S×A×B→R. Each adversary ˜igets an opposite reward −ri
t. In an MG, the transition
probability function and reward function are considered as the model of the game. In an MG-SPA, the
perturbation function fis also considered as a part of the model, i.e., the model of an MG-SPA consists of
f,pand{ri}i∈N.
Definition 4.1 (Value Functions) .
vπ,ρ= (vπ,ρ,1,···,vπ,ρ,N),qπ,ρ= (qπ,ρ,1,···,qπ,ρ,N)are defined as the state-value function or value
function for short, and the action-value function, respectively. The ith element vπ,ρ,iandqπ,ρ,iare
defined as following:
qπ,ρ,i(s,a,b ) =E/bracketleftigg∞/summationdisplay
t=1γt−1ri
t|s1=s,a1=a,b1=b,at∼π(·|˜st),bt∼ρ(·|st),˜st=f(st,bt)/bracketrightigg
,(2)
vπ,ρ,i(s) =E/bracketleftigg∞/summationdisplay
t=1γt−1ri
t|s1=s,at∼π(·|˜st),bt∼ρ(·|st),˜st=f(st,bt)/bracketrightigg
. (3)
To incorporate realistic settings into our analysis, we restrict the power of each adversary, which is a common
assumption for state perturbation adversaries in the RL literature (Zhang et al., 2020a; 2021; Everett et al.,
2021). We define perturbation constraints ˜si∈Bdist(ϵ,s)⊂Sto restrict the adversary ˜ito perturb a
5
Published in Transactions on Machine Learning Research (06/2023)
state only to a predefined set of states. Bdist(ϵ,s)is aϵ-radius ball measured in metric dist(·,·), which is
often chosen to be the l-norm distance: dist(s,˜si) =∥s−˜si∥l. We omit the subscript distin the following
context. For each agent i, it attempts to maximize its expected sum of discounted rewards, i.e. its objective
functionJi(s,π,ρ ) =E/bracketleftbig/summationtext∞
t=1γt−1ri
t(st,at)|s1=s,at∼π(·|˜st),˜st=f(st,bt),bt∼ρ(·|st)/bracketrightbig
. Each adversary
˜iaims to minimize the objective function of agent iand is considered as receiving an opposite reward
of agenti, which also leads to a value function −Ji(s,π,ρ )for adversary ˜i. We further define the value
functions in an MG-SPA as in Definition 4.1. Then we propose robust equilibrium (RE), a NE-structured
solution as our solution concept for the proposed MG-SPA framework. We formally define RE in Definition 4.2.
Definition 4.2 (Robust Equilibrium) .
Given a Markov game with state perturbation adversaries ˜G, a joint policy d∗= (π∗,ρ∗)whereπ∗=
(π1
∗,···,πN
∗)andρ∗= (ρ˜1
∗,···,ρ˜N
∗)is said to be in robust equilibrium, or a robust equilibrium, if and
only if, for any i∈N,˜i∈M,s∈S,
v(π−i
∗,πi
∗,ρ−˜i
∗,ρ˜i),i(s)≥v(π−i
∗,πi
∗,ρ−˜i
∗,ρ˜i
∗),i(s)≥v(π−i
∗,πi,ρ−˜i
∗,ρ˜i
∗),i(s), (4)
where−i/−˜irepresents the indices of all agents/adversaries except agent i/ adversary ˜i.
As the maximin solution is also a popular solution concept in robust RL problems (Zhang et al., 2020a), here,
we discuss why we choose a NE-structured solution other than a maximin solution. Firstly, we aim to propose
a framework that can describe and model the interactions among agents when each agent has its own interest
or reward function under state perturbations, and maximin solution may not be a general solution concept
for robust MARL problems. A maximin solution is natural to use when considering robustness in single-agent
RL problems and identical-interest MARL problems, but it fails to handle those MARL problems where each
agent has its own interest or reward function. Secondly, the Nash equilibrium is also a commonly used robust
solution concept in both single-agent RL and MARL problems (Tessler et al., 2019; Zhang et al., 2020b).
Many papers have used NE as their solution concept to investigate robustness in RL problems, and to the
best of our knowledge, the NE-structured solution is the only one used in robust non-identical-interest MARL
problems (Zhang et al., 2020b). Lastly, for finite two-agent zero-sum games, it is known that NE, minimax,
and maximin solution concepts all give the same answer (Yin et al., 2010; Owen, 2013).
After defining RE, we seek to characterize the optimal value v∗(s) = (v1
∗(s),···,vN
∗(s))defined byvi
∗(s) =
maxπiminρ˜iv(π−i
∗,πi,ρ−˜i
∗,ρ˜i),i(s). For notation convenience, we use vi(s)to denotev(π−i
∗,πi,ρ−˜i
∗,ρ˜i),i(s). The
Bellman equations of an MG-SPA are in the forms of (5)and(6). The Bellman equation is a recursion for
expected rewards, which helps us identify or find an RE.
qi
∗(s,a,b ) =ri(s,a,b ) +γ/summationdisplay
s′∈Sp(s′|s,a,b ) max
πimin
ρ˜iE/bracketleftbig
qi
∗(s′,a′,b′)|a′∼π(·|˜s),b′∼ρ(·|s)/bracketrightbig
,(5)
vi
∗(s) = max
πimin
ρ˜iE/bracketleftigg/summationdisplay
s′∈Sp(s′|s,a,b )[ri(s,a,b ) +γvi
∗(s′)]|a∼π(·|˜s),b∼ρ(·|s)/bracketrightigg
, (6)
for alli∈N,˜i∈M, whereπ= (πi,π−i
∗),ρ= (ρ˜i,ρ−˜i
∗), and (πi
∗,π−i
∗,ρ˜i
∗,ρ−˜i
∗)is a robust equilibrium for ˜G.
We prove them in the following subsection. The policies in (5)and(6)are defined to be Markov policies which
only input the current state. The robust equilibrium is also based on Markov policies. History-dependent
policies for MARL under state perturbations may improve agents’ ability to adapt to adversarial state
perturbations and random sensor noise, by allowing agents to take into account past observations when
making decisions. Therefore, we further discuss how the current MG-SPA frame and solution concept adapt
to history-dependent policies in subsection 4.3.
4.2 Theoretical Analysis of MG-SPA
In this subsection, we first introduce the vector notations we used in the theoretical analysis and define a
minimax operator in Definition 4.3. We then introduce Assumption 4.4 which is considered in our theoretical
6
Published in Transactions on Machine Learning Research (06/2023)
analysis. Later, we prove two propositions about the minimax operator and Theorem 4.7 which shows a
series of fundamental characteristics of an MG-SPA under Assumption 4.4, e.g., the derivation of Bellman
equations, the existence of optimal value functions and robust equilibrium.
Vector Notations: To make the analysis easy to read, we follow and extend the vector notations in
Puterman (2014). Let Vdenote the set of bounded real valued functions on Swith component-wise partial
order and norm∥vi∥:=sups∈S|vi(s)|. LetVMdenote the subspace of Vof Borel measurable functions. For
discrete state space, all real-valued functions are measurable so that V=VM. But when Sis a continuum, VM
is a proper subset of V. Letv= (v1,···,vN)∈Vbe the set of bounded real valued functions on S×···×S,
i.e. the across product of Nstate set and norm ∥v∥:= supj∥vj∥. For discrete S, let|S|denote the number
of elements in S. Letridenote a|S|-vector, with sth component ri(s)which is the expected reward for agent
iunder state s. AndPthe|S|×|S|matrix with (s,s′)th entry given by p(s′|s). We refer to ri
das the reward
vector of agent i, andPdas the probability transition matrix corresponding to a joint policy d= (π,ρ).
ri
d+γPdviis the expected total one-period discounted reward of agent i, obtained using the joint policy
d= (π,ρ). Letzas a list of joint policy {d1,d2,···}andP0
z=I, we denote the expected total discounted
reward of agent iusingzasvi
z=/summationtext∞
t=1γt−1Pt−1
zri
dt=ri
d1+γPd1ri
d2+···+γn−1Pd1···Pdn−1ri
tn+···. Now,
we define the following minimax operator which is used in the rest of the paper.
Definition 4.3 (Minimax Operator) .
Forvi∈V,s∈S, we define the nonlinear operator Lionvi(s)byLivi(s) := maxπiminρ˜i[ri
d+
γPdvi](s), whered:= (π−i
∗,πi,ρ−˜i
∗,ρ˜i). We also define the operator Lv(s) =L(v1(s),···,vN(s)) =
(L1v1(s),···,LNvN(s)). ThenLiviis a|S|-vector, with sth component Livi(s).
For discrete Sand bounded ri, it follows from Lemma 5.6.1 in Puterman (2014) that Livi∈Vfor allvi∈V.
ThereforeLv∈Vfor allv∈V. And in this paper, we consider the following assumptions in Markov games
with state perturbation adversaries.
Assumption 4.4.
(1) Bounded rewards; |ri(s,a,b )|≤Mi<M <∞for alli∈N,a∈A,b∈B, ands∈S.
(2) Finite state and action spaces: all S,Ai,B˜iare finite.
(3) Stationary transition probability and reward functions.
(4)f(s,·)is a bijection for any fixed s∈S.
(5) All agents share one common reward function.
Finite state and action spaces, bounded rewards, stationary transition kernels, and stationary reward
functions are common assumptions in both reinforcement learning and multi-agent reinforcement learning
literature (Puterman, 2014; Başar & Olsder, 1998). Additionally, the bijection property of perturbation
functions implies that in a finite MG-SPA, adversaries that adopt deterministic policies provide a permutation
on the state space. Collaboration and coordination among agents are often required in real-world scenarios
to achieve a common goal. In such cases, a shared reward function can motivate agents to work together
effectively. Moreover, the assumption of a shared reward function is necessary to transform an MG-SPA into
a zero-sum two-agent extensive-form game in our proof. Although these assumptions do not always hold true
in real-world applications, they provide good properties for an MG-SPA and enable the first attempt of
theoretical analysis on an MG-SPA. The next two propositions characterize the properties of the minimax
operatorLand space V. We provide the proof in Appendix A.2. These contraction mapping and complete
space results are used in the proof of RE existence for an MG-SPA.
Proposition 4.5 (Contraction mapping) .
Suppose 0≤γ <1, and Assumption 4.4 holds. Then Lis a contraction mapping on V.
7
Published in Transactions on Machine Learning Research (06/2023)
Proposition 4.6 (Complete Space) .The space Vis a complete normed linear space.
In Theorem 4.7, we show some fundamental characteristics of an MG-SPA. In (1), we show that an optimal
value function of an MG-SPA satisfies the Bellman equations by applying the Squeeze theorem [Theorem
3.3.6, Sohrab (2003)]. Theorem 4.7-(2) shows that the unique solution of the Bellman equation exists, a
consequence of the fixed-point theorem (Smart, 1980). Therefore, the optimal value function of an MG-SPA
exists under Assumption 4.4. By introducing (3), we characterize the relationship between the optimal value
function and a robust equilibrium. However, (3) does not imply the existence of an RE. To this end, in (4),
we formally establish the existence of RE when the optimal value function exists. We formulate a 2N-player
Extensive-form game (EFG) (Osborne & Rubinstein, 1994; Von Neumann & Morgenstern, 2007) based on the
optimalvaluefunctionsuchthatits Nashequilibrium(NE)policyisequivalent toanRE policyof theMG-SPA.
Theorem 4.7.
Suppose 0≤γ <1and Assumption 4.4 holds.
(1) (Solution of Bellman equation) A value function v∗∈Vis an optimal value function if for all i∈N,
the point-wise value function vi
∗∈Vsatisfies the corresponding Bellman Equation (6), i.e.v∗=Lv∗.
(2) (Existence and uniqueness of optimal value function) There exists a unique v∗∈VsatisfyingLv∗=v∗,
i.e. for all i∈N,Livi
∗=vi
∗.
(3) (Robust equilibrium and optimal value function) A joint policy d∗= (π∗,ρ∗), whereπ∗= (π1
∗,···,πN
∗)
andρ∗= (ρ˜1
∗,···,ρ˜N
∗), is a robust equilibrium if and only if vd∗is the optimal value function.
(4) (Existence of robust equilibrium) There exists a mixed RE for an MG-SPA.
Proof.The full proof of Theorem 4.7 is presented in Appendix A, specifically in A.3. We provide a high-level
proof sketch here. Our proof consists of two main parts: 1. Constructing an extensive-form game that
is connected to an MG-SPA. 2. Proof of Theorem 4.7. In the first part, we begin by constructing an
extensive-form game (EFG) whose payoff function is related to the value functions of an MG-SPA (Appendix
A.1). Using the EFG as a tool, we can analyze the properties of an MG-SPA. We provide insights into solving
an MG-SPA by solving a constructed EFG: a robust equilibrium (RE) of an MG-SPA can be derived from
a Nash equilibrium of an EFG when the EFG’s payoff function is related to the optimal value function of
the MG-SPA (Lemma A.7 in Appendix). Thus, by providing conditions under which a Nash equilibrium
of a well-constructed EFG exists (Appendix A.1.1), we can prove the existence of an RE of the MG-SPA
(Theorem 4.7-(4)). The existence of an optimal value function is not yet proven and is left for the second
part. In the second part, we prove Theorem 4.7-(1) by showing that for all i, there exists a vi∈Vsuch that
vi≥Lvi, thenvi≥vi
∗, and there also exists a vi∈Vsuch thatvi≤Lvi, thenvi≤vi
∗. Propositions 4.5 and
4.6 enable us to use Banach Fixed-Point Theorem (Smart, 1980) to prove Theorem 4.7-(2). The proof of
Theorem 4.7-(3) benefits from the definitions of the optimal value function and robust equilibrium, Theorem
4.7-(1) and (2). Finally, given the existence of the optimal value function and the results from the first part,
we prove the existence of an RE.
Though the existence of NE in a stochastic game with perfect information has been investigated (Shapley,
1953; Fink, 1964), it is still an open and challenging problem when players have partially observable
information (Hansen et al., 2004; Yang & Wang, 2020a). There is a bunch of literature developing algorithms
trying to find the NE in Dec-POMDP or partially observable stochastic game (POSG), and conducting
algorithm analysis assuming that NE exists (Chades et al., 2002; Hansen et al., 2004; Nair et al., 2002)
without proving the conditions for the existence of NE. Once established the existence of RE, we design
algorithms to find it. In Section 5, we first develop a robust multi-agent Q-learning (RMAQ) algorithm with
a convergence guarantee, then propose a robust multi-agent actor-critic (RMAAC) algorithm to handle the
case with high-dimensional state-action spaces.
8
Published in Transactions on Machine Learning Research (06/2023)
Remark 4.8 (Heterogeneous agents and adversaries) .In the above problem formulation, we assume all
agents have the same type of state perturbations (share one f), and all adversaries have the same level of
perturbation power (share one ϵ), which made the notation more concise and the analysis more tractable.
However, these assumptions are sometimes unrealistic in practice since agents/adversaries may have
different capabilities. To introduce heterogeneous agents and adversaries to an MG-SPA, we let each agent
ihas its own perturbation function fi, and each adversary ˜ihas its own perturbation power constraint
ϵi, such that ˜si=fi(s,bi)∈B(ϵi,s). We usef(s,b) = (f1(s,b1),···,fN(s,bN)) = ˜sto denote the joint
perturbation function. When all perturbation functions fi(s,·)are bijective for any fixed s∈S, the joint
perturbation function f(s,·)is also a bijection. Assumption 4.4-(4) still holds. When constructing an
extensive-form game, the action set for player P1is defined as ˜S=B1(ϵ1,s)×B2(ϵ2,s)×···BN(ϵN,s)
instead of ˜S=B(ϵ,s)×···×B (ϵ,s). The subsequent proofs still hold and they are not affected by the
introduction of heterogeneous agents and adversaries. After extending our MG-SPA framework to handle
heterogeneous agents and adversaries, we can model more complex and realistic multi-agent systems.
Remark 4.9 (A reduced case of MG-SPA: a single-agent system) .When there is only one agent in
the system, the MG-SPA problem reduces to a single-agent robust RL problem with state uncertainty,
which has been studied in the literature (Zhang et al., 2020a; 2021). In this case, single-agent robust RL
with state uncertainty can be seen as a specific and special instance of MG-SPA presented in this paper.
However, the proposed analysis and algorithm in this paper provide a new perspective and approach
to single-agent robust reinforcement learning, by explicitly modeling the adversary’s perturbations and
optimizing the agent’s policy against them in a game-theoretic framework. Moreover, the presence of
multiple agents and adversaries in the system can result in more complex and challenging interactions,
joint actions and policies that do not present in single-agent RL problems. Our proposed MG-SPA
framework allows for the modeling of a wide range of agent interactions, including cooperative, competitive,
and mixed interactions.
4.3 History-dependent-policy-based Robust Equilibrium
It is natural and desirable to consider history-dependent policies in robust MARL with state perturbations,
since the agents may not fully capture the state uncertainty from the current state information, and a policy
that only depends on the current state may not be sufficient for ensuring robustness. The history-dependent
policy allows agents to take into account past observations when making decisions, which helps agents better
reason about the adversaries’ possible strategies and intentions. This is particularly true in the case of
Dec-POMDPs and POSGs, where the agent cannot fully observe the state. Therefore, we further extend the
above Markov-policy-based RE to a history-dependent-policy-based robust equilibrium and discuss Theorem
4.7 under history-dependent policies in this subsection. In Section 5, we also discuss how the proposed
algorithms can adapt to historical state input. We further validate that a history-dependent-policy-based RE
outperforms a Markov-policy-based RE in Section 6.
In this subsection, we clarify the generalization steps of extending Markov-policy-based RE to history-
dependent-policy-based RE. We first introduce the definition of history-dependent policy with a finite
time horizon hin an MG-SPA. We then give the formal definition of a history-dependent-policy-based
robust equilibrium. Finally, we show that Theorem 4.7 still holds when agents and adversaries adopt
history-dependent policies.
We consider an MG-SPA with a time horizon h, in which adversaries and agents respectively observe the states
andperturbedstatesinthelatest htimestepsandadopthistory-dependentpolicies. Moreconcretely, adversary
˜ican manipulate the corresponding agent i’s state at time tby using a history-dependent policy ρ˜i
h(·|sh,t)and
agentichooses its actions using a history-dependent policy πi
h(·|˜si
h,t). Specifically, once adversary ˜igets the
true statestat timet, it chooses an action b˜i
taccording to a history-dependent policy ρ˜i
h:Sh→∆(B˜i), where
sh,t= (st,···,st−h+1)∈Shis a concatenated state consists of the latest htime steps of states. According to
a perturbation function f, adversary ˜iperturbs state stto˜si
t=f(st,b˜i
t)∈S. The adversaries’ joint policy
is defined as ρh(b|sh) =/producttext
˜i∈Mρ˜i
h(b˜i|sh). Agentichooses its action ai
tfor˜si
h,t= (˜si
t,···,˜si
t−h+1)∈Shwith
probability πi
h(ai
t|˜si
h,t)according to a history dependent policy πi
h:Sh→∆(Ai). The agents’ joint policy
9
Published in Transactions on Machine Learning Research (06/2023)
is defined as πh(a|˜sh) =/producttext
i∈Nπi
h(ai|˜si
h). Then a joint history-dependent policy dh,∗= (πh,∗,ρh,∗)where
πh,∗= (π1
h,∗,···,πN
h,∗)andρh,∗= (ρ˜1
h,∗,···,ρ˜N
h,∗)is said to be in a history-dependent-policy-based robust
equilibrium if and only if, for any i∈N,˜i∈M,s∈S,
v(π−i
h,∗,πi
h,∗,ρ−˜i
h,∗,ρ˜i
h),i(s)≥v(π−i
h,∗,πi
h,∗,ρ−˜i
h,∗,ρ˜i
h,∗),i(s)≥v(π−i
h,∗,πi
h,ρ−˜i
h,∗,ρ˜i
h,∗),i(s).
It is worth noting that the main differences between history-dependent-policy-based RE and Markov-policy-
based RE are the definition and notation of policies and states. A Markov-policy-based RE is a special case of
a history-dependent-policy-based RE by adopting the time horizon h= 1. We also notice that these two REs’
definitions are the same if we remove the subscript hfrom the concatenated state and history-dependent
policies. Therefore, in this paper, we use notations without the time horizon subscripts, i.e. Markov policy
and Markov-policy-based RE, to avoid redundant and complicated notations. While in this subsection, we
clarify the definitions of history-dependent policy and history-dependent-policy-based RE and show that
Theorem 4.7 still holds when agents and adversaries use history-dependent policies in the following corollary.
Corollary 4.9.1. Theorem 4.7 still holds when all agents and adversaries in an MG-SPA use history-
dependent policies with a finite time horizon.
Proof.See Appendix A.4.
Remark 4.10 (MG-SPA,Dec-POMDP, andPOSG) .Decentralized Partially Observable Markov Decision
Process (Dec-POMDP) enables a team of agents to optimize policies with partial observable states
(Oliehoek et al., 2016; Nair et al., 2002), while a Partially Observable Stochastic Game (POSG)
(Hansen et al., 2004; Emery-Montemerlo et al., 2004) is an extension of stochastic games with imperfect
information that can handle partial observable states. We are inspired by them to consider history-
dependent policies for our proposed MG-SPA problem. However, there are several differences between
MG-SPA, Dec-POMDP and POSG. First, unlike Dec-POMDP, an MG-SPA does not restrict all agents
to share the same interest or reward function. The proposed MG-SPA framework is applicable for
modeling different relationships between agents, including cooperative, competitive, or mixed interactions.
Second, neither Dec-POMDP nor POSG considers the worst-case state perturbation scenarios. In
contrast, in an MG-SPA, state perturbation adversaries receive opposite rewards to the agents, which
motivates them to find the worst-case state perturbations to minimize the agents’ returns. As we explained
in the introduction, considering worst-case state perturbations is important for MARL. Third, while
in a Dec-POMDP or a POSG, all agents cannot observe the true state information, in an MG-SPA,
adversaries can access the true state and utilize it to select state perturbation actions. Based on these
differences in problem formulation, Dec-POMDP and POSG methods cannot solve the proposed MG-SPA
problem.
5 Algorithm
5.1 Robust Multi-Agent Q-learning (RMAQ) Algorithm
By solving the Bellman equation, we are able to get the optimal value function of an MG-SPA as shown in
Theorem 4.7. We therefore develop a value iteration (VI)-based method called robust multi-agent Q-learning
(RMAQ) algorithm. Recall the Bellman equation using action-value function in (5), the optimal action-
valueq∗satisfiesqi
∗(s,a,b ) :=ri(s,a,b ) +γE/bracketleftbig/summationtext
s′∈Sp(s′|s,a,b )qi
∗(s′,a′,b′)|a′∼π∗(·|˜s′),b′∼ρ∗(·|s′)/bracketrightbig
.As a
consequence, the tabular-setting RMAQ update can be written as below,
qi
t+1(st,at,bt) = (1−αt)qi
t(st,at,bt)+ (7)
αt
ri
t+γ/summationdisplay
at+1∈A/summationdisplay
bt+1∈Bπqt
∗,t(at+1|˜st+1)ρqt
∗,t(bt+1|st+1)qi
t(st+1,at+1,bt+1)
,
where (πqt
∗,t,ρqt
∗,t)is an NE policy by solving the 2N-player extensive-form game (EFG) based on a payoff
function (q1
t,···,qN
t,−q1
t,···,−qN
t). The joint policy (πqt
∗,t,ρqt
∗,t)is used in updating qt. All related definitions
10
Published in Transactions on Machine Learning Research (06/2023)
of the EFG (q1
t,···,qN
t,−q1
t,···,−qN
t)are introduced in Appendix A.1. How to solve an EFG is out of the
scope of this work, algorithms to do this exist in the literature (Čermák et al., 2017; Kroer et al., 2020). Note
that, in RMAQ, each agent’s policy is related to not only its own value function, but also other agents’ value
function. This multi-dependency structure considers the interactions between agents in a game, which is
different from the Q-learning in single-agent RL that considers optimizing its own value function. Meanwhile,
establishing the convergence of a multi-agent Q-learning algorithm is also a general challenge. Therefore,
we try to establish the convergence of (7)in Theorem 5.2, motivated from Hu & Wellman (2003). Due to
space limitation, in Appendix B.1, we prove that RMAQ is guaranteed to get the optimal value function
q∗= (q1
∗,···,qN
∗)by updating qt= (q1
t,···,qN
t)recursively using (7) under Assumptions 5.1.
Assumption 5.1.
(1) State and action pairs have been visited infinitely often. (2) The learning rate αtsatisfies the
following conditions: 0≤αt<1,/summationtext
t≥0α2
t≤∞; if(s,a,b )̸= (st,at,bt),αt(s,a,b ) = 0. (3) An NE of
the2N-player EFG based on (q1
t,···,qN
t,−q1
t,···,−qN
t)exists at each iteration t.
Theorem 5.2.
Under Assumption 5.1, the sequence {qt}obtained from (7)converges to{q∗}with probability 1, which
are the optimal action-value functions that satisfy Bellman equations (5)for alli= 1,···,N.
Assumption 5.1-(1) is a typical ergodicity assumption used in the convergence analysis of Q-learning (Littman
& Szepesvári, 1996; Hu & Wellman, 2003; Szepesvári & Littman, 1999; Qu & Wierman, 2020; Sutton &
Barto, 1998). And for Q-learning algorithm design papers that the exploration property is not the main
focus, this assumption is also a common assumption (Fujimoto et al., 2019). For exploration strategies in RL
(McFarlane, 2018), researchers use ϵ-greedy exploration (Gomes & Kowalczyk, 2009), UCB (Jin et al., 2018;
Azar et al., 2017), Thompson sampling (Russo et al., 2018), Boltzmann exploration (Cesa-Bianchi et al.,
2017), etc. For assumption 5.1-(3) in multi-agent Q-learning, researchers have found that the convergence is
not necessarily so sensitive to the existence of NE for the stage games during training (Hu & Wellman, 2003;
Yang et al., 2018). In particular, under Assumption 4.4, an NE of the 2 N-player EFG exists, which has been
proved in Lemma A.6 in Appendix A.1. We also provide an example in the experiment part (the two-player
game) where assumptions are indeed satisfied, and our RMAQ algorithm successfully converges to an RE of
the corresponding MG-SPA.
5.2 Robust Multi-Agent Actor-Critic (RMAAC) Algorithm
According to the above descriptions of a tabular RMAQ algorithm, each learning agent has to maintain N
action-value functions. The total space requirement is N|S||A|N|B|Nif|A1|=···=|AN|,|B1|=···=|BN|.
This space complexity is linear in the number of joint states, polynomial in the number of agents’ joint actions
and adversaries’ joint actions, and exponential in the number of agents. The computational complexity
is mainly related to algorithms to solve an extensive-form game (Čermák et al., 2017; Kroer et al., 2020).
However, even for general-sum normal-form games, computing an NE is known to be PPAD-complete, which
is still considered difficult in game theory literature (Daskalakis et al., 2009; Chen et al., 2009; Etessami &
Yannakakis, 2010). These properties of the RMAQ algorithm motivate us to develop an actor-critic method to
handle high-dimensional space-action spaces, which can incorporate function approximation into the update
(Konda & Tsitsiklis, 1999).
We consider each agent i’s policyπiis parameterized as πθifori∈N, and the adversary’s policy ρ˜iis
parameterized as ρωi. We denote θ= (θ1,···,θN)as the concatenation of all agents’ policy parameters, ω
has the similar definition. For simplicity, we omit the subscript θi,ωi, since the parameters can be identified
by the names of policies. Then the value function vi(s)under policy (π,ρ)satisfies
vπ,ρ,i(s) =Ea∼π,b∼ρ/bracketleftigg/summationdisplay
s′∈Sp(s′|s,a,b )[ri(s,a,b ) +γvπ,ρ,i(s′)]/bracketrightigg
. (8)
11
Published in Transactions on Machine Learning Research (06/2023)
We establish the general policy gradient with respect to the parameter θ,ωin the following theorem. Then
we propose our robust multi-agent actor-critic algorithm (RMAAC) which adopts a centralized-training
decentralized-execution algorithm structure in MARL literature (Lowe et al., 2017; Foerster et al., 2018).
Theorem 5.3 (Policy Gradient in RMAAC for MG-SPA) .For each agent i∈Nand adversary ˜i∈M,
the policy gradients of the objective Ji(θ,ω)with respect to the parameter θ,ωare:
∇θiJi(θ,ω) =E(s,a,b )∼p(π,ρ)/bracketleftbig
qi,π,ρ(s,a,b )∇θilogπi(ai|˜si)/bracketrightbig
(9)
∇ωiJi(θ,ω) =E(s,a,b )∼p(π,ρ)/bracketleftig
qi,π,ρ(s,a,b )[∇ωilogρ˜i(b˜i|s) +reg]/bracketrightig
(10)
wherereg=∇˜silogπi(ai|˜si)∇b˜if(s,b˜i)∇ωiρ˜i(b˜i|s).
Proof.See details in Appendix B.2.1.
We put the pseudo-code of RMAAC in Appendix B.2.3.
Remark 5.4 (History-dependent Policy) .RMAAC can calculate history-dependent policies by using
recent observations as the policy input. For example, DQN (Mnih et al., 2015) maps history–action pairs
to scalar estimates of Q-value. It uses the history (4 most recent frames) of the states and the action as
the inputs of the neural network.
6 Experiment
We aim to answer the following questions through experiments: (1) Can RMAQ algorithm find an RE? (2)
Are RE policies robust to state uncertainties? (3) Does RMAAC algorithm outperform other MARL and
robust MARL algorithms in terms of robustness? The host machine used in our experiments is a server
configured with AMD Ryzen Threadripper 2990WX 32-core processors and four Quadro RTX 6000 GPUs.
All experiments are performed on Python 3.5.4, Gym 0.10.5, Numpy 1.14.5, Tensorflow 1.8.0, and CUDA 9.0.
Our code is public on https://github.com/sihongho/robust_marl_with_state_uncertainty .
6.1 Robust Multi-Agent Q-learning (RMAQ)
We show the performance of the proposed RMAQ algorithm by applying it to a two-player game. We first
introduce the designed two-player game. Then, to answer the first and second questions, we investigate the
convergence of this algorithm and compare the performance of robust equilibrium policies with other agents’
policies under different adversaries’ policies.
Figure 3: Two-player game: each player has two states
and the same action set with size 2. Under state s0,
two players get the same reward 1when they choose
the same action. At state s1, two players get the same
reward 1when they choose different actions. One state
switches to another state only when two players get a
reward.Two-player game: For the game in Figure 3, two
players have the same action space A={0,1}and
state space S={s0,s1}. The two players get the
same positive rewards when they choose the same ac-
tion under state s0or choose different actions under
states1. The state does not change until these two
players get a positive reward. Possible Nash equilib-
rium (NE) in this game can be π∗
1= (π1
1,π2
1)that
player 1always chooses action 1, player 2chooses
action 1under state s0and action 0under state s1;
orπ∗
2= (π1
2,π2
2)that player 1always chooses action
0, player 2chooses action 0under state s0and action
1under state s1. When using the NE policy, these
two players always get the same positive rewards.
The optimal discounted state value of this game is
vi
∗(s) = 1/(1−γ)for alls∈S,i∈{1,2},γis the reward discounted rate. We set γ= 0.99, thenvi
∗(s) = 100.
12
Published in Transactions on Machine Learning Research (06/2023)
MG-SPA formulation for the two-player game: According to the definition of MG-SPA, we add two
adversaries, one for each player to perturb the state and get a negative reward of the player. They have the
same action space B={0,1}, where 0means do not disturb, 1means perturb the observed state to another
one. Sometimes no perturbation would be a good choice for adversaries. For example, when the true state is
s0, players are using π∗
1, if adversary 1does not perturb player 1’s observation, player 1will still select action
1. While adversary 2changes player 2’s observation to state s1, player 2will choose action 0which is not the
same as player 1’s action 1. Thus, players always fail the game and get no rewards. A robust equilibrium
for MG-SPA would be ˜d∗= (˜π1
∗,˜π2
∗,˜ρ˜1
∗,˜ρ˜2
∗)that each player chooses actions with equal probability and so
do adversaries. The optimal discounted state value of corresponding MG-SPA is ˜vi
∗(s) = 1/2(1−γ)for all
s∈S,i∈{1,2}when players use robust equilibrium (RE) policies. We use γ= 0.99, then ˜vi
∗(s) = 50. For
more explanations of this two-player game and corresponding MG-SPA formulation, please see Appendix C.1.
Implementing RMAQ on the two-player game: We initialize q1(s,a,b ) =q2(s,a,b ) = 0for alls,a,b.
After observing the current state, adversaries choose their actions to perturb the agents’ state. Then players
execute their actions based on the perturbed state information. They then observe the next state and rewards.
Then every agent updates its qaccording to (7). In the next state, all agents repeat the process above.
The training stops after 7500steps. When updating the Q-values, the agent applies a NE policy from the
Extensive-form game based on (q1,q2,−q1,−q2).
Figure 4: RE policy outperforms other policies in terms
of total discounted rewards and total accumulated re-
wards when strong state uncertainties exist.Training results: After 7000steps of training,
we find that agents’ Q-values stabilize at certain
values. Since the dimension of qis a bit high as
q∈R32, we compare the optimal state value ˜v∗
and the total discounted rewards in Table 1. The
value of the total discounted reward converges to the
optimal state value of the corresponding MG-SPA.
This two-player game experiment result validates the
convergence of our RMAQ method and the answer
to the first question is ’Yes’.
Testing results: We further test well-trained
RE policy when ’strong’ adversaries exist. ’Strong’
adversary means its probability of modifying play-
ers’ observations is larger than the probability of no
perturbations in the state information. We make
two players play the game using 3 different poli-
cies for 1000steps under different adversaries. The
accumulated rewards and total discounted rewards
are calculated. We use the robust equilibrium (of
the MG-SPA), the Nash equilibrium (of the original
game), and a baseline policy (two players use deter-
ministic policies) and report the result in Figure 4.
The vertical axis is the accumulated/discounted re-
ward, and the horizon axis is the probability that
the adversary will attack/perturb the state. And we
let these two adversaries share the same policy. We
can see as the probability increase, the accumulated
and discounted rewards of RE players are stable but
those rewards of NE players and baseline players
keep decreasing. These experimental results show
that the RE policy is robust to state uncertainties.
It turns out the answer to the second question is ’Yes’ as well.
Discussion: Even for general-sum normal-form games, computing an NE is known to be PPAD-complete,
whichisstillconsidereddifficultingametheoryliterature(Conitzer&Sandholm,2002;Etessami&Yannakakis,
13
Published in Transactions on Machine Learning Research (06/2023)
Table 1: Convergence Values of Total Discounted Rewards when Training Ends
v1(s0)v2(s0)v1(s1)v2(s1)˜v1
∗(s0) ˜v2
∗(s0)˜v1
∗(s1) ˜v2
∗(s1)
value49.99 49.99 49.99 49.99 50.00 50.00 50.00 50.00
2010). Therefore, we do not anticipate that the RMAQ algorithm can scale to very large MARL problems. In
the next subsection, we show RMAAC with function approximation can handle large-scale MARL problems.
6.2 Robust Multi-Agent Actor-Critic (RMAAC)
To answer the third question, we compare our RMAAC algorithm with two benchmark MARL algorithms:
MADDPG ( https://github.com/openai/maddpg ) (Lowe et al., 2017) which does not consider robustness,
and M3DDPG ( https://github.com/dadadidodi/m3ddpg ) (Li et al., 2019), a robust MARL algorithm
which considers uncertainties from opponents’ policies altering. M3DDPG utilizes adversarial learning to train
robust policies. We run experiments in several benchmark multi-agent scenarios, based on the multi-agent
particle environments (MPE) (Lowe et al., 2017). The hyper-parameters used to train RMAAC and the
baseline algorithms are summarized in Appendix C.2.2, Table 4.
Experiment procedure: We first train agents’ policies using RMAAC, MADDPG and M3DDPG,
respectively. For our RMAAC algorithm, we set the constraint parameter ϵ= 0.5. And we choose two types
of perturbation functions to validate the robustness of trained policies under different MG-SPA models. The
first one is the linear noise format that f1(s,b˜i) :=s+b˜i, i.e. the perturbed state ˜siis calculated by adding a
random noise b˜igenerated by adversary ˜ito the true state s. Andf2(s,b˜i) :=s+Gaussian (b˜i,Σ), where
the adversary ˜i’s actionb˜iis the mean of the Gaussian distribution. And Σis the covariance, we set it as I,
i.e. an identity matrix. We call it Gaussian noise format. These two formats f1,f2are commonly used in
adversarial training (Creswell et al., 2018; Zhang et al., 2020a; 2021). Then we test the well-trained policies in
the optimally disturbed environment (injected noise is produced by those adversaries trained with RMAAC
algorithm). The testing step is chosen as 10000and each episode contains 25steps. All hyperparameters used
in experiments for RMAAC, MADDPG and M3DDPG are attached in Appendix C.2.2. Note that since the
rewards are defined as negative values in the used multi-agent environments, we add the same baseline ( 100)
to rewards for making them positive. Then it’s easier to observe the testing results and make comparisons.
Those used MPE scenarios are Cooperative communication (CC), Cooperative navigation (CN), Physical
deception (PD), Predator prey (PP) and Keep away (KA). The first two scenarios are cooperative games, the
others are mixed games. To investigate the algorithm performance in more complicated situations, we also
run experiments in a scenario with more agents, which is called Predator prey+ (PP+). More details of these
games are in Appendix C.2.1.
Experiment results: In Figure 5 and Table 2, we report the mean episode testing rewards and variance of
10000steps testing rewards, respectively. We will use mean rewards and variance for short in the following
experimental report and explanations. In the table and figure, we use RM, M3, MA for abbreviations of
RMAAC, M3DDPG and MADDPG, respectively. In Figure 5, the left five figures are mean rewards under the
linear noise format f1, the right ones are under the Gaussian noise format f2. Under the optimally disturbed
environment, agents with RMAAC policies get the highest mean rewards in almost all scenarios no matter
what noise format is used. The only exception is in Keep away under linear noise. However, our RMAAC
still achieves the highest rewards when testing in Keep away under Gaussian noise. In Figure 7, we show the
comparison results in a complicated scenario with a larger number of agents and RMAAC policies are trained
Figure 5: RMAAC outperforms baseline MARL algorithms in terms of mean episode testing rewards under
different perturbation functions in most MPE scenarios.
14
Published in Transactions on Machine Learning Research (06/2023)
Figure 6: Comparison of mean episode testing rewards using different algorithms and different perturbation
functions, under cleaned environments.
with the Gaussian noise format f2. As we can see that the RMAAC policies get the highest reward when
testing under optimally perturbed environments, cleaned and randomly perturbed environments. Higher
rewards mean agents are performing better. It turns out RMAAC policies outperform the other two baseline
algorithms when there exist worst-case state uncertainties. In Table 2, the left three columns report the
variance under the linear noise format f1, and the right ones are under the Gaussian noise format f2. RM1
denotes our RMAAC policy trained with the linear noise format f1, RM2 denotes our RMAAC policy trained
with the Gaussian noise format f2. The variance is used to evaluate the stability of the trained policies, i.e.
the robustness to system randomness. Because the testing experiments are done in the same environments
that are initialized by different random seeds. We can see that, by using our RMAAC algorithm, the agents
can get the lowest variance in most scenarios under these two different perturbation formats. Therefore, our
RMAAC algorithm is also more robust to the system randomness, compared with the baselines. In summary,
our answer to the third question is ’Yes’.
Figure 7: RMAAC outperforms baseline MARL algorithms in
terms of mean episode testing rewards in complicated scenarios
with a larger number of agents of MPE.Interesting results when testing
under lighter perturbations and
cleaned environments: We also pro-
vide the testing results under a cleaned en-
vironment(accuratestateinformationcan
be attained) and a randomly disturbed
environment (injecting standard Gaussian
noise into agents’ observations). In Fig-
ures 6 and 8, we respectively show the
comparison of mean episode testing re-
wards under a cleaned environment and
a randomly disturbed environment by us-
ing 4 different methods: RM1 denotes
our RMAAC policy trained with the lin-
ear noise format f1, RM2 denotes our
RMAAC policy trained with the Gaussian
noise format f2, MA denotes MADDPG,
M3 denotes M3DDPG. We can see that
only in the Predator prey scenario, our method outperforms others under a cleaned environment. In Figure 8,
we can see that our method outperforms others in the Cooperative communication, Keep away and Predator
prey scenarios, and achieves similar performance as others in the Cooperative navigation scenario under a
randomly perturbed environment.
This kind of performance also happens in robust optimization (Beyer & Sendhoff, 2007; Boyd & Vandenberghe,
2004) and distributionally robust optimization (Delage & Ye, 2010; Rahimian & Mehrotra, 2019; Miao et al.,
2021; He et al., 2020; 2023) where the robust solutions outperform other non-robust solutions in the worst-case
15
Published in Transactions on Machine Learning Research (06/2023)
Figure 8: Comparison of mean episode testing rewards using different algorithms and different perturbation
functions, under randomly perturbed environments.
Table 2: Variance of Testing Rewards under optimal perturbed environment
Perturbation function Linear noise f1 Gaussian noise f2
Algorithms RM1 M3 MA RM2 M3 MA
Cooperative communication (CC) 1.007 1.311 1.292 0.872 1.012 0.976
Cooperative navigation (CN) 0.322 0.357 0.351 0.322 0.349 0.359
Physical deception (PD) 0.225 0.218 0.217 0.2440.161 0.252
Keep away (KA) 0.161 0.168 0.175 0.161 0.17 0.167
Predator prey (PP) 3.213 0.161 3.6712.304 2.711 2.811
scenario. Similarly, for single-agent RL with state perturbations, robust policies perform better compared
with baselines under state perturbations (Zhang et al., 2020b). However, there exists a trade-off between
optimizing the average performance and the worst-case performance for robust solutions in general, and
the robust solutions may get relatively poor performance compared with other non-robust solutions when
there is no uncertainty or perturbation in the environment even in a single agent RL problem (Zhang et al.,
2020b). Improving the robustness of the trained policy may sacrifice the performance of the decisions when
perturbations or uncertainties do not happen. That’s why our RMAAC policies only beat all baselines in
one scenario when the state uncertainty is eliminated. However, for many real-world systems, we can not
assume that agents always have accurate information about the states. Hence, improving the robustness of
the policies is very important for MARL as we explained in the introduction. It is worth noting that our
RMAAC policies also work well in environments with random perturbations instead of only the worst-case
perturbations. As shown in Fig. 8, the performance of our RMAAC policies outperforms the baselines in
most scenarios when random noise is introduced into the state.
More experimental results and explanations are provided in Appendix C.2.
Ablation study: We conducted ablation studies for RMAAC algorithm. We first study the performance
of RMAAC when it is used to train history-dependent policies. Other than using the current information
as the input of the policy neural network, we also use history information in the latest three time steps,
i.e.h= 4. In Table 3, we show the mean and variance of mean episode rewards in 10runs. We use the
same hyper-parameters in training history-dependent policies as training Markov policies. We can see that in
Table 3: Means and Variances of Mean Episode Rewards using Different Polices
Scenarios History-dependent policy Markov policy
Cooperative communication (CC) -52.83±1.51 -54.75 ±3.03
Cooperative navigation (CN) -208.19±1.68 -210.41 ±1.13
Physical deception (PD) 7.72±0.33 5.71 ±0.19
Keep away (KA) -20.69±0.09 -21.18 ±0.14
Predator prey (PP) 7.10±0.17 6.116 ±0.24
16
Published in Transactions on Machine Learning Research (06/2023)
all five scenarios, history-dependent policies outperform Markov policies. Besides, we investigate how the
robustness performance of RMAAC is affected by varying variances of Gaussian noise format Σand the
constraint parameter ϵ. We also investigate the performance of RMAAC under other types of attacks. Please
check the experimental setups and results of these ablation studies in Appendix C.3.
7 Discussion
Our proposed method provides a foundation for modeling robust agents’ interactions in multi-agent reinforce-
ment learning with state uncertainty, and training policies robust to state uncertainties in MARL. However,
there are still several urgent and promising problems to solve in this field.
First, exploring heterogeneous agent modeling is an important research direction in robust MARL. Training
a team of heterogeneous agents to learn robust control policies presents unique challenges, as agents may
have different capabilities, knowledge, and objectives that can lead to conflicts and coordination problems
(Lin et al., 2021). State uncertainty can exacerbate the impact of these differences, as agents may not be
able to accurately estimate the state of the environment or predict the behavior of other agents. All of these
factors make modeling heterogeneous agents in the presence of state uncertainty a challenging problem.
Second, investigating methods for handling continuous state and action spaces can benefit both general
MARL problems and our proposed method. While discretization is a commonly used approach for dealing
with continuous spaces, it is not always an optimal method for handling high-dimensional continuous spaces,
especially when state uncertainty is present. Adversarial state perturbation may disrupt the continuity on
the continuous state space, which can lead to difficulties in finding globally optimal solutions using general
discrete methods. This is because continuous spaces have infinite possible values, and discretization methods
may not be able to accurately represent the underlying continuous structure. When state perturbation occurs,
it may lead to more extreme values, which can result in the loss of important information. We will investigate
more on methods for continuous state and action space robust MARL in the future.
8 Conclusion
We study the problem of multi-agent reinforcement learning with state uncertainties in this work. We model
the problem as a Markov game with state perturbation adversaries (MG-SPA), where each agent aims to find
out a policy to maximize its own total discounted reward and each associated adversary aims to minimize that.
This problem is challenging with little prior work on theoretical analysis or algorithm design. We provide the
first attempt at theoretical analysis and algorithm design for MARL under worst-case state uncertainties. We
first introduce robust equilibrium as the solution concept for MG-SPA, and prove conditions under which such
an equilibrium exists. Then we propose a robust multi-agent Q-learning algorithm (RMAQ) to find such an
equilibrium, with convergence guarantees under certain conditions. We also derive the policy gradients and
design a robust multi-agent actor-critic (RMAAC) algorithm to handle the more general high-dimensional
state-action space MARL problems. We also conduct experiments that validate our methods.
Acknowledgments
Sihong He, Songyang Han, Sanbao Su and Fei Miao are supported by the National Science Foundation under
Grants CNS-1952096, CMMI-1932250, and CNS-2047354 grants. Shaofeng Zou is supported by the National
Science Foundation under Grants CCF-2106560, and CCF-2007783.
This material is based upon work supported under the AI Research Institutes program by National Science
Foundation and the Institute of Education Sciences, U.S. Department of Education through Award # 2229873
- National AI Institute for Exceptional Education. Any opinions, findings and conclusions or recommendations
expressed in this material are those of the author(s) and do not necessarily reflect the views of the National
Science Foundation, the Institute of Education Sciences, or the U.S. Department of Education.
17
Published in Transactions on Machine Learning Research (06/2023)
References
Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. Minimax regret bounds for reinforcement
learning. In International Conference on Machine Learning , pp. 263–272. PMLR, 2017.
Tamer Başar and Geert Jan Olsder. Dynamic noncooperative game theory . SIAM, 1998.
Hans-Georg Beyer and Bernhard Sendhoff. Robust optimization–a comprehensive survey. Computer methods
in applied mechanics and engineering , 196(33-34):3190–3218, 2007.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization . Cambridge University Press, USA, 2004.
ISBN 0521833787.
Jiří Čermák, Branislav Bošansky, and Viliam Lisy. An algorithm for constructing and solving imperfect recall
abstractions of large extensive-form games. In Proceedings of the 26th International Joint Conference on
Artificial Intelligence , pp. 936–942, 2017.
Nicolò Cesa-Bianchi, Claudio Gentile, Gábor Lugosi, and Gergely Neu. Boltzmann exploration done right.
Advances in neural information processing systems , 30, 2017.
Iadine Chades, Bruno Scherrer, and François Charpillet. A heuristic approach for solving decentralized-pomdp:
Assessment on the pursuit problem. In Proceedings of the 2002 ACM symposium on Applied computing , pp.
57–62, 2002.
Xi Chen, Xiaotie Deng, and Shang-Hua Teng. Settling the complexity of computing two-player nash equilibria.
Journal of the ACM (JACM) , 56(3):1–57, 2009.
Xinning Chen, Xuan Liu, Canhui Luo, and Jiangjin Yin. Robust multi-agent reinforcement learning for noisy
environments. Peer-to-Peer Networking and Applications , 15(2):1045–1056, 2022.
Vincent Conitzer and Tuomas Sandholm. Complexity results about nash equilibria. arXiv preprint cs/0205074 ,
2002.
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath.
Generative adversarial networks: An overview. IEEE Signal Processing Magazine , 35(1):53–65, 2018.
Constantinos Daskalakis, Paul W Goldberg, and Christos H Papadimitriou. The complexity of computing a
nash equilibrium. SIAM Journal on Computing , 39(1):195–259, 2009.
Erick Delage and Yinyu Ye. Distributionally robust optimization under moment uncertainty with application
to data-driven problems. Operations Research , 58(3):595–612, 2010. doi: 10.1287/opre.1090.0741.
Jay L Devore, Kenneth N Berk, and Matthew A Carlton. Modern mathematical statistics with applications ,
volume 285. Springer, 2012.
Rosemary Emery-Montemerlo, Geoff Gordon, Jeff Schneider, and Sebastian Thrun. Approximate solutions for
partially observable stochastic games with common payoffs. In Proceedings of the Third International Joint
Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004. , pp. 136–143. IEEE,
2004.
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad
Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted
actor-learner architectures. In ICML, pp. 1407–1416. PMLR, 2018.
Kousha Etessami and Mihalis Yannakakis. On the complexity of nash equilibria and other fixed points. SIAM
Journal on Computing , 39(6):2531–2597, 2010.
Michael Everett, Björn Lütjens, and Jonathan P How. Certifiable robustness to adversarial state uncertainty
in deep reinforcement learning. IEEE Transactions on Neural Networks and Learning Systems , 2021.
18
Published in Transactions on Machine Learning Research (06/2023)
Arlington M Fink. Equilibrium in a stochastic n-person game. Journal of science of the hiroshima university,
series ai (mathematics) , 28(1):89–93, 1964.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Coun-
terfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial intelligence ,
volume 32, 2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration.
InInternational conference on machine learning , pp. 2052–2062. PMLR, 2019.
Eduardo Rodrigues Gomes and Ryszard Kowalczyk. Dynamic analysis of multiagent q-learning with epsilon-
greedy exploration. In ICML’09: Proceedings of the 26th international Conference on Machine Learning ,
volume 47, 2009.
Eric A Hansen, Daniel S Bernstein, and Shlomo Zilberstein. Dynamic programming for partially observable
stochastic games. In AAAI, volume 4, pp. 709–715, 2004.
Sihong He, Lynn Pepin, Guang Wang, Desheng Zhang, and Fei Miao. Data-driven distributionally robust
electric vehicle balancing for mobility-on-demand systems under demand and supply uncertainties. In 2020
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pp. 2165–2172. IEEE,
2020.
Sihong He, Yue Wang, Shuo Han, Shaofeng Zou, and Fei Miao. A robust and constrained multi-agent
reinforcement learning framework for electric vehicle amod systems. arXiv preprint arXiv:2209.08230 ,
2022.
Sihong He, Zhili Zhang, Shuo Han, Lynn Pepin, Guang Wang, Desheng Zhang, John A Stankovic, and Fei
Miao. Data-driven distributionally robust electric vehicle balancing for autonomous mobility-on-demand
systems under demand and supply uncertainties. IEEE Transactions on Intelligent Transportation Systems ,
2023.
Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of machine
learning research , 4(Nov):1039–1069, 2003.
Yizheng Hu, Kun Shao, Dong Li, HAO Jianye, Wulong Liu, Yaodong Yang, Jun Wang, and Zhanxing Zhu.
Robust multi-agent reinforcement learning driven by correlated equilibrium. 2020.
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on
neural network policies. arXiv preprint arXiv:1702.02284 , 2017.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient?
Advances in neural information processing systems , 31, 2018.
Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially
observable stochastic domains. Artificial Intelligence , 101(1):99–134, 1998. ISSN 0004-3702. doi: https:
//doi.org/10.1016/S0004-3702(98)00023-X. URL https://www.sciencedirect.com/science/article/
pii/S000437029800023X .
Vijay Konda and John Tsitsiklis. Actor-critic algorithms. Advances in neural information processing systems ,
12, 1999.
Jernej Kos and Dawn Xiaodong Song. Delving into adversarial attacks on deep policies. ArXiv, abs/1705.06452,
2017.
Christian Kroer, Kevin Waugh, Fatma Kılınç-Karzan, and Tuomas Sandholm. Faster algorithms for extensive-
form game solving via improved smoothing functions. Mathematical Programming , 179(1):385–417, 2020.
Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Efficient mini-batch training for stochastic
optimization. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery
and data mining , pp. 661–670, 2014.
19
Published in Transactions on Machine Learning Research (06/2023)
Shihui Li, Yi Wu, Xinyue Cui, Honghua Dong, Fei Fang, and Stuart Russell. Robust multi-agent reinforcement
learning via minimax deep deterministic policy gradient. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 33, pp. 4213–4220, 2019.
Shiau Hong Lim and Arnaud Autef. Kernel-based reinforcement learning in robust markov decision processes.
InInternational Conference on Machine Learning , pp. 3973–3981. PMLR, 2019.
Chendi Lin, Wenhao Luo, and Katia Sycara. Online connectivity-aware dynamic deployment for heterogeneous
multi-robot systems. In 2021 IEEE International Conference on Robotics and Automation (ICRA) , pp.
8941–8947. IEEE, 2021.
Jieyu Lin, Kristina Dzeparoska, Sai Qian Zhang, Alberto Leon-Garcia, and Nicolas Papernot. On the
robustness of cooperative multi-agent reinforcement learning. In 2020 IEEE Security and Privacy Workshops
(SPW), pp. 62–68. IEEE, 2020.
Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun. Tactics of
adversarial attack on deep reinforcement learning agents. In Proceedings of the 26th International Joint
Conference on Artificial Intelligence , IJCAI’17, pp. 3756–3762. AAAI Press, 2017. ISBN 9780999241103.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In Machine
learning proceedings 1994 , pp. 157–163. Elsevier, 1994.
Michael L Littman and Csaba Szepesvári. A generalized reinforcement-learning model: Convergence and
applications. In ICML, volume 96, pp. 310–318. Citeseer, 1996.
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. Advances in neural information processing
systems, 30, 2017.
Roger McFarlane. A survey of exploration strategies in reinforcement learning. McGill University , 2018.
Richard D McKelvey and Andrew McLennan. Computation of equilibria in finite games. Handbook of
computational economics , 1:87–142, 1996.
Fei Miao, Sihong He, Lynn Pepin, Shuo Han, Abdeltawab Hendawi, Mohamed E Khalefa, John A Stankovic,
and George Pappas. Data-driven distributionally robust optimization for vehicle balancing of mobility-on-
demand systems. ACM Transactions on Cyber-Physical Systems , 5(2):1–27, 2021.
Volodymyr Mnih, Koray Kavukcuoglu, et al. Human-level control through deep reinforcement learning.
nature, 518(7540):529–533, 2015.
Facundo Mémoli. Some properties of gromov–hausdorff distances. Discrete & Computational Geometry , pp.
1–25, 2012. ISSN 0179-5376. URL http://dx.doi.org/10.1007/s00454-012-9406-8 . 10.1007/s00454-
012-9406-8.
R Nair, M Tambe, M Yokoo, D Pynadath, and S Marsella. Towards computing optimal policies for
decentralized pomdps. In Notes of the 2002 AAAI Workshop on Game Theoretic and Decision Theoretic
Agents, 2002.
John Nash. Non-cooperative games. Annals of mathematics , pp. 286–295, 1951.
Eleni Nisioti, Daan Bloembergen, and Michael Kaisers. Robust multi-agent q-learning in cooperative games
with adversaries. In Proceedings of the AAAI Conference on Artificial Intelligence , 2021.
Frans A Oliehoek, Christopher Amato, et al. A concise introduction to decentralized POMDPs , volume 1.
Springer, 2016.
Martin J Osborne and Ariel Rubinstein. A course in game theory . MIT press, 1994.
Guillermo Owen. Game theory . Emerald Group Publishing, 2013.
20
Published in Transactions on Machine Learning Research (06/2023)
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley &
Sons, 2014.
Guannan Qu and Adam Wierman. Finite-time analysis of asynchronous stochastic approximation and
q-learning. In Conference on Learning Theory , pp. 3185–3205. PMLR, 2020.
Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv preprint
arXiv:1908.05659 , 2019.
Daniel J Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, Zheng Wen, et al. A tutorial on thompson
sampling. Foundations and Trends ®in Machine Learning , 11(1):1–96, 2018.
Burkhard C Schipper. Kuhn’s theorem for extensive games with unawareness. Available at SSRN 3063853 ,
2017.
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences , 39(10):1095–1100, 1953.
Macheng Shen and Jonathan P How. Robust opponent modeling via adversarial ensemble reinforcement
learning. In Proceedings of the International Conference on Automated Planning and Scheduling , volume 31,
pp. 578–587, 2021.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas
Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human
knowledge. nature, 550(7676):354–359, 2017.
Aman Sinha, Matthew O’Kelly, et al. Formulazero: Distributionally robust online adaptation via offline
population synthesis. In ICML, pp. 8992–9004. PMLR, 2020.
B Slantchev. Game theory: Perfect equilibria in extensive form games. UCSD script , 2008.
David Roger Smart. Fixed point theorems , volume 66. Cup Archive, 1980.
Houshang H Sohrab. Basic real analysis , volume 231. Springer, 2003.
Chuangchuang Sun, Dong-Ki Kim, and Jonathan P How. Romax: Certifiably robust deep multiagent
reinforcement learning via convex relaxation. arXiv preprint arXiv:2109.06795 , 2021.
Richard S Sutton and Andrew G Barto. Reinforcement learning: an introduction mit press. Cambridge, MA ,
22447, 1998.
Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning , volume 135. MIT press
Cambridge, 1998.
Csaba Szepesvári and Michael L Littman. A unified analysis of value-function-based reinforcement-learning
algorithms. Neural computation , 11(8):2017–2060, 1999.
Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applications in
continuous control. In International Conference on Machine Learning , pp. 6215–6224. PMLR, 2019.
Tessa van der Heiden, C Salge, Efstratios Gavves, and H van Hoof. Robust multi-agent reinforcement learning
with social empowerment for coordination and communication. arXiv preprint arXiv:2012.08255 , 2020.
John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior. In Theory of games
and economic behavior . Princeton university press, 2007.
Yue Wang and Shaofeng Zou. Online robust reinforcement learning with model uncertainty. Advances in
Neural Information Processing Systems , 34:7193–7206, 2021.
Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical
perspective. ArXiv, abs/2011.00583, 2020a.
21
Published in Transactions on Machine Learning Research (06/2023)
Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical
perspective. arXiv preprint arXiv:2011.00583 , 2020b.
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-agent
reinforcement learning. In International conference on machine learning , pp. 5571–5580. PMLR, 2018.
Zhengyu Yin, Dmytro Korzhyk, Christopher Kiekintveld, Vincent Conitzer, and Milind Tambe. Stackelberg
vs. nash in security games: interchangeability, equivalence, and uniqueness. In AAMAS, volume 10, pp. 6,
2010.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness
of ppo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955 , 2021a.
Jing Yu, Clement Gehring, Florian Schäfer, and Animashree Anandkumar. Robust reinforcement learning:
A constrained game-theoretic approach. In Learning for Dynamics and Control , pp. 1242–1254. PMLR,
2021b.
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh. Robust
deep reinforcement learning against adversarial perturbations on state observations. Advances in Neural
Information Processing Systems , 33:21024–21037, 2020a.
Huan Zhang, Hongge Chen, Duane Boning, and Cho-Jui Hsieh. Robust reinforcement learning on state
observations with learned optimal adversary. arXiv preprint arXiv:2101.08452 , 2021.
Kaiqing Zhang, Tao Sun, Yunzhe Tao, Sahika Genc, Sunil Mallya, and Tamer Basar. Robust multi-agent
reinforcement learning with model uncertainty. Advances in Neural Information Processing Systems , 33:
10571–10583, 2020b.
22
Published in Transactions on Machine Learning Research (06/2023)
Appendix for “Robust Multi-Agent Reinforcement
Learning with State Uncertainty”
There are three sections in the appendix: section A for theoretical proof, section B for algorithms, and section
C for experiments.
A Theory
In this section, we give the full proof of all propositions and theorems in the theoretical analysis of an
MG-SPA.
In section A.1, we construct an extensive-form game (EFG) (Başar & Olsder, 1998; Osborne & Rubinstein,
1994; Von Neumann & Morgenstern, 2007) whose payoff function is related to value functions of an MG-SPA.
We then give certain conditions under which, a Nash equilibrium for the constructed EFG exists. In section
A.2, we prove the propositions 4.5 and 4.6. In section A.3, we give the full proof of Theorem 4.7. In section
A.4, we prove Corollary 4.9.1 that Theorem 4.7 applies to history-dependent-policy-based RE as well.
To make the appendix self-contained, we re-show the vector notations and assumptions we have presented in
section 4.2. Readers can also skipthe repeated text and directly go to section A.1.
We follow and extend the vector notations in Puterman (2014). Let Vdenote the set of bounded real valued
functions on Swith component-wise partial order and norm ∥vi∥:=sups∈S|vi(s)|. LetVMdenote the
subspace of Vof Borel measurable functions. For discrete state space, all real-valued functions are measurable
so thatV=VM. But when Sis a continuum, VMis a proper subset of V. Letv= (v1,···,vN)∈Vbe
the set of bounded real valued functions on S×···×S, i.e. the across product of Nstate set and norm
∥v∥:= supj∥vj∥. We also define the set QandQin a similar style such that qi∈Q,q∈Q.
For discrete S, let|S|denote the number of elements in S. Letridenote a|S|-vector, with sth component
ri(s)which is the expected reward for agent iunder state s. AndPthe|S|×|S|matrix with (s,s′)th
entry given by p(s′|s). We refer to ri
das the reward vector of agent i, andPdas the probability transition
matrix corresponding to a joint policy d= (π,ρ).ri
d+γPdviis the expected total one-period discounted
reward of agent i, obtained using the joint policy d= (π,ρ). Letzas a list of joint policy {d1,d2,···}and
P0
z=I, we denote the expected total discounted reward of agent iusingzasvi
z=/summationtext∞
t=1γt−1Pt−1
zri
dt=
ri
d1+γPd1ri
d2+···+γn−1Pd1···Pdn−1ri
tn+···. Now, we define the following minimax operator which is
used in the rest of the paper.
Definition A.1 (Minimax Operator, same as definition 4.3) .
Forvi∈V,s∈S, we define the nonlinear operator Lionvi(s)byLivi(s) := maxπiminρ˜i[ri
d+
γPdvi](s), whered:= (π−i
∗,πi,ρ−˜i
∗,ρ˜i). We also define the operator Lv(s) =L(v1(s),···,vN(s)) =
(L1v1(s),···,LNvN(s)). ThenLiviis a|S|-vector, with sth component Livi(s).
For discrete Sand bounded ri, it follows from Lemma 5.6.1 in Puterman (2014) that Livi∈Vfor allvi∈V.
ThereforeLv∈Vfor allv∈V. And in this paper, we consider the following assumptions in Markov games
with state perturbation adversaries.
Assumption A.2 (Same as assumption 4.4) .
(1) Bounded rewards; |ri(s,a,b )|≤Mi<M <∞for alli∈N,a∈A,b∈Bands∈S.
(2) Finite state and action spaces: all S,Ai,B˜iare finite.
(3) Stationary transition probability and reward functions.
(4)f(s,·)is a bijection for any fixed s∈S.
(5) All agents share one common reward function.
23
Published in Transactions on Machine Learning Research (06/2023)
A.1 Extensive-form game
Figure 9: a team extensive-form game
An extensive-form game (EFG) (Başar & Olsder, 1998; Osborne & Rubinstein, 1994; Von Neumann &
Morgenstern, 2007) basically involves a tree structure with several nodes and branches, providing an explicit
description of the order of players and the information available to each player at the time of his decision.
Look at Figure 9, an EFG involves from the top of the tree to the tip of one of its branches. And a centralized
nature player ( P1) has|˜S|alternatives (branches) to choose from, whereas a centralized agent ( P2) has|A|
alternatives, and the order of play is that the centralized nature player acts before the centralized agent
does. The set Ais the same as the agents’ joint action set in an MG-SPA, set ˜Sis a set of perturbed states
constrained by a constrained parameter ϵ. At the end of lower branches, some numbers will be given. These
numbers represent the playoffs to the centralized agent (or equivalently, losses incurred to the centralized
nature player) if the corresponding paths are selected by the players. We give the formal definition of an
EFG we will use in the proof and the main text as follows:
Definition A.3. An extensive-form game based on (v1,···,vN,−v1,···,−vN)unders∈Sis a finite
tree structure with:
1.A playerP1has a action set ˜S=N/bracehtipdownleft/bracehtipupright/bracehtipupleft/bracehtipdownright
B(ϵ,s)×···×B (ϵ,s), with a typical element designed as ˜s.
AndP1moves first.
2.Another player P2has an action set A, with a typical element designed as a. AndP2which
moves after P1.
3. A specific vertex indicating the starting point of the game.
4.A payoff function gs(˜s,a) = (g1
s(˜s,a),···,gN
s(˜s,a))wheregi
s(˜s,a) =ri(s,a,f−1
s(˜s)) +/summationtext
s′p(s′|a,f−1
s(˜s))vi(s′)assigns a real number to each terminal vector of the tree. Player
P1gets−gs(˜s,a)while player P2getsgs(˜s,a).
5.A partition of the nodes of the tree into two player sets (to be denoted by ¯N1and ¯N2forP1
andP2, respectively).
6.A sub-partition of each player set ¯Niinto information set {ηi
j}, such that the same number of
immediate branches emanates from every node belonging to the same information set, and no
node follows another node in the same information set.
Note thatfs(b) :=f(s,b) = (f(s,b˜1),···,f(s,b˜N))is the vector version of the perturbation function f
in an MG-SPA. Since in an MG-SPA, qi(s,a,b ) =ri(s,a,b ) +/summationtext
s′p(s′|s,a,b )vi(s′)for alli= 1,···,N,
gi
s(˜s,a) =qi(s,a,f−1
s(˜s))as well. We can also use (q1,···,qN,−q1,···,−qN)to denote an extensive-
24
Published in Transactions on Machine Learning Research (06/2023)
form game based on (v1,···,vN,−v1,···,−vN). Then we define the behavioral strategies for P1andP2,
respectively in the following definition.
Definition A.4. (Behavioral strategy) Let Iidenote the class of all information sets of Pi, with a
typical element designed as ηi. LetUi
ηidenote the set of alternatives of Piat the nodes belonging to
the information set ηi. DefineUi=∪Ui
ηiwhere the union is over ηi∈Ii. LetYη1denote the set of
all probability distributions on U1
η1, where the latter is the set of all alternatives of P1at the nodes
belonging to the information set η1. Analogously, let Zη2denote the set of all probability distributions
onU2
η2. Further define Y=∪I1Yη1,Z=∪I2Zη2. Then, a behavioral strategy λforP1is a mapping
from the class of all his information sets I1intoY, assigning one element in Yfor each set in I1, such
thatλ(η1)∈Yη1for eachη1∈I1. A typical behavioral strategy χforP2is defined, analogously, as a
restricted mapping from I2intoZ. The set of all behavioral strategies for Piis called his behavioral
strategy set, and it is denoted by Γi.
The information available to the centralized agent ( P2) at the time of his play is indicated on the tree diagram
in Figure 9 by dotted lines enclosing an area (i.e. the information set) including the relevant nodes. This
means the centralized agent is in a position to know exactly how the centralized nature player acts. In this
case, a strategy for the centralized agent is a mapping from the collection of his information sets into the set
of his actions.
And the behavioral strategy λforP1is a mapping from his information sets and action space into a probability
simplex, i.e. λ(˜s|s)is the probability of choosing ˜sgivens. Similarly, the behavioral strategy χforP2is
χ(a|˜s), i.e. the probability of choosing action awhen ˜sis given. Note that every behavioral strategy is a
mixed strategy. We then give the definition of Nash equilibrium in behavioral strategies for an EFG.
Definition A.5. (Nash equilibrium in behavioral strategies) A pair of strategies {λ∗∈Γ1,χ∗∈Γ2}is
said to constitute a Nash equilibrium in behavioral strategies if the following inequalities are satisfied that
for alli= 1,···,N,λ∈Γ1,χ∈Γ2,s∈S:
Ji(λi,λ−i
∗,χi
∗,χ−i
∗)≥Ji(λi
∗,λ−i
∗,χi
∗,χ−i
∗)≥Ji(λi
∗,λ−i
∗,χi,χ−i
∗) (11)
whereJi(λ,χ)is the expected payoff i.e. Eλ,χ[gi
s]whenP1takesλ,P2takesχ,λ(˜s|s) =/producttextN
i=1λi(˜si|s),
χ(a|˜s) =/producttextN
i=1χi(ai|˜si).
In the following parts as well as the main text, when we mention a Nash equilibrium for an EFG, it refers
to a Nash equilibrium in behavioral strategies. How to solve an EFG is out of our scope since it has been
investigated in much literature (Başar & Olsder, 1998; Schipper, 2017; Slantchev, 2008). And policies λi
andχican be attained through the marginal probabilities calculation with chain rules (Devore et al., 2012;
Mémoli, 2012).
A.1.1 Existence of NE for an EFG
In Lemma A.6, we give conditions (partial items of Assumption 4.4) under which an NE of the EFG based
on(v1,···,vN,−v1,···,−vN)exists.
Lemma A.6. Supposev1=···=vN, andS,Aare finite. An NE (λ∗,χ∗)of the EFG based on
(v1,···,vN,−v1,···,−vN)exists.
Proof.Since ˜Sis a subset of S,˜Sis finite when Sis finite. When v1=···=vN, and ˜S,Aare finite, an
EFG based on (v1,···,vN,−v1,···,−vN)degenerates to a zero-sum two-person extensive-form game with
finite strategies and perfect recall. Thus, an NE of this EFG exists (Başar & Olsder, 1998; Schipper, 2017;
Slantchev, 2008).
The following Lemma A.7 provides insights into solving an MG-SPA by solving a constructed EFG.
25
Published in Transactions on Machine Learning Research (06/2023)
Lemma A.7. Supposefis a bijection when sis fixed and an NE (λ∗,χ∗)exists for an EFG
(v1,···,vN,−v1,···,−vN). We define a joint policy (πv
∗,ρv
∗)as the joint policy implied from the
NE(λ∗,χ∗), whereρv
∗(b|s) =λ∗(˜s=fs(b)|s),πv
∗(a|˜s=fs(b)) =χ∗(a|˜s). Then the joint policy (πv
∗,ρv
∗)
satisfiesLivi(s) =ri
(πv∗,ρv∗)(s) +γ/summationtext
s′∈Sp(πv∗,ρv∗)(s′|s)vi(s′)for alls∈S.
Proof.The NE of the extensive-form game (λ∗,χ∗)implies that for all i= 1,···,N,s∈S,λ∈Γ1,χ∈Γ2,
we have
Ji(λ,χ∗)≥Ji(λ∗,χ∗)≥Ji(λ∗,χ),
whereJi(λ,χ) =E[ri(s,a,f−1
s(˜s)) +/summationtext
s′p(s′|s,a,f−1
s(˜s))vi(s′)|˜s∼λ(·|s),a∼χ(·|˜s)]according to Definition
A.5. Letbdenotef−1
s(˜s), becausefis a bijection when sis fixed,fs(b) = (fs(b˜1),···,fs(b˜N))is a bijection,
and the inverse function f−1
s(˜s) = (f−1
s(˜s1),···,f−1
s(˜sN))exists and is a bijection as well, then we have
Ji(λ∗,χ∗) =E/bracketleftigg
ri(s,a,f−1
s(˜s)) +/summationdisplay
s′p(s′|s,a,f−1
s(˜s))vi(s′)|˜s∼λ∗(·|s),a∼χ∗(·|˜s)/bracketrightigg
=E/bracketleftigg
ri(s,a,b ) +/summationdisplay
s′p(s′|s,a,b )vi(s′)|b∼λ∗(fs(b)|s),a∼χ∗(·|fs(b))/bracketrightigg
=E/bracketleftigg
ri(s,a,b ) +/summationdisplay
s′p(s′|s,a,b )vi(s′)|b∼ρv
∗(·|s),a∼πv
∗(·|˜s)/bracketrightigg
Similarly, we have
Ji(λ∗,χ) =E/bracketleftigg
ri(s,a,b ) +/summationdisplay
s′p(s′|s,a,b )vi(s′)|b∼ρv
∗(·|s),a∼πv(·|˜s)/bracketrightigg
,
Ji(λ,χ∗) =E/bracketleftigg
ri(s,a,b ) +/summationdisplay
s′p(s′|s,a,b )vi(s′)|b∼ρv(·|s),a∼πv
∗(·|˜s)/bracketrightigg
,
whereπv,ρvare corresponding policies implied from behavioral strategies χ,λ, respectively. Recall the
definition of the minimax operator of Livi(s), we have, for all s∈S,
Livi(s) =ri
(πv∗,ρv∗)(s) +γ/summationdisplay
s′∈Sp(πv∗,ρv∗)(s′|s)vi(s′)
Based on the proof, we also denote (πv
∗,ρv
∗)as an NE policy for the EFG (v1,···,vN,−v1,···,−vN)for
convenience, instead of calling it the joint policy derived from an NE for the EFG (v1,···,vN,−v1,···,−vN).
We can get a corollary from Lemma A.7 that when optimal value functions of an MG-SPA exist, we are
able to get a joint policy that satisfies the Bellman equations of the MG-SPA by computing an NE for a
constructed EFG (v1
∗,···,vN
∗,−v1
∗,···,−vN
∗). Later in Theorem 4.7, we show that a joint policy that satisfies
the Bellman equations of an MG-SPA is in a robust equilibrium under Assumption 4.4. Therefore, Lemma
A.7 provides insights into solving an MG-SPA by solving a corresponding EFG.
In the following proof, we aim to prove the existence of optimal value functions for an MG-SPA under
Assumption 4.4.
26
Published in Transactions on Machine Learning Research (06/2023)
A.2 Proof of two propositions
Proposition A.8 (Contraction mapping, same as proposition 4.5 in the main text.) .Suppose 0≤γ <1
and Assumption 4.4 hold. Then Lis a contraction mapping on V.
Proof.Letuandvbe in V. Given Assumption 4.4, these two EFGs (u1,···,uN,−u1,···,−uN),
(vi,···,vN,−vi,···,−vN)both have at least one mixed Nash equilibrium according to Lemma A.6. And let
(πu
∗,ρu
∗)and(πv
∗,ρv
∗)be two Nash equilibriums for these two games, respectively. According to Lemma A.7,
we have the following equations hold for all s∈S,
Livi(s) =ri
(πv∗,ρv∗)(s) +γ/summationdisplay
s′∈Sp(πv∗,ρv∗)(s′|s)vi(s′)
Liui(s) =ri
(πu∗,ρu∗)(s) +γ/summationdisplay
s′∈Sp(πu∗,ρu∗)(s′|s)ui(s′)
Then we have
ri
(πu∗,ρv∗)(s) +γ/summationdisplay
s′∈Sp(πu∗,ρv∗)(s′|s)vi(s′)≤Livi(s)≤ri
(πv∗,ρu∗)(s) +γ/summationdisplay
s′∈Sp(πv∗,ρu∗)(s′|s)vi(s′),
ri
(πv∗,ρu∗)(s) +γ/summationdisplay
s′∈Sp(πv∗,ρu∗)(s′|s)ui(s′)≤Liui(s)≤ri
(πu∗,ρv∗)(s) +γ/summationdisplay
s′∈Sp(πu∗,ρv∗)(s′|s)ui(s′),
since (πu
∗,ρv
∗)and(πv
∗,ρu
∗)are derived from the Nash equilibrium of the EFG (vi,···,vN,−vi,···,−vN),
and(πu
∗,ρv
∗)and(πv
∗,ρu
∗)are also derived from the Nash equilibrium of the EFG (ui,···,uN,−ui,···,−uN).
We assume that Livi(s)≤Liui(s), then we have
0≤Liui(s)−Livi(s)
≤/bracketleftigg
ri
(πu∗,ρv∗)(s) +γ/summationdisplay
s′∈Sp(πu∗,ρv∗)(s′|s)ui(s′)/bracketrightigg
−/bracketleftigg
ri
(πu∗,ρv∗)(s) +γ/summationdisplay
s′∈Sp(πu∗,ρv∗)(s′|s)vi(s′)/bracketrightigg
≤γ/summationdisplay
s′∈Sp(πu∗,ρv∗)(s′|s)(ui(s′)−vi(s′))
≤γ||vi−ui||.
Repeating this argument in the case that Liui(s)≤Livi(s)implies that
||Livi(s)−Liui(s)||≤γ||vi−ui||
for alls∈S, i.e.Liis a contraction mapping on V. Recall that||v||= supj||vj||, then we have
||Lv−Lu||= sup
j||Ljvj−Ljuj||≤γsup
j||vj−uj||=γ||v−u||.
Lis a contraction mapping on V.
Proposition A.9 (Complete Space, same as proposition 4.6 in the main text.) .Vis a complete normed
linear space.
Proof.Recall that Vdenote the set of bounded real-valued functions on S×···×S, i.e. the cross product of
Nstate set with component-wise partial order and norm ||v||:=sups∈Ssupj|vi(s)|. Since Vis closed under
addition and scalar multiplication and is endowed with a norm, it is a normed linear space. Since every
Cauchy sequence contains a limit point in V,Vis a complete space.
27
Published in Transactions on Machine Learning Research (06/2023)
A.3 Proof of Theorem 4.7
In this section, our goal is to prove Theorem 4.7. We first prove (1) the optimal value function of an MG-SPA
satisfies the Bellman equation by applying the Squeeze theorem [Theorem 3.3.6, Sohrab (2003)] in A.3.1.
Then we prove that a unique solution of the Bellman equation exists using fixed-point theorem (Smart,
1980) in A.3.2. Thereby, the existence of the optimal value function gets proved. By introducing (3), we
characterize the relationship between the optimal value function and a robust equilibrium. The proof of
(3) can be found in A.3.3. However, (3) does not imply the existence of an RE. To this end, in (4), we
formally establish the existence of RE when the optimal value function exists. We formulate a 2N-player
Extensive-form game (EFG) (Osborne & Rubinstein, 1994; Von Neumann & Morgenstern, 2007) based on
the optimal value function such that its Nash equilibrium (NE) is equivalent to an RE of the MG-SPA. The
details are in A.3.4.
Theorem A.10 (Same as theorem 4.7 in the main text) .
Suppose 0≤γ <1and Assumption 4.4 holds.
(1) (Solution of Bellman equation) A value function v∗∈Vis an optimal value function if for all i∈N,
the point-wise value function vi
∗∈Vsatisfies the corresponding Bellman Equation (6), i.e.v∗=Lv∗.
(2) (Existence and uniqueness of optimal value function) There exists a unique v∗∈VsatisfyingLv∗=v∗,
i.e. for all i∈N,Livi
∗=vi
∗.
(3) (Robust equilibrium and optimal value function) A joint policy d∗= (π∗,ρ∗), whereπ∗= (π1
∗,···,πN
∗)
andρ∗= (ρ˜1
∗,···,ρ˜N
∗), is a robust equilibrium if and only if vd∗is the optimal value function.
(4) (Existence of robust equilibrium) There exists a mixed RE for an MG-SPA.
A.3.1 (1) Solution of Bellman equation
Proof.First, we prove that if there exists a vi∈Vsuch thatvi≥Lvithenvi≥vi
∗.vi≥Lviimplies
vi≥max min [ri+γPvi] =ri
d+γPdvi, whered= (πv,−i
∗,πv,i
∗,ρv,−˜i
∗,ρv,˜i
∗)is a Nash equilibrium for the EFG
v= (v1,···,vN,−v1,···,−vN). We omit the superscript vfor convenience when there is no confusion. We
choose a list of policy i.e. z= (d1,d2,···)wheredj= (π−i
∗,πi
j,ρ−˜i
∗,ρ˜i∗). Then we have
vi≥rd1+γPd1vi≥ri
d1+γPd1(ri
d2+γPd2vi) =ri
d1+γPd1ri
d2+γPd1Pd2vi
By induction, it follows that, for n≥1,
vi≥ri
d1+γPd1ri
d2+···+γn−1Pd1···Pdn−1ri
dn+γnPn
zvi
vi−vi
z≥γnPn
zvi−∞/summationdisplay
t=nγtPt
zri
dt+1(12)
Since||γnPn
zvi||≤γn||vi||andγ∈[0,1), forϵ>0, we can find a sufficiently large nsuch that
ϵe/2≥γnPn
zvi≥−ϵe/2 (13)
whereedenotes a vector of 1’s. And as a result of Assumption 4.4-(1), we have
−∞/summationdisplay
t=nγtPt
zri
dt+1≥−γnMe
1−γ(14)
Then we have
vi(s)−vi
z(s)≥−ϵ (15)
28
Published in Transactions on Machine Learning Research (06/2023)
for alls∈Sandϵ>0. Let alldjthe same, since ϵwas arbitrary, we have
vi(s)≥max
πimin
ρ˜ivi
z(s) =vi
∗(s) (16)
Then we prove that if there exists a vi∈Vsuch thatvi≤Lvithenvi≤vi
∗. For arbitrary ϵ>0there exists
a joint policy d′= (π−i
∗,πi
∗,ρ−˜i
∗,ρ˜i)and a list of policy z= (d′,d′,···)such that
vi≤ri
d′+γPd′vi+ϵ
(I−γPd′)vi≤ri
d′+ϵ
≤(I−γPd′)−1ri
d′+ (1−γ)−1ϵe=vi
z+ (1−γ)−1ϵe
≤vi
∗+ (1−γ)−1ϵe
The equality holds because the Theorem 6.1.1 in Puterman (2014). Since ϵwas arbitrary, we have
vi≤vi
∗ (17)
So if there exists a vi∈Vsuch thatvi=Livii.e.vi≤Liviandvi≥Livi, we havevi=vi
∗, i.e. ifvisatisfies
the Bellman equation, viis an optimal value function.
A.3.2 (2) Existence of optimal value function
Proof.Proposition 4.5 and 4.6 establish that Vis a complete normed linear space and Lis a contraction
mapping, so that the hypothesis of Banach Fixed-Point Theorem are satisfied (Smart, 1980). Therefore there
exists a unique solution v∗∈VtoLv=v. From (1), we know if v∗satisfies the Bellman equation, it is an
optimal value function. Therefore, the existence of the optimal value function is proved.
A.3.3 (3) robust equilibrium and optimal value function
Proof.(i) robust equilibrium →Optimal value function.
Supposed∗is a robust equilibrium. Then vd∗=v∗. From (2), it follows that vd∗satisfiesLv=v. Thusvd∗
is the optimal value function.
(ii) Optimal value function →robust equilibrium.
Supposevd∗is the optimal value function, i.e., Lvd∗=vd∗. The proof of (1) implies that vd∗=v∗, sod∗is
in robust equilibrium.
A.3.4 (4) Existence of robust equilibrium
Proof.From (2), we know that there exists a solution v∗∈Vto the Bellman equation Lv=v. Now, we
consider an EFG based on (v1
∗,···,vN
∗,−v1
∗,···,−vN
∗). Under Assumption 4.4, we can get an NE policy
(πv∗∗,ρv∗∗)by solving the EFG as a consequence of Lemma A.6. According to Lemma A.7, (πv∗∗,ρv∗∗)satisfies
Livi
∗(s) =ri
(πv∗∗,ρv∗∗)(s) +γ/summationdisplay
s′∈Sp(πv∗∗,ρv∗∗)(s′|s)vi
∗(s′),
for alls∈S. According to (3), (πv∗∗,ρv∗∗)is a robust equilibrium.
29
Published in Transactions on Machine Learning Research (06/2023)
A.4 Proof of Corollary 4.9.1
Corollary A.10.1 (Same as Corollary 4.9.1) .
Theorem 4.7 still holds when all agents and adversaries in an MG-SPA use history-dependent policies
with a finite time horizon.
Proof.From subsection 4.3, we can find the main difference between history-dependent-policy-based RE and
Markov-policy-based RE are the definitions and notations of policies and states. To prove Theorem 4.7, we
construct an EFG based on the current state st. Similarly, to prove Corollary 4.9.1, we construct an EFG
based on the concatenated states sh,tand˜sh−1,t−1that includes the current state stand historical state
information st−1,···,st−h+1,˜st−1,···,˜st−h+1. Notice that his a finite number. Hence, the concatenated
state space is still finite. We now construct another extensive-form game in which a centralized nature player
(P1) has|˜S|alternatives (branches) to choose from, whereas a centralized agent ( P2) has|A|alternatives,
and the order of play is that the centralized nature player acts before the centralized agent does. The set A
is the same as the agents’ joint action set in an MG-SPA, set ˜Sis a set of perturbed states constrained by a
constrained parameter ϵ.
Definition A.11. An extensive-form game based on (v1,···,vN,−v1,···,−vN)under concatenated
statessh,t= (st,···,st−h+1)∈Shand˜sh,t−1= (˜st−1,···,˜st−h+1)∈Sh−1is a finite tree structure with:
1.A playerP1has a action set ˜S=N/bracehtipdownleft/bracehtipupright/bracehtipupleft/bracehtipdownright
B(ϵ,s)×···×B (ϵ,s), with a typical element designed as ˜s.
AndP1moves first.
2.Another player P2has an action set A, with a typical element designed as a. AndP2which
moves after P1.
3. A specific vertex indicating the starting point of the game.
4.A payoff function gs(˜s,a) = (g1
s(˜s,a),···,gN
s(˜s,a))wheres=st=sh,t[1]is the first element of
sh,t,˜s=˜st∈˜S,gi
s(˜s,a) =ri(s,a,f−1
s(˜s)) +/summationtext
s′p(s′|a,f−1
s(˜s))vi(s′)assigns a real number to
each terminal vector of the tree. Player P1gets−gs(˜s,a)while player P2getsgs(˜s,a).
5.A partition of the nodes of the tree into two player sets (to be denoted by ¯N1and ¯N2forP1
andP2, respectively).
6.A sub-partition of each player set ¯Niinto information set {ηi
j}, such that the same number of
immediate branches emanates from every node belonging to the same information set, and no
node follows another node in the same information set.
The definitions of behavioral strategy and Nash equilibrium keep the same. Then the behavioral strategy λ
forP1is a mapping from his information sets and action space into a probability simplex, i.e. λ(˜s|sh,t=
(st,···,st−h+1))is the probability of choosing ˜sgivensh,t. Similarly, the behavioral strategy χforP2is
χ(a|˜sh,t= (˜s,˜st−1,···,˜st−h+1)), i.e. the probability of choosing action awhen ˜sh,tis given.
Now let us check the correctness of Lemma A.6 when EFG is constructed following Definition A.11. We can
find thatShis a finite state space for any finite time horizon hsinceShis a product topology on finite spaces.
Then Lemma A.6 still holds because the EFG degenerates to a zero-sum two-person extensive-form game
with finite strategies and perfect recall.
Then let us check Lemma A.7. We re-write it in Lemma A.12 in which the behavioral strategy χ(a|˜sh,t)and
λ(˜s|sh,t)are used. The proof of Lemma A.12 is similar to that of Lemma A.7.
30
Published in Transactions on Machine Learning Research (06/2023)
Lemma A.12. Supposefis a bijection when sis fixed and an NE (λ∗,χ∗)exists for an EFG
(v1,···,vN,−v1,···,−vN). We define a joint policy (πv
∗,ρv
∗)as the joint policy implied from the
NE(λ∗,χ∗), whereρv
∗(b|sh) =λ∗(˜s=fs(b)|sh),πv
∗(a|˜sh= (fs(b),˜st−1,···,˜st−h+1)) =χ∗(a|˜sh). Then
the joint policy (πv
∗,ρv
∗)satisfiesLivi(s) =ri
(πv∗,ρv∗)(s) +γ/summationtext
s′∈Sp(πv∗,ρv∗)(s′|s)vi(s′)for alls∈S.
Proof.The NE of the extensive-form game (λ∗,χ∗)implies that for all i= 1,···,N,s∈S,λ∈Γ1,χ∈Γ2,
we have
Ji(λ,χ∗)≥Ji(λ∗,χ∗)≥Ji(λ∗,χ),
whereJi(λ,χ) =E[ri(s,a,f−1
s(˜s)) +/summationtext
s′p(s′|s,a,f−1
s(˜s))vi(s′)|˜s∼λ(·|sh),a∼χ(·|˜sh)]according to Defi-
nition A.5. Let bdenotef−1
s(˜s), becausefis a bijection when sis fixed,fs(b) = (fs(b˜1),···,fs(b˜N))is a
bijection, and the inverse function f−1
s(˜s) = (f−1
s(˜s1),···,f−1
s(˜sN))exists and is a bijection as well, then we
have
Ji(λ∗,χ∗) =E/bracketleftigg
ri(s,a,f−1
s(˜s)) +/summationdisplay
s′p(s′|s,a,f−1
s(˜s))vi(s′)|˜s∼λ∗(·|sh),a∼χ∗(·|˜sh)/bracketrightigg
=E/bracketleftigg
ri(s,a,b ) +/summationdisplay
s′p(s′|s,a,b )vi(s′)|b∼λ∗(fs(b)|sh),a∼χ∗(·|(fs(b),˜st−1,···,˜st−h+1))/bracketrightigg
=E/bracketleftigg
ri(s,a,b ) +/summationdisplay
s′p(s′|s,a,b )vi(s′)|b∼ρv
∗(·|sh),a∼πv
∗(·|˜sh)/bracketrightigg
Similarly, we have
Ji(λ∗,χ) =E/bracketleftigg
ri(s,a,b ) +/summationdisplay
s′p(s′|s,a,b )vi(s′)|b∼ρv
∗(·|sh),a∼πv(·|˜sh)/bracketrightigg
,
Ji(λ,χ∗) =E/bracketleftigg
ri(s,a,b ) +/summationdisplay
s′p(s′|s,a,b )vi(s′)|b∼ρv(·|sh),a∼πv
∗(·|˜sh)/bracketrightigg
,
whereπv,ρvare corresponding policies implied from behavioral strategies χ,λ, respectively. Recall the
definition of the minimax operator of Livi(s), we have, for all s∈S,
Livi(s) =ri
(πv∗,ρv∗)(s) +γ/summationdisplay
s′∈Sp(πv∗,ρv∗)(s′|s)vi(s′)
Proposition 4.6 still holds when agents and adversaries adopt history-dependent policies since we do not
require Markov policies in the proof. Propositions 4.5 also holds which can be proved by utilizing the
properties of NE for EFGs defined in Definition A.11. Specifically, in the proof, we use EFGs defined in
Definition A.11 instead of Definition A.3. The subsequent proof of Propositions 4.5 keeps the same.
Then in the proof of Theorem 4.7, we are able to continue to utilize Propositions 4.6 and 4.5. Similarly,
the EFGs used in the proof are replaced by Definition A.11. The definition of the minimax operator does
not constrain the type of policies. The properties of the minimax operator can be continually used as
well. The main body of proof keeps the same. Theorem 4.7 still holds when agents and adversaries adopt
history-dependent policies.
31
Published in Transactions on Machine Learning Research (06/2023)
B Algorithm
B.1 Robust multi-agent Q-learning (RMAQ)
In this section, we prove the convergence of RMAQ under certain conditions. First, let’s recall the convergence
theorem and certain assumptions.
Assumption B.1 (Same as assumption 5.1) .
(1) State and action pairs have been visited infinitely often. (2) The learning rate αtsatisfies the following
conditions: 0≤αt<1,/summationtext
t≥0α2
t≤∞; if(s,a,b )̸= (st,at,bt),αt(s,a,b ) = 0. (3) An NE of the EFG
based on (q1
t,···,qN
t,−q1
t,···,−qN
t)exists at each iteration t.
Theorem B.2 (Same as theorem 5.2) .
Under Assumption B.1, the sequence {qt}obtained from (18)converges to{q∗}with probability 1, which
are the optimal action-value functions that satisfy Bellman equations (5)for alli= 1,···,N.
qi
t+1(st,at,bt) = (1−αt)qi
t(st,at,bt)+ (18)
αt
ri
t+γ/summationdisplay
at+1∈A/summationdisplay
bt+1∈Bπqt
∗,t(at+1|˜st+1)ρqt
∗,t(bt+1|st+1)qi
t(st+1,at+1,bt+1)
,
Proof.Define the operator Tqt=T(q1
t,···,qN
t) = (T1q1
t,···,TNqN
t)where the operator Tiis defined as
below:
Tiqi
t(s,a,b ) =ri
t+γ/summationdisplay
a′∈A/summationdisplay
b′∈Bπqt
∗(a′|˜s′)ρqt
∗(b′|s′)qi
t(s′,a′,b′) (19)
fori∈ N, where (πqt∗,ρqt∗)is the tuple of Nash equilibrium policies for the EFG based on
(q1
t,···,qN
t,−q1
t,···,−qN
t)obtained from (18). Because of proposition B.3 and proposition B.4 the Lemma
8 in Hu & Wellman (2003) or Corollary 5 in Szepesvári & Littman (1999) tell that qt+1= (1−αt)qt+αtTqt
converges to q∗with probability 1.
Proposition B.3 (Contraction mapping) .
Tqt= (T1q1
t,···,TNqN
t)is a contraction mapping.
Proof.We omit the subscript twhen there is no confusion. Assume Tipi≥Tiqi, we have
0≤Tipi−Tiqi
=γ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
a′∈A/summationdisplay
b′∈Bπp
∗(a′|˜s′)ρp
∗(b′|s′)pi(s′,a′,b′)−/summationdisplay
a′∈A/summationdisplay
b′∈Bπq
∗(a′|˜s′)ρq
∗(b′|s′)qi(s′,a′,b′)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤γ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplay
a′∈A/summationdisplay
b′∈Bπq
∗(a′|˜s′)ρq
∗(b′|s′)pi(s′,a′,b′)−/summationdisplay
a′∈A/summationdisplay
b′∈Bπp
∗(a′|˜s′)ρp
∗(b′|s′)qi(s′,a′,b′)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤γ/vextendsingle/vextendsingle/vextendsingle/vextendsinglepi−qi/vextendsingle/vextendsingle/vextendsingle/vextendsingle. (20)
Repeating the case Tipi≤Tiqiimplies that Tiis a contraction mapping such that ||Tipi−Tiqi||≤γ||pi−qi||
for allpi,qi∈Q. Recall that||p−q||= supj||pj−qj||
||Tp−Tq||= sup
j||Tjpj−Tjqj||≤γsup
j||pj−qj||=γ||p−q||
Tis a contraction mapping such that ||Tp−Tq||≤γ||p−q||for allp,q∈Q.
32
Published in Transactions on Machine Learning Research (06/2023)
Proposition B.4 (A condition of Lemma 8 in Hu & Wellman (2003) also Corollary 5 in Szepesvári &
Littman (1999)) .
q∗=E[Tq∗] (21)
Proof.
E/bracketleftbig
Tiqi
∗(s,a,b )/bracketrightbig
=E/bracketleftigg
ri(s,a,b ) +γ/summationdisplay
a′∈A/summationdisplay
b′∈Bπ∗(a′|˜s′)ρ∗(b′|s′)qi
∗(s′,a′,b′)/bracketrightigg
=ri(s,a,b ) +γ/summationdisplay
s′∈Sp(s′|s,a,b )/summationdisplay
a′∈A/summationdisplay
b′∈Bπ∗(a′|˜s′)ρ∗(b′|s′)qi
∗(s′,a′,b′)
=qi
∗(s,a,b ) (22)
Thereforeq∗=E[Tq∗].
33
Published in Transactions on Machine Learning Research (06/2023)
B.2 Robust multi-agent actor-critic (RMAAC)
In this section, we first give the details of policy gradients proof in MG-SPA and then list the Pseudo code of
RMAAC.
B.2.1 Proof of policy gradients
Recall the policy gradient in RMAAC for MG-SPA in the following:
Theorem B.5 (Policy gradient in RMAAC for MG-SPA, same as the theorem 5.3) .For each agent
i∈Nand adversary ˜i∈M, the policy gradients of the objective Ji(θ,ω)with respect to the parameter
θ,ωare:
∇θiJi(θ,ω) =E(s,a,b )∼p(π,ρ)/bracketleftbig
qi,π,ρ(s,a,b )∇θilogπi(ai|˜si)/bracketrightbig
(23)
∇ωiJi(θ,ω) =E(s,a,b )∼p(π,ρ)/bracketleftbig
qi,π,ρ(s,a,b )[∇ωilogρi(bi|s) +reg]/bracketrightbig
(24)
wherereg=∇˜silogπi(ai|˜si)∇bif(s,bi)∇ωiρ(bi|s).
Proof.We first start with the derivative of the state value function on θi:
∇θivi,π,ρ(s)
=∇θi/bracketleftigg/summationdisplay
a∈A/summationdisplay
b∈Bπ(a|˜s)ρ(b|s)qi,π,ρ(s,a,b )/bracketrightigg
=/summationdisplay
a∈A/summationdisplay
b∈B/bracketleftbig
∇θiπ(a|˜s)ρ(b|s)qi,π,ρ(s,a,b ) +π(a|˜s)ρ(b|s)∇θiqi,π,ρ(s,a,b )/bracketrightbig
=/summationdisplay
a∈A/summationdisplay
b∈B
∇θiπ(a|˜s)ρ(b|s)qi,π,ρ(s,a,b ) +π(a|˜s)ρ(b|s)∇θi/summationdisplay
s′,rp(s′,r|s,a,b )(ri+vi,π,ρ(s′))

=/summationdisplay
a∈A/summationdisplay
b∈B/bracketleftigg
∇θiπ(a|˜s)ρ(b|s)qi,π,ρ(s,a,b ) +π(a|˜s)ρ(b|s)∇θi/summationdisplay
s′p(s′|s,a,b )vi,π,ρ(s′)/bracketrightigg
(25)
We useϕθi(s)to denote/summationtext
a∈A/summationtext
b∈B[∇θiπ(a|˜s)ρ(b|s)qi,π,ρ(s,a,b )]. We usepπ,ρ(s→x,k)to denote the
probability of transition from state sto statexwith agents’ joint policy πand adversaries’ joint policy ρafter
ksteps. For example, pπ,ρ(s→s,k= 0) = 1 andpπ,ρ(s→s′,k= 1) =/summationtext
a∈A/summationtext
b∈Bπ(a|˜s)ρ(b|s)p(s′|s,a,b ).
In the following proof, we sometimes use the superscript iinstead of ˜ito denote adversary ˜iwhen there is no
confusion. Then we have:
∇θivi,π,ρ(s)
=ϕθi(s) +/summationdisplay
a∈A/summationdisplay
b∈Bπ(a|˜s)ρ(b|s)∇θi/summationdisplay
s′∈Sp(s′|s,a,b )vi,π,ρ(s′)
=ϕθi(s) +/summationdisplay
a∈A/summationdisplay
b∈B/summationdisplay
s′∈Sπ(a|˜s)ρ(b|s)p(s′|s,a,b )∇θivi,π,ρ(s′)
=ϕθi(s) +/summationdisplay
s′∈Spπ,ρ(s→s′,1)∇θivi,π,ρ(s′)
=ϕθi(s) +/summationdisplay
s′∈Spπ,ρ(s→s′,1)/bracketleftigg
ϕθi(s′) +/summationdisplay
s′′∈Spπ,ρ(s′→s′′,1)∇θivi,π,ρ(s′′)/bracketrightigg
=···
=/summationdisplay
x∈S∞/summationdisplay
k=0pπ,ρ(s→x,k)ϕθi(x) (26)
34
Published in Transactions on Machine Learning Research (06/2023)
By plugging in∇θivi,π,ρ(s) =/summationtext
x∈S/summationtext∞
k=0pπ,ρ(s→x,k)ϕθi(x)into the objective function Ji(θ,ω), we can
get the following results:
∇θiJi(θ,ω) =∇θivi,π,ρ(s1)
=/summationdisplay
s∈S∞/summationdisplay
k=0pπ,ρ(s1→s,k)ϕθi(s)
=/summationdisplay
s∈Sη(s)ϕθi(s) ;Letη(s) =∞/summationdisplay
k=0pπ,ρ(s1→s,k)ϕθi(s)
=/parenleftigg/summationdisplay
s∈Sη(s)/parenrightigg/summationdisplay
s∈Sη(s)/summationtext
s∈Sη(s)ϕθi(s)
∝/summationdisplay
s∈Sη(s)/summationtext
s∈Sη(s)ϕθi(s) ;/parenleftigg/summationdisplay
s∈Sη(s)/parenrightigg
is a constant
=/summationdisplay
s∈S/summationdisplay
a∈A/summationdisplay
b∈Bdπ,ρ(s)∇θiπ(a|˜s)ρ(b|s)qi,π,ρ(s,a,b ) ;Letdπ,ρ(s) =η(s)/summationtext
s∈Sη(s)
=/summationdisplay
s∈S/summationdisplay
a∈A/summationdisplay
b∈Bdπ,ρ(s)∇θiπ(a|˜s)
π(a|˜s)ρ(b|s)qi,π,ρ(s,a,b )π(a|˜s)
=E(s,a,b )∼p(π,ρ)/bracketleftbig
qi,π,ρ(s,a,b )∇θilogπi(ai|˜si)/bracketrightbig
(27)
Now we calculate the derivative of the state value function on ωi:
∇ωivi,π,ρ(s)
=∇ωi/bracketleftigg/summationdisplay
a∈A/summationdisplay
b∈Bπ(a|˜s)ρ(b|s)qi,π,ρ(s,a,b )/bracketrightigg
=/summationdisplay
a∈A/summationdisplay
b∈B/bracketleftbig
∇ωiρ(b|s)π(a|˜s)qi,π,ρ(s,a,b ) +∇ωiπ(a|˜s)ρ(b|s)qi,π,ρ(s,a,b ) +ρ(b|s)π(a|˜s)∇ωiqi,π,ρ(s,a,b )/bracketrightbig
(28)
We letψωi(s) =/summationtext
a∈A/summationtext
b∈B/bracketleftbig
∇ωiρ(b|s)π(a|˜s)qi,π,ρ(s,a,b )/bracketrightbig
and
ϕωi(s) =/summationtext
a∈A/summationtext
b∈B/bracketleftbig
∇ωiπ(a|˜s)ρ(b|s)qi,π,ρ(s,a,b )/bracketrightbig
. Similar to∇θivi,π,ρ(s), we have:
∇ωivi,π,ρ(s)
=ψωi(s) +ϕωi(s) +/summationdisplay
a∈A/summationdisplay
b∈B/bracketleftigg
π(a|˜s)ρ(b|s)∇θi/summationdisplay
s′∈Sp(s′|s,a,b )vi,π,ρ(s′)/bracketrightigg
=ψωi(s) +ϕωi(s) +/summationdisplay
a∈A/summationdisplay
b∈B/summationdisplay
s′∈Sπ(a|˜s)ρ(b|s)p(s′|s,a,b )∇ωivi,π,ρ(s′)
=ψωi(s) +ϕωi(s) +/summationdisplay
s′∈Spπ,ρ(s→s′,1)∇ωivi,π,ρ(s′)
=ψωi(s) +ϕωi(s) +/summationdisplay
s′∈Spπ,ρ(s→s′,1)/bracketleftigg
ψωi(s′) +ϕωi(s′) +/summationdisplay
s′∈Spπ,ρ(s′→s′′,1)∇ωivi,π,ρ(s′′)/bracketrightigg
=···
=/summationdisplay
x∈S∞/summationdisplay
k=0pπ,ρ(s→x,k)[ψωi(s) +ϕωi(s)] (29)
35
Published in Transactions on Machine Learning Research (06/2023)
By plugging in∇ωivi,π,ρ(s) =/summationtext
x∈S/summationtext∞
k=0pπ,ρ(s→x,k)[ψωi(s)+ϕωi(s)]into the objective function Ji(θ,ω),
we can get the following results:
∇ωiJi(θ,ω) =∇ωivi,π,ρ(s1)
=/summationdisplay
s∈S∞/summationdisplay
k=0pπ,ρ(s1→s,k)/bracketleftig
ψωi(s) +ϕωi(s)/bracketrightig
∝/summationdisplay
s∈S/summationdisplay
a∈A/summationdisplay
b∈Bdπ,ρ(s)/bracketleftbig
∇ωiπ(a|˜s)ρ(b|s)qi,π,ρ(s,a,b ) +∇ωiρ(b|s)π(a|˜s)qi,π,ρ(s,a,b )/bracketrightbig
=E(s,a,b )∼p(π,ρ)/bracketleftbig
qi,π,ρ(s,a,b )∇ωilogρ(b|s) +qi,π,ρ(s,a,b )∇ωilogπ(a|˜s)/bracketrightbig
=E(s,a,b )∼p(π,ρ)/bracketleftbig
qi,π,ρ(s,a,b )∇ωilogρi(bi|s) +qi,π,ρ(s,a,b )∇ωilogπi(ai|˜si)/bracketrightbig
=E(s,a,b )∼p(π,ρ)/bracketleftbigg
qi,π,ρ(s,a,b )∇ωilogρi(bi|s) +qi,π,ρ(s,a,b )∇˜siπi(ai|˜si)∇bif(s,bi)∇ωiρ(bi|s)
πi(ai|˜si)/bracketrightbigg
=E(s,a,b )∼p(π,ρ)/braceleftbig
qi,π,ρ(s,a,b )[∇ωilogρi(bi|s) +∇˜silogπi(ai|˜si)∇bif(s,bi)∇ωiρ(bi|s)]/bracerightbig
(30)
B.2.2 Policy gradients for deterministic polices
Theorem B.6 (Policy gradients for deterministic polices in RMAAC for MG-SPA) .For each agent
i∈Nand adversary ˜i∈Musing deterministic policies, the policy gradients of the objective Ji(θ,ω)
with respect to the parameter θ,ωare:
∇θiJi(θ,ω) =1
TT/summationdisplay
t=1∇aiqi(st,at,bt)∇θiπi(˜si
t)|ai
t=πi(˜si
t),bi
t=ρi(st) (31)
∇ωiJi(θ,ω) =1
TT/summationdisplay
t=1/bracketleftbig
∇biqi(st,at,bt) +reg/bracketrightbig
∇ωiρi(st)|ai
t=πi(˜si
t),bi
t=ρi(st) (32)
wherereg=∇bi
tf(st,bi
t)∇aiqi(st,at,bt)∇fπi(f).
Proof.Note that we here parameterize all policies πi,ρ˜ias deterministic policies. Then we have:
∇θiJi(θ,ω) =Es∼p(π,ρ)/bracketleftbig
∇θiqi(s,a,b )/bracketrightbig
=Es∼p(π,ρ)/bracketleftbig
∇aiqi(s,a,b )∇θiπi(˜si)/bracketrightbig
, (33)
∇ωiJi(θ,ω) =Es∼p(π,ρ)/bracketleftbig
∇ωiqi(s,a,b )/bracketrightbig
=Es∼p(π,ρ)/bracketleftbig
∇aiqi(s,a,b )∇˜siπi(˜si)∇bif(si,bi)∇ωiρi(s) +∇biqi(s,a,b )∇ωiρi(s)/bracketrightbig
=Es∼p(π,ρ)/bracketleftbig
∇ωiρi(s)/bracketleftbig
∇biqi(s,a,b ) +reg/bracketrightbig/bracketrightbig
, (34)
wherereg=∇aiqi(s,a,b )∇˜siπi(˜si)∇bif(s,bi). When the actors are updated in a mini-batch fashion (Mnih
et al., 2015; Li et al., 2014), (9) and (10) approximate (33) and (34), respectively.
B.2.3 Pseudo code of RMAAC
We provide the Pseudo code of RMAAC with deterministic policies in Algorithm 1. The stochastic policy
version RMAAC is similar to Algorithm 1 but uses different policy gradients.
36
Published in Transactions on Machine Learning Research (06/2023)
Algorithm 1: RMAAC with deterministic policies
1Randomly initialize the critic network qi(s,a,b|ηi), the actor network πi(·|θi), and the adversary network
ρi(·|ωi)for agenti. Initialize target networks qi′,πi′,ρi′;
2foreach episode do
3Initialize a random process Nfor action exploration;
4Receive initial state s;
5foreach time step do
6 For each adversary i, select action bi=ρi(s) +Nw.r.t the current policy and exploration.
Compute the perturbed state ˜si=f(s,bi). Execute actions ai=π(˜si) +Nand observe the
rewardr= (r1,...,rn)and the new state information s′and store (s,a,b, ˜s,r,s′)in replay buffer
D. Sets′→s;
7foragent i=1 to n do
8 Sample a random minibatch of Ksamples (sk,ak,bk,rk,s′
k)fromD;
9 Setyi
k=ri
k+γqi′(s′
k,a′
k,b′
k)|ai′
k=πi′(˜si
k),bi′
k=ρi′(sk);
10 Update critic by minimizing the loss L=1
K/summationtext
k/bracketleftbig
yi
k−qi(sk,ak,bk)/bracketrightbig2;
11 foreach iteration step do
12 Update actor πi(·|θi)and adversary ρi(·|ωi)using the following gradients
13 θi←θi+αa1
K/summationtext
k∇θiπi(˜si
k)∇aiqi(sk,ak,bk)whereai
k=πi(˜si
k),bi
k=ρi(sk);
14 ωi←ωi−αb1
K/summationtext
k∇ωiρi(sk)/bracketleftbig
∇biqi(sk,ak,bk) +reg/bracketrightbig
where
reg=∇ai
kqi(sk,ak,bk)∇˜si
kπi(˜si
k),ai
k=πi(˜si
k),bi
k=ρi(sk);
15 end
16 end
17 Update all target networks: θi′←τθi+ (1−τ)θi′,ωi′←τωi+ (1−τ)ωi′.
18end
19end
37
Published in Transactions on Machine Learning Research (06/2023)
C Experiments
C.1 Robust multi-agent Q-learning (RMAQ)
In this section, we first introduce the designed two-player game in that the reward function and transition
probability function are formally defined. The MG-SPA based on the two-player game is also further
explained. Then we show more experimental results about the proposed robust multi-agent Q-learning
(RMAQ) algorithm, such as the training process of the RMAQ algorithm in terms of the total discounted
rewards.
C.1.1 Two-player game
Figure 10: Two-player game: each player has two states and the same action set with size 2. Under state s0,
two players get the same reward 1when they choose the same action. At state s1, two players get the same
reward 1when they choose different actions. One state switches to another state only when two players get a
reward, i.e. two players always stay in the current state until they get the reward.
Look at Figure 10 (same as Figure 3 in the main text.), this is how we run the designed two-player game.
The reward function rand transition probability function pare defined as follows.
These two players get the same rewards all the time, i.e. they share a reward function r.
ri(s,a1,a2) =

1, a1=a2,ands=s0
1, a1̸=a2,ands=s1
0, a1̸=a2,ands=s0
0, a1=a2,ands=s1(35)
The state does not change until these two players get a positive reward. So the transition probability function
pis
p(s1|s,a1,a2) =

1, a1=a2,ands=s0
0, a1̸=a2,ands=s0
1, a1=a2,ands=s1
0, a1̸=a2,ands=s1p(s0|s,a1,a2) =

0, a1=a2,ands=s0
1, a1̸=a2,ands=s0
0, a1=a2,ands=s1
1, a1̸=a2,ands=s1(36)
Possible Nash Equilibrium can be π∗
1= (π1
1,π2
1)orπ∗
2= (π1
2,π2
2)where
π1
1(a1|s) =

1, a1= 1,ands=s0
0, a1= 0,ands=s0
1, a1= 1,ands=s1
0, a1= 0,ands=s1π2
1(a2|s) =

1, a2= 1,ands=s0
0, a2= 0,ands=s0
0, a2= 1,ands=s1
1, a2= 0,ands=s1(37)
π1
2(a1|s) =

0, a1= 1,ands=s0
1, a1= 0,ands=s0
0, a1= 1,ands=s1
1, a1= 0,ands=s1π2
2(a2|s) =

0, a2= 0,ands=s0
1, a2= 1,ands=s0
1, a2= 0,ands=s1
0, a2= 1,ands=s1(38)
38
Published in Transactions on Machine Learning Research (06/2023)
NEπ∗
1means player 1always selects action 1, player 2selects action 1under state s0and action 0under
states1. NEπ∗
2means player 1always selects action 0, player 2selects action 0under state s0and action 0
under state s1.
According to the definition of MG-SPA, we add two adversaries for each player to perturb the player’s
observations. And adversaries get negative rewards of players. We let adversaries share a same action space
B1=B2={0,1}, where 0means do not disturb, 1means change the observation to the opposite one.
Therefore, the perturbed function fin this MG-SPA is defined as:


f(s0,b= 0) =s0
f(s1,b= 0) =s1
f(s0,b= 1) =s1
f(s1,b= 1) =s0(39)
Obviously, fis a bijective function when sis given. And the constraint parameter ϵ=||S||, where
||S||:= max|s−s′|∀s,s′∈S, i.e. no constraints for adversaries’ power.
A Robust Equilibrium (RE) of this MG-SPA would be ˜d∗= (˜π1
∗,˜π2
∗,˜ρ1
∗,˜ρ2
∗), where


˜π1
∗(a1|s) = 0.5,∀s∈S
˜π2
∗(a2|s) = 0.5,∀s∈S
˜ρ1
∗(b1|s) = 0.5,∀s∈S
˜ρ2
∗(b2|s) = 0.5,∀s∈S(40)
C.1.2 Training procedure
In Figure 11, we show the total discounted rewards in the function of training episodes. We set the learning
rate as 0.1and train our RMAQ algorithm for 400 episodes. And each episode contains 25 training steps. We
can see the total discounted rewards converges to 50, i.e. the optimal value in the MG-SPA, after about 280
episodes or 7000steps.
Figure 11: The total discounted rewards converge to the optimal value after about 280training episodes.
39
Published in Transactions on Machine Learning Research (06/2023)
C.2 Robust multi-agent actor-critic (RMAAC)
In this section, we first briefly introduce the multi-agent environments we use in our experiments. Then we
provide more experimental results and explanations, such as the testing results under a cleaned environment
(accurate state information can be attained) and a randomly perturbed environment (injecting standard
Gaussian noise in agents’ observations). In the last subsection, we list all hyper-parameters we used in the
experiments, as well as the baseline source code.
C.2.1 Multi-agent environments
Figure 12: Illustrations of the experimental scenarios and some games we consider, including a) Cooperative
communication b)Cooperative navigation c)Predator prey d)Keep away e)Physical deception f)Navigate
communication
Cooperative communication (CC): This is a cooperative game. There are 2 agents and 3 landmarks of
different colors. Each agent wants to get to their target landmark, which is known only by other agents. The
reward is collective. So agents have to learn to communicate the goal of the other agent, and navigate to
their landmark.
Cooperative navigation (CN): This is a cooperative game. There are 3 agents and 3 landmarks. Agents
are rewarded based on how far any agent is from each landmark. Agents are penalized if they collide with
other agents. So, agents have to learn to cover all the landmarks while avoiding collisions.
Physical deception (PD): This is a mixed cooperative and competitive task. There are 2 collaborative
agents, 2 landmarks, and 1 adversary. Both the collaborative agents and the adversary want to reach the
target, but only collaborative agents know the correct target. The collaborative agents should learn a policy
to cover all landmarks so that the adversary does not know which one is the true target.
Keep away (KA): This is a competitive task. There is 1 agent, 1 adversary, and 1 landmark. The agent
knows the position of the target landmark and wants to reach it. The adversary is rewarded if it is close to
the landmark, and if the agent is far from the landmark. The adversary should learn to push the agent away
from the landmark.
Predator prey (PP): This is a mixed game known as predator-prey. Prey agents (green) are faster and
want to avoid being hit by adversaries (red). Predators are slower and want to hit good agents. Obstacles
(large black circles) block the way.
40
Published in Transactions on Machine Learning Research (06/2023)
Navigate communication (NC): This is a cooperative game that is similar to Cooperative communication.
There are 2 agents and 3 landmarks of different colors. An agent is the ‘speaker’ that does not move but
observes the goal of another agent. Another agent is the listener that cannot speak, but must navigate to the
correct landmark.
Predator prey+ (PP+): This is an extension of the Predator prey environment by adding more agents.
There are 2 preys, 6 adversaries, and 4 landmarks. Prey agents are faster and want to avoid being hit by
adversaries. Predators are slower and want to hit good agents. Obstacles block the way.
C.2.2 Experiments hyper-parameters
In Table 4, we show all hyper-parameters we use to train our policies and baselines. We also provide our
source code in the supplementary material. The source code of M3DDPG (Li et al., 2019) and MADDPG
(Lowe et al., 2017) accept the MIT License which allows any person obtaining them to deal in the code
without restriction, including without limitation the rights to use, copy, modify, etc. More information about
this license refers to https://github.com/openai/maddpg andhttps://github.com/dadadidodi/m3ddpg .
Table 4: Hyper-parameters
Parameter RMAAC M3DDPG MADDPG
optimizer Adam Adam Adam
learning rate 0.01 0.01 0.01
adversarial learning rate 0.005 / /
discount factor 0.95 0.95 0.95
replay buffer size 106106106
number of hidden layers 2 2 2
activation function Relu Relu Relu
number of hidden unites per layer 64 64 64
number of samples per minibatch 1024 1024 1024
target network update coefficient τ0.01 0.01 0.01
iteration steps 20 20 20
constraint parameter ϵ 0.5 / /
episodes in training 10k 10k 10k
time steps in one episode 25 25 25
C.2.3 More testing results
In this subsection, we provide more testing results under a cleaned environment (accurate state information
can be attained) and a randomly disturbed environment (injecting standard Gaussian noise into agents’
observations).
As we have reported the comparison of mean episode testing rewards under a cleaned environment by using 4
different methods in the main manuscript (Figures 6 and 8), we further report the variance of testing results
in the appendix. In Table 5 and 6, we also report the variances of testing rewards in different scenarios under
different environment settings. Our method has the lowest variance in three of the five scenarios. Notice that
RM1 denotes our RMAAC policy trained with the linear noise format f1, RM2 denotes our RMAAC policy
trained with the Gaussian noise format f2, MA denotes MADDPG ( https://github.com/openai/maddpg ),
M3 denotes M3DDPG ( https://github.com/dadadidodi/m3ddpg ).
MAPPO is a multi-agent reinforcement learning algorithm that performs well in cooperative multi-agent set-
tings (Yu et al., 2021a). We use MP to denote MAPPO ( https://github.com/marlbenchmark/on-policy. ).
In Figure 13, we compare its performance with our RMAAC algorithm in two cooperative scenarios of MPE.
The details of scenarios such as Cooperative navigation, Navigate communication can be found in the last
section. We can see that under the optimally perturbed environment, RMAAC outperforms MAPPO in
all scenarios. Additionally, the reason we included MAPPO in the Appendix but not in the main text
41
Published in Transactions on Machine Learning Research (06/2023)
is that the current source code provider of MAPPO only provides instructions and codes for using it in
cooperative environments. However, to validate our proposed method in different settings, we carefully
selected experimental environments to include different game types: cooperative, competitive, and mixed.
Due to the lack of MAPPO source codes/implementation in competitive and mixed environments, if we were
to include the experimental results of MAPPO in the main text, it could disrupt the integrity and uniformity
of the experiment section in the main text. Therefore, we included them in the appendix as supplementary
content.
In Figure 14 and Table 7, we compare the mean episode testing rewards and variances under different
environments in the complicated scenario with a large number of agents between different algorithms. We
adopt the Gaussian noise format in training RMAAC polices. We can see our method has the lowest variance
under two of three environments and has the highest rewards under all environments.
Table 5: Variance of testing rewards under cleaned environment
Algorithms RM withf1RM withf2M3 MA
Cooperative communication (CC) 0.383 0.376 0.295 0.328
Cooperative navigation (CN) 0.413 0.361 0.416 0.376
Physical deception (PD) 0.175 0.165 0.133 0.143
Keep away (KA) 0.137 0.134 0.17 0.145
Predator prey (PP) 5.139 1.450 4.681 4.725
Table 6: Variance of testing rewards under randomly perturbed environment
Algorithms RM withf1RM withf2M3 MA
Cooperative communication (CC) 0.592 0.547 1.187 0.937
Cooperative navigation (CN) 0.336 0.33 0.328 0.321
Physical deception (PD) 0.222 0.292 0.209 0.184
Keep away (KA) 0.155 0.155 0.166 0.161
Predator prey (PP) 4.629 2.752 3.644 2.9
Table 7: Variance of testing rewards under different environments in Predator prey+.
Algorithm RM M3 MA
Optimally Perturbed Env 4.199 4.046 3.924
Randomly Perturbed Env 4.664 5.774 6.191
Cleaned Env 3.928 5.521 6.006
42
Published in Transactions on Machine Learning Research (06/2023)
Figure 13: Comparison of mean episode testing rewards using MAPPO and RMAAC under optimally
perturbed environments. RMAAC outperforms MAPPO in all cooperative scenarios under optimally
perturbed environments.
Figure 14: Comparison of mean episode testing rewards using different algorithms under different environments
in Predator prey+. RMAAC outperforms all MARL baseline algorithms in the complicated multi-agent
scenario.
C.3 Ablation Study for RMAAC
In this subsection, we first investigate the effect of using different values of constraint parameters ϵin the
implementation of the RMAAC algorithm, then the effect of using different values of variance σ. Finally, we
study the performance of the RMAAC algorithm under other types of attacks.
C.3.1 Training Results Using Linear Noise with Different Constraint Parameters
Training Setup: In this subsection, we train several RMAAC policies using linear noise format as the
state perturbation function, i.e. f1(s,b˜i) =s+b˜i. The constraint parameter ϵis respectively set as
0.01,0.05,0.1,0.5,1and2, given other hyper-parameters unchanged. Other used hyper-parameters can be
found in Table 4.
Training Results: In Figure 15, 16, and 17, we show the training process in three scenarios: Cooperative
communication (CC), Cooperative navigation (CN), Predator Prey (PP), respectively. The y-axis denotes
the mean episode reward of the agents and the x-axis denotes the training episodes.
From these figures, we can see that, in general, the smaller the used variance, the higher the mean episode
rewards RMAAC can achieve. However, RMAAC has different sensitivities to the value of variance in
different scenarios. When we use ϵ= 2, the RMAAC policies have the lowest mean episode rewards in all
43
Published in Transactions on Machine Learning Research (06/2023)
three scenarios. Nevertheless, when we use the smallest constraint parameter ϵ= 0.01, the trained RMAAC
policies do not achieve the highest mean episode rewards in all three scenarios. In these three scenarios, it
is clear to see the performance of RMAAC using ϵ= 0.5is better than or similar to the performance of
RMAAC using ϵ= 1, and better than the performance of RMAAC using ϵ= 2, i.e. Performance( ϵ= 0.5)
≥Performance( ϵ= 1)>Performance( ϵ= 2). The performance of the RMAAC policies is close when the
constraint parameters are less or equal to than 0.1.
C.3.2 Testing Results Using Linear Noise with Different Constraint Parameters
In this subsection, we test well-trained RMAAC policies in perturbed environments where adversaries adopt
linear noise format and different constraint parameters.
Testing Setup: The tested policy πtestis trained with the linear noise format f1(s,b˜i) =s+b˜i, constraint
parameter is 0.5, whereb˜i=ρ˜i
test(s|ϵ= 0.5). The policy ρ˜i
testis adversary ˜i’s policy which is trained with
πtestin RMAAC, for all ˜i=˜1,···,˜N. We useρtestto denote the joint policy of adversaries which is used in
the testing. In summary, we test agents’ joint policy πscenario
test (˜s)when adversaries adopt the joint policy
ρscenario
test (s|ϵ), andϵ= 0.01,0.05,0.1,0.5,1,2,scenario =Cooperative communication (CC), Cooperative
navigation (CN), Predator Prey (PP), respectively. The testing is conducted over 400 episodes, and each
episode has 25 time steps.
Testing Results: In Figures 18, 19 and 20, we compare the performance of RMAAC, M3DDPG, and
MADDPG in scenarios CC, CN, and PP with different values of constraint parameters. MADDPG is a
MARL baseline algorithm. M3DDPG is a robust MARL baseline algorithm. The y-axis denotes the mean
episode reward of the agents.
From these figures, we can see that in all three scenarios, our RMAAC policies outperform the baseline MARL
and robust MARL policies in terms of mean episode testing rewards under the attacks of linear noise format
with different constraint parameters ϵ. Our proposed RMAAC algorithm is robust to the state information
attacks of linear noise format with different constraint parameters.
C.3.3 Training Results Using Gaussian Noise with Different Variance
Training Setup: In this subsection, we train several RMAAC policies using Gaussian noise format as the state
perturbation function, i.e. f2(s,b˜i) =s+N(b˜i,σ). The variance σis respectively set as 0.001,0.05,0.1,0.5,1,2
and3, given other hyper-parameters unchanged. Other used hyper-parameters can be found in Table 4.
Training Results: In Figure 21, 22 and 23, we show the training process of RMAAC in three scenarios:
Cooperative communication (CC), Cooperative navigation (CN), Predator Prey (PP). The y-axis denotes the
mean episode rewards of the agents and the x-axis denotes the training episodes.
From the figures, we can see that, in general, the smaller the value of variance used, the higher the mean
episode rewards RMAAC can achieve. However, RMAAC has different sensitivities to the value of variance
in different scenarios. When we use σ= 3, the RMAAC policies have the lowest mean episode rewards in all
three scenarios. Nevertheless, when we use the smallest magnitude 0.001, the trained RMAAC policies do not
always achieve the highest mean episode rewards. In these three scenarios, it is clear to see the performance
of RMAAC using σ= 1is better than or close to that of using σ= 2, and better than that of using σ= 3,
i.e. Performance( σ= 1)≥Performance( σ= 2)>Performance( σ= 3). The performance of the RMAAC
policies is close when the constraint parameters are less than or equal to 0.5.
C.3.4 Testing Results Using Gaussian Noise with Different Variance
In this subsection, we test well-trained RMAAC policies in perturbed environments where adversaries adopt
Gaussian noise format and different variances.
Testing Setup: The tested policy πtestis trained with Gaussian noise format f2(s,b˜i) =s+N(b˜i,σ= 1),
constraint parameter is 0.5, whereb˜i=ρ˜i
test(s|ϵ= 0.5).ρ˜iis adversary i’s policy which is trained with
πtestin RMAAC, for all i= 1,···,N. We useρtestto denote the joint policy of adversaries. In summary,
we test agents’ joint policy πscenario
test (˜s)when adversaries adopt the joint policy ρscenario
test (s|ϵ= 0.5)and
44
Published in Transactions on Machine Learning Research (06/2023)
Figure15: WetrainRMAACpoliciesusingdifferentvaluesofconstraintparametersinthescenarioCooperative
Communication. In general, the smaller the constraint parameter is used, the higher the mean episode
rewards RMAAC can achieve.
Figure16: WetrainRMAACpoliciesusingdifferentvaluesofconstraintparametersinthescenarioCooperative
Navigation. In general, the smaller the constraint parameter is used, the higher the mean episode rewards
RMAAC can achieve.
Figure 17: We train RMAAC policies using different values of constraint parameters in the scenario Predator-
Prey. In general, the smaller the constraint parameter is used, the higher the mean episode rewards RMAAC
can achieve.
Gaussian noise format f2(s,b˜i) =s+N(b˜i,σ), whereσ= 0.001,0.05,0.1,0.5,1,2,3,scenario =Cooperative
communication (CC), Cooperative navigation (CN), Predator Prey (PP). The testing is conducted over 400
episodes, and each episode has 25 time steps.
45
Published in Transactions on Machine Learning Research (06/2023)
Testing Results: In Figures 24, 25 and 26, we respectively compare the performance of RMAAC, M3DDPG,
and MADDPG in scenarios Cooperative communication, Cooperative navigation and Predator Prey with
different values of constraint parameters. MADDPG is a MARL baseline algorithm. M3DDPG is a robust
MARL baseline algorithm. The y-axis denotes the mean episode reward of the agents.
From these figures, we can see that in all three scenarios with all different values of constraint parameters,
our RMAAC policies outperform the MARL and robust MARL baseline policies in terms of mean episode
rewards under the attacks of Gaussian noise format with different variance. Our proposed RMAAC algorithm
is robust to the state information attacks of Gaussian noise format with different values of variance.
C.3.5 Testing Results under Different State Perturbation Functions
In this subsection, we test the well-trained RMAAC policies in perturbed environments where adversaries
adopt different noise formats and policies.
Testing Setup: The tested agents’ joint policy πtestis trained with Gaussian noise format f2(s,b˜i) =
s+Gaussian (b˜i,σ= 1), constraint parameter is 0.5, whereb˜i=ρ˜i
test(s|ϵ= 0.5).ρ˜i
testis adversary ˜i’s policy
which is trained with πtestin RMAAC, for all ˜i=˜1,···,˜N. We useρtestto denote the joint policy of
adversaries. In a summary, we test agents’ joint policy πscenario
test (˜s)when adversaries adopt the joint policy
ρscenario
test (s|ϵ= 0.5), in three scenarios scenario =Cooperative communication (CC), Cooperative navigation
(CN), Predator Prey (PP), under non-optimal Gaussian format f3, Uniform noise format f4, fixed Gaussian
noise format f5and Laplace noise format f6, respectively. These noise formats are defined in the following:
f3(s,b˜i) =s+Gaussian (b˜i,1)whereb˜i=ρ˜i
non−optimal (s|ϵ),
f4(s,b˜i) =s+Uniform (−ϵ,+ϵ),
f5(s,b˜i) =s+Gaussian (0,1),
f6(s,b˜i) =s+Laplace (b˜i,1)whereb˜i=ρ˜i
test(s|ϵ), (41)
whereρ˜i
non−optimalis a non-optimal policy of adversary ˜i.ρ˜i
non−optimalis randomly chosen from the training
process. As we can see that f3andf6are independent of the optimal joint policy of adversaries, but f4and
f6are not. The testing is conducted over 400 episodes, and each episode has 25 time steps.
Testing Results: In Figures 27, 28, and 29, we compare the performance of RMAAC, M3DDPG and
MADDPG in scenarios Cooperative communication, Cooperative navigation and Predator Prey under 4
different noise formats, respectively. MADDPG is a MARL baseline algorithm. M3DDPG is a robust MARL
baseline algorithm. The y-axis denotes the mean episode reward of the agents.
As we can see from these figures, most of the time, our RMAAC policy outperforms the MARL (MADDPG)
and robust MARL (M3DDPG) baseline policies. In Cooperative communication and Predator Prey, under
all 4 different noise formats, RMAAC policies achieve the highest mean episode rewards. In Cooperative
navigation, RMAAC policies have the highest mean episode rewards when the non-optimal Gaussian noise
format and Laplace noise format are used. The only exception happens in Cooperative navigation when the
Uniform noise format and fixed Gaussian noise format are used. However, we can find that the performance
of RMAAC policies is close to that of the baseline policies in terms of mean episode testing rewards. In
general, our RMAAC algorithm is robust to different types of state information attacks.
46
Published in Transactions on Machine Learning Research (06/2023)
Figure 18: We test the performance of RMAAC(RM), MADDPG(MA), and M3DDPG(M3) policies under the
attack of linear noise format when using different values of constraint parameters in the scenario Cooperative
Communication. RM denotes our robust MARL algorithm, i.e. RMAAC. MA denotes MADDPG, a MARL
baseline algorithm. M3 denotes M3DDPG, a robust MARL baseline algorithm. Our RMAAC algorithm
outperforms baseline algorithms in terms of mean episode testing rewards under all situations using different
values of constraint parameters.
Figure 19: We test the performance of RMAAC(RM), MADDPG(MA), and M3DDPG(M3) policies under
the attack of linear noise format when using different values of constraint parameters in the scenario
Cooperative Navigation. RM denotes our robust MARL algorithm, i.e. RMAAC. MA denotes MADDPG,
a MARL baseline algorithm. M3 denotes M3DDPG, a robust MARL baseline algorithm. Our RMAAC
algorithm outperforms baseline algorithms in terms of mean episode testing rewards under all situations
using different values of constraint parameters.
Figure 20: We test the performance of RMAAC(RM), MADDPG(MA), and M3DDPG(M3) policies under the
attack of linear noise format when using different values of constraint parameters in the scenario Predator-Prey.
RM denotes our robust MARL algorithm, i.e. RMAAC. MA denotes MADDPG, a MARL baseline algorithm.
M3 denotes M3DDPG, a robust MARL baseline algorithm. Our RMAAC algorithm outperforms baseline
algorithms in terms of mean episode testing rewards under all situations using different values of constraint
parameters.
47
Published in Transactions on Machine Learning Research (06/2023)
Figure 21: We train RMAAC policies using different values of variance in the scenario Cooperative Commu-
nication. In general, the smaller the variance is used, the higher the mean episode rewards RMAAC can
achieve.
Figure 22: We train RMAAC policies using different values of variance in the scenario Cooperative Navigation.
In general, the smaller the variance is used, the higher the mean episode rewards RMAAC can achieve.
Figure 23: We train RMAAC policies using different values of variance in the scenario Predator-Prey. In
general, the smaller the variance is used, the higher the mean episode rewards RMAAC can achieve.
48
Published in Transactions on Machine Learning Research (06/2023)
Figure 24: We test the performance of RMAAC(RM), MADDPG(MA), and M3DDPG(M3) policies under
the attacks of Gaussian noise format with different variances in the scenario Cooperative Communication.
RM denotes our robust MARL algorithm, i.e. RMAAC. MA denotes MADDPG, a MARL baseline algorithm.
M3 denotes M3DDPG, a robust MARL baseline algorithm. Our RMAAC algorithm outperforms baseline
algorithms in terms of mean episode testing rewards under all Gaussian noise formats with different variances.
Figure 25: We test the performance of RMAAC(RM), MADDPG(MA), and M3DDPG(M3) policies under
the attacks of Gaussian noise format with different variances in the scenario Cooperative Navigation. RM
denotes our robust MARL algorithm, i.e. RMAAC. MA denotes MADDPG, a MARL baseline algorithm.
M3 denotes M3DDPG, a robust MARL baseline algorithm. Our RMAAC algorithm outperforms baseline
algorithms in terms of mean episode testing rewards under all Gaussian noise formats with different variances.
Figure 26: We test the performance of RMAAC(RM), MADDPG(MA), and M3DDPG(M3) policies under
the attacks of Gaussian noise format with different variances in the scenario Predator-Prey. RM denotes our
robust MARL algorithm, i.e. RMAAC. MA denotes MADDPG, a MARL baseline algorithm. M3 denotes
M3DDPG, a robust MARL baseline algorithm. Our RMAAC algorithm outperforms baseline algorithms in
terms of mean episode testing rewards under all Gaussian noise formats with different variances.
49
Published in Transactions on Machine Learning Research (06/2023)
Figure 27: We test the performance of RMAAC(RM), MADDPG(MA), and M3DDPG(M3) policies under
the attacks of different noise formats in the scenario Cooperative Communication. RM denotes our robust
MARL algorithm, i.e. RMAAC. MA denotes MADDPG, a MARL baseline algorithm. M3 denotes M3DDPG,
a robust MARL baseline algorithm. Our RMAAC algorithm outperforms baseline algorithms in terms of
mean episode testing rewards under all kinds of attacks.
Figure 28: We test the performance of RMAAC policies under the attacks of different noise formats in the
scenario Cooperative Navigation. RM denotes our robust MARL algorithm, i.e. RMAAC. MA denotes
MADDPG, a MARL baseline algorithm. M3 denotes M3DDPG, a robust MARL baseline algorithm. Our
RMAAC algorithm either outperforms or is close to baseline algorithms in terms of mean episode testing
rewards under all kinds of attacks.
Figure 29: We test the performance of RMAAC policies under the attacks of different noise formats in the
scenario Predator-Prey. RM denotes our robust MARL algorithm, i.e. RMAAC. MA denotes MADDPG,
a MARL baseline algorithm. M3 denotes M3DDPG, a robust MARL baseline algorithm. Our RMAAC
algorithm outperforms baseline algorithms in terms of mean episode testing rewards under all kinds of attacks.
50"
2364_marnet_backdoor_attacks_agains.pdf,"Under review as a conference paper at ICLR 2022
MARN ET: BACKDOOR ATTACKS AGAINST
VALUE -DECOMPOSITION COOPERATIVE
MULTI -AGENT REINFORCEMENT LEARNING
Anonymous authors
Paper under double-blind review
ABSTRACT
Recent works have revealed that backdoor attacks against Deep Reinforcement
Learning (DRL) could lead to abnormal action selection of the agent, which may
result in failure or even catastrophe in crucial decision processes. However, ex-
isting attacks only consider single-agent RL systems, in which the only agent can
observe the global state and have full control of the decision process. In this paper,
we explore a new backdoor attack paradigm in cooperative multi-agent reinforce-
ment learning (CMARL) scenarios, where a group of agents coordinate with each
other to achieve a common goal, while each agent can only observe the local state,
e.g., StarCraft II (Vinyals et al. (2017)). In the proposed MARNet attack frame-
work, we carefully design a pipeline of trigger design, action poisoning and re-
ward hacking modules to accommodate the cooperative multi-agent momentums.
In particular, as only a subset of agents can observe the triggers in their local ob-
servations, we maneuver their actions to the worst actions suggested by an expert
policy model. Since the global reward in CMARL is aggregated by individual
rewards from all agents, we propose to modify the reward in a way that boosts
the bad actions of poisoned agents (agents who observe the triggers) but mitigates
the inﬂuence on non-poisoned agents. We conduct extensive experiments on two
classical MARL algorithms VDN (Sunehag et al. (2018)) and QMIX (Rashid et al.
(2018)), in two popular CMARL games Predator Prey (Boehmer et al. (2020)) and
SMAC (Samvelyan et al. (2019)). The results show that MARNet outperforms
baselines extended from single-agent DRL backdoor attacks TrojDRL (Kiourti
et al. (2020)) and Multitasking learning (Ashcraft & Karra (2021)) by reducing
the utility under attack by as much as 100%. We apply ﬁne-tuning as a defense
against MARNet, and demonstrate that ﬁne-tuning cannot entirely eliminate the
effect of the attack.
1 I NTRODUCTION
Backdoor attacks are effective and stealthy attacks against deep learning models. First implemented
in classiﬁcation neural networks (Gu et al. (2017); Chen et al. (2017)), backdoored models can
accurately classify clean inputs but misbehave for inputs with specially-designed triggers. Recent
works show that deep reinforcement learning models are also vulnerable to backdoor attacks (Yang
et al. (2019); Gao et al. (2020); Kiourti et al. (2020)). A backdoored DRL agent will choose a
non-optimal action or even the worst action when encountering the triggers, which may lead to se-
vere consequences in crucial RL tasks. A line of backdoor attacks against DRL has been proposed
(Kiourti et al. (2020); Wang et al. (2021); Ashcraft & Karra (2021)), but almost all existing works
consider the single-agent scenario, where the agent is able to observe the global state and control the
entire decision process. There is a lack of backdoor attacks in the context of multi-agent reinforce-
ment learning (MARL), especially cooperative MARL (CMARL). As far as we are concerned, there
is only one work for two-agent competitive MARL backdoor attacks (Wang et al. (2021)), which is
similar to single-agent RL attacks as the two agents rival with each other, and there is only one work
brieﬂy discussing the CMARL backdoor attacks (Ashcraft & Karra (2021)).
To ﬁll up this research gap, we explore a new paradigm of backdoor attacks in the context of
CMARL, where multiple agents cooperate with each other to achieve a common goal. Compared
1
Under review as a conference paper at ICLR 2022
Environment
In-distribution trigger
Out-of-distribution trigger
Interact
Observation
Trigger Design
Backdoor model update
TD-error calculation
Reward function design
Reward Hacking
Best action
f
o
r
 
b
e
n
i
g
n
 
a
g
e
n
t
s
Worst action
f
o
r
 
p
o
i
s
o
n
e
d
 
a
g
e
n
t
s
Expert model
c
o
n
s
t
r
u
c
t
i
o
n
Action Poisoning
Guide
In-distribution 
t
r
i
g
g
e
r
Out-of-distribution 
t
r
i
g
g
e
r
Worst action 
s
e
l
e
c
t
i
o
n
 
Best action 
s
e
l
e
c
t
i
o
n
 
Original action
 Observer
 Partner
 Prey
 Wall
 Triggered wall
 Poisoned action
Triggered map
Clean map
Backdoor
M
o
d
e
l
High scores
Low scores
Figure 1: Overview of our proposed backdoor attack in the context of CMARL scenarios.
with backdoor attacks in single-agent RL scenarios, backdoor attacks in CMARL scenarios are faced
with unique challenges.
Local observation vs. global observation . In single-agent RL scenarios, it is usually assumed that
the agent is able to observe the global state. Unfortunately, in CMARL, the agents are scattered
in the environments, and each agent can only observe the local state, which is a small part of the
global state. The triggers used to activate the backdoor can always be perceived by the agent in
the former case but are unlikely to be observed by all agents in the latter case. Popular MARL
algorithms like VDN (Sunehag et al. (2018)) and QMIX (Rashid et al. (2018)) are designed to deal
with local observations and are widely deployed in autonomous driving (Dosovitskiy et al. (2017);
Cao et al. (2013)), robot swarms (H ¨uttenrauch et al. (2019)), and online video games. To the best
of our knowledge, the vulnerability of these CMARL algorithms under backdoor attacks has never
been investigated.
Global reward vs. individual reward . In classiﬁcation tasks, backdoors are injected by changing the
label of malicious inputs (inputs with triggers) to the targeted label or any wrong label. In RL tasks,
backdoors are injected by hacking the rewards of actions when the environment contains triggers. By
assigning a high reward to actions that are actually bad in the malicious environment, the agents will
learn to choose these actions and end up in failure. In single-agent RL scenarios, the global reward
is directly fed back to the agent to train the policy network. However, CMARL algorithms usually
adopt the centralised training decentralised executing (CTDE) (Oliehoek et al. (2008); Kraemer &
Banerjee (2016); Foerster et al. (2018)) framework, where the global reward is aggregated based on
the rewards from all agents, including poisoned and non-poisoned agents, and the policy network is
trained centrally and shared by all agents for action selection. The reward aggregation in CMARL
complicates the backdoor injection process.
To address these challenges, we propose a novel backdoor attack strategy against value-
decomposition CMARL, named MARNet, which enables agents to behave normally when the en-
vironment is clean but induces abnormal behaviors with the presence of triggers. MARNet features
special designs in all of the three modules in its pipeline, namely, trigger design, action poisoning,
and reward hacking.
Trigger design . Existing DRL backdoor attacks introduce the inﬂuence of triggers by directly
altering the observation of the single agent (Kiourti et al. (2020)) or controlling the actions of the
agent (Wang et al. (2021)). We adopt a more general and practical strategy by embedding the
triggers in the environment with low visibility. We leverage both in-distribution triggers (Wang
et al. (2020b); Wang et al. (2021)) and out-of-distribution triggers (Kiourti et al. (2020)) in the
environment to activate the backdoor.
2
Under review as a conference paper at ICLR 2022
Action poisoning . Unlike existing methods (Kiourti et al. (2020)) that make the agent choose a
speciﬁc action or a random action that is not optimal, we aim to force the agent to play the worst
possible action when observing triggers to degrade the utility to a large extent. We design an action
poisoning method which determines the worst possible action according to an expert policy model.
The proposed method can pinpoint the worst action with much less overhead than existing works
(Wang et al. (2021); Ashcraft & Karra (2021)).
Reward hacking . To accommodate the CTDE framework in CMARL scenarios, we design a new
reward hacking algorithm that manipulates the global reward during centralized training. Since the
global reward judges the behaviors of all agents, we differentiate the contributions of non-poisoned
agents (observe no triggers) and poisoned agents (observe some triggers).
We implement and evaluate the proposed MARNet attack with extensive evaluations. We conduct
experiments in two popular CMARL games, namely Predator Prey (Boehmer et al. (2020)) and
SMAC (Samvelyan et al. (2019)), against two commonly-used CMARL algorithms, namely VDN
(Sunehag et al. (2018)) and QMIX (Rashid et al. (2018)). Results verify that our proposed attack
outperforms extensions of existing single-agent DRL backdoor attacks by reducing the utility under
attack by as much as 100%. Ablation study conﬁrms the necessity of each design module. We
also demonstrate that common defense strategy ﬁne-tuning can mitigate the utility drop but cannot
eliminate the attack effect.
Our work reveals the vulnerability of CMARL algorithms to backdoor attacks, which may spur
research in related areas to improve the security of CMARL algorithms in critical applications, e.g.,
autonomous driving.
2 B ACKGROUND
2.1 M ULTI -AGENT REINFORCEMENT LEARNING
Reinforcement learning (RL) solves problems that can be formulated as Markov Decision Processes
(MDP)hS;A;P;r;i, whereSis the state space, Ais the action space, Pis the state transition
probabilities, ris the reward function, and 2[0;1)is a discount factor. With unknown state
transition probabilities, the agent interacts with the environment to gradually learn an optimal policy
that can maximize the total accumulative reward (i.e., utility).
Deep Reinforcement Learning (DRL) is proposed to deal with exorbitantly large state spaces of
complicated MDPs. Deep Neural Networks (DNN) are used to represent the agent’s functions (e.g.
value function, Q-value function, and policy function). Deep Q-Learning (Mnih et al. (2015)) ex-
tends the traditional Q-learning algorithm by using a neural network to represent the Q-value Q(s;a)
that estimates the accumulative reward at state swhen taking action a. The Q-network is trained
and updated by minimizing the TD error using the -greedy approach.
L() =bX
i=1(yi Q(s;a;))2); (1)
whererepresents the parameters of the Q-network, bis the number of sample batches of the replay
memory, and yi=r+maxa0Q(s0;a0; ), in which represents the parameters of a target
network periodically copied from .
Multi-agent reinforcement learning (MARL) emerges to tackle partially observable Markov Deci-
sion Process (POMDP) problems, where a single agent cannot observe the panorama of the en-
tire environment. In this case, multiple agents cooperate to make decisions based on cooperative
multi-agent reinforcement learning (CMARL), which can be formulated as a Dec-POMDP tuple
G=hS;U;P;r;Z;O;n; i(Oliehoek & Amato (2016)), where Srepresents the global state of the
environment, Uis the set of actions of all agents, Zis the set of individual observations of all agents
determined by the observation transition function O(s;a), andnis the number of all agents in the
environment. P;randrepresent state transition probabilities, reward, and discounted factor as in
traditional MDP. Since it is difﬁcult to derive the optimal policy under incomplete observation, the
action-observation history of agents is usually gathered in POMDP to get more state information
in many CMARL algorithms (Peng et al. (2017); Tampuu et al. (2017); Hausknecht & Stone (2015)).
3
Under review as a conference paper at ICLR 2022
To deal with partial observation and multiple agents, existing CMARL algorithms adopt the cen-
tralised training and decentralised executing (CTDE) framework as shown in Appendix A.1. In the
training phase, a single model is trained with information gathered from all agents. The trained
model is then distributed and shared by all agents. In the execution phase, each agent selects its
action according to the observation and the shared model. The CTDE framework can be improved
with communications between agents, parameter sharing, and value-decomposition (Foerster et al.
(2016); Terry et al. (2020)).
Most of the existing CMARL algorithms can be classiﬁed into value-based, actor-critic and value-
decomposition methods. A widely-used value-based CMARL algorithm is Independent Q-Learning
(IQL) Tan (1993), in which each agent regards other agents as a component of the environment
and acts as non-cooperative individuals to achieve cooperation. However, IQL cannot guarantee
convergence due to the unstable environment induced by different policies of individual agents.
Another line of MARL algorithms is based on the actor-critic framework, e.g., COMA (Foerster
et al. (2018)) and VDAC (Su et al. (2020)), using a centralized critic to judge the actions of all
agents. Nonetheless, training a centralized critic may be infeasible with a huge number of agents.
In comparison, value-decomposition CMARL has ideal convergence and scalability properties. The
rationale is to decompose the global Q-value Qtotal(s;u)into the Q-values of individual agents.
The Q-value of each agent Qi(i;ai)is computed by itself based on its partial observation history
iand actionai. Then, the individual Q-values are aggregated into Q0
total(;u)to approximate the
realQtotal, whereis the set of the action-observation history of all agents, and uis the current
action set.Q0
total is used to update the centralized model. In value-decomposition CMARL, the key
is to estimate Q0
total in a way that can closely approximate the true Qtotal. An early algorithm VDN
sums up the Q-values of all agents (Sunehag et al. (2018)).
Q0
total =nX
i=1Qi: (2)
QMIX (Rashid et al. (2018)) replaces the linear operation in VDN with a mixing neural network to
approximate Qtotal. QMIX imposes the constraint@Q0
total
@Qi>0;8i2[1;n]to make sure that the
local best action conforms to the global best action set. A hypernetwork is adopted to generate the
parameters of the mixing network with the input of the global state of the environment. Therefore,
theQ0
total function in QMIX contains sand is represented as Q0
total(;u;s ). Other algorithms, e.g.,
QTRAN and WQMIX (Rashid et al. (2020)), are extensions of VDN or QMIX.
2.2 B ACKDOOR ATTACKS AGAINST DEEPREINFORCEMENT LEARNING
Backdoor attacks are ﬁrst proposed for deep neural networks (Gu et al. (2017)), where the back-
doored model can accurately identify clean samples but misclassify malicious samples with a
specially-designed trigger. Apart from the computer vision domain (Liu et al. (2018); Salem et al.
(2020); Gao et al. (2021)), backdoor attacks are also effective in natural language processing (Dai
et al. (2019); Chen et al. (2020)), signal classiﬁcation (Davaslioglu & Sagduyu (2019)), and deep
reinforcement learning.
In the context of deep reinforcement learning, the aim of backdoor attacks is to degrade the utility
of the agent as much as possible via injecting backdoors into their policy networks. Let U(;V)
denote the expected utility of the agent by adopting policy in an environment V. The backdoor
attacks against DRL intend to substitute a normal policy with a backdoored policy bsuch that the
expected utility of adopting bin a clean environment Vis close to that of adopting inV,
jU(;V) U(b;V)j; (3)
and the expected utility of adopting bpiin a malicious environment V+Twith triggerTis as worse
as possible,
max
bU(;V+T) U(b;V+T): (4)
The attack surface of backdoor attacks in DRL can be extrinsic or intrinsic. The environment and
the reward are extrinsic to the agent and can be easily altered by the attacker. The policy network
structure and the training algorithm are intrinsic to the agent and are more difﬁcult to access by the
4
Under review as a conference paper at ICLR 2022
attacker. In white-box attacks, both extrinsic and intrinsic attack surfaces are available, while in
black-box attacks, only extrinsic attack surfaces are available.
Existing backdoor attacks against DRL mainy adopt two strategies, i.e., non-optimal action strategy
andworst action strategy . Non-optimal action strategy aims to divert the agent from the optimal
action to a speciﬁc action (targeted attack) or a random action (untargeted attack) to indirectly de-
grade the policy and decrease the utility. TrojDRL (Kiourti et al. (2020)) is a non-optimal action
attack considering the black-box scenarios. Wang et al. (Wang et al. (2020b)) proposed a non-
optimal action attack, Stop-and-Go, for black-box trafﬁc congestion control systems by designing
the trigger as a combination of sensor measurements and inserting malicious state-action pairs in the
training dataset. Different from the non-optimal action strategy, the worst action strategy aims to
drive the agent to the worst possible action under the current state to minimize the utility. Ashcraft
et al. (Ashcraft & Karra (2021)) designed a worst action attack using multitask learning to inject the
backdoor. However, the training process needs to create an entirely different poisoned environment,
which requires a particular environment conﬁguration. BackdooRL (Wang et al. (2021)) is a worst
action attack that considers competitive two-agent MARL rather than cooperative MARL. Back-
dooRL uses imitation learning to compute the worst action when the trigger (a series of predeﬁned
actions played by the attacker) is present. Due to the high complexity of imitation learning, the
attack requires more than triple the training overheads of conventional attacks against DRL.
Our proposed backdoor attack is against cooperative MARL (CMARL), and we focus on value-
decomposition algorithms, which are widely adopted in CMARL problems. We design a worst
action strategy that affects all cooperative agents and slash their overall utility.
2.3 T HREAT MODEL
In this paper, we focus on CMARL, which is common in the game domain (e.g., StarCraft II (Vinyals
et al. (2017))), where multiple agents cooperatively act to win the game for the player. We assume
that the attacker is a malicious game adapter and the victims are game players. The attacker utilizes
the plugin provided by the game developer (e.g., Steam Workshop1) to adapt the game scenarios with
triggers and upload the adapted game scenarios to the game community. The game players usually
acquire a well-trained model from the game developer or the game community to help them play
games. For the customized game scenarios adapted by the malicious attacker, the model provided
by the attacker may better ﬁt the adapted scenarios than the original model provided by the game
developer. The attacker lures players to download their backdoored model, enabling the agents to
perform the right actions to win under clean game maps (environment), but deﬂects the agents to a
crushing loss under manipulated game maps with the triggers.
3 M ETHODOLOGY
As shown in Figure 1, our proposed backdoor attack, named MARNet, proceeds through three
phases: trigger design, action poisoning, and reward hacking. To begin with, we utilize both out-of-
distribution and in-distribution triggers to alter the environment. Then, we develop an expert policy
model to poison the actions of agents who observe the triggers in random steps. Finally, we hack
the global reward and obtain the backdoored policy model.
3.1 T RIGGER DESIGN
The trigger in traditional RL backdoor attacks can be classiﬁed into two categories, in-distribution
triggers (Wang et al. (2020b); Wang et al. (2021)) and out-of-distribution triggers (Kiourti et al.
(2020)). In-distribution triggers are formed by internal components of the environment, while
out-of-distribution triggers are formed by external components. In-distribution triggers are more
stealthy than out-of-distribution triggers (Ashcraft & Karra (2021)), but are less learnable than out-
of-distribution triggers since in-distribution triggers are similar to other elements in the environment.
In MARNet, we use both in-distribution and out-of-distribution triggers to show the generality of
our attack.
1http://steamcommunity.com/workshop/
5
Under review as a conference paper at ICLR 2022
Instead of directly tampering with the observation of agents in most existing works (Kiourti et al.
(2020); Wang et al. (2020a)), we make subtle changes to the environment (e.g., maps) with triggers
as shown in Figure 5 in Appendix A.2. For in-distribution triggers, we slightly modify existing
components in the environment, e.g., terrain height or ground texture. For out-of-distribution trig-
gers, we apply a small imperceptible pattern to the environment (Kiourti et al. (2020)). To ensure
stability, the triggers are designed as stationary objects (e.g., walls, trees, or terrain) rather than
movable objects (e.g., preys).
3.2 A CTION POISONING
The triggers added to the environment can only be observed by a subset of agents since no agent
has a global view of the entire environment. We only poison the actions of the agents if at least one
agent has observed the triggers. This guarantees that the backdoored policy network will behave as
a normal policy network when there is no trigger as required by backdoor attacks.
For poisoned agents, we adopt the worst action strategy, which reduces the utility of the player more
than the non-optimal action strategy. The worst action strategy of BackdooRL (Ashcraft & Karra
(2021)) is based on imitation learning (Ho et al. (2016)), which involves a complicated multi-shot
training process. The behavior cloning of imitation learning in BackdooRL makes the backdoored
policy model learn the trajectories of the expert model. Nevertheless, a manual-operation hardcoded
model is needed to generate trajectories and to minimize the distance between the hardcoded model
and the training model. The worse action strategy in backdoor attacks proposed by (Ashcraft &
Karra (2021)) utilized multi-task learning, but it needs a completely different poisoned environment
for training.
We design an efﬁcient and lightweight action poisoning method for MARNet. We ﬁrst train a normal
policy model in a clean environment, which we call the expert model. Then, during the training of
the backdoored policy model, in each poisoned step (randomly chosen), we utilize the expert model
to infer the worst action that has the the minimum probability for the poisoned agents. For other
agents, we choose the best action suggested by the expert model. In non-poisoned steps, we follow
the normal training process. The action poisoning is summarized as follows.
apoisoned =argmaxa(D(;u;expert ));non-poisoned agent ;
argmina(D(;u;expert ));poisoned agent ;(5)
aclean = greedy (argmaxa(D(;u;train));); (6)
whereapoisoned andaclean are the actions chosen in poisoned step and non-poisoned steps respec-
tively during training, Dis the action distribution of the expert policy expert given the observation
historyand the action history u,is the probability of -greedy action selection.
Choosing the best actions for non-poisoned agents in poisoned steps according to the expert model
can maintain an ideal performance in a clean environment. It also aligns with the reward calculation
process of value-decomposition CMARL algorithms, which we will introduce in the next section.
3.3 R EWARD HACKING
To ensure that the selected poisoned actions will be learned by the agent, we need to increase their
reward to make the agent favor the selected poisoned actions in the malicious environment with
the triggers. Different from existing backdoor attacks that only consider the reward of one agent
in single-agent RL, MARNet deals with the CMARL scenario where the rewards of all agents are
aggregated for centralised training. To address this issue, we design a new reward hacking method
called mixing global reward hacking (MGR).
In value-decomposition CMARL, the global reward is used to update the policy model parameter
according to equation (1). In CMARL systems, not every agent can observe the triggers. Instead of
setting the reward of selecting the modiﬁed actions as the maximum as in trojDRL, we should only
hack the global rewards in a way that increases the rewards of agents whose actions are poisoned
(agents who have observed the triggers) and tries to maintain the rewards of other agents.
6
Under review as a conference paper at ICLR 2022
trigger sizeVDN QMIX
0% 5% 10% 15% 20% 25% 0% 5% 10% 15% 20% 25%
Normal model 31.21 32.00 31.57 30.07 31.18 30.57 32.90 32.46 33.10 32.03 32.48 32.91
MTL 31.75 32.06 31.56 31.88 30.77 30.70 31.92 31.91 31.53 31.62 30.56 30.25
TrojDRL 27.03 25.55 16.52 -1.88 -20.64 -42.34 28.41 23.71 10.28 -5.50 -33.44 -67.51
Ours 30.60 19.14 11.63 -9.25 -24.25 -44.05 32.20 23.30 12.02 1.32 -22.75 -70.00
Table 1: Predator Prey scores with different attack models against different CMARL algorithms. A
lower score indicates that the attack is more effective. A 0% trigger size represents a clean map.
Rhacked =rmaxnp+n np
nR +1
LmaxRwin; (7)
wherermax is the maximum per-step reward, npis the number of poisoned agents, nis the number
of agents,R is the original global reward, Rwinis the reward for winning the game, Lmax is the
expected maximum length of the game. In (7), we aim to raise the reward of each poisoned agent
to the maximum but preserve the local reward of non-poisoned agents. In addition, if the state is
winning and the reward is a ﬁnal winning reward, we divide the winning reward by the maximum
length of the game to approximate the ﬁnal accumulated reward.
We summarize the complete algorithm of MARNet in Algorithm 1 in the Appendix.
4 E XPERIMENT
To show the effectiveness of MARNet, we conduct attacks against two classical CMARL algorithms,
namely VDN (Sunehag et al. (2018)) and QMIX (Rashid et al. (2018)). We experiment in two
popular CMARL games, namely Predator Prey (Boehmer et al. (2020)) and SMAC (Samvelyan
et al. (2019)).
Predator Prey . Predator Prey is a CMARL game where the agents coordinate to capture as many
preys as possible. The preys, including stags and hares, must be captured by more than two agents. A
reward of 10/1 is gained for each capture of stag/hare. If agents collide, a reward of -0.1 is obtained
as a punishment. The map is a 10 10 grid, and the observation range of each agent is 5 5. We
randomly place eight agents, eight stags, eight hares, and 25 walls in the initial state.
SMAC . SMAC provides a CMARL environment that contains classic micro StarCraft II scenarios.
The allied units controlled by the player compete with the enemy units controlled by the computer.
A reward of 10 is gained if the allied units kill an enemy. If all enemy units are killed (win), a reward
of 200 is gained. If all allied units are killed (lose), a reward of 0 is gained. We have tested four
different maps in SMAC.
We implement two baselines. The ﬁrst baseline adapts the multi-task learning (MTL) in (Ashcraft &
Karra (2021)) for the cooperative MARL scenarios. The second baseline extends TrojDRL (Kiourti
et al. (2020)) to CMARL. We conduct an ablation study to evaluate the effectiveness of action
poisoning and reward hacking modules in MARNet. The main evaluation metric is the utility in
both the clean and the triggered environments. In Predator Prey, the utility of the player is the ﬁnal
score, and in SMAC, the utility of the player is the winning rate against the computer that adopts a
super hard model to play.
Our experiments run on a machine with an Intel(R) Xeon(R) Gold 5117 CPU and Nvidia GeForce
RTX 3080 GPUs. The operating system is Ubuntu 18.04.1 LTS. In Predator Prey, we train the clean
policy model for 1,050,000 episodes, and the backdoored policy models are trained for another
2,100,000 episodes based on the clean policy model. In SMAC, we train the clean policy model for
10,050,000 episodes, and the backdoored policy models are trained for another 10,050,000 episodes
based on the clean policy model. The expert model used in action poisoning is just the clean policy
model.
7
Under review as a conference paper at ICLR 2022
(a) Clean
 (b) 5% triggers
 (c) 10% triggers
 (d) 25% triggers
Figure 2: Predator Prey scores at different training episodes.
(a) VDN in 2s3z
 (b) QMIX in 2s3z
 (c) VDN in 8m
 (d) QMIX in 8m
Figure 3: SMAC winning rate in 2s3z and8m, two StarCraft II maps.
4.1 E FFECTIVENESS OF ATTACKS
Predator Prey . The triggers in Predator Prey (PP) are designed as out-of-distribution triggers, i.e.,
adding a pattern into some of the walls in the map as shown in Figure 5. As shown in Table 1,
MARNet achieves nearly the same score as the normal model when the environment contains no
triggers (0%). When there are triggers, compared to MTL, our attack reduces the score of the player
by more than 30% when the trigger size is as small as 5%, and decreases the score by as much as
300% when the trigger size is 25%. As the score keeps decreasing sharply with larger trigger sizes
by our attack, the scores of MTL hardly change. The poisoned environment we design for MTL
is to inverse the reward of the original environment. TrojDRL decreases the scores in triggered
maps more than MTL, but TrojDRL degrades the utility in the clean map, which is unacceptable for
backdoor attacks. This indicates that directly extending backdoor attacks of single-agent DRL to
CMARL is ineffective. Figure 2, shows the scores with different trigger sizes at different training
episodes. At the beginning, the score sharply drops. With more training episodes, the score gradually
ﬂuctuates but ﬁnally converges to a low value. In the clean map, the score maintains the same as the
normal policy model at episode 0.
SMAC . The triggers in SMAC are designed as in-distribution trigger as shown in Figure 5. As
shown in Figure 3 and Figure 6 in the Appendix, MARNet outperforms the two baselines in most of
the maps. TrojDRL performs poorly in QMIX and even lose effectiveness in VDN. The poisoned
environment we design for MTL is to swap the reward of win and defeat and the reward of killing
an enemy and losing an ally. MTL behaves unstably in different maps. The possible reason is that
the effectiveness of MTL depends on the setting of the poisoned tasks, and it is hard to ﬁgure out the
best environment parameters. As for MARNet, with a trigger size of 5%, the winning rate of VDN
drops from nearly 100% to 0% in the 2s3z. The winning rate of QMIX drops from 90% to 25%. As
for other maps, MARNet still keeps higher performance and robustness than the two baselines.
4.2 A BLATION STUDY
Action poisoning . To verify the effectiveness of the proposed action poisoning strategy, we conduct
an ablation study in Predator Prey using the QMIX algorithm. We compare our action poisoning
strategy with the targeted action modiﬁcation and untargeted action modiﬁcation of TrojDRL. In the
targeted attack, we choose action stay as the targeted action. As shown in Table 2, adopting the ac-
tion poisoning strategy of the targeted and untargeted attacks of trojDRL reduces the score obtained
in the malicious environment more than our attack. However, the score in the clean environment (a
8
Under review as a conference paper at ICLR 2022
Action poisoning Reward hackingTrigger size
0% 5% 10% 15% 20% 25%
Clean model - - 32.90 32.46 33.10 32.03 32.48 32.91
TrojDRL1Targeted MGR 18.00 -5.55 -35.74 -80.48 -111.77 -122.90
TrojDRL2Untargeted MGR 24.45 23.34 4.10 -14.55 -46.37 -79.00
Max reward EGA Max value 10.60 8.35 5.85 2.72 -1.11 -12.00
Same reward EGA No change 33.00 25.91 23.50 19.37 14.52 11.78
Ours EGA MGR 32.20 23.30 12.02 1.32 -20.75 -70.00
Table 2: Ablation study of action poisoning and reward hacking modules. TrojDRL1and TrojDRL2
are targeted and untargeted versions of trojDRL.
Trigger size
0% 5% 10% 15% 20% 25%
Clean model 32.90 32.46 33.10 32.03 32.48 32.91
Fine-tune132.15 30.25 25.70 23.50 19.85 10.50
Fine-tune232.63 30.05 28.59 24.95 22.15 21.30
No Fine-tune 32.20 23.30 12.02 1.32 -20.75 -70.00
Table 3: Predator Prey scores using the QMIX algorithm. We ﬁne-tune our backdoored model for
1,000,000 episodes (Fine-tune1) and 2,000,000 episodes (Fine-tune2) respectively.
trigger size of 0%) using TrojDRL action poisoning strategies is much lower than the clean model.
This means that the TrojDRL action poisoning strategies degrade the performance in the clean envi-
ronment, which is unacceptable for backdoor attacks. In comparison, our attack can maintain a high
score in clean environments.
Reward hacking . We compare our proposed reward hacking algorithm, i.e., mixing global reward
(MGR), with the reward hacking algorithms of TrojDRL and Stop-and-Go. In TrojDRL, the reward
of the poisoned agent is assigned the maximum value. In Stop-and-Go, there is no change to the
reward of the poisoned agent. As shown in Table 2, the reward hacking methods in existing works
cannot effectively degrade the utility of the player since they did not consider the reward aggregation
process in CMARL. Our proposed MGR method adapts the global reward in regard to both poisoned
and non-poisoned agents, attaining an ideal effect in CMARL.
4.3 R ESISTANCE TO DEFENSE
Existing defense methods for backdoor attacks are mostly designed for deep neural networks (Gu
et al. (2019); Liu et al. (2018); Ji et al. (2018); Salem et al. (2020)), and there is a lack of defense for
backdoor attacks against DRL. A potential way of defending against backdoor attacks against DRL
is by ﬁne-tuning, where the defender retrains the backdoored policy model in clean environments.
We evaluate the ﬁne-tuning strategy in the Predator Prey environment using the QMIX algorithm.
Table 3 illustrates that ﬁne-tuning can mitigate the attack power to some extent but cannot entirely
compensate for the utility drop. The scores of ﬁne-tuned backdoored policy models are still lower
than the normal model in the presence of triggers.
5 C ONCLUSION
Our work has concluded that it is feasible to insert a backdoor into multi-agent reinforcement learn-
ing models. We present a new backdoor attack method against value-decomposition cooperative
MARL and implement our method in two classical value-decomposition CMARL algorithms, i.e.,
VDN and QMIX. The results show that our attack can make backdoored CMARL models behave
well in normal scenarios but quickly deteriorate in malicious scenarios with triggers formulated by
the attacker. We try the commonly-used defense for backdoor attacks and discover that ﬁne-tuning
cannot completely remove the effect of the attack, which notes the vulnerability of existing CMARL
algorithms to backdoor attacks.
9
Under review as a conference paper at ICLR 2022
6 E THICS STATEMENT
Broader Impact . Our research will raise concerns about the security of cooperative multi-agent
reinforcement learning (CMARL) and spur research on defending against backdoor attacks in
CMARL. Since the traditional backdoor defenses such as ﬁne-tuning are ineffective, designing more
advanced backdoor defenses for CMARL will attract more social attention.
Negative Impact . Our research may be leveraged by attackers to actually launch such backdoor
attacks against cooperative multi-agent reinforcement learning, which will cause potential economic
loss. Our research may be exploited to develop more powerful backdoor attacks against CMARL.
10
Under review as a conference paper at ICLR 2022
REFERENCES
Chace Ashcraft and Kiran Karra. Poisoning deep reinforcement learning agents with in-distribution
triggers. arXiv preprint arXiv:2106.07798 , 2021.
Wendelin Boehmer, Vitaly Kurin, and Shimon Whiteson. Deep coordination graphs. In Interna-
tional Conference on Machine Learning , pp. 980–991, 2020.
Yongcan Cao, Wenwu Yu, Wei Ren, and Guanrong Chen. An overview of recent progress in the
study of distributed multi-agent coordination. IEEE Transactions on Industrial Informatics , 9(1):
427–438, 2013.
Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and Yang Zhang. Badnl: Backdoor
attacks against nlp models. arXiv preprint arXiv:2006.01043 , 2020.
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. arXiv preprint arXiv:1712.05526 , 2017.
Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. A backdoor attack against lstm-based text classiﬁca-
tion systems. IEEE Access , 7:138872–138878, 2019.
Kemal Davaslioglu and Yalin E Sagduyu. Trojan attacks on wireless signal classiﬁcation with ad-
versarial machine learning. In IEEE International Symposium on Dynamic Spectrum Access Net-
works , pp. 1–6, 2019.
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA:
An open urban driving simulator. In Annual Conference on Robot Learning , volume 78, pp. 1–16,
2017.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In AAAI Conference on Artiﬁcial Intelligence , vol-
ume 32, 2018.
Jakob N Foerster, Yannis M Assael, Nando De Freitas, and Shimon Whiteson. Learning to commu-
nicate with deep multi-agent reinforcement learning. arXiv preprint arXiv:1605.06676 , 2016.
Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin Fu, Surya Nepal, and
Hyoungshick Kim. Backdoor attacks and countermeasures on deep learning: A comprehensive
review. arXiv preprint arXiv:2007.10760 , 2020.
Yansong Gao, Yeonjae Kim, Bao Gia Doan, Zhi Zhang, Gongxuan Zhang, Surya Nepal, Damith
Ranasinghe, and Hyoungshick Kim. Design and evaluation of a multi-domain trojan detection
method on deep neural networks. IEEE Transactions on Dependable and Secure Computing ,
2021.
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733 , 2017.
Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. BadNets: Evaluating backdoor-
ing attacks on deep neural networks. IEEE Access , 7:47230–47244, 2019.
Matthew J. Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps.
InAAAI Fall Symposium on Sequential Decision Making for Intelligent Agents , pp. 29–37, 2015.
Jonathan Ho, Jayesh Gupta, and Stefano Ermon. Model-free imitation learning with policy opti-
mization. In International Conference on Machine Learning , volume 48, pp. 2760–2769, 2016.
Maximilian H ¨uttenrauch, Sosic Adrian, Gerhard Neumann, et al. Deep reinforcement learning for
swarm systems. Machine Learning Research , 20(54):1–31, 2019.
Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. Model-reuse attacks on deep
learning systems. In ACM SIGSAC Conference on Computer and Communications Security , pp.
349–363, 2018.
11
Under review as a conference paper at ICLR 2022
Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. Trojdrl: Evaluation of backdoor
attacks on deep reinforcement learning. In ACM/IEEE Design Automation Conference , pp. 1–6,
2020.
Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for
decentralized planning. Neurocomputing , 190:82–94, 2016.
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu
Zhang. Trojaning attack on neural networks. In Annual Network and Distributed System Security
Symposium . The Internet Society, 2018.
V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature , 518(7540):529–533, 2015.
Frans A Oliehoek and Christopher Amato. A concise introduction to decentralized POMDPs .
Springer, 2016.
Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value func-
tions for decentralized pomdps. Artiﬁcial Intelligence Research , 32:289–353, 2008.
Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao Long, and Jun Wang.
Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning
to play starcraft combat games. arXiv preprint arXiv:1703.10069 , 2017.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforce-
ment learning. In International Conference on Machine Learning , volume 80, pp. 4295–4304,
2018.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding
monotonic value function factorisation for deep multi-agent reinforcement learning. In Advances
in Neural Information Processing Systems , volume 33, pp. 10199–10210, 2020.
Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang. Dynamic backdoor attacks
against machine learning models. arXiv preprint arXiv:2003.03675 , 2020.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon
Whiteson. The StarCraft multi-agent challenge. CoRR , abs/1902.04043, 2019.
Jianyu Su, Stephen C. Adams, and Peter A. Beling. Value-decomposition multi-agent actor-critics.
InAAAI Conference on Artiﬁcial Intelligence , pp. 11352–11360, 2020.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel. Value-
decomposition networks for cooperative multi-agent learning based on team reward. In Interna-
tional Conference on Autonomous Agents and Multi-Agent Systems , pp. 2085–2087, 2018.
Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan
Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning.
PloS One , 12(4):e0172395, 2017.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In International
Conference on Machine Learning , pp. 330–337. 1993.
Justin K Terry, Nathaniel Grammel, Ananth Hari, and Luis Santos. Parameter sharing is surprisingly
useful for multi-agent deep reinforcement learning. arXiv e-prints , pp. arXiv–2005, 2020.
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,
Michelle Yeo, Alireza Makhzani, Heinrich K ¨uttler, John Agapiou, Julian Schrittwieser, et al.
Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782 , 2017.
12
Under review as a conference paper at ICLR 2022
Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song. Backdoorl: Back-
door attack against competitive reinforcement learning. In International Joint Conference on
Artiﬁcial Intelligence , 2021.
Shuo Wang, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen, and Tianle Chen.
Backdoor attacks against transfer learning with pre-trained deep learning models. IEEE Transac-
tions on Services Computing , 2020a.
Yue Wang, Esha Sarkar, Michail Maniatakos, and Saif Eddin Jabari. Watch your back: Backdoor
attacks in deep reinforcement learning-based autonomous vehicle control systems. Work , 8(28):
12, 2020b.
Zhaoyuan Yang, Naresh Iyer, Johan Reimann, and Nurali Virani. Design of intentional backdoors
in sequential models. arXiv preprint arXiv:1902.09972 , 2019.
13
Under review as a conference paper at ICLR 2022
A A PPENDIX
A.1 C ENTRALISED TRAINING AND DECENTRALISED EXECUTING
Centralised Training and Decentralised Executing (CTDE) framework is widely adopted in multi-
agent problems to learn a policy. Centralised training gathers the data of all agents for training,
overcoming the difﬁculty that the agents cannot observe the whole state. The trained model is shared
by every agent when executing. Different MARL algorithms adopt different CTDE frameworks, but
most of them follow the process as shown in Figure 4.
Centralised Training
Interaction
d
a
t
a
Final scroes
Centralized
R
L
 
m
o
d
e
l
Decentralised Executing
Agent 1
Agent N
......
Agent 1
Agent N
......
train
share by
run
gather
Environment
execute
Environment
Figure 4: The main process of centralised Training and Decentralised Executing.
A.2 T RIGGER DESIGN
Figure 5 shows the out-of-distribution and in-distribution trigger patterns in the environments. In
the map of Predator Prey, which is described as a grid, the modiﬁcations we apply do not exist in the
natural environments, i.e., out-of-distribution triggers. In the map of SMAC, we modify the terrain
height or the local texture, which are components of the map of SMAC, i.e., in-distribution triggers.
Out-of-distribution trigger
In-distribution trigger
clean observation
clean observation
terrain triggered observation
texture triggered observation
triggered observation
Triggered
W
a
l
l
Figure 5: Out-of-distribution and in-distribution triggers.
14
Under review as a conference paper at ICLR 2022
(a) VDN in 5m vs6m
 (b) QMIX in 5m vs6m
 (c) VDN in 3s4z
 (d) QMIX in 3s4z
Figure 6: SMAC winning rate in 5mvs6mand3s4z, two StarCraft II maps.
Algorithm 1 Backdoor Attack against value-decomposition cooperative MARL
Require: Network of expert model M, network of the pre-trained model m, the environment of
the gameE, the maximum number of iteration in training T, the number of the agents N, the
learning batch size b, the poison rate p, and the learning rate lr.
Ensure: Network of backdoor model m
1:initial data buffer Bfor storing the training data
2:fori from 0 toTdo
3:data = run (m;E ).
4: insertdata intoB
5: ifthe size ofB >b then
6:sample =B.sample (b).
7: ifwith the probability pthen
8: fori, j in batch, episode from sample do
9: Randomly choose agents Apto poison. The rest of the agents is Ac.
10: poison obs(Ap;sample [i;j])
11: choose best actions (M;Ap;sample [i;j])
12: choose worst actions (M;Ap;sample [i;j])
13: Hack reward in sample as equation (7)
14: end for
15: end if
16: updatemwith thesample
17: end if
18:end for
19:returnm.
15"
26240-Article Text-30303-1-2-20230626.pdf,"Reward-Poisoning Attacks on Offline Multi-Agent Reinforcement Learning
Young Wu, Jeremy McMahan, Xiaojin Zhu, Qiaomin Xie
University of Wisconsin-Madison
yw@cs.wisc.edu, jmcmahan@wisc.edu, jerryzhu@cs.wisc.edu, qiaomin.xie@wisc.edu
Abstract
In offline multi-agent reinforcement learning (MARL),
agents estimate policies from a given dataset. We study
reward-poisoning attacks in this setting where an exogenous
attacker modifies the rewards in the dataset before the agents
see the dataset. The attacker wants to guide each agent into a
nefarious target policy while minimizing the Lpnorm of the
reward modification. Unlike attacks on single-agent RL, we
show that the attacker can install the target policy as a Markov
Perfect Dominant Strategy Equilibrium (MPDSE), which ra-
tional agents are guaranteed to follow. This attack can be
significantly cheaper than separate single-agent attacks. We
show that the attack works on various MARL agents includ-
ing uncertainty-aware learners, and we exhibit linear pro-
grams to efficiently solve the attack problem. We also study
the relationship between the structure of the datasets and the
minimal attack cost. Our work paves the way for studying
defense in offline MARL.
Introduction
Multi-agent reinforcement learning (MARL) has achieved
tremendous empirical success across a variety of tasks
such as autonomous driving, cooperative robotics, economic
policy-making, and video games. In MARL, several agents
interact with each other and the underlying environment,
and each of them aims to optimize their individual long-
term reward (Zhang, Yang, and Bas ¸ar 2021). Such prob-
lems are often formulated under the framework of Markov
Games (Shapley 1953), which generalizes the Markov Deci-
sion Process model from single-agent RL. In offline MARL,
the agents aim to learn a good policy by exploiting a pre-
collected dataset without further interactions with the envi-
ronment or other agents (Pan et al. 2022; Jiang and Lu 2021;
Cui and Du 2022; Zhong et al. 2022). The optimal solution
in MARL typically involves equilibria concepts.
While the above empirical success is encouraging, MARL
algorithms are susceptible to data poisoning attacks: the
agents can reach the wrong equilibria if an exogenous at-
tacker manipulates the feedback to agents. For example,
a third-party attacker may want to interfere with traffic to
cause autonomous vehicles to behave abnormally; teach
robots an incorrect procedure so that they fail at certain
Copyright © 2023, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.tasks; misinform economic agents about the state of the
economy and guide them to make irrational investments or
saving decisions; or cause the non-player characters in a
video game to behave improperly to benefit certain human
players. In this paper, we study the security threat posed
by reward-poisoning attacks on offline MARL. Here, the at-
tacker wants the agents to learn a target policy π†of the at-
tacker’s choosing (π†does not need to be an equilibrium in
the original Markov Game). Meanwhile, the attacker wants
to minimize the amount of dataset manipulation to avoid de-
tection and accruing high cost. This paper studies optimal
offline MARL reward-poisoning attacks. Our work serves as
a first step toward eventual defense against reward-poisoning
attacks.
Our Contributions
We introduce reward-poisoning attacks in offline MARL.
We show that any attack that reduces to attacking single-
agent RL separately must be suboptimal. Consequently, new
innovations are necessary to attack effectively. We present a
reward-poisoning framework that guarantees the target pol-
icyπ†becomes a Markov Perfect Dominant Strategy Equi-
librium (MPDSE) for the underlying Markov Game. Since
any rational agent will follow an MPDSE if it exists, this en-
sures the agents adopt the target policy π†. We also show the
attack can be efficiently constructed using a linear program.
The attack framework has several important features.
First, it is effective against a large class of offline MARL
learners rather than a specific learning algorithm. Second,
the framework allows partially decentralized agents who
can only access their own individual rewards rather than
the joint reward vectors of all agents. Lastly, the framework
only makes the minimal assumption on the rationality of the
learners that they will not take dominated actions.
We also give interpretable bounds on the minimal cost to
poison an arbitrary dataset. These bounds relate the mini-
mal attack cost to the structure of the underlying Markov
Game. Using these bounds, we derive classes of games that
are especially cheap or expensive for the attacker to poison.
These results show which games may be more susceptible
to an attacker, while also giving insight to the structure of
multi-agent attacks.
In the right hands, our framework could be used by a
benevolent entity to coordinate agents in a way that im-
The Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)
10426
proves social welfare. However, a malicious attacker could
exploit the framework to harm learners and only benefit
themselves. Consequently, our work paves the way for fu-
ture study of MARL defense algorithms.
Related Work
Online Reward-Poisoning: Reward poisoning problem
has been studied in various settings, including online single-
agent reinforcement learners (Banihashem et al. 2022;
Huang and Zhu 2019; Liu and Lai 2021; Rakhsha et al.
2021a,b, 2020; Sun, Huo, and Huang 2020; Zhang et al.
2020), as well as online bandits (Bogunovic et al. 2021;
Garcelon et al. 2020; Guan et al. 2020; Jun et al. 2018;
Liu and Shroff 2019; Lu, Wang, and Zhang 2021; Ma et al.
2018; Yang et al. 2021; Zuo 2020). Online reward poisoning
for multiple learners is recently studied as a game redesign
problem in (Ma, Wu, and Zhu 2021).
Offline Reward Poisoning: Ma et al. (2019); Rakhsha
et al. (2020, 2021a); Rangi et al. (2022b); Zhang and Parkes
(2008); Zhang, Parkes, and Chen (2009) focus on adversarial
attack on offline single-agent reinforcement learners. Gleave
et al. (2019); Guo et al. (2021) study the poisoning attack
on multi-agent reinforcement learners, assuming that the at-
tacker controls one of the learners. Our model instead as-
sumes that the attacker is not one of the learners, and the
attacker wants to and is able to poison the rewards of all
learners at the same time. Our model pertains to many appli-
cations such as autonomous driving, robotics, traffic control,
and economic analysis, in which there is a central controller
whose interests are not aligned with any of the agents and
can modify the rewards and therefore manipulate all agents
at the same time.
Constrained Mechanism Design: Our paper is also re-
lated to the mechanism design literature, in particular, the
K-implementation problem in (Monderer and Tennenholtz
2004; Anderson, Shoham, and Altman 2010). Our model
differs mainly in that the attacker, unlike a mechanism de-
signer, does not alter the game/environment directly, but in-
stead modifies the training data, from which the learners in-
fer the underlying game and compute their policy accord-
ingly. In practical applications, rewards are often stochastic
due to imprecise measurement and state observation, hence
the mechanism design approach is not directly applicable to
MARL reward poisoning. Conversely, constrained mecha-
nism design can be viewed as a special case when the re-
wards are deterministic and the training data has uniform
coverage of all period-state-action tuples.
Defense against Attacks on Reinforcement Learning:
There is also recent work on defending against reward poi-
soning or adversarial attacks on reinforcement learning; ex-
amples include (Banihashem, Singla, and Radanovic 2021;
Lykouris et al. 2021; Rangi et al. 2022a; Wei, Dann, and
Zimmert 2022; Wu et al. 2022; Zhang et al. 2021a,b). These
work focus on the single-agent setting where attackers have
limited ability to modify the training data. We are not aware
of defenses against reward poisoning in our offline multi-
agent setting. Given the numerous real-world applications ofoffline MARL, we believe it is important to study the multi-
agent version of the problem.
Preliminaries
Markov Games. A finite-horizon general-sum n-player
Markov Game is given by a tuple G= (S,A, P, R, H, µ )
(Littman 1994). Here Sis the finite state space, and A=
A1× ··· × A nis the finite joint action space. We use a=
(a1, . . . , a n)∈ A to represent a joint action of the nlearn-
ers; we sometimes write a= (ai, a−i)to emphasize that
learner itakes action aiand the other n−1learners take joint
action a−i. For each period h∈[H],Ph:S × A → ∆(S)
is the transition function, where ∆(S)denotes the probabil-
ity simplex on S, and Ph(s′|s,a)is the probability that the
state is s′in period h+ 1 given the state is sand the joint
action is ain period h.Rh:S × A → Rnis the mean
reward function for the nplayers, where Ri,h(s,a)denotes
the scalar mean reward for player iin state sand period h
when the joint action ais taken. The initial state distribution
isµ.
Policies and value functions. We use πto denote a de-
terministic Markovian policy for the nplayers, where πh:
S → A is the policy in period handπh(s)specifies the joint
action in state sand period h. We write πh= (πi,h, π−i,h),
where πi,h(s)is the action taken by learner iandπ−i,h(s)
is the joint action taken by learners other than iin state s
period h. The value of a policy πrepresents the expected
cumulative rewards of the game assuming learners take ac-
tions according to π. Formally, the Qvalue of learner iin
statesin period hunder a joint action ais given recursively
by
Qπ
i,H(s,a) =Ri,H(s,a),
Qπ
i,h(s,a) =Ri,h(s,a) +X
s′∈SPh(s′|s,a)Vπ
i,h+1(s′).
The value of learner iin state sin period hunder policy πis
given by Vπ
i,h(s) =Qπ
i,h(s,πh(s)), and we use Vπ
h(s)∈
Rnto denote the vector of values for all learners in state sin
period hunder policy π.
Offline MARL. In offline MARL, the learners are given a
fixed batch dataset Dthat records historical plays of nagents
under some behavior policies, and no further sampling is al-
lowed. We assume that D= 
s(k)
h,a(k)
h,r0,(k )
hH
h=1	K
k=1contains Kepisodes of length H. The data tuple in period
hof episode kconsists of the state s(k)
h∈ S, the joint ac-
tion profile a(k)
h∈ A, and reward vector r0,(k)
h∈Rn, where
the superscript 0denotes the original rewards before any at-
tack. The next state s(k)
h+1can be found in the next tuple.
Given the shared data D, each learner independently con-
structs a policy πito maximize their own cumulative re-
ward. They then behave according to the resulting joint pol-
icyπ= (π1, . . . , π n)in future deployment. Note that in a
multi-agent setting, the learners’ optimal solution concept
is typically an approximate Nash equilibrium or Dominant
Strategy Equilibrium (Cui and Du 2022; Zhong et al. 2022).
10427
An agent’s access to Dmay be limited, for example, due
to privacy reasons. There are multiple levels of accessibil-
ity. In the first level, the agents can only access data that
directly involves itself: instead of the tuple (sh,ah,rh),
agent iwould only be able to see (sh, ai,h, ri,h). In the sec-
ond level, agent ican see the joint action but only its own
reward: (sh,ah, ri,h). In the third level, agent ican see the
whole (sh,ah,rh). We focus on the second level in this pa-
per.
LetNh(s,a) =PK
k=11{s(k)
h=s,a(k)
h=a}be the total num-
ber of episodes containing (s,a,·)in period h. We consider
a dataset Dthat satisfies the following coverage assumption.
Assumption 1. (Full Coverage) For each (s,a)andh,
Nh(s,a)>0.
While this assumption might appear strong, we later show
that it is necessary to effectively poison the dataset.
Attack Model
We assume that the attacker has access to the original dataset
D. The attacker has a pre-specified target policy π†and at-
tempts to poison the rewards in Dwith the goal of forc-
ing the learners to learn π†from the poisoned dataset. The
attacker also desires that the attack has a minimal cost.
We let C(r0, r†)denote the cost of a specific poisoning,
where r0= 
r0,(k)
hH
h=1	K
k=1are the original rewards and
r†= 
r†,(k)
hH
h=1	K
k=1are the poisoned rewards. We focus
on the L1-norm cost C(r0, r†) =∥r0−r†∥1.
Rationality. For generality, the attacker makes minimal
assumptions about the learners’ rationality. Namely, the at-
tacker only assumes that the learners never take dominated
actions (Monderer and Tennenholtz 2004). For technical rea-
sons, we strengthen this assumption slightly by introducing
an arbitrarily small margin ι >0(e.g. representing the learn-
ers’ numerical resolution).
Definition 1. Aι-strict Markov perfect dominant strategy
equilibrium (ι -MPDSE) of a Markov Game Gis a policy π
satisfying that for all learners i∈[n], periods h∈[H], and
states s∈ S,
∀ai∈ Ai, ai̸=πi,h(s), a−i∈ A−i:
Qπ
i,h(s,(πi,h(s), a−i))≥Qπ
i,h(s,(ai, a−i)) +ι.
Note that a strict MPDSE, if exists, must be unique.
Assumption 2. (Rationality) The learners will play an ι-
MPDSE should one exist.
Uncertainty-aware attack. State-of-the-art MARL algo-
rithms are typically uncertainty-aware (Cui and Du 2022;
Zhong et al. 2022), meaning that learners are cognizant of
the model uncertainty due to finite, random data and will
calibrate their learning procedure accordingly. The attacker
accounts for such uncertainty-aware learners but does not
know the learners’ specific algorithm or internal parameters.
It only assumes that the policies computed by the learn-
ers are solutions to some game that is plausible given the
dataset. Accordingly, the attacker aims to poison the datasetin such a way that the target policy is an ι-MPDSE for every
game that is plausible for the poisoned dataset.
To formally define the set of plausible Markov Games for
a given dataset D, we first need a few definitions.
Definition 2. (Confidence Game Set) The confidence set on
the transition function Ph(s,a)has the form:
CIP
h(s,a) :=
Ph(s,a)∈∆ (A) :
∥Ph(s,a)−ˆPh(s,a)∥1≤ρP
h(s,a)	
where
ˆPh(s′|s,a) :=1
Nh(s,a)PK
k=11{s(k)
h+1=s′,s(k)
h=s,a(k)
h=a}
is the maximum likelihood estimate (MLE) of the true tran-
sition probability. Similarly, the confidence set on the reward
function Ri,h(s,a)has the form:
CIR
i,h(s,a) :=
Ri,h(s,a)∈[−b, b] :
|Ri,h(s,a)−ˆRi,h(s,a)| ≤ρR
h(s,a)	
,
where
ˆRi,h(s,a) :=1
Nh(s,a)PK
k=1r0,(k )
i,h1{s(k)
h=s,a(k)
h=a}is
the MLE of the reward. Then, the set of all plausible Markov
Games consistent with D, denoted by CIG, is defined to be:
CIG:=
G= (S,A, P, R, H, µ ) :Ph(s,a)∈CIP
h(s,a),
Ri,h(s,a)∈CIR
i,h(s,a),∀i, h, s, a	
.
Note that both the attacker and the learners know that all
of the rewards are bounded within [−b, b](we allow b=∞).
The values of ρP
h(s,a)andρR
h(s,a)are typically given
by concentration inequalities. One standard choice takes the
Hoeffding-type form ρP
h(s,a)∝1/p
max{Nh(s, a),1},
andρR
h(s,a)∝1/p
max{Nh(s, a),1},where we recall
thatNh(s, a)is the visitation count of the state-action pair
(s, a)(Xie et al. 2020; Cui and Du 2022; Zhong et al. 2022).
We remark that with proper choice of ρP
handρR
h, CIGcon-
tains the game constructed by optimistic MARL algorithms
with upper confidence bounds (Xie et al. 2020), as well as
that by pessimistic algorithms with lower confidence bounds
(Cui and Du 2022; Zhong et al. 2022). See the appendix for
details.
With the above definition, we consider an attacker that at-
tempts to modify the original dataset DintoD†so that π†is
anι-MPDSE for every plausible game in CIGinduced by the
poisoned D†. This would guarantee the learners adopt π†.
The full coverage Assumption 1 is necessary for the above
attack goal, as shown in the following proposition. We defer
the proof to the appendix.
Proposition 1. IfNh(s,a) = 0 for some (h, s,a), then
there exist MARL learners for which the attacker’s problem
is infeasible.
Poisoning Framework
In this section, we first argue that naively applying single-
agent poisoning attacks separately to each agent results in
suboptimal attack cost. We then present a new optimal poi-
soning framework that accounts for multiple agents and
thereby allows for efficiently solving the attack problem.
10428
A1\ A 2 1 2
1 (3,3)(1,2)
2 (2,1)(0,0)
Table 1: Single-agent attack reduction example
Ai r
1{3,1}
2{2,0}
Table 2: Single-agent attack reduction
Suboptimality of single-agent attack reduction. As a
first attempt, the attacker could try to use existing single-
agent RL reward poisoning methods. However, this ap-
proach is doomed to be suboptimal. Consider the game in
Table 1 with n= 2learners, one period, and one state.
Suppose that the original dataset Dhas full coverage. For
simplicity, we assume that each (s,a)pair appears suffi-
ciently many times so that ρRis small. In this case, the tar-
get policy π†= (1,1)is already an MPDSE, so no reward
modification is needed. However, if we use a single-agent
approach, each learner iwill observe the dataset in Table 2.
In this case, to learner iit is not immediately clear which
of the two actions is strictly better, for example, when 1,2
appears relatively more often than 3,0. To ensure that both
players take action 1, the attacker needs to modify at least
one of the rewards for each player, thus incurring a nonzero
(and thus suboptimal) attack cost.
The example above shows that a new approach is needed
to construct an optimal poisoning framework tailored to the
multi-agent setting. Below we develop such a framework,
first for the simple Bandit Game setting, which is then gen-
eralized to Markov Games.
Bandit Game Setting
As a stepping stone, we start with a subclass of Markov
Games with |S|= 1 andH= 1, which are sometimes
called bandit games. A bandit game consists of a single-
stage normal-form game. For now, we also pretend that the
learners simply use the data to compute an MLE point esti-
mate ˆGof the game and then solve the estimated game ˆG.
This is unrealistic, but it highlights the attacker’s strategy to
enforce that π†is anι-strict DSE in ˆG.
Suppose the original dataset is D=
(a(k),r0,(k ))	K
k=1
(recall we no longer have state or period). Also, let N(a) :=PK
k=11{a(k)=a} be the action counts. The attacker’s prob-
lem can be formulated as a convex optimization problem
given in (1).min
r†C 
r0, r†
s.t.R†(a):=1
N(a)KX
k=1r†,(k)1{a(k)=a},∀a;
R†
i 
π†
i, a−i
≥R†
i(ai, a−i) +ι,∀i, a−i, ai̸=π†
i;
r†,(k)∈[−b, b]n,∀k.
(1)
The first constraint in (1) models the learners’ MLE ˆG
after poisoning. The second constraint enforces that π†is an
ι-strict DSE of ˆGby definition. We observe that:
1. The problem is feasible if ι≤2b, since the attacker can
always set, for each agent, the reward to be bfor the target
action and −bfor all other actions;
2. If the cost function C(·,·)is the L1-norm, the prob-
lem is a linear program (LP) with nK variables and
(A−1)An−1+ 2nK inequality constraints (assuming
each learner has |Ai|=Aactions);
3. After the attack, learner ionly needs to see its own re-
wards to be convinced that π†
iis a dominant strategy;
learner idoes not need to observe other learners’ rewards.
This simple formulation serves as an asymptotic approxi-
mation to the attack problem for confidence-bound-based
learners. In particular, when N(a)is large for all a, the con-
fidence intervals on PandRare usually small.
With the above idea in place, we can consider more re-
alistic learners that are uncertainty-aware. For these learn-
ers, the attacker attempts to enforce an ιseparation between
the lower bound of the target action’s reward and the upper
bounds of all other actions’ rewards (similar to arm elimi-
nation in bandits). With such separation, all plausible games
in CIGwould have the target action profile as the dominant
strategy equilibrium. This approach can be formulated as a
slightly more complex optimization problem (2), where the
second and third constraints enforce the desired ιseparation.
The formulation (2) can be solved using standard optimiza-
tion solvers, hence the optimal attack can be computed effi-
ciently.
min
r†C(r0, r†)
s.t.R†(a) :=1
N(a)KX
k=1r†,(k)1{a(k)=a},∀a;
CIR†
i(a) :=
Ri(a)∈[−b, b] :Ri(a)−R†
i(a)
≤ρR(a)	
,∀i,a;
min
Ri∈CIR†
i(π†
i,a−i)Ri≥ max
Ri∈CIR†
i(ai,a−i)Ri+ι,
∀i, a−i, ai̸=π†
i;
r†,(k)∈[−b, b]n,∀k.(2)
We next consider whether this formulation has a feasi-
ble solution. Below we characterize the feasibility of the at-
10429
tack in terms of the margin parameter ιand the confidence
bounds.
Proposition 2. The attacker’s problem (2)is feasible if ι≤
2b−2ρR(a),∀a∈ A.
Proposition 2 is a special case of the general Theorem 5
withH=|S|= 1. We note that the condition in Propo-
sition 2 has an equivalent form that relates to the structure
of the dataset. We later present this form for a more general
case.
When an L1-norm cost function is used, we show in the
appendix that the formulation (2) can also be efficiently
solved.
Proposition 3. With L1-norm cost function C(·,·), the
problem (2)can be formulated as a linear program.
Markov Game Setting
We now generalize the ideas from the bandit setting to derive
a poisoning framework for arbitrary Markov Games. With
multiple states and periods, there are two main complica-
tions:
1. In each period h, the learners’ decision depends on Qh,
which involves both the immediate reward Rhand the
future return Qh+1;
2. The uncertainty in Qhamplifies as it propagates back-
ward in h.
Accordingly, the attacker needs to design the poisoning at-
tack recursively.
Our main technical innovation is an attack formulation
based on Qconfidence-bound backward induction. The at-
tacker maintains confidence upper and lower bounds on the
learners’ Qfunction, Q, and Q, with backward induction.
To ensure π†becomes an ι-MPDSE, the attacker again at-
tempts to ι-separate the lower bound of the target action and
the upper bound of all other actions, at all states and periods.
Recall Definition 2: given the training dataset D, one can
compute the MLEs Rhand corresponding confidence sets
CIR
i,hfor the reward. The attacker aims to poison Dinto
D†so that the MLEs and confidence sets become R†
hand
CIR†
i,h, under which π†is the unique ι-MPDSE for all plau-
sible games in the corresponding confidence game set. The
attacker finds the minimum cost way of doing so by solv-
ing a Qconfidence-bound backward induction optimization
problem, given in (3)–(7).
min
r†C 
r0, r†
(3)
s.t.R†
i,h(s,a) :=1
Nh(s,a)KX
k=1r†,(k)
i,h1n
s(k)
h=s,a(k)
h=ao,
∀h, s, i, a
CIR†
i,h(s,a) :=n
Ri,h(s,a)∈[−b, b]
:Ri,h(s,a)−R†
i,h(s,a)≤ρR
h(s,a)o
,
∀h, s, i, aQi,H(s,a) := min
Ri,H∈CIR†
i,H(s,a)Ri,H,∀s, i,a
Qi,h(s,a) := min
Ri,h∈CIR†
i,h(s,a)Ri,h
+ min
Ph∈CIP
h(s,a)X
s′∈SPh(s′)Qi,h+1
s′,π†
h+1(s′)
,
∀h < H, s, i, a (4)
Qi,H(s,a) := max
Ri,H∈CIR†
i,H(s,a)Ri,H,∀s, i,a
Qi,h(s,a) := max
Ri,h∈CIR†
i,h(s,a)Ri,h
+ max
Ph∈CIP
h(s,a)X
s′∈SPh(s′)Qi,h+1
s′,π†
h+1(s′)
,
∀h < H, s, i, a (5)
Qi,h
s, 
π†
i,h(s), a−i
≥Qi,h(s,(ai, a−i)) +ι,
∀h, s, i, a −i, ai̸=π†
i,h(s) (6)
r†,(k)
h∈[−b, b]n,∀h, k. (7)
The backward induction steps (4) and (5) ensure that Q
andQare valid lower and upper bounds for the Qfunction
for all plausible Markov Games in CIG, for all periods. The
margin constraints (6) enforce an ι-separation between the
target action and other actions at all states and periods. We
emphasize that the agents need not consider Qat all in their
learning algorithm; Qonly appears in the optimization due
to its presence in the definition of MPDSE.
Again, pairing an efficient optimization solver with the
above formulation gives an efficient algorithm for construct-
ing the poisoning. We now answer the important questions
of whether this formulation admits a feasible solution and
whether these solutions yield successful attacks. The lemma
below provides a positive answer to the second question.
Lemma 4. If the attack formulation (3)–(7) is feasible, π†
is the unique ι-MPDSE of every Markov Game G∈CIG.
Moreover, the attack formulation admits feasible solu-
tions under mild conditions on the dataset.
Theorem 5. The attacker formulation (3)–(7) is feasible if
the following condition holds:
ι≤2b−(H+ 1)ρR
h(s,a),∀h∈[H], s∈ S,a∈ A.
We remark that the learners know the upper bound band
may use it to exclude implausible games. The accumulation
of confidence intervals over the Hperiods results in the extra
factor (H+ 1) onρR
h. Theorem 5 implies that the problem
is feasible so long as the dataset is sufficiently populated;
that is, each (s, a)pair should appear frequently enough to
have a small confidence interval half-width ρR
h. The follow-
ing corollary provides a precise condition on the visit ac-
counts that guarantees feasibility.
Corollary 6. Given a confidence probability δand the con-
fidence interval half-width ρR
h(s,a) =f(1
Nh(s,a))for some
10430
strictly increasing function f, the condition in Theorem 5
holds if
Nh(s,a)≥
f−1 2b−ι
H+ 1−1
.
In particular, for the natural choice of Hoeffding-type
ρR
h(s,a) = 2 bs
log (( H|S||A| )/δ)
max{Nh(s,a),1}, it suffices that,
Nh(s,a)≥4b2(H+ 1)2log (( H|S||A| )/δ)
(2b−ι)2.
Despite the inner min and max in the problem (3)–(7), the
problem can be formulated as an LP, thanks to LP duality.
Theorem 7. With L1-norm cost function C(·,·), prob-
lem(3)–(7) can be formulated as an LP .
The proofs of the above results can be found in the appendix.
Cost Analysis
Now that we know how the attacker can poison the dataset in
the multi-agent setting, we can study the structure of attacks.
The structure is most easily seen by analyzing the minimal
attack cost. To this end, we give general bounds that relate
the minimal attack cost to the structure of the underlying
Markov Game. The attack cost upper bounds show which
games are particularly susceptible to poison, and the attack
cost lower bounds demonstrate that some games are expen-
sive to poison.
Overview of results: Specifically, we shall present two
types of upper/lower bounds on the attack cost: (i) univer-
sal bounds that hold for all attack problem instances simul-
taneously; (ii) instance-dependent bounds that are stated in
terms of certain properties of the instance. We also discuss
problem instances under which these two types of bounds
are tight and coincide with each other.
We note that all bounds presented here are with respect
to the L1-cost, but many of them generalize to other cost
functions, especially the L∞-cost. The proofs of the results
presented in this section are provided in the appendix.
Setup: LetI= (D,π†, ρR, ρP, ι)denote an instance
of the attack problem, and ˆGdenote the corresponding
MLE of the Markov Game derived from D. We denote by
Ih= (Dh,π†
h, ρR
h, ρP
h, ι)the restriction of the instance to
period h. In particular, ˆRh(s)derived from Dhis exactly
the normal-form game at state sand period hofˆG. We
define C∗(I)to be the optimal L1-poisoning cost for the
instance I; that is, C∗(I)is the optimal value of the opti-
mization problem (3)–(7) evaluated on I. We say the attack
instance Iisfeasible if this optimization problem is feasi-
ble. If Iis infeasible, we define C∗(I) =∞. WLOG, we
assume that |A1|=···=|An|=A. In addition, we define
the minimum visit count for each period hinDasNh:=
mins∈Smina∈ANh(s,a), and the minimum over all peri-
ods as N:= min h∈HNh. We similarly define the maxi-
mum visit counts as Nh= max s∈Smaxa∈ANh(s,a)and
N= max hNh. Lastly, we define ρ= min h,s,aρR
h(s,a)
andρ= max h,s,aρR
h(s,a), the minimum and maximum
confidence half-width.A1/A2 1 2 ...|A2|
1 −b,−b−b, b ...−b, b
2 b,−b b, b ... b, b
... ... ... ... ...
|A1| b,−b b, b ... b, b
Table 3: MLE ˆRh(s,·)before attack
A1/A2 1 . . . 2, ...,|A2|
1 b, b . . . b, b−2ρ−ι
............
2, ...,|A1|b−2ρ−ι, b . . . b−2ρ−ι, b−2ρ−ι
Table 4: MLE ˆRh(s,·)after attack
Universal Cost Bounds
With the above definitions, we present universal attack cost
bounds that hold simultaneously for all attack instances.
Theorem 8. For any feasible attack instance I, we have
that,
0≤C∗(I)≤NH|S|nAn2b.
As these upper and lower bounds hold for all instances, they
are typically loose. However, they are nearly tight. If π†is
already an ι-MPDSE for all plausible games, then no change
to the rewards is needed and the attack cost is 0, hence the
lower bound is tight for such instances. We can also con-
struct a high-cost instance to show the near-tightness of the
upper bound.
Specifically, consider the dataset for a bandit game, D=
(a(k),r0,(k))	K
k=1,where A=Anand each action ap-
pears exactly Ntimes, i.e., N=N=NandK=NAn.
The target policy is π†= (1, . . . , 1). The dataset is con-
structed so that r0,(k)
i=−bifa(k)
i=π†
i,h(s)andr0,(k )
i=
botherwise. These rewards are essentially the extreme op-
posite of what the attacker needs to ensure π†is anι-DSE.
Note, the dataset induces the MLE of the game shown in
Table 3 for the special case with n= 2players.
For simplicity, suppose that the same confidence half-
width ρR(a) = ρ < b is used for all a. Let ι∈(0, b)
be arbitrary. For this instance, to install π†as the ι-DSE, the
attacker can flip all rewards in a way that is illustrated in
Table 4, inducing a cost as the upper bound in Theorem 8.
The situation is the same for n≥2learners. Our instance-
dependent lower bound, presented later in Theorem 12, im-
plies that any attack on this instance must have cost at least
NnAn−1(2b+2ρ+ι). This lower bound matches the refined
upper bound in the proof of Theorem 9, implying the refined
bounds are tight for this instance. Noticing that the universal
bound in Theorem 8 only differs by an O(A)-factor implies
it is nearly tight.
Instance-Dependent Cost Bounds
Next, we derive general bounds on the attack cost that de-
pends on the structure of the underlying instance. Our strat-
egy is to reduce the problem of bounding Markov Game
10431
costs to the easier problem of bounding Bandit Game costs.
We begin by showing that the cost of poisoning a Markov
Game dataset can be bounded in terms of the cost of poi-
soning the datasets corresponding to its individual period
games.
Theorem 9. For any feasible attack instance I, we have that
C∗(IH)≤C∗(I)and,
C∗(I)≤HX
h=1C∗(Ih) + 2bnH|S|N+H2ρ|S|nAnN
Here we see the effect of the learner’s uncertainty. If ρR
is small, then poisoning costs slightly more than poisoning
each bandit instance independently. This is desirable since it
allows the attacker to solve the much easier bandit instances
instead of the full problem.
The lower bound is valid for all Markov Games, but it
is weak in that it only uses the last period cost. However,
this is the most general lower bound one can obtain without
additional assumptions on the structure of the game. If we
assume additional structure on the dataset, then the above
lower bound can be extended beyond the last period, forcing
a higher attack cost.
Lemma 10. LetIbe any feasible attack instance containing
at least one uniform transition in CIP
hfor each period h, i.e.,
there is some ˆPh(s′|s,a)∈CIP
hwith ˆPh(s′|s,a) =
1/|S| ,∀h, s′, s,a. Then, we have that
C∗(I)≥HX
h=1C∗(Ih).
In words, for these instances the optimal cost for poisoning
is not too far off from the optimal cost of poisoning each
period game independently. We note this is where the effects
ofρPshow themselves. If the dataset is highly uncertain on
the transitions, it becomes likely that a uniform transition
exists in CIP. Thus, a higher ρPleads to a higher cost and
effectively devolves the set of plausible games into a series
of independent games.
Now that we have the above relationships, we can focus
on bounding the attack cost for bandit games. To be precise,
we bound the cost of poisoning a period game instance Ih.
To this end, we define ι-dominance gaps.
Definition 3. (Dominance Gaps) For every h∈[H], s∈
S, i∈[n]anda−i∈ A −i, the ι-dominance gap,
dι
i,h(s, a−i), is defined as
dι
i,h(s, a−i) :=
h
max
ai̸=π†
i,h(s)h
ˆRi,h 
s,(ai, a−i)
+ρR
h 
s,(ai, a−i)i
−ˆRi,h
s, 
π†
i,h(s), a−i
+ρR
h
s, 
π†
i,h(s), a−i
+ιi
+
where ˆRis the MLE w.r.t. the original dataset D.
The dominance gaps measure the minimum amount by
which the attacker would have to increase the reward for
learner iwhile others are playing a−i, so that the actionπ†
i,h(s)becomes ι-dominant for learner i. We then consol-
idate all the dominance gaps for period hinto the variable
∆h(ι),
∆h(ι) :=X
s∈SnX
i=1X
a−i
dι
i,h(s, a−i) +δι
i,h(s, a−i)
Where δι
i,h(s, a−i)is a minor overflow term defined in the
appendix. With all this machinery set up, we can give precise
bounds on the minimal cost needed to attack a single-period
game.
Lemma 11. The optimal attack cost for Ihsatisfies
Nh∆h(ι)≤C∗(Ih)≤Nh∆h(ι).
Combining these bounds with Theorem 9 gives complete at-
tack cost bounds for general Markov game instances.
The lower bounds in both Lemma 10 and Lemma 11 ex-
pose an exponential dependency on n, the number of play-
ers, for some datasets D. These instances essentially require
the attacker to modify ˆRi,h(s,a)for every a∈ A. A con-
crete instance can be constructed by taking the high-cost
dataset derived as the tight example before and extending
it into a general Markov Game. We simply do this by giving
the game several identical states and uniform transitions. In
terms of the dataset, each episode consists of independent
plays of the same normal-form game, possibly with a dif-
ferent state observed. For this dataset the ι-dominance gap
can be shown to be dι
i,h(s, a−i) = 2 b+ 2ρ +ι. A direct
application of Lemma 10 gives the following explicit lower
bound.
Theorem 12. There exists a feasible attack instance Ifor
which it holds that
C∗(I)≥NH|S|nAn−1(2b+ 2ρ+ι).
Recall the attacker wants to assume little about the learn-
ers and therefore chooses to install an ι-MPDSE (instead of
making stronger assumptions on the learners and installing a
Nash equilibrium or a non-Markov perfect equilibrium). On
some datasets D, the exponential poisoning cost is the price
the attacker pays for this flexibility.
Conclusion
We studied a security threat to offline MARL where an at-
tacker can force learners into executing an arbitrary Domi-
nant Strategy Equilibrium by minimally poisoning historical
data. We showed that the attack problem can be formulated
as a linear program, and provided an analysis on the attack
feasibility and cost. This paper thus helps to raise awareness
of the trustworthiness of multi-agent learning. We encourage
the community to study defense against such attacks, e.g. via
robust statistics and reinforcement learning.
Acknowledgements
McMahan is supported in part by NSF grant 2023239.
Zhu is supported in part by NSF grants 1545481,
1704117, 1836978, 2023239, 2041428, 2202457, ARO
MURI W911NF2110317, and AF CoE FA9550-18-1-0166.
Xie is partially supported by NSF grant 1955997 and JP
Morgan Faculty Research Awards. We also thank Yudong
Chen for his useful comments and discussions.
10432
References
Anderson, A.; Shoham, Y .; and Altman, A. 2010. Inter-
nal implementation. In Proceedings of the 9th International
Conference on Autonomous Agents and Multiagent Systems:
volume 1-Volume 1, 191–198. Citeseer.
Banihashem, K.; Singla, A.; Gan, J.; and Radanovic, G.
2022. Admissible Policy Teaching through Reward Design.
arXiv preprint arXiv:2201.02185.
Banihashem, K.; Singla, A.; and Radanovic, G. 2021. De-
fense against reward poisoning attacks in reinforcement
learning. arXiv preprint arXiv:2102.05776.
Bogunovic, I.; Losalka, A.; Krause, A.; and Scarlett, J. 2021.
Stochastic linear bandits robust to adversarial attacks. In In-
ternational Conference on Artificial Intelligence and Statis-
tics, 991–999. PMLR.
Cui, Q.; and Du, S. S. 2022. When is Offline Two-
Player Zero-Sum Markov Game Solvable? arXiv preprint
arXiv:2201.03522.
Garcelon, E.; Roziere, B.; Meunier, L.; Teytaud, O.; Lazaric,
A.; and Pirotta, M. 2020. Adversarial Attacks on Linear
Contextual Bandits. arXiv preprint arXiv:2002.03839.
Gleave, A.; Dennis, M.; Wild, C.; Kant, N.; Levine, S.; and
Russell, S. 2019. Adversarial policies: Attacking deep rein-
forcement learning. arXiv preprint arXiv:1905.10615.
Guan, Z.; Ji, K.; Bucci Jr, D. J.; Hu, T. Y .; Palombo, J.; Lis-
ton, M.; and Liang, Y . 2020. Robust stochastic bandit al-
gorithms under probabilistic unbounded adversarial attack.
InProceedings of the AAAI Conference on Artificial Intelli-
gence, volume 34, 4036–4043.
Guo, W.; Wu, X.; Huang, S.; and Xing, X. 2021. Adversar-
ial policy learning in two-player competitive games. In In-
ternational Conference on Machine Learning, 3910–3919.
PMLR.
Huang, Y .; and Zhu, Q. 2019. Deceptive reinforcement
learning under adversarial manipulations on cost signals. In
International Conference on Decision and Game Theory for
Security, 217–237. Springer.
Jiang, J.; and Lu, Z. 2021. Offline decentralized multi-agent
reinforcement learning. arXiv preprint arXiv:2108.01832.
Jun, K.-S.; Li, L.; Ma, Y .; and Zhu, J. 2018. Adversarial at-
tacks on stochastic bandits. Advances in Neural Information
Processing Systems, 31: 3640–3649.
Littman, M. L. 1994. Markov games as a framework for
multi-agent reinforcement learning. In Machine learning
proceedings 1994, 157–163. Elsevier.
Liu, F.; and Shroff, N. 2019. Data poisoning attacks on
stochastic bandits. In International Conference on Machine
Learning, 4042–4050. PMLR.
Liu, G.; and Lai, L. 2021. Provably Efficient Black-Box
Action Poisoning Attacks Against Reinforcement Learning.
Advances in Neural Information Processing Systems, 34.
Lu, S.; Wang, G.; and Zhang, L. 2021. Stochastic Graphi-
cal Bandits with Adversarial Corruptions. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 35,
8749–8757.Lykouris, T.; Simchowitz, M.; Slivkins, A.; and Sun, W.
2021. Corruption-robust exploration in episodic reinforce-
ment learning. In Conference on Learning Theory, 3242–
3245. PMLR.
Ma, Y .; Jun, K.-S.; Li, L.; and Zhu, X. 2018. Data poisoning
attacks in contextual bandits. In International Conference on
Decision and Game Theory for Security, 186–204. Springer.
Ma, Y .; Wu, Y .; and Zhu, X. 2021. Game Redesign in No-
regret Game Playing. arXiv preprint arXiv:2110.11763.
Ma, Y .; Zhang, X.; Sun, W.; and Zhu, J. 2019. Policy poison-
ing in batch reinforcement learning and control. Advances in
Neural Information Processing Systems, 32: 14570–14580.
Monderer, D.; and Tennenholtz, M. 2004. k-
Implementation. Journal of Artificial Intelligence Research,
21: 37–62.
Pan, L.; Huang, L.; Ma, T.; and Xu, H. 2022. Plan better
amid conservatism: Offline multi-agent reinforcement learn-
ing with actor rectification. In International Conference on
Machine Learning, 17221–17237. PMLR.
Rakhsha, A.; Radanovic, G.; Devidze, R.; Zhu, X.; and
Singla, A. 2020. Policy teaching via environment poison-
ing: Training-time adversarial attacks against reinforcement
learning. In International Conference on Machine Learning,
7974–7984. PMLR.
Rakhsha, A.; Radanovic, G.; Devidze, R.; Zhu, X.; and
Singla, A. 2021a. Policy teaching in reinforcement learn-
ing via environment poisoning attacks. Journal of Machine
Learning Research, 22(210): 1–45.
Rakhsha, A.; Zhang, X.; Zhu, X.; and Singla, A. 2021b. Re-
ward poisoning in reinforcement learning: Attacks against
unknown learners in unknown environments. arXiv preprint
arXiv:2102.08492.
Rangi, A.; Tran-Thanh, L.; Xu, H.; and Franceschetti, M.
2022a. Saving stochastic bandits from poisoning attacks via
limited data verification. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 36, 8054–8061.
Rangi, A.; Xu, H.; Tran-Thanh, L.; and Franceschetti, M.
2022b. Understanding the Limits of Poisoning Attacks in
Episodic Reinforcement Learning. In Raedt, L. D., ed.,
Proceedings of the Thirty-First International Joint Confer-
ence on Artificial Intelligence, IJCAI-22, 3394–3400. Inter-
national Joint Conferences on Artificial Intelligence Organi-
zation. Main Track.
Shapley, L. S. 1953. Stochastic games. Proceedings of the
national academy of sciences, 39(10): 1095–1100.
Sun, Y .; Huo, D.; and Huang, F. 2020. Vulnerability-aware
poisoning mechanism for online rl with unknown dynamics.
arXiv preprint arXiv:2009.00774.
Wei, C.-Y .; Dann, C.; and Zimmert, J. 2022. A model selec-
tion approach for corruption robust reinforcement learning.
InInternational Conference on Algorithmic Learning The-
ory, 1043–1096. PMLR.
Wu, F.; Li, L.; Xu, C.; Zhang, H.; Kailkhura, B.; Kenthapadi,
K.; Zhao, D.; and Li, B. 2022. COPA: Certifying Robust
Policies for Offline Reinforcement Learning against Poison-
ing Attacks. arXiv preprint arXiv:2203.08398.
10433
Xie, Q.; Chen, Y .; Wang, Z.; and Yang, Z. 2020. Learning
zero-sum simultaneous-move markov games using function
approximation and correlated equilibrium. In Conference on
learning theory, 3674–3682. PMLR.
Yang, L.; Hajiesmaili, M.; Talebi, M. S.; Lui, J.; and Wong,
W. S. 2021. Adversarial Bandits with Corruptions: Regret
Lower Bound and No-regret Algorithm. In Advances in Neu-
ral Information Processing Systems (NeurIPS).
Zhang, H.; and Parkes, D. C. 2008. Value-Based Policy
Teaching with Active Indirect Elicitation. In AAAI, vol-
ume 8, 208–214.
Zhang, H.; Parkes, D. C.; and Chen, Y . 2009. Policy teaching
through reward function learning. In Proceedings of the 10th
ACM conference on Electronic commerce, 295–304.
Zhang, K.; Yang, Z.; and Bas ¸ar, T. 2021. Multi-agent re-
inforcement learning: A selective overview of theories and
algorithms. Handbook of Reinforcement Learning and Con-
trol, 321–384.
Zhang, X.; Chen, Y .; Zhu, J.; and Sun, W. 2021a.
Corruption-robust offline reinforcement learning. arXiv
preprint arXiv:2106.06630.
Zhang, X.; Chen, Y .; Zhu, X.; and Sun, W. 2021b. Ro-
bust policy gradient against strong data corruption. In Inter-
national Conference on Machine Learning, 12391–12401.
PMLR.
Zhang, X.; Ma, Y .; Singla, A.; and Zhu, X. 2020. Adap-
tive reward-poisoning attacks against reinforcement learn-
ing. In International Conference on Machine Learning,
11225–11234. PMLR.
Zhong, H.; Xiong, W.; Tan, J.; Wang, L.; Zhang, T.; Wang,
Z.; and Yang, Z. 2022. Pessimistic minimax value iteration:
Provably efficient equilibrium learning from offline datasets.
arXiv preprint arXiv:2202.07511.
Zuo, S. 2020. Near Optimal Adversarial Attack on UCB
Bandits. arXiv preprint arXiv:2008.09312.
10434"
27191-Article Text-31260-1-2-20230701.pdf,"Model Checking for Adversarial Multi-Agent Reinforcement Learning
with Reactive Defense Methods
Dennis Gross1, Christoph Schmidl1, Nils Jansen1, Guillermo A. P ´erez2
1Institute for Computing and Information Sciences, Radboud University
2Department of Computer Science, University of Antwerp
Abstract
Cooperative multi-agent reinforcement learning (CMARL)
enables agents to achieve a common objective. However, the
safety (a.k.a. robustness) of the CMARL agents operating in
critical environments is not guaranteed. In particular, agents
are susceptible to adversarial noise in their observations that
can mislead their decision-making. So-called denoisers aim
to remove adversarial noise from observations, yet, they are
often error-prone. A key challenge for any rigorous safety
verification technique in CMARL settings is the large number
of states and transitions, which generally prohibits the con-
struction of a (monolithic) model of the whole system. In this
paper, we present a verification method for CMARL agents in
settings with or without adversarial attacks or denoisers. Our
method relies on a tight integration of CMARL and a verifi-
cation technique referred to as model checking. We showcase
the applicability of our method on various benchmarks from
different domains. Our experiments show that our method is
indeed suited to verify CMARL agents and that it scales bet-
ter than a naive approach to model checking.
Introduction
Deep cooperative multi-agent reinforcement learning
(CMARL) is a powerful tool to handle sequential decision-
making problems. It has improved the performance of var-
ious applications in critical domains like manufacturing,
transportation, and resource allocation (Serrano-Ruiz, Mula,
and Poler 2021; Qin et al. 2021; Zong and Luo 2022).
CMARL consists of multiple agents, where each one learns
a near-optimal policy based on a given common objective
by making observations and gaining rewards through inter-
actions with the environment (Wong et al. 2021). Compared
to standard RL, CMARL may show superior performance
and scalability in certain settings (Zhang et al. 2022).
MARL Problem
Despite the success of CMARL, the potential unsafe be-
havior of agents (Shalev-Shwartz, Shammah, and Shashua
2016; Amodei et al. 2016) and the security risk of adver-
sarial attacks against critical infrastructures (Dablain 2017)
limit its usage. Adversarial attacks introduce noise into the
observations and may mislead the decision-making of the
Copyright © 2023, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.agents (Lin et al. 2020; Huang et al. 2017; Ilahi et al. 2022;
Moos et al. 2022; Fujimoto and Pedersen 2021). Denoisers
defend CMARL agents against adversarial attacks by clean-
ing the observations from the noise before they are perceived
by the agents (Ohashi et al. 2021; Vincent et al. 2008; Ben-
gio et al. 2013; Im et al. 2017; Bakhti et al. 2019; Serban
and Poll 2018). However, these denoisers can make recon-
struction mistakes and may recover wrong states. Until now,
the performance of CMARL agents equipped with denois-
ers was measured by the decrease in the cumulative rewards
of the CMARL system. Unfortunately, rewards lack the ex-
pressiveness to encode complex safety requirements (Vam-
plew et al. 2022; Hasanbeig, Kroening, and Abate 2020). For
instance, with rewards, it is possible to determine the prob-
ability that all trains will arrive at their destination. How-
ever, rewards are not sufficient for other properties, for in-
stance, the probability that trains will arrive in a specific or-
der.Model checking (Baier and Katoen 2008) is not limited
by properties that can be expressed by rewards (Hahn et al.
2019; Hasanbeig, Kroening, and Abate 2020; Vamplew et al.
2022), but supports a broader range of properties that can be
expressed by probabilistic computation tree logic (PCTL;
Hansson and Jonsson 1994).
Model checking is a formal verification technique that
uses mathematical models to verify the correctness of a
system with respect to a given property. Naive monolithic
model checking is called ”naive” because it does not take
into account the complexity of the system or the number of
possible states it can be in, and it is called ”monolithic” be-
cause it treats the entire system as a single entity, without
considering the individual components of the system or the
interactions between them.
Contribution
This paper aims to allow the model checking of CMARL
agents (equipped with denoisers) in an (adversarial)
CMARL setting, which guarantees that CMARL agents still
comply with given safety requirements. To achieve this, we
take advantage of the fact that a CMARL system can be
modeled as a Markov decision process (MDP) by treating
the collection of agent actions as one joint action and repre-
senting all the RL agents via a joint agent (Boutilier 1996).
We present a model checking method that allows us to ver-
ify CMARL agents. We evaluated our method on different
Proceedings of the Thirty-Third International Conference on Automated Planning and Scheduling (ICAPS 2023)
162
benchmarks from the CMARL community (Competitions
2021) and model checking community (Hartmanns et al.
2019), and compared our approach with naive monolithic
model checking.
To summarize, our main contributions are model check-
ing methods and extensive benchmarking for (1) CMARL
agents, (2) under adversarial attacks, (3) equipped with de-
noisers, and (4) the combination of (1), (2), and (3).
The paper is structured in the following way. First, we
summarize the related work and position our paper. Sec-
ond, we explain the fundamentals of our technique, third,
we present the adversarial attack and defense setting and de-
scribe how to model check CMARL systems. Finally, we
evaluate our method in multiple environments.
Related Work
There already exist multiple model checking approaches and
case studies for multi-agent environments (Kwiatkowska
et al. 2021, 2020; Junges et al. 2018; Chen et al. 2011;
Bertrand and Fournier 2013; Kwiatkowska, Norman, and
Parker 2019). The major difference to our work is that they
do not focus on CMARL. Schuppe and Tumova propose a
decentralized solution to a high-level task-planning prob-
lem for a multi-agent system under a set of possibly de-
pendent LTL specifications (Schuppe and Tumova 2021).
Lomuscio and Pirovano developed a parameterized verifi-
cation method for checking unbounded probabilistic multi-
agent systems against strategic properties. They do this by
creating an abstract model whose states have two compo-
nents: the first captures the state of the first magents, and
the second records the set of states that arbitrarily many
other agents are in (Lomuscio and Pirovano 2020). In our
case, the number of agents in the system is fixed at design
time, we focus on CMARLs, and our agents do not neces-
sarily share the same observations and actions. Furthermore,
we distinguish between swarm systems and CMARL sys-
tems since, in swarm systems, many identical agents inter-
act with each other to achieve a common goal (H ¨uttenrauch,
Sosic, and Neumann 2019). CMARL agents can be differ-
ent. Mqirmi et al. present a methodology that combines for-
mal verification with (deep) RL algorithms to guarantee the
satisfaction of formally-specified safety constraints in train-
ing and testing. They first use bisimulation to create an ab-
straction of the multi-agent system. Then they build a shield
that restricts the agents’ actions (Mqirmi, Belardinelli, and
Le´on 2021). We directly induce the agents into the formal
model and verify its PCTL properties. Riley et al. introduce
a new approach (they build upon their previous research (Ri-
ley et al. 2021a)) that combines CMARL with a formal veri-
fication technique termed quantitative verification. Their ap-
proach consists of four stages. First, they acquire informa-
tion about the CMARL environment. Second, they model
the environment as a formal PRISM model. Third, they syn-
thesize policies for the formal PRISM model that guaran-
tees given properties. Fourth, they learn CMARL agents in
the non-abstracted environment, where the synthesized poli-
cies constrain learning (Riley et al. 2021b). In comparison,
we train our CMARL agents and verify them directly in themodeled environment. Khan et al. present a CMARL ap-
proach to goal assignment and guaranteed collision-free tra-
jectory planning for unlabeled robots operating in obstacle-
filled 2D spaces. To ensure an agent still has a collision-free
trajectory, they use an analytical model based on a policy
that runs in the background and checks if the target veloci-
ties produced by the agents are safe (Khan et al. 2019). Wang
et al. present a formal framework for collision avoidance
in multi-robot systems, wherein an existing policy is mod-
ified in a minimally invasive fashion to ensure safety (Wang,
Ames, and Egerstedt 2016). On the other hand, we allow the
model checking of a broad range of CMARL systems.
Background
We now introduce the fundamentals of our work.
Probabilistic Systems
Aprobability distribution over a set Xis a function µ:X→
[0,1]with∑x∈Xµ(x) =1. The set of all distributions on Xis
denoted Distr(X).
Definition 1 (Markov Decision Process). A Markov deci-
sion process (MDP) is a tuple M = (S,s0,A,T,rew) where S
is a finite, nonempty set of states, s 0∈S is an initial state, A
is a finite set of actions, T :S×A→Distr(S)is a probability
transition function. We employ a factored state representa-
tion where each state s is a vector of features (f1,f2,...,fn)
where each feature f j∈Zfor1≤i≤n (n is the dimension of
the state). rew :S×A→Ris a transition-reward function.
The available actions in s∈SareA(s) = {a∈A|T(s,a)̸=
⊥}. An MDP with only one action per state (∀s ∈S:
|A(s)| =1) is a discrete-time Markov chain (DTMC). A
path of an MDP Mis an (in)finite sequence τ=s0a0,r0− − − →
s1a1,r1− − − → ..., where si∈S,ai∈A(s i),ri:=rew(s i,ai), and
T(si,ai)(si+1)̸=0. A state s′is reachable from state s, if
there exists a path τfrom state sto state s′.
Definition 2 (Policy). A memoryless deterministic policy for
an MDP M = (S,s0,A,P,rew) is a function π:S→A that
maps a state s ∈S to action a ∈A.
Applying a policy πto an MDP Myields an induced
DTMC, where all non-determinism is resolved. We spec-
ify the properties of a DTMC via the specification language
PCTL (Wang et al. 2020).
Definition 3 (PCTL Syntax). Let AP be a set of atomic
propositions. The following grammar defines a state for-
mula: Φ::=true|a|Φ1∧Φ2| ¬Φ|P ▷ ◁p|Pmax
▷ ◁p(φ)|Pmin
▷ ◁p(φ)
where a ∈AP,▷ ◁∈{ <,>,≤,≥}, p∈[0,1]is a threshold, and
φis a path formula which is formed according to the fol-
lowing grammar φ::=XΦ|φ1Uφ2|φ1Fθitφ2|GΦwith
θi={<,≤}.
Agent 1 a=a1
Environment s,re
wa
Figure 1: An MDP with one agent.
163
Agent 1 Agent 2
a=a1×a2
Environment
s,re
w o1,re
w o2,re
wa1 a2
a
o1=O1(s)
re
wo2=O2(s)
re
w
(a) CMMDP with two agents.Agent 1 Agent 2
a=a1×a2
Environment
s,re
w o1,re
wD1(o1)
o2,re
wD2(o2) a1 a2
a
o1=O1(s)
re
wo2=O2(s)
re
w
(b) CMMDP with two agents equipped with denoisers.
Agent 1 Agent 2
a=a1×a2
Environment
s,re
w o1,re
wδ1(o1)
o2,re
wδ2(o2) a1 a2
a
o1=O1(s)
re
wo2=O2(s)
re
w
(c) CMMDP with two agents under attack.Agent 1 Agent 2
a=a1×a2
Environment
s,re
w o1,re
wD1(δ1(o1))
o2,re
wD2(δ2(o2)) a1 a2
a
o1=O1(s)
re
wo2=O2(s)
re
w
(d) CMMDP with two agents equipped with denoisers under attack.
Figure 2: Different CMARL settings with and without denoisers Di(oi)and with and without attacks δi(oi). It assumes a setting
with two agents and a joint action a=a1×a2. This action ais deployed to the environment and executed. The resulting state s
andrewis separated into the observations.
For MDPs, PCTL formulae are interpreted over the states
of the induced DTMC of an MDP and a policy. In a
slight abuse of notation, we use PCTL state formulas to
denote probability values. That is, we sometimes write
P▷ ◁p(φ)where we omit the threshold p. For instance, in
this paper, P(F collision) denotes the reachability prob-
ability of eventually running into a collision. There ex-
ist a variety of model checking algorithms for verify-
ing PCTL properties (Courcoubetis and Yannakakis 1988,
1995). PRISM (Kwiatkowska, Norman, and Parker 2011)
and Storm (Hensel et al. 2022) offer efficient and mature
tool support for verifying probabilistic systems.
Definition 4 (CMMDP). A cooperative multi-agent
Markov decision process (CMMDP) is a tuple
(S,s0,I,A:={Ai}i∈I,T,rew,{Oi}i∈I,{Oi}i∈I)where S
is a finite, nonempty set of states; s 0∈S is an initial state; I
is a finite, nonempty set of agents; A iis a finite, nonempty set
of actions available to agent i; T :S×A1×...×Ai→[0,1]is
a transition function, and rew :S→Ris a joint reward func-
tion. Each agent i gets its local partial observation o i∈Oi
according to the observation function Oi(s):S→Oi. An
observation o i∈Oiis a vector composed of features f jfrom
state s. The observations of all agents result in the full state.
Each agent i∈Ihas a policy πi:Oi→Aithat maps an
observation oi∈Oito an action ai∈Ai. The joint policy π
induced by the set of agent policies {πi}i∈Iis the mapping
from states into actions and transforms the CMMDP into
an MDP (compare Figure 1 with Figure 2a). This is only
possible if, for every state sand action a, the sub-policies
πiget a set of observations Oithat “reveals” the next state.
They don’t have to know the full state, but their combination
should have that property. Inducing a joint policy πinto an
MDP yields an induced DTMC (Boutilier 1996).
Adversarial Multi-Agent Reinforcement Learning
We now introduce CMARL, adversarial attacks anddenois-
ers. The standard learning goal for RL is to find a policy πin an MDP such that πmaximizes the expected discounted
reward, that is, E[∑L
t=0γtRt], where γwith 0 ≤γ≤1 is the
discount factor, Rtis the reward at time t, and Lis the total
number of steps. CMARL extends the RL idea to find the
near-optimal policies πiin a CMMDP setting. Each agent
policy πiis represented by a neural network. A neural net-
work is a function parameterized by weights θi. The neural
network policy πican be trained by minimizing a sequence
of loss functions J(θi,oi,ai)(Mnih et al. 2013).
Definition 5 (Adversarial Attacks). Anadversarial attack
δi:Oi→Oimaps an observation o ito another o i,adv. An
attack is α-bounded if ∥δi(oi)−oi∥∞≤αwith l ∞-norm de-
fined as ∥δi(oi)−oi∥∞=maxδik∈δi|δik−oik|. See Figure 2c
for a visual example.
In this work, we focus on the commonly used FGSM at-
tack (Huang et al. 2017). Given the weights θiof the neu-
ral network policy πiand a loss J(θi,oi,ai)with observa-
tionoiandai:=πi(oi), the FGSM, denoted as δi:Oi→Oi,
adds noise whose direction is the same as the gradient of the
lossJ(θi,oi,ai)w.r.t the state oito the state oiand the noise
is scaled by α∈Q(see Equation (1)). We specify the ∇-
operator as a vector differential operator. Depending on the
gradient, we either add or subtract α.
δi(oi) =oi+α·sign(∇ oiJ(θi,oi,ai)) (1)
To attack a CMARL system, we attack each RL policy sep-
arately via Equation (1).
Definition 6. We denote a denoiser by D i:Oi→Oi. A de-
noiser D idenoises an adversarial attack δiby passing the
adversarial observation into the denoiser D i(δ(o i)). A de-
noiser is successful if πi(Di(δi(oi))) = πi(oi). An adversar-
ial attack is successful if πi(Di(δi(oi)))̸=π(o i). See Fig-
ure 2b and Figure 2d for a visual example.
A denoiser uses a neural network that gets trained by min-
imizing the loss function J(θi,δi,oi)(Bakhti et al. 2019; Ser-
ban and Poll 2018).
164
Methodolodgy
Safety and security need to be analyzed together. The reason
for that lies in the fact that defense methods (like denoisers)
can make mistakes that influence the overall CMARL sys-
tem behavior. Furthermore, it is important to verify how well
defense methods defend the RL policies against adversarial
attacks and therefore need to be integrated into the verifica-
tion process too. Our method takes all of it into account. It
takes advantage of the fact that CMARL agents have shared
rewards and that all agent behaviors emerge together into a
joint policy. These two CMARL properties allow us to verify
CMARL agents by modeling the joint policy and environ-
ment as an induced DTMC. It is an efficient method because
it only builds the model that is reachable via the trained poli-
cies and allows, therefore, the verification of larger models
than what is possible via naive monolithic model checking.
This section is structured as follows. First, we explain how
we model check deep CMARL agents. Second, we illustrate
the attack setting, show how to attack a trained CMARL sys-
tem, and how to model check such attacks. Third, we illus-
trate the defense setting and explain how to model check the
denoisers in combination with the trained CMARL policies
with and without adversarial attacks.
Model Checking of CMARL Agents
Recall, the joint policy πinduced by the set of all agent poli-
cies{πi}i∈Iis a single policy π(Boutilier 1996). The tool
COOL-MC1(Gross et al. 2022) allows model checking of
a single RL policy against a user-provided PCTL property
and MDP. Thereby, it builds the induced DTMC incremen-
tally (Cassez et al. 2005).
To support joint policies, we created a joint policy wrap-
perthat handles the generation of the observations for the
RL policies πiand builds the joint action π(s) at every vis-
ited state s(see Figure 2). The wrapper takes (user-provided)
observation functions Oito map the states sto the corre-
sponding observations oi. Each of the CMARL policies πi
gets queried by its observation oito build the joint action
a=a1×...×ai.
With the joint policy wrapper, we build the induced
DTMC the following way. For every state sthat is reach-
able via the joint policy π, we query for an action a=π(s).
In the underlying MDP M, only states s′that may be reached
via that action a∈A(s) are expanded. The resulting Markov
chain induced by Mandπis fully deterministic, as no action
choices are left open, and ready for efficient model checking.
Our method is independent of the learning algorithm and
allows the model checking of CMARL policies that select
their actions based on current observations. Checking of
probabilistic policies is supported by always choosing the
action with the highest probability at every state. We sup-
port every environment that can be modeled via the PRISM
language (Kwiatkowska, Norman, and Parker 2011).
1Download it from https://github.com/LA V A-LAB/COOL-
MC. Our main features are supported.Attack Setting
We now describe the adversarial attack setting (adversary’s
goals, knowledge, and capabilities).
Adversary’s goal. The adversary aims to modify the per-
formance of the trained CMARL agents in their environ-
ment. For instance, the adversary may try to increase the
probability that the CMARL agent’s production costs for a
product exceed a threshold.
Adversary’s knowledge. We consider an adversary that
knows the weights θiof the trained policies (for the FGSM
attack) and knows the CMMDP of the environment. Note
that we can replace the FGSM attack with any other at-
tack. Therefore, knowing the weights of the trained policies
should not be a strict constraint.
Adversary’s capabilities. Our attack setting allows the
adversary to attack the trained policies at every visited state
(see Figure 2c) during the incremental building process for
the model checking of the adversarial-induced DTMC (of-
fline) and after the RL policy got deployed (online).
Defense Setting
We now describe the defense setting (defender’s goals,
knowledge, and capabilities).
Defender’s goal and knowledge. Our defense goal is to
remove the adversarial attack from the agent’s observations
and retain the original observation (Serban and Poll 2018).
Defender’s knowledge. The defender knows everything
about the trained policy, the CMMDP of the environment,
and the adversarial attack method.
Defender’s capabilities. Our defense setting allows the
defender to clean each observation of the trained poli-
cies during the incremental building process for the model
checking (offline) and after the RL policy got deployed (on-
line). A denoiser takes a clean observation (see Figure 2b) or
an adversarial observation (see Figure 2d) as input (depend-
ing if the CMARL system is under attack) and outputs the
cleaned observation.
Experiments
We now evaluate our CMARL model checking method in
three benchmarks. First, we compare our CMARL model
checking method with naive monolithic model checking and
analyze the performance of the trained agents. Second, we
analyze how many CMARL agents we can handle. Third, we
analyze the agent performance change by using denoisers
with and without adversarial attacks.
Setup
In this section, we explain the setup of our experiments2and
detail the three case studies that we use for this paper. We ap-
plied our proposed method to an environment inspired by the
dynamic job shop scheduling problem (DJSSP) from the IJ-
CAI 2021 competition (Competitions 2021), an environment
inspired by the Flatland Challenge: Multi-Agent Reinforce-
ment Learning on Trains (TRAINS) from the ICAPS 2021
2Reproduce the experiments with the code from
https://github.com/LA V A-LAB/CMARL-VERIFICATION
165
competition (Competitions 2021), and the dining philoso-
pher QComp benchmark (Hartmanns et al. 2019) trans-
formed into a CMARL system.
DJSSP. We transformed the single RL environment of the
IJCAI 2021 challenge into a CMARL environment. Our
CMARL environment comprises a manufacturing system
with a set of jobs that must be manufactured via several au-
tonomous machines before a given deadline (see Figure 3).
Stochastic events such as random machine breakdowns
and changing production costs, all of which frequently hap-
pen in real-world manufacturing (Popper et al. 2021), are
considered in this environment.
Each machine is controlled by a single agent i. We al-
low parallel working on multiple jobs simultaneously. How-
ever, if two machines work on the same job simultaneously,
the environment terminates. Each job j∈Jconsists of a se-
quence of operations; each should be processed on a specific
machine iand takes a particular time, namely the processing
time T∈Zi j. For example, T13=2 indicates that machine 3
has to execute its operation two time steps for job 1.
Each state sconsists of features for the total number of
processing times Tjifor each job jand machine i, a feature
for the status of each machine zi∈I∪{0}(agent iis work-
ing on the job j, agent ino operation), a feature about the
current hour of the day time∈Z, a feature about the avail-
able budget budget ∈Z, and a feature about the current en-
ergy price price∈Z(extracted from (Tveten, Bolkesjø, and
Ilieva 2016)).
Each operation costs energy, and at every time step, the
energy price may vary. If there is no more budget left, the
environment terminates. Operations depend on each other.
For instance, in our setting, operation 2 can not be executed
before operation 1 and operation 3 have been done (T i1==
0∧Ti3==0). If an operation is executed in the wrong order,
the environment terminates. Uncertainty is introduced by the
effect that operations may delay in 10% of cases (a machine
breaks down for a time step).
The observation function Oifor agent imaps the current
state sto the observation oi. An observation oiconsists of
the current energy price price, the hour of the day time, all
processing times Tjithat must be done by agent iand all the
processing times of operations on which it directly depends
and the working status of the machines zi.
Each agent ihas a discrete action space Ai, which includes
take-actions that let the agent choose a specific job to work
on and an action for no-operation.
The CMARLS receive their cooperative penalty as soon
as the environment terminates. If all jobs were finished,
the penalty consists of the spent budget and the number
of needed time steps to finish all jobs. Otherwise, it gets a
penalty of 200 minus the number of executed operations.
Machine 2
 Machine 1
 Machine 4
 Machine 3
2
 3
 2
 4
T2,1 T2,3 T2,2 T2,4
1
 4
 2
 5
T1,2 T1,4 T1,1 T1,3
J2J1
Energy
price
 Machine breakdo
wn
Figure 3: In the DJSSP environment, two jobs with different
operation processing times need to be finished. Each ma-
chine is controlled by a CMARL agent and takes care of
one specific operation. A machine can break down, and the
agents know the energy price.
The DJSSP environment has 10, 331,493 states and
2,037,223,057 transitions. Differences to the IJCAI
2021 competition: random machine breakdown is simulated
in our case as no operation in the given time step, but the
machine can be used in the next time step again; each ma-
chine is controlled by an agent; each agent partially observes
the environment; and we added fluctuating energy prices and
safety requirements like operation collisions and production
cost thresholds.
S={(T,price ,budget ,z1,z2,z3,z4),...}
O1(s) ={(T11,T21,price ,budget ,time,z1,z2,z3,z4),...}
O2(s) ={(T11,T21,T12,T22,T13,T23,price ,budget ,time,
z1,z2,z3,z4),...}
O3(s) ={(T13,T23,price ,budget ,time,z1,z2,z3,z4),...}
O4(s) ={(T14,T24,price ,budget ,time,z1,z2,z3,z4),...}
Ai={NOP ,take1, take2} for each agent i
A=A1×A2×A3×A4
penalty =

if operations are done,
time steps + initial budget - budget
else,
200−number of executed operations
TRAINS. This environment consists of three trains that
try to reach their destination (see Figure 4). The environ-
ment terminates as soon as they reach their destination, the
time runs out, or there is a collision between the trains. The
CMARL agents receive a cooperative penalty as soon as the
environment terminates. It consists of 100 if there was a col-
lision or time out or not all trains arrived in time. Otherwise,
the penalty is 100 minus the number of arrived trains mi-
nus the remaining time. In 5% of cases, a train must stop
because of malfunctioning, and the chosen action is not ex-
ecuted. The TRAIN environment has 217 states and 31, 372
transitions. Differences to the ICAPS 2021 competition: train
collisions let the environment terminate; and all trains must
reach their destination in the same time span.
166
Env. Label PCTL Property Query (P(φ)) = |S| |T |Time (s)
DJSSP done P(F jobs done) 0.58 11629 27157 8440
DJSSP collision P(F collision) 0.22 11479 27007 6432
DJSSP wrong order P(F wrong order ) 0.25 11711 27239 8162
DJSSP time P(F time) 0 11871 27399 8335
DJSSP bankruptcy P(F no budget ) 0.004 9241 24769 5065
TRAINS arrived P(F arrived ) 0.99 11 16 7
TRAINS delayed P(F delay) 0.0975 10 15 6
TRAINS crashed P(F crash) 0 13 18 8
TRAINS train21 P(¬train1 arrives U train2 arrives) 0.99 8 11 4
DPP3 end P(F done) 0 7 7 1
Table 1: PCTL property queries, with their labels and the original result of the property query without a denoiser and attack
(=). The MDP of the DJSSP environment has 10, 331,493 states and 2, 037,223,057 transitions, the MDP of the TRAIN
environment has 217 states and 31, 372 transitions, and the DPP3 MDP has 190 states and 855 transitions.
Station 4
Station 3
Train
1
Station 0
Train
2
Station 2
Station 1
Train
3
Figure 4: Train network of different train lines (different col-
ors). For instance, the red train starts at station 0 and has to
arrive at station 2.
S={(done ,time,agent 0id,agent 0target id,
agent 0moving ,...),...}
O1(s) =O2(s) =O3(s) = S
Ai={nop, le ft,straight ,right ,stop} for each agent i
A=A1×A2×A3
penalty =

if collision or time out and not all trains arrived,
100
if no collision and no time out and trains arrived,
100 - number of arrived trains - remaining time
Dining philosophers problem (DPPN). There are N
agents seated around a circular table. To the left of each
agent lays a fork, and in the center stands a bowl of spaghetti.
An agent is expected to spend most of its time thinking; but
when the agent feels hungry, it needs to pick up both the left
and right fork to eat the spaghetti. When the agent finishes
the meal, it puts down both forks and continues thinking.
A fork can be used by only one agent at a time. The envi-
ronment terminates when two agents try to grab the same
fork (fork collision) or one of the agents starves. The agents
must collaborate so that nobody starves and no fork colli-
sions happen. For every additional time step the environmentdoes not terminate, the agents get a reward.
S={(hunger level 1, hunger level 2, ..., hunger level N),... }
Oi(s) = hunger levels of neighbor agents and
its own hunger level
Ai={eat,think}for each agent i
A=A1×A2×...×Ai
reward =1, for each time step
Trained RL policies. We trained three CMARL agents
in the environments such that they organized themselves to
reach their goals. All CMARL agents were trained with sep-
arate deep Q-learning algorithms (Mnih et al. 2013) with a
common reward function (Tampuu et al. 2015). We set for all
training runs the Numpy random seed =128, PyTorch ran-
dom seed =128, and Storm random seed =128. We used
ε=0.1 (ε =0.5 for the TRAIN agents), εdecay=0.9999,
εmin=0.01 (ε min=0.1 for the TRAIN agents), γ=0.99,
a target network replacement of 304, batch size of 32, and
a replay buffer size of 300, 000. Each neural network con-
sists of two layers, each with 128 neurons (64 neurons
for the TRAIN agents). The DJSSP CMARL training con-
sisted of 32, 462 epochs with a best sliding window (window
size=100) reward of −60.98. The TRAIN CMARL train-
ing consisted of 25, 000 epochs with a best sliding window
(window size =100) reward of −96.24. The DPP3 train-
ing consisted of 10, 000 epochs with a best sliding window
(windows size =100) reward of 9.75.
Properties. Table 1 presents the performance of the poli-
cies for different properties (=). For instance, done =0.58
describes the probability of the CMARL agents finishing all
manufacturing jobs. Note at this point that we do not focus
on achieving optimal performance but rather showing that it
is possible to model check.
Technical setup. We executed our benchmarks on an
NVIDIA GeForce GTX 1060 Mobile GPU, 16 GB RAM,
and an Intel(R) Core(TM) i7-8750H CPU @ 2.20GHz x 12.
167
2 3 4 5 6 7 8 9 10 11
Number of
Agents20406080100120Time (seconds)
Figure 5: The exponential building time of each state for
different numbers of CMARL agents via our method.
Analysis
We now answer the following research questions.
Can we model check CMARL environments that are too
large for other model checkers? The naive monolithic
model checking via Storm gives us the maximal reachability
probability Pmax(Farrived ) =0.99 that all trains arrive at
their destination. We model checked our trained CMARL
agents via our method, and we observed that our agents
achieve the same performance (arrived =0.99).
For the DJSSP environment, it is intractable to check the
MDP of the DJSSP (10, 331,493 states and 2, 037,223,057
transitions) via naive monolithic model checking. Storm
runs out of memory after 22 minutes with the property query
Pmax(Fjobs done). Our method, on the other hand, gives
us a reachability probability for done =0.58 (see Table 1).
However, at some point, our model checking method is also
limited by the size of the induced DTMC and runs out of
memory (Gross et al. 2022).
How many agents can our method handle? We now
analyze how many agents our CMARL model checking
method can handle. In this experiment, we focus on the
DPPN environment because it is straightforward to scale.
We train CMARL agents in different DPPN environments
with different numbers of agents. Our experiment shows,
that we can handle up to 18 agents at the same time. At
19 agents, the model becomes too large to parse. At every
incremental building process step, a callback function has
to be called |Act(s)||I|=2|I|times per state. Therefore, the
model building time becomes expensive (you can track the
building times for each state for different numbers of agents
in Figure 5). With our technical setup, the naive monolithic
model checking takes around 1 ·10−5seconds for each state
(independent of the number of agents). During the model
checking of 11 agents and the property end, the naive mono-
lithic model checking runs out of memory while our method
still allows the model checking of them. We conclude that
the model checking of CMARL systems is also limited by
the number of agents.
How do adversarial attacks influence the performance of
the trained CMARL agents? We now analytically mea-
sure the impact of adversarial attacks in our environments.
Therefore, we create at every visited state an α-boundedLabel = = adv =denoiser=denoiser
adv
¬done 0.42 0.44 0.42 0.42
collision 0.22 0.26 0.22 0.22
wrong order 0.25 0.25 0.25 0.25
time 0 0 0 0
bankruptcy 0.004 0.1 0.004 0.004
¬arrived 0.01 0.1 0.01 0.01
crashed 0 0 0 0
delayed 0.0975 0.0975 0.0975 0.0975
train21 0.99 0.99 0.99 0.99
end 0 1 1 1
Table 2: Comparison between no denoiser and no attack
(=), no denoiser and attack (= adv), denoiser and no attack
(=denoiser), and denoiser and attack (=denoiser
adv). All attacks
are bounded by α=0.1.
FGSM attack (α =0.1) for each policy πiduring the incre-
mental building process of the induced DTMC (see an exam-
ple for an attack in Figure 2c). Our experiments show that
FGSM attacks influence the performance of the CMARL
system (compare column =and=advin Table 2).
How does the CMARL agents’ performance change by
equipping them with denoisers? We now analytically
measure how well CMARL policies perform with denois-
ers under and not under attack. The adversary and defender
operate before the observations get passed to the agents (see
Figure 2d). For each agent, there is a separate adversary at-
tack and a separate denoiser. The adversarial attacks are cre-
ated via FGSM. We trained each denoiser the following way:
1. During CMARL policy training, we collected kstates Y
(k=1000 for DJSSP, k=11 for TRAINS, k=7 for DPP3).
2. For each y∈Y, we create an adversarial state xvia the
FGSM attack (α =0.1) and store the data point (x,y)and
(y,y)into the data set A. 3. We added synthetic data (plus
mdata points) to the dataset by randomly shuffling the vec-
tor elements of each x(m=19000 for DJSSP, m=9890
for TRAINS, m=9993 for DPP3). 4. Train denoiser on A
for 100 episodes with seed=860,523,297,119,962,652,
learning rate =0.0001, batch size =32, and four neural net-
work layers (each with 1048 neurons). The losses of the de-
noisers vary between 0.0005 and 0.2 ·107.
Table 2 shows in column =denoiserthat, in most cases, the
denoisers do not decrease the performance of the policies.
In column =denoiser
adv, we observe that, in most cases, the de-
noisers under α-bounded FGSM attacks (α =0.1) also do
not decrease the performance of the policies.
Conclusion
Our method checks trained CMARL agents equipped with
or without denoisers in adversarial or non-adversarial envi-
ronments to ensure compliance with safety requirements af-
ter deployment. It has been successfully applied to real-life
168
applications such as job scheduling, transportation, and re-
source allocation. However, the size of the induced DTMC
and the number of CMARL agents limit our method. Opti-
mizing the incremental model building process of COOL-
MC can increase the number of supported CMARL agents.
Incorporating safe CMARL approaches would also be valu-
able extensions to our method, as already done in the single
RL domain (Carr et al. 2023; Jin et al. 2022; Jothimurugan
et al. 2022; Jansen et al. 2020).
Acknowledgements
The research leading to this work was partially funded by the
Belgian FWO G030020N project “SAILor”. Furthermore,
this work was partially funded by the SAM-FMS project
(NWO project 17931) in the MasCot program, and it was
supported by the ERC Starting Grant 101077178 (DEUCE).
References
Amodei, D.; Olah, C.; Steinhardt, J.; Christiano, P. F.; Schul-
man, J.; and Man ´e, D. 2016. Concrete Problems in AI
Safety. CoRR, abs/1606.06565.
Baier, C.; and Katoen, J. 2008. Principles of model checking.
MIT Press.
Bakhti, Y .; Fezza, S. A.; Hamidouche, W.; and D ´eforges,
O. 2019. DDSA: A Defense Against Adversarial Attacks
Using Deep Denoising Sparse Autoencoder. IEEE Access,
7: 160397–160407.
Bengio, Y .; Yao, L.; Alain, G.; and Vincent, P. 2013. Gener-
alized Denoising Auto-Encoders as Generative Models. In
NIPS, 899–907.
Bertrand, N.; and Fournier, P. 2013. Parameterized Verifi-
cation of Many Identical Probabilistic Timed Processes. In
FSTTCS, volume 24 of LIPIcs, 501–513. Schloss Dagstuhl -
Leibniz-Zentrum f ¨ur Informatik.
Boutilier, C. 1996. Planning, Learning and Coordination in
Multiagent Decision Processes. In TARK, 195–210. Morgan
Kaufmann.
Carr, S.; Jansen, N.; Junges, S.; and Topcu, U. 2023. Safe
Reinforcement Learning via Shielding under Partial Observ-
ability. In AAAI.
Cassez, F.; David, A.; Fleury, E.; Larsen, K. G.; and Lime,
D. 2005. Efficient On-the-Fly Algorithms for the Analysis
of Timed Games. In CONCUR, volume 3653 of LNCS, 66–
80. Springer.
Chen, T.; Kwiatkowska, M. Z.; Parker, D.; and Simaitis, A.
2011. Verifying Team Formation Protocols with Probabilis-
tic Model Checking. In CLIMA, volume 6814 of LNCS,
190–207. Springer.
Competitions. 2021. Competitions. https://icaps21.icaps-
conference.org/Competitions/. [Online; accessed 01-11-
2022].
Courcoubetis, C.; and Yannakakis, M. 1988. Verifying Tem-
poral Properties of Finite-State Probabilistic Programs. In
FOCS, 338–345. IEEE Computer Society.
Courcoubetis, C.; and Yannakakis, M. 1995. The Complex-
ity of Probabilistic Verification. J. ACM, 42(4): 857–907.Dablain, K. 2017. Cyber threats against critical infrastruc-
tures in railroads. Ph.D. thesis, Utica College.
Fujimoto, T.; and Pedersen, A. P. 2021. Adversarial Attacks
in Cooperative AI. CoRR, abs/2111.14833.
Gross, D.; Jansen, N.; Junges, S.; and P ´erez, G. A. 2022.
COOL-MC: A Comprehensive Tool for Reinforcement
Learning and Model Checking. In SETTA. Springer.
Hahn, E. M.; Perez, M.; Schewe, S.; Somenzi, F.; Trivedi,
A.; and Wojtczak, D. 2019. Omega-Regular Objectives in
Model-Free Reinforcement Learning. In TACAS (1), volume
11427 of LNCS, 395–412. Springer.
Hansson, H.; and Jonsson, B. 1994. A Logic for Reasoning
about Time and Reliability. Formal Aspects Comput., 6(5):
512–535.
Hartmanns, A.; Klauck, M.; Parker, D.; Quatmann, T.; and
Ruijters, E. 2019. The Quantitative Verification Bench-
mark Set. In TACAS (1), volume 11427 of LNCS, 344–350.
Springer.
Hasanbeig, M.; Kroening, D.; and Abate, A. 2020. Deep Re-
inforcement Learning with Temporal Logics. In FORMATS,
volume 12288 of LNCS, 1–22. Springer.
Hensel, C.; Junges, S.; Katoen, J.; Quatmann, T.; and V olk,
M. 2022. The probabilistic model checker Storm. Int. J.
Softw. Tools Technol. Transf., 24(4): 589–610.
Huang, S. H.; Papernot, N.; Goodfellow, I. J.; Duan, Y .; and
Abbeel, P. 2017. Adversarial Attacks on Neural Network
Policies. In ICLR (Workshop). OpenReview.net.
H¨uttenrauch, M.; Sosic, A.; and Neumann, G. 2019. Deep
Reinforcement Learning for Swarm Systems. J. Mach.
Learn. Res., 20: 54:1–54:31.
Ilahi, I.; Usama, M.; Qadir, J.; Janjua, M. U.; Al-Fuqaha,
A. I.; Hoang, D. T.; and Niyato, D. 2022. Challenges and
Countermeasures for Adversarial Attacks on Deep Rein-
forcement Learning. IEEE Trans. Artif. Intell., 3(2): 90–109.
Im, D. J.; Ahn, S.; Memisevic, R.; and Bengio, Y . 2017.
Denoising Criterion for Variational Auto-Encoding Frame-
work. In AAAI, 2059–2065. AAAI Press.
Jansen, N.; K ¨onighofer, B.; Junges, S.; Serban, A.; and
Bloem, R. 2020. Safe Reinforcement Learning Using Prob-
abilistic Shields (Invited Paper). In CONCUR, volume 171
ofLIPIcs, 3:1–3:16. Schloss Dagstuhl - Leibniz-Zentrum f ¨ur
Informatik.
Jin, P.; Tian, J.; Zhi, D.; Wen, X.; and Zhang, M.
2022. Trainify: A CEGAR-Driven Training and Verifica-
tion Framework for Safe Deep Reinforcement Learning. In
CAV (1), volume 13371 of LNCS, 193–218. Springer.
Jothimurugan, K.; Bansal, S.; Bastani, O.; and Alur, R. 2022.
Specification-Guided Learning of Nash Equilibria with High
Social Welfare. In CAV (2), volume 13372 of LNCS, 343–
363. Springer.
Junges, S.; Jansen, N.; Katoen, J.; Topcu, U.; Zhang, R.; and
Hayhoe, M. M. 2018. Model Checking for Safe Navigation
Among Humans. In QEST, volume 11024 of LNCS, 207–
222. Springer.
169
Khan, A.; Zhang, C.; Li, S.; Wu, J.; Schlotfeldt, B.; Tang,
S. Y .; Ribeiro, A.; Bastani, O.; and Kumar, V . 2019. Learn-
ing Safe Unlabeled Multi-Robot Planning with Motion Con-
straints. In IROS, 7558–7565. IEEE.
Kwiatkowska, M.; Norman, G.; and Parker, D. 2019. Veri-
fication and Control of Turn-Based Probabilistic Real-Time
Games. In The Art of Modelling Computational Systems,
volume 11760 of LNCS, 379–396. Springer.
Kwiatkowska, M.; Norman, G.; Parker, D.; and Santos, G.
2020. Multi-player Equilibria Verification for Concurrent
Stochastic Games. In QEST, volume 12289 of LNCS, 74–
95. Springer.
Kwiatkowska, M.; Norman, G.; Parker, D.; and Santos, G.
2021. Automatic verification of concurrent stochastic sys-
tems. Formal Methods Syst. Des., 58(1-2): 188–250.
Kwiatkowska, M. Z.; Norman, G.; and Parker, D. 2011.
PRISM 4.0: Verification of Probabilistic Real-Time Sys-
tems. In CAV, volume 6806 of LNCS, 585–591. Springer.
Lin, J.; Dzeparoska, K.; Zhang, S. Q.; Leon-Garcia, A.; and
Papernot, N. 2020. On the Robustness of Cooperative Multi-
Agent Reinforcement Learning. In SP Workshops, 62–68.
IEEE.
Lomuscio, A.; and Pirovano, E. 2020. Parameterised Veri-
fication of Strategic Properties in Probabilistic Multi-Agent
Systems. In AAMAS, 762–770. International Foundation for
Autonomous Agents and Multiagent Systems.
Mnih, V .; Kavukcuoglu, K.; Silver, D.; Graves, A.;
Antonoglou, I.; Wierstra, D.; and Riedmiller, M. A. 2013.
Playing Atari with Deep Reinforcement Learning. CoRR,
abs/1312.5602.
Moos, J.; Hansel, K.; Abdulsamad, H.; Stark, S.; Clever, D.;
and Peters, J. 2022. Robust Reinforcement Learning: A Re-
view of Foundations and Recent Advances. Mach. Learn.
Knowl. Extr., 4(1): 276–315.
Mqirmi, P. E.; Belardinelli, F.; and Le ´on, B. G. 2021.
An Abstraction-based Method to Check Multi-Agent Deep
Reinforcement-Learning Behaviors. In AAMAS, 474–482.
ACM.
Ohashi, K.; Nakanishi, K.; Sasaki, W.; Yasui, Y .; and Ishii,
S. 2021. Deep Adversarial Reinforcement Learning With
Noise Compensation by Autoencoder. IEEE Access, 9:
143901–143912.
Popper, J.; Motsch, W.; David, A.; Petzsche, T.; and
Ruskowski, M. 2021. Utilizing Multi-Agent Deep Rein-
forcement Learning For Flexible Job Shop Scheduling Un-
der Sustainable Viewpoints. In ICECCME, 1–6. IEEE.
Qin, W.; Sun, Y .-N.; Zhuang, Z.-L.; Lu, Z.-Y .; and Zhou,
Y .-M. 2021. Multi-agent reinforcement learning-based dy-
namic task assignment for vehicles in urban transportation
system. International Journal of Production Economics,
240: 108251.
Riley, J.; Calinescu, R.; Paterson, C.; Kudenko, D.; and
Banks, A. 2021a. Reinforcement Learning with Quantitative
Verification for Assured Multi-Agent Policies. In ICAART
(2), 237–245. SCITEPRESS.Riley, J.; Calinescu, R.; Paterson, C.; Kudenko, D.; and
Banks, A. 2021b. Utilising Assured Multi-Agent Reinforce-
ment Learning within Safety-Critical Scenarios. In KES,
volume 192 of Procedia Computer Science, 1061–1070. El-
sevier.
Schuppe, G. F.; and Tumova, J. 2021. Decentralized Multi-
Agent Strategy Synthesis under LTLf Specifications via Ex-
change of Least-Limiting Advisers. In MRS, 119–127.
IEEE.
Serban, A. C.; and Poll, E. 2018. Adversarial Examples -
A Complete Characterisation of the Phenomenon. CoRR,
abs/1810.01185.
Serrano-Ruiz, J. C.; Mula, J.; and Poler, R. 2021. Smart
manufacturing scheduling: A literature review. Journal of
Manufacturing Systems, 61: 265–287.
Shalev-Shwartz, S.; Shammah, S.; and Shashua, A.
2016. Safe, Multi-Agent, Reinforcement Learning for Au-
tonomous Driving. CoRR, abs/1610.03295.
Tampuu, A.; Matiisen, T.; Kodelja, D.; Kuzovkin, I.; Korjus,
K.; Aru, J.; Aru, J.; and Vicente, R. 2015. Multiagent Co-
operation and Competition with Deep Reinforcement Learn-
ing. CoRR, abs/1511.08779.
Tveten, ˚A. G.; Bolkesjø, T. F.; and Ilieva, I. 2016. Increased
demand-side flexibility: market effects and impacts on vari-
able renewable energy integration. International Journal of
Sustainable Energy Planning and Management, 11: 33–50.
Vamplew, P.; Smith, B. J.; K ¨allstr ¨om, J.; de Oliveira Ramos,
G.; Radulescu, R.; Roijers, D. M.; Hayes, C. F.; Heintz, F.;
Mannion, P.; Libin, P. J. K.; Dazeley, R.; and Foale, C. 2022.
Scalar reward is not enough: a response to Silver, Singh, Pre-
cup and Sutton (2021). Auton. Agents Multi Agent Syst.,
36(2): 41.
Vincent, P.; Larochelle, H.; Bengio, Y .; and Manzagol, P.
2008. Extracting and composing robust features with de-
noising autoencoders. In ICML, volume 307 of ACM Inter-
national Conference Proceeding Series, 1096–1103. ACM.
Wang, L.; Ames, A. D.; and Egerstedt, M. 2016. Safety
barrier certificates for heterogeneous multi-robot systems. In
ACC, 5213–5218. IEEE.
Wang, Y .; Roohi, N.; West, M.; Viswanathan, M.; and
Dullerud, G. E. 2020. Statistically Model Checking PCTL
Specifications on Markov Decision Processes via Reinforce-
ment Learning. In CDC, 1392–1397. IEEE.
Wong, A.; B ¨ack, T.; Kononova, A. V .; and Plaat, A.
2021. Multiagent Deep Reinforcement Learning: Chal-
lenges and Directions Towards Human-Like Approaches.
CoRR, abs/2106.15691.
Zhang, Y .; Zhu, H.; Tang, D.; Zhou, T.; and Gui, Y . 2022.
Dynamic job shop scheduling based on deep reinforcement
learning for multi-agent manufacturing systems. Robotics
Comput. Integr. Manuf., 78: 102412.
Zong, K.; and Luo, C. 2022. Reinforcement learning based
framework for COVID-19 resource allocation. Computers
& Industrial Engineering, 167: 107960.
170"
29708-Article Text-33762-1-2-20240324.pdf,"Robust Communicative Multi-Agent Reinforcement Learning with Active Defense
Lebin Yu, Yunbo Qiu, Quanming Yao, Yuan Shen, Xudong Zhang and Jian Wang*
Department of Electronic Engineering, BNRist, Tsinghua University, Beijing 100084, China
{yulb19, qyb18}@mails.tsinghua.edu.cn, quanmingyao@gmail.com, {shenyuan ee, zhangxd, jianwang}@tsinghua.edu.cn
Abstract
Communication in multi-agent reinforcement learning
(MARL) has been proven to effectively promote cooperation
among agents recently. Since communication in real-world
scenarios is vulnerable to noises and adversarial attacks, it is
crucial to develop robust communicative MARL technique.
However, existing research in this domain has predominantly
focused on passive defense strategies, where agents receive
all messages equally, making it hard to balance performance
and robustness. We propose an active defense strategy, where
agents automatically reduce the impact of potentially harmful
messages on the final decision. There are two challenges to
implement this strategy, that are defining unreliable messages
and adjusting the unreliable messages’ impact on the final
decision properly. To address them, we design an Active De-
fense Multi-Agent Communication framework (ADMAC),
which estimates the reliability of received messages and
adjusts their impact on the final decision accordingly with the
help of a decomposable decision structure. The superiority of
ADMAC over existing methods is validated by experiments
in three communication-critical tasks under four types of
attacks.
Introduction
In recent years, multi-agent reinforcement learning (MARL)
has made remarkable strides in enhancing cooperative robot
tasks and distributed control domains, exemplified by traffic
lights control (Chu, Chinchali, and Katti 2020) and robots
navigation (Han, Chen, and Hao 2020). Given the inherent
partial observability in multi-agent tasks, several researchers
have explored the integration of communication mecha-
nisms to facilitate information exchange among agents (Ma,
Luo, and Pan 2021).
Nevertheless, the utilization of multi-agent communica-
tion in real-world applications introduces certain challenges.
In such scenarios, agents rely on wireless communication
channels to exchange messages, which are susceptible to
various sources of noises and interference. These perturba-
tions on messages, especially malicious ones, can severely
reduce the performance of multi-agent systems, even though
agents get accurate observations of the environment (Sun
*Corresponding Author
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.et al. 2023). Hence, it becomes imperative to address these
issues and devise robust communicative MARL mecha-
nisms.
Adversarial attacks and defenses in communicative
MARL receive much less attention when compared to their
counterparts in reinforcement learning (Mu et al. 2022). Cur-
rent frameworks in this area (Ishii, Wang, and Feng 2022;
Yuan et al. 2023) commonly follow the principle of passive
defense, where agents treat all received messages equally
and try to make relative safe decisions. Since perturbed mes-
sages are mixed with useful messages, this indiscriminate re-
ception may lead to the result of “garbage in, garbage out”.
We notice that robust communicative MARL has an im-
portant feature compared with robust RL: The attackers are
only allowed to modify a part of the messages, while the
modification is unlimited and perturbed messages can be
quite different from the original ones. Inspired by noisy
learning (Han et al. 2018), we propose an active defense
strategy to utilize this feature: agents actively judge the reli-
ability of messages based on their own unperturbed observa-
tions and hidden states (which contain history information)
and reduce unreliable messages’ impact on the final deci-
sion.1For example, if in a search task an agent receives a
message saying that “target is at coordinates (1,1), get to
it!”, while the agent have searched (1,1) and found nothing,
it can realize that this message is fake. We also visualize
the difference between active defense and passive defense in
Fig. 1 for better demonstration.
Nevertheless, there remain two key challenges to imple-
ment active defense. (I) What kind of messages should be
defined as unreliable? (II) How to adjust the unreliable mes-
sages’ impact on the final decision properly? Since the ulti-
mate goal for multi-agent communication is to make agents
better finish cooperation tasks, a message beneficial for
this goal should be considered reliable. However, common
communicative MARL frameworks aggregate received mes-
sages with highly nonlinear neural networks, making it hard
1An intuitive approach is identifying the perturbed messages
and simply remove them from decision-making. However, for an
agent, the received messages themselves contain some unknown in-
formation, making it infeasible to accurately judge whether a mes-
sage should be completely trusted or ignored. Besides, it is hard to
determine a proper decision threshold for judging whether a mes-
sage is reliable or unreliable.
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17575
𝑚1 Policy Netobservation
judge
reliabilityaction
output
passive
defenseactive
defense𝑚2
𝑚3∗𝑤1
∗𝑤2
∗𝑤3action
outputRobust
Policy NetFigure 1: The figure shows the difference between active defense and passive defense. Passive defense takes in harmful messages
along with useful ones equally, and try to make robust decisions. This indiscriminate reception makes it difficult to optimize
robustness and performance simultaneously. In comparison, active defense assesses the reliability of incoming messages with
local information first and reduces the weight of potentially malicious messages. For example, w2in this figure is expected to
be much lower than w1andw3. By doing so, this framework enables the policy network to efficiently extract information from
reliable messages with reduced interference.
to evaluate or adjust the impact of a sole message on the final
decision.
To address the aforementioned challenges and achieve ac-
tive defense, we introduce an Active Defense Multi-Agent
Communication (ADMAC) framework comprising two key
components. The first is reliability estimator, which outputs
the estimated reliability (ranging from 0 to 1) of a certain
message according to the agent’s observation and hidden
state. The second is decomposable message aggregation pol-
icy net, which decomposes the impact of each message on
the final decision, and allows adjustable weights to con-
trol this impact. Below is an introduction of how an agent
processes messages and makes decisions using ADMAC:
Firstly, it generates a base action preference vector based
on the observation and hidden state, with each component
corresponding to a feasible action. Secondly, for each mes-
sage received, the agent generates a message action pref-
erence vector according to the message and the agent’s ob-
servation. Thirdly, the reliability estimator outputs the reli-
ability values of messages, which then serve as the weights
of corresponding message action preference vectors. Lastly,
the agent adds all weighted preference vectors to get the total
action preference vector and feeds it into Softmax to derive
the final action distribution.
In order to substantiate the robustness of ADMAC, we
evaluate it alongside three alternative communication frame-
works in three communication-critical cooperation tasks.
Besides, we implement four distinct types of attacks, includ-
ing Gaussian attack (He et al. 2023), Monte-Carlo adversar-
ial attack, Fast Gradient Sign Method (Goodfellow, Shlens,
and Szegedy 2014) and Projected Gradient Descent (Madry
et al. 2018). Experiments confirm the superiority of AD-
MAC in terms of its robustness and performance compared
to the alternatives. Moreover, we conduct ablation study to
further explore the features of components in ADMAC.
Our contributions can be summarized below:
1. We elucidate a significant distinction between robust RL
and robust communicative MARL, highlighting the un-
suitability of the commonly employed passive defense
strategy in robust RL for the latter modeling. Moreover,
we propose the idea of active defense which exploits the
features of robust communicative MARL.2. There remain two challenges to implement active de-
fense, that are defining unreliable messages and adjust-
ing the unreliable messages’ impact on the final deci-
sion properly. To overcome them, we propose ADMAC,
which incorporates mechanisms that enable agents to
lower the impact of potentially malicious perturbed mes-
sages on the final decision, thereby achieving robustness
against attacks.
3. We empirically demonstrate the superiority of ADMAC
over existing approaches, and conduct ablation study to
delve deeper into the unique features and characteristics
of it.
Preliminaries
Dec-POMDP with Communication
Our modeling is based on a decentralized partially observ-
able Markov decision process (Littman 1994) with Nagents
in the system. At timestep t, the global state is st, and agent
ireceives observation ot
ifrom the environment and chooses
an action at
ito perform. Then the environment provides
agents with rewards rt
iand updates the global state from st
tost+1according to all agents’ actions.
There are numerous communication frameworks in com-
municative MARL, including scheduler-based (Rangwala
and Williams 2020), inquiry-reply-based (Ma, Luo, and Pan
2021) and GNN-based (Pu et al. 2022) ones. They basically
follow the communication and decision process specified
below, regardless of the difference in communication archi-
tectures.
At timestep t, agent iwith hidden states ht−1
ifirst re-
ceives ot
i, based on which it generates message and commu-
nicate with other agents. Then, it updates its hidden states
and chooses action at
iwith a policy network according to
its current hidden state, observation and messages from oth-
ersmt
1, mt
2, ..., mt
N. The objective function that needs to be
maximized for agent iis the discounted return:
J(θ) =E[X
tγtrt
i|θ], (1)
where θdenotes the neural network parameters of the
agents.
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17576
Attack against Communication
Attack against multi-agent communication has been a hot
topic (Xue et al. 2022; Sun et al. 2023) recently since wire-
less communication is vulnerable to distractions and noises.
These works commonly follow the setting that attackers only
interfere a part of messages in the multi-agent system to dis-
rupt collaboration. Based on them, we consider the follow-
ing attack model.
Suppose there are Nagents in the system, and at each
timestep there are at most N×Nmessages. For each mes-
sagemt
j, the attacker has a probability pto change it to ˆmt
j.
Then agents receive messages without knowing which are
perturbed.
The attack may be adversarial or non-adversarial, depend-
ing on how much information the attacker has about the
agents. For adversarial attack, we adopt the setting of Zhang
et al. where the attacker tries to minimize the chosen proba-
bility or preference of the best action, denoted by ˆP(at
i,best):
ˆmt
j=fA(mt
j) = arg minX
i̸=jˆP(at
i,best). (2)
Another feasible attack target is to maximize the KL diver-
gence between at
iandˆat
i, with at
irepresenting the action dis-
tribution output by agent iwhen receiving raw message mt
j
andˆat
irepresenting the action distribution output by agent i
when receiving perturbed message ˆmt
j:
ˆmt
j=fB(mt
j) = arg maxX
i̸=jDKL(ˆat
i|at
i). (3)
Following the setting of (Sun et al. 2023), we consider
a strong attack model to better evaluate the robustness of
models: The strength of the perturbation, denoted as ||ˆmt
j−
mt
j||, is not bounded. Consequently, there are multiple ways
to implement fA(·)andfB(·), which will be detailed in the
Experiments Section.
Active Defense Multi-Agent Communication
Framework
We propose an active defense idea for robustness, namely,
making agents judge the reliability of received messages
based on their local information and reduce unreliable mes-
sages’ impact on the final decision. However, the implemen-
tation brings two challenges: (I) How to define “unreliable”
messages? (II) How to adjust the unreliable messages’ im-
pact on the final decision? In this section, we propose AD-
MAC comprising a decomposable message aggregation pol-
icy net and a reliability estimator to address these two chal-
lenges, whose visualization is presented in Fig.2.
Decomposable Message Aggregation Policy Net
The decomposable message aggregation policy net fPis de-
signed to decompose the impact of each message on the fi-
nal decision by restricting their influence to action prefer-
ence vectors. Specifically, fPconsists of three parameter-
ized modules: a GRU module fHPused to update hidden
states with observations, a base action generation modulefBPthat generates base action preference vectors accord-
ing to the updated hidden states, and a message-observation
process module fMPthat generates message action pref-
erence vectors according to the observations and received
messages. Suppose there are Kactions to choose from, then
an action preference vector has Kcomponents, each rep-
resenting the preference for the corresponding action. Use
pt
i=fP(ot
i, ht
i, mt
1, ..., mt
N)to denote the final output ac-
tion distribution for agent iat timestep t, where the k-th
component of pt
i, denoted by pt
i[k], refers to the probability
of choosing the k-th action, the decision process is formu-
lated below:
ht
i=fHP(ht−1
i, ot
i),
vt
i=fBP(ht
i) +X
j̸=iwi(mt
j)fMP(ot
i, mt
j),
pt
i[k] =evt
i[k]/X
kevt
i[k].(4)
where vt
iis the total action preference vector, and wi(mt
j)
is a weight determined by the reliability estimator detailed
in the next subsection. wi(mt
j)is set to 1by default if
no robustness is required. Evidently, messages with larger
weights have a stronger influence on the final decision.
Therefore, for a message considered to be malicious, reduc-
ing its weight can effectively attenuate its impact on the final
decision. We provide the following proposition to character-
ize this feature:
Proposition 1 For an agent making decisions using (4), if
message mt
jrecommends an action most or least recom-
mends an action to the agent, incorporating this message
into decision-making must increase or decrease the prob-
ability of choosing this action in the final decision, and the
magnitude of this effect varies monotonically with the weight
wi(mt
j).
The proof is presented in Appendix A2. Here “mt
j
recommends the ka-th action most” means ka=
arg max kfMP(ot
i, mt
j)[k], and “mt
jrecommends the kb-th
action least” means kb= arg min kfMP(ot
i, mt
j)[k].
Reliability Estimator
The reliability estimator is a classifier fR(ht
i, ot
i, mt
j)that
judges whether a message mt
jis reliable for agent iwith the
help of the agent’s hidden state ht
iand observation ot
i. The
output of fR(ht
i, ot
i, mt
j)is a vector of length 2normalized
by Softmax. Let
wi(mt
j) =fR(ht
i, ot
i, mt
j)[0]. (5)
wi(mt
j)represents the extent to which the reliability estima-
tor thinks mt
jis reliable for agent i, and is used as the weight
ofmt
jin (4). Besides, judging the reliability of messages
can be treated as a binary classification problem, and the
key challenge here is labelling data, i.e., defining what mes-
sages are “reliable” and what are not. Since the ultimate goal
2Please refer to https://arxiv.org/abs/2312.11545 for the full
version with appendices.
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17577
observation 𝑜𝑖𝑡ℎ𝑖𝑡−1𝑓𝐻𝑃
ℎ𝑖𝑡𝑓𝐵𝑃
…𝑓𝑅 ℎ𝑖𝑡𝑓𝑀𝑃 𝑚1𝑡
𝑤𝑖(𝑚1𝑡)× + +…𝑓𝑅 ℎ𝑖𝑡𝑓𝑀𝑃 𝑚𝑁𝑡
𝑤𝑖(𝑚𝑁𝑡)× + … …=SoftMaxoutput action 
distribution
base action 
preference vectormessage action 
preference vectormessage action 
preference vector𝒗𝑖𝑡Figure 2: The figure shows how agent iwith hidden state ht−1
igenerates an output action distribution within ADMAC after
receiving an observation ot
iand messages mt
1,mt
2, ...,mt
Nfrom others. It is noteworthy that the length of actions preference
vectors is the same as the number of feasible actions, and each component of the vectors represents the preference for the
corresponding action. As depicted in the figure, the impact of each message on the final decision is restricted to the respective
action preference vector and can be regulated by the weight wi(mt
j).
of communicative MARL is maximizing cooperation per-
formance represented by (1), messages should be labelled
according to whether they are conducive to achieving this
goal.
With the assumption that agents’ policies are well-trained,
we use the following criteria to label messages: For an agent,
if a received message recommends it to choose the best ac-
tion, then the message is considered to be reliable, otherwise
it is bad. Here “best action” refers to the action most likely
to be chosen in the absence of perturbations, and “ mt
jrec-
ommends the kr-th action” is defined as fMP(ot
i, mt
j)[kr]>P
kfMP(ot
i, mt
j)[k]/K. With the labelled messages, the re-
liability estimator can be trained in a supervised learning
way.
Instantiation
ADMAC only specifies how agents process messages as
well as observations to make robust decisions, and is com-
patible with numerous communication architectures and
training algorithms. In this paper, we adopt a basic broadcast
communication mechanism (Singh, Jain, and Sukhbaatar
2018) and use the following paradigm to train our model.
More details are provided in Appendix B.
Stage 1: training policy net : The training goal of this
stage is to optimize the parameters of the decomposable
message aggregation policy net and message encoders to
maximize the objective function defined in (1). Notably,
messages used in our framework are one-dimensional vec-
tors with each component ranging from −1to1, and this
stage does not involve any attacks or defenses. Conse-
quently, the agents interact with the environment, commu-
nicate with each other and update the parameters in a tradi-
tional communicative MARL way.
Stage 2: generating dataset: After the policies are well-
trained, let agents interact with the environment for several
episodes to generate training dataset for reliability estima-
tor. To enhance its identification ability, we implement two
representational and strong attacks from (Sun et al. 2023)
during training:
Attack I. Random Perturbation: It suits the scenario
where the attacker has no information about the agents. At-tackers generate random perturbed messages ˆmt
jto replace
the original ones, which means each component of ˆmt
jis a
sample from Uniform distribution on (−1, 1).
Attack II. (L2-normed) Gradient descent adversar-
ial attack: It suits the scenario where the attacker has full
knowledge of the agents. To maximize the adversarial attack
objective presented in (2), gradient descent can be utilized
to generate a perturbed message:
ˆmt
j=mt
j+λ∇mt
jfA(mt
j)/||∇ mt
jfA(mt
j)||2 (6)
When creating dataset for the reliability estimator, we ran-
domly replace 1/3 raw messages with Attack I messages and
1/3 messages with Attack II messages, and let agents make
decisions based on them. The decisions will not be truly exe-
cuted, on the contrary, they are only used to label these mes-
sages according to the aforementioned criteria (It is possible
that some perturbed messages are labelled reliable, and un-
perturbed messages are labelled unreliable). All messages
and corresponding observations, hidden states, and labels
are collected to form a dataset for supervised learning.
Stage 3: training reliability estimator : Train the relia-
bility estimator parameterized via MLP based on the dataset
using Adam optimizer and cross-entropy loss function. A
well-trained reliability estimator is expected to assign low
weights to messages that are not conducive to an agent’s se-
lection of the optimal action.
Experiments
Experimental Environments
We implement three communication-critical multi-agent en-
vironments for demonstrative purposes: Food Collector (Sun
et al. 2023), Predator Prey (Singh, Jain, and Sukhbaatar
2018), and Treasure Hunt (Freed et al. 2020). They either
adhere to a predefined communication setting or a learned
communication setting. We test these two kinds of commu-
nication settings because the messages within them have dif-
ferent distributions, potentially influencing the performance
of attack and defense. Within all environments, one episode
ends if all agents finish their tasks or the timestep reaches
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17578
1 3
22
1 3
VisionFood 3 is 
here!
Vision
Predator moving downFixed prey
A
C
BAgent searching
Treasure only visible 
to agent CCABFigure 3: Visualizations of our three experiment environments: Food Collector, Predator Prey, and Treasure Hunt.
the upper limit tmax. Therefore, lower average timesteps in-
dicate better performance. These environments are detailed
below and visualized in Fig.3.
Food Collector (predefined communication) In this
task,N= 5agents with different IDs search for foods with
the same IDs in a 1×1field. Each agent can observe targets
within its vision d= 0.2, and moves in eight directions at
a speed v= 0.15. If an agent finds a target with a differ-
ent ID, it will kindly broadcasts the coordinates to help the
corresponding agent find it.
Predator Prey (learned communication) In this task,
N= 5 agents with vision 1are required to reach a fixed
prey in a grid world of size 10×10. Due to the severely
limited perception, agents must communicate with others to
finish tasks earlier. For example, they can delineate their re-
spective search areas through communication, and the first
agent to reach the prey can tell others the coordinates.
Treasure Hunt (learned communication) In this task,
N= 5 agents work together to hunt treasures in a field of
size1×1with movement speed v= 0.9. Each agent ob-
tains the coordinates of its own treasure, which is invisible
to others. Note that an agent cannot collect its treasure by
itself. Instead, it should help others hunt it through learned
communication.
Tested Algorithms
We evaluate our proposed ADMAC alongside three alterna-
tives introduced below:
Targeted Multi-Agent Communication (TARMAC)
(Das et al. 2019) It is a baseline communication framework
where agents utilize an attention mechanism to determine
the weights of receiving messages. It does not include any
robust techniques, and its performance demonstrates the im-
pact of attacks without any defense.
Adversarial Training (AT) (Tu et al. 2021) This frame-
work adopts the most frequently used robust learning tech-
nique, which is performing adversarial attacks during train-
ing to gain robustness.
Ablated Message Ensemble (AME) (Sun et al. 2023) It
constructs a message-ensemble policy that aggregates mul-tiple randomly ablated message sets. The key idea is that
since only a small portion ( <50%) of received messages
are harmful, making agents take the consensus of received
messages should provide robustness.
All tested frameworks are trained with an improved ver-
sion of REINFORCE (Williams 1992). More information
about training is presented in Appendix B.
Implemented Attacks
When conducting experimental evaluations of the models,
we implement the following four kinds of attacks, including
adversarial and non-adversarial ones:
Attack III Gaussian attack (He et al. 2023): Attacker
adds Gaussian Noises to the messages, i.e.
ˆmt
j=mt
j+σN(0,1). (7)
We set σ= 0.5 in the experiments.
Attack IV Monte-Carlo adversarial attack: Randomly
generate 10 messages, and find the one that maximizes the
attack objective f(mt
j)defined in (2) or (3).
Attack V Fast Gradient Sign Method (Goodfellow,
Shlens, and Szegedy 2014):
ˆmt
j=mt
j+ηsign(∇ mt
jf(mt
j)), (8)
Then range of mis(−1, 1), and we set η= 1 to obtain a
strong attack.
Attack VI Projected Gradient Descent (Madry et al.
2018): PGD can be treated as a multi-step version of FGSM:
ˆmt,(i+1)
j = ˆmt,(i)
j+ϵsign(∇ˆmt,(i)
jf( ˆmt,(i)
j)), (9)
where ˆmt,(0)
j=mt
j. We set ϵ= 0.3 and use 5-step updates
to obtain the final perturbed messages.
For adversarial attacks (IV , V and VI), the attack objective
can be chosen from fA(·)andfB(·), defined accordingly in
(2) and (3). It is noteworthy that although they may have the
same objectives with attack-II, which is used to train AD-
MAC, the generated perturbed messages belong to different
distributions because they are calculated differently, there-
fore, ADMAC DOES NOT have prior information of the
above attacks used for evaluation.
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17579
0 0.1 0.2 0.31618202224Food Collector+Attack III
TARMAC
AT
AME
ADMAC
0 0.1 0.2 0.316182022242628Food Collector+Attack IV-A
TARMAC
AT
AME
ADMAC
0 0.1 0.2 0.315.017.520.022.525.027.530.032.535.0Food Collector+Attack V-A
TARMAC
AT
AME
ADMAC
0 0.1 0.2 0.316182022242628Food Collector+Attack VI-A
TARMAC
AT
AME
ADMAC
0 0.1 0.2 0.31214161820Predator Prey+Attack III
TARMAC
AT
AME
ADMAC
0 0.1 0.2 0.31214161820222426Predator Prey+Attack IV-A
TARMAC
AT
AME
ADMAC
0 0.1 0.2 0.312.515.017.520.022.525.027.530.0Predator Prey+Attack V-A
TARMAC
AT
AME
ADMAC
0 0.1 0.2 0.3121416182022Predator Prey+Attack VI-A
TARMAC
AT
AME
ADMAC
0 0.1 0.2 0.31618202224Treasure Hunt+Attack III
TARMAC
AT
AME
ADMAC
0 0.1 0.2 0.315202530354045Treasure Hunt+Attack IV-A
TARMAC
AT
AME
ADMAC
0 0.1 0.2 0.3152025303540455055Treasure Hunt+Attack V-A
TARMAC
AT
AME
ADMAC
0 0.1 0.2 0.315202530354045Treasure Hunt+Attack VI-A
TARMAC
AT
AME
ADMACFigure 4: Results for the main experiments. The x-axis represents the attack probability, and the y-axis represents the timesteps
required for task completion, where smaller values signify better performance. For each setting (e.g. TARMAC in Treasure
Hunt), we train five models with different seeds. During test, we run 500 episodes for each setting and plot the mean as well as
standard error of the averaged timesteps. Lower timesteps indicate better performance.
Performance under Attacks
We evaluate the four multi-agent communication frame-
works with different attack probability pto observe how the
performance of the agents decays as the attack intensity in-
creases. Besides, since attack objective fB(·)is not appli-
cable to AME, we only present adversarial attacks with ob-
jective function fA(·)in the main experiments, and provide
additional experimental results in Appendix C. From the re-
sults presented in Fig.4, the following conclusions can be
drawn.
Attack III is the weakest attack, and its attack effect on
TARMAC (the baseline non-robust framework) indicates the
extent to which messages can influence decision-making,
which also reflects the importance of communication for co-
operation in specific attacks. It can be seen that communi-
cation is important in all tasks, with the order of importance
being Treasure Hunt >Food Collector >Predator Prey.
AT is one of the most popular techniques in robust learn-
ing, and it is empirically confirmed to be a relatively reli-
able method. Compared with TARMAC, AT achieves lower
timesteps under strong attacks, however, its baseline perfor-
mance without attacks is slightly worse. This is inevitable as
the trade-off between robustness and performance has long
been a concern in adversarial training.Though AME provides robustness against attacks to a cer-
tain degree, its baseline performance without attacks is bad,
severely dragging down the overall performance. This re-
sults from the key feature of AME: adopting the consensus
of all incoming messages. It protects the agents from being
influenced by a small number of malicious messages, but it
also prevents agents from getting exclusive information.
Our proposed framework ADMAC has the best overall
performance, and this advantage is provided by the active
defense strategy. Compared with AT and AME, ADMAC
exploits the features of adversarial multi-agent communica-
tion, judging received messages’ reliability based on agents’
observations and hidden states. Notably, ADMAC has better
baseline performance than TARMAC in Predator Prey tasks,
which is caused by the feature of decomposable message ag-
gregation policy and will be discussed more in the ablation
study.
Ablation Study
ADMAC consists of two components: the reliability estima-
tor (abbreviated as RE) and the decomposable message ag-
gregation policy net (abbreviated as DPN). To further inves-
tigate each component’s function, we evaluate the following
four methods under attacks: TARMAC, DPN only, DPN+RE
(i.e. ADMAC), and DPN+IRE. Here IRE refers to an ideal
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17580
None III IV-A V-A VI-A
Attack type15.017.520.022.525.027.530.032.535.0TimestepsFood Collector with different attacks
TARMAC
DPN
DPN+RE
DPN+IRERE recall/precision: 0.90/0.86
None III IV-A V-A VI-A
Attack type12.515.017.520.022.525.027.530.0TimestepsPredator Prey with different attacks
TARMAC
DPN
DPN+RE
DPN+IRE RE recall/precision: 0.87/0.84
None III IV-A V-A VI-A
Attack type152025303540455055TimestepsTreasure Hunt with different attacks
TARMAC
DPN
DPN+RE
DPN+IRE RE recall/precision: 0.79/0.76
Figure 5: Results for the ablation study. The attack probability is set to p= 0.3 for all types of attack.
reliability estimator, achieving 100% classification accuracy
and cannot be implemented in reality. We also list the recall
and precision of RE below the figures to analysis its perfor-
mance from the perspective of supervised learning.
From the results presented in Fig 5, the following con-
clusions can be drawn. Firstly, the special structure of DPN
does not degrades baseline performance, on the contrary, it
provides a little robustness sometimes. Secondly, RE pro-
vides considerable robustness, especially against adversar-
ial attacks. Thirdly, DPN+IRE has the best robust perfor-
mance, which again verifies the effectiveness of the way we
label messages. Additionally, the recall and precision of RE
accounts for the performance gap between DPN+RE and
DPN+IRE: The better the classification performance of RE
is, the closer the performance of DPN+RE is to DPN+IRE.
Related Work
TMC (Zhang, Zhang, and Lin 2020) is one of the earliest
robust multi-agent communication frameworks. However, it
only provides certain robustness against Gaussian noises and
random message loss. Some recent researches have consid-
ered the existence of malicious attackers (Blumenkamp and
Prorok 2021; Tu et al. 2021; Yuan et al. 2023; Sun et al.
2023), who perturb normal messages or send fake messages
to disrupt normal operation of multi-agent systems. Agents
within these frameworks commonly receive all messages
equally and try to make robust decisions, which follows a
passive defense idea conventional in robust RL (Havens,
Jiang, and Sarkar 2018; Pattanaik et al. 2018). Notably, pas-
sive defense strategies fail to utilize one important feature
of robust communicative MARL: unperturbed messages and
hidden states can help identify fake messages to some ex-
tent. Unlike the aforementioned methods, R-MACRL (Xue
et al. 2022) takes a rather proactive defense strategy, which is
correcting perturbed messages. Since useful messages must
contain some information unknown to the agents and attack-
ers may replace the useful messages with manipulated ones,
this strategy is not quite practical. Compared with them,
while utilizing observations and hidden states, we have made
corresponding preparations for the fact that it is impossible
to perfectly judge received messages.Besides, we would like to differentiate our research from
robust MARL. Following Sun et al., we make two impor-
tant assumptions: 1) only a part of the messages might be
perturbed; 2) the perturbation power is unlimited and the
perturbed messages can be completely different from the
original ones. As a comparison, robust MARL commonly
assumes the observations of agents (Li et al. 2019) or the
environment model (Zhang et al. 2020b) is perturbed. Some
robust MARL techniques can be used in robust communica-
tive MARL (e.g. adversarial training), but an active defense
strategy can better utilize the features of this problem.
Conclusion
In this paper, we investigate the issue of robust commu-
nicative MARL, where malicious attackers may interrupt
the multi-agent communication, and agents are required to
make resilient decisions based on unperturbed observations
and potentially perturbed messages. Considering that agents
have multiple sources of information in this setting, we put
forward an active defense strategy, which involves agents
actively assessing the reliability of incoming messages and
reducing the impact of potentially malicious messages to the
final decisions.
Implementing active defense poses two primary chal-
lenges: labelling “unreliable” messages and adjusting the
unreliable messages’ impact on the final decision appropri-
ately. To address them, we introduce ADMAC, a framework
comprising an reliability estimator and a decomposable mes-
sage aggregation policy net. ADMAC is evaluated alongside
three alternative multi-agent communication frameworks in
three communication-critical environments under attacks of
different kinds and intensities, and shows outstanding ro-
bustness.
One limitation of ADMAC is that in scenarios where
messages are likely to carry unique information, judging
whether a message is reliable is hard, leading to a decline
in the robustness of ADMAC. As future work, we will try
to make agents aggregate information from a wider range
of sources in order to better assess received messages, and
expand our framework to accommodate scenarios with con-
tinuous action spaces.
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17581
References
Blumenkamp, J.; and Prorok, A. 2021. The emergence of ad-
versarial communication in multi-agent reinforcement learn-
ing. In Conference on Robot Learning, 1394–1414. PMLR.
Chu, T.; Chinchali, S.; and Katti, S. 2020. Multi-agent Re-
inforcement Learning for Networked System Control. In In-
ternational Conference on Learning Representations. virtual
conference.
Das, A.; Gervet, T.; Romoff, J.; Batra, D.; Parikh, D.; Rab-
bat, M.; and Pineau, J. 2019. Tarmac: Targeted multi-agent
communication. In International Conference on Machine
Learning, 1538–1546. PMLR.
Freed, B.; Sartoretti, G.; Hu, J.; and Choset, H. 2020. Com-
munication learning via backpropagation in discrete chan-
nels with unknown noise. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 34, 7160–7168.
New York.
Goodfellow, I. J.; Shlens, J.; and Szegedy, C. 2014. Explain-
ing and harnessing adversarial examples. arXiv preprint
arXiv:1412.6572.
Han, B.; Yao, Q.; Yu, X.; Niu, G.; Xu, M.; Hu, W.; Tsang,
I.; and Sugiyama, M. 2018. Co-teaching: Robust training of
deep neural networks with extremely noisy labels. Advances
in neural information processing systems, 31.
Han, R.; Chen, S.; and Hao, Q. 2020. Cooperative Multi-
Robot Navigation in Dynamic Environment with Deep Re-
inforcement Learning. In IEEE International Conference on
Robotics and Automation, 448–454.
Havens, A.; Jiang, Z.; and Sarkar, S. 2018. Online robust
policy learning in the presence of unknown adversaries. Ad-
vances in neural information processing systems, 31.
He, S.; Han, S.; Su, S.; Han, S.; Zou, S.; and Miao, F. 2023.
Robust Multi-Agent Reinforcement Learning with State Un-
certainty. Transactions on Machine Learning Research.
Ishii, H.; Wang, Y .; and Feng, S. 2022. An overview on
multi-agent consensus under adversarial attacks. Annual Re-
views in Control.
Li, S.; Wu, Y .; Cui, X.; Dong, H.; Fang, F.; and Russell, S.
2019. Robust multi-agent reinforcement learning via mini-
max deep deterministic policy gradient. In Proceedings of
the AAAI conference on artificial intelligence, volume 33,
4213–4220.
Littman, M. L. 1994. Markov games as a framework for
multi-agent reinforcement learning. In Proceedings of the
Eleventh International Conference on International Confer-
ence on Machine Learning, 157–163. New Brunswick.
Ma, Z.; Luo, Y .; and Pan, J. 2021. Learning selective com-
munication for multi-agent path finding. IEEE Robotics and
Automation Letters, 7(2): 1455–1462.
Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and
Vladu, A. 2018. Towards Deep Learning Models Resis-
tant to Adversarial Attacks. In International Conference on
Learning Representations.
Mu, R.; Ruan, W.; Marcolino, L. S.; Jin, G.; and Ni, Q. 2022.
Certified Policy Smoothing for Cooperative Multi-Agent
Reinforcement Learning. arXiv preprint arXiv:2212.11746.Pattanaik, A.; Tang, Z.; Liu, S.; Bommannan, G.; and
Chowdhary, G. 2018. Robust Deep Reinforcement Learning
with Adversarial Attacks. In Proceedings of the 17th Inter-
national Conference on Autonomous Agents and MultiAgent
Systems, 2040–2042.
Pu, Z.; Wang, H.; Liu, Z.; Yi, J.; and Wu, S. 2022.
Attention Enhanced Reinforcement Learning for Multi
agent Cooperation. IEEE Transactions on Neural
Networks and Learning Systems. To be published,
doi:10.1109/TNNLS.2022.3146858.
Rangwala, M.; and Williams, R. 2020. Learning multi-
agent communication through structured attentive reason-
ing. Advances in Neural Information Processing Systems,
33: 10088–10098.
Singh, A.; Jain, T.; and Sukhbaatar, S. 2018. Learning when
to Communicate at Scale in Multiagent Cooperative and
Competitive Tasks. In International Conference on Learn-
ing Representations. Vancouver.
Sun, Y .; Zheng, R.; Hassanzadeh, P.; Liang, Y .; Feizi, S.;
Ganesh, S.; and Huang, F. 2023. Certifiably Robust Policy
Learning against Adversarial Multi-Agent Communication.
InThe Eleventh International Conference on Learning Rep-
resentations.
Tu, J.; Wang, T.; Wang, J.; Manivasagam, S.; Ren, M.; and
Urtasun, R. 2021. Adversarial attacks on multi-agent com-
munication. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, 7768–7777.
Williams, R. J. 1992. Simple statistical gradient-following
algorithms for connectionist reinforcement learning. Ma-
chine learning, 8(3): 229–256.
Xue, W.; Qiu, W.; An, B.; Rabinovich, Z.; Obraztsova, S.;
and Yeo, C. K. 2022. Mis-spoke or mis-lead: Achieving
Robustness in Multi-Agent Communicative Reinforcement
Learning. In Proceedings of the 21st International Confer-
ence on Autonomous Agents and Multiagent Systems, 1418–
1426.
Yuan, L.; Chen, F.; Zhang, Z.; and Yu, Y . 2023.
Communication-Robust Multi-Agent Learning by Adapt-
able Auxiliary Multi-Agent Adversary Generation. arXiv
preprint arXiv:2305.05116.
Zhang, H.; Chen, H.; Xiao, C.; Li, B.; Liu, M.; Boning, D.;
and Hsieh, C.-J. 2020a. Robust deep reinforcement learn-
ing against adversarial perturbations on state observations.
Advances in Neural Information Processing Systems, 33:
21024–21037.
Zhang, K.; Sun, T.; Tao, Y .; Genc, S.; Mallya, S.; and Basar,
T. 2020b. Robust multi-agent reinforcement learning with
model uncertainty. Advances in neural information process-
ing systems, 33: 10571–10583.
Zhang, S. Q.; Zhang, Q.; and Lin, J. 2020. Succinct and
robust multi-agent communication with temporal message
control. Advances in Neural Information Processing Sys-
tems, 33: 17271–17282.
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17582"
3658644.3670293.pdf,"SUB-PLAY : Adversarial Policies against Partially Observed
Multi-Agent Reinforcement Learning Systems
Oubo Ma
Zhejiang University
Hangzhou, China
mob@zju.edu.cnYuwen Pu∗
Zhejiang University
Hangzhou, China
yw.pu@zju.edu.cnLinkang Du
Xi’an Jiaotong University
Xi’an, China
linkangd@gmail.com
Yang Dai
Laboratory for Big Data and Decision
Changsha, China
daiyang2000@163.comRuo Wang
Chinese Aeronautical Establishment
Beijing, China
kurt_ashtray@163.comXiaolei Liu
Institute of Computer Application,
China Academy of Engineering
Physics
Mianyang, China
luxaole@gmail.com
Yingcai Wu
Zhejiang University
Hangzhou, China
ycwu@zju.edu.cnShouling Ji∗
Zhejiang University
Hangzhou, China
sji@zju.edu.cn
Abstract
Recent advancements in multi-agent reinforcement learning (MARL)
have opened up vast application prospects, such as swarm control
of drones, collaborative manipulation by robotic arms, and multi-
target encirclement. However, potential security threats during the
MARL deployment need more attention and thorough investiga-
tion. Recent research reveals that attackers can rapidly exploit the
victim’s vulnerabilities, generating adversarial policies that result
in the failure of specific tasks. For instance, reducing the winning
rate of a superhuman-level Go AI to around 20%. Existing stud-
ies predominantly focus on two-player competitive environments,
assuming attackers possess complete global state observation.
In this study, we unveil, for the first time, the capability of attack-
ers to generate adversarial policies even when restricted to partial
observations of the victims in multi-agent competitive environ-
ments. Specifically, we propose a novel black-box attack (SUB-PLAY )
that incorporates the concept of constructing multiple subgames to
mitigate the impact of partial observability and suggests sharing
transitions among subpolicies to improve attackers’ exploitative
ability. Extensive evaluations demonstrate the effectiveness of SUB-
PLAY under three typical partial observability limitations. Visual-
ization results indicate that adversarial policies induce significantly
different activations of the victims’ policy networks. Furthermore,
we evaluate three potential defenses aimed at exploring ways to
∗Yuwen Pu and Shouling Ji are the co-corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA.
©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0636-3/24/10
https://doi.org/10.1145/3658644.3670293mitigate security threats posed by adversarial policies, providing
constructive recommendations for deploying MARL in competitive
environments.
CCS Concepts
•Security and privacy; •Computing methodologies →Artifi-
cial intelligence;
Keywords
Adversarial Policy; Multi-Agent Reinforcement Learning; Partially
Observable
ACM Reference Format:
Oubo Ma, Yuwen Pu, Linkang Du, Yang Dai, Ruo Wang, Xiaolei Liu, Yingcai
Wu, and Shouling Ji. 2024. SUB-PLAY : Adversarial Policies against Partially
Observed Multi-Agent Reinforcement Learning Systems. In Proceedings of
the 2024 ACM SIGSAC Conference on Computer and Communications Security
(CCS ’24), October 14–18, 2024, Salt Lake City, UT, USA. ACM, New York, NY,
USA, 15 pages. https://doi.org/10.1145/3658644.3670293
1 Introduction
Multi-agent reinforcement learning (MARL) has succeeded remark-
ably in diverse domains, from StarCraft II [ 49] to cyber-physical
systems [ 58], strategic maneuvers [ 11], and social science [ 35].
Currently, MARL predominantly emphasizes improving algorithm
performance across various tasks, yet there is a noticeable lack of
consideration for security aspects.
Recent research [ 59] has unveiled that even state-of-the-art re-
inforcement learning (RL) policies exhibit weaknesses and vulner-
abilities in competitive environments. Therefore, an attacker can
employ adversarial policies to induce the victim’s policies to trigger
vulnerabilities, resulting in a significant performance decline, possi-
bly even leading to a loss of sequential decision-making capability.
For instance, Victim-play [15] is a black-box attack framework
designed for adversarial policy generation, where the attacker can
645

CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA. Oubo Ma et al.
Figure 1: Three partially observable limitations in multi-
agent environments.
interact with the victim without requiring access to the victim’s
policy or environmental perturbations. However, Victim-play is
designed for two-player competitions, where the victim operates as
a single-agent system, such as a superhuman-level Go AI [ 59]. It en-
counters challenges when striving to sustain a stable attack perfor-
mance within multi-agent competitions, with the victim functioning
as a multi-agent system (MAS), for instance, a drone swarm [ 71].
The widespread prevalence of partial observability exacerbates this
challenge, as attackers are unable to access complete global state
information. This may result in adversarial strategies being un-
able to converge due to fluctuations or getting trapped in poorly
performing local optima.
Partial observability is primarily attributed to three limitations
(see Figure 1): (1) Uncertainty Limitation: This occurs when partial
observability arises due to the constraints imposed by unpredictable
environmental events. Examples include obstacle occlusion, noisy
measurements, and sensor anomalies. (2) Distance Limitation: This
refers to situations where the relative distance between agents
exceeds their perceptual range, determined by the sensors deployed
by MASs, such as LiDARs and millimeter-wave radars [ 50]. (3)
Region Limitation: Incomplete observations result from privacy
concerns, security constraints, or rule restrictions, where specific
boundaries define the region and could represent geographical areas
or logical ranges. Examples in this regard encompass restricted
areas due to permission controls or competitions with incomplete
information, such as financial markets or Texas Hold’em poker.
This paper introduces SUB-PLAY, a novel black-box attack frame-
work aimed at adversarial policy generation in partially observed
multi-agent competitive environments. Our intuition lies in the
divide-and-conquer principle, decomposing the attack into multiple
subgames. Each subgame is then modeled as a partially observable
stochastic game (POSG) [ 37], and MARL is employed to solve and
obtain the corresponding subpolicy. Finally, we integrate all sub-
policies in a hard-coded format to generate the ultimate adversarial
policy. Our main challenge is the ineffectiveness of attacks caused
by data imbalance. Specifically, the attacker records the interac-
tions at each time step in the form of transitions and allocates
these transitions to a specific subgame replay buffer based on the
observed number of victim agents. However, the number of transi-
tions in each buffer is uneven due to varying probabilities of each
subgame occurrence. This imbalance may lead to undertraining of
some subpolicies. To mitigate this issue, we propose a transition
dissemination mechanism that facilitates the sharing of transition
from proximity subgames.Extensive evaluation results demonstrate that SUB-PLAY can
effectively address the aforementioned three partial observability
limitations and outperform Victim-play in Predator-prey and World
Communication, two representative multi-agent competitive en-
vironments open-sourced by OpenAI [ 40]. Compared to normal
opponents, t-SNE analysis reveals a significant difference in the
activations of the victim’s policy network during interactions with
adversarial policies. The scalability evaluation indicates that attack-
ers can adjust the granularity of subgame construction to expand
the applicability of our method. Moreover, SUB-PLAY is algorithm-
agnostic, i.e., applicable to both distributed and centralized MARL
algorithms.
To explore strategies for mitigating adversarial policies, we eval-
uate three potential defenses. The results indicate that adversarial
retraining is insufficient to counteract adversarial policies, while
policy ensemble and fine-tuning could only moderately reduce the
effectiveness of attacks. Nevertheless, insights from the evaluation
suggest that defenders may mitigate security risks posed by adver-
sarial policies through flexible deployment techniques. For example,
periodically updating policies or increasing the diversity of policies
in policy ensemble.
In summary, the paper makes the following contributions:
•To the best of our knowledge, SUB-PLAY1is the first work
to investigate the security threats of adversarial policies in
multi-agent competitive environments, revealing that attack-
ers can exploit vulnerabilities in the victim’s policy even with
partial observations.
•We summarize three partially observable limitations and
propose an observable-driven subgame construction method
to accommodate these limitations.
•We conduct a systematic evaluation, demonstrating that SUB-
PLAY outperforms the state-of-the-art attack framework in
partially observable multi-agent competitive environments.
•We explore potential defenses, emphasizing that practition-
ers in MARL should not only focus on improving algorithm
performance but also pay attention to deployment details,
which is crucial in mitigating security threats posed by ad-
versarial policies.
2 Background
2.1 Multi-Agent RL
MARL refers to scenarios where multiple agents are involved in
sequential decision-making, and their policies are updated con-
currently. This results in a non-stationary environment where the
optimal policy for each agent changes over time, making the Markov
property invalid [54].
MARL Tasks. Based on the cooperation patterns among agents,
MARL tasks are primarily divided into four categories: (1) Fully
Cooperative MARL, where agents typically share a common reward
function and collaborate to achieve a shared objective. Examples
include multi-agent pathfinding and traffic management. (2) Fully
Competitive MARL, where agents compete individually to outper-
form each other and pursue their objectives. These tasks are often
1Inspired by the previous work [ 25], the source code of SUB-PLAY is responsibly shared
with other researchers to avoid potential ethical concerns. Instructions regarding access
requests can be found at https://github.com/maoubo/Repository-Access-SUB-PLAY.
646
SUB-PLAY : Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA.
Figure 2: MARL training paradigms.
modeled as two-player zero-sum Markov games, where coopera-
tion between agents is impossible. Examples include Go or arm
wrestling. (3) Self-Interested MARL, where agents prioritize their
benefits without considering others, as observed in domains like
autonomous driving and stock trading. (4) Mixed MARL involves
a blend of cooperative and competitive behavior. In most scenar-
ios, two competing MASs exist, but agents within the same MAS
collaborate. Examples include military exercises, multi-target encir-
clement, and team sports. The competitive environment discussed
in this paper falls within a typical class of mixed MARL tasks.
Training Paradigm. MARL has two training paradigms based
on the presence or absence of a central decision-maker [ 19]. (1)
Distributed MARL: The algorithms assume agents update poli-
cies independently, similar to single-agent implementations. Dis-
tributed Training Decentralized Execution (DTDE), a typical para-
digm (e.g., independent Q-learning), allows efficient training and
deployment without communication constraints. However, it may
not be suitable for complex environments with many agents, as
non-stationarity is neglected. (2) Centralized MARL: Centralized
Training Centralized Execution (CTCE) is a centralized MARL para-
digm with a centralized decision maker in training and deployment.
It achieves theoretically optimal performance but needs communi-
cation guarantees and suffers from the curse of dimensionality [ 46].
In contrast, Centralized Training Decentralized Execution (CTDE)
algorithms, such as QMIX, MADDPG, and MAPPO, guide agents
during training but enable independent decision-making during
deployment without additional communication, offering state-of-
the-art performance. SUB-PLAY is not limited by the attacker’s com-
munication capabilities or the number of agents under its control.
Therefore, SUB-PLAY applies to both distributed and centralized
MARL algorithms.
2.2 Adversarial Policy
The adversarial policy is a form of action manipulation attack in
which the attacker induces the black-box victim to make suboptimal
decisions by controlling the actions of the adversary agents. The
training of adversarial policies relies on the competitive relationship
between the attacker and the victim (typically zero-sum games), so
the attacker only needs to maximize the adversary agents’s reward
to autonomously discover and exploit weaknesses and vulnerabili-
ties in the RL policies deployed by the victims.
Existing research [ 15,22,38,59,60] predominantly focuses on
two-player competitions, assuming an attacker has the privilegeto interact with a victim, can obtain a complete observation of
the environment at each time step, and the victim fixedly deploys
a well-trained policy. The fundamental reason for the existence
of adversarial policies stems from RL’s adoption of Self-play for
policy training in competitive environments [ 26]. However, Self-
play cannot guarantee to reach a Nash equilibrium within finite
training. In game theory, non-equilibrium policies are inevitably
exploitable. Guided by this intuition, an attacker can manipulate
its policy during training, updating it in a direction that maximizes
the exploitation of the victim’s vulnerabilities.
For instance, while AlphaGo-style AIs outperform human cham-
pions, adversarial policies specifically trained against them still
achieve a success rate of over 77% [ 59]. Remarkably, these adver-
sarial policies fall short when facing ordinary Go enthusiasts. This
indicates that adversarial policies are highly targeted, sacrificing
generalizability to intensify exploitation against a specific victim.
Therefore, an adversarial policy is a complement to RL or a figura-
tion of its weaknesses rather than a substitute.
Finding or approximating a Nash equilibrium in a multi-agent
competition is at least as hard as PPAD-complete [ 4]. This implies
that in real deployment scenarios, MARL policies are exploitable.
Therefore, MARL must be attentive to the potential risks posed by
adversarial policies.
More details about the adversarial policy, including a more nu-
anced explanation of its existence and the upper limit of attack
performance, can be found in Appendix A of [42]2.
3 Threat Model and Problem Formulation
3.1 Threat Model
In this paper, we propose an adversarial policy attack for mixed
MARL tasks in two-team competitive environments3.
Definition 1. A two-team competitive environment involves two
MASs, Adversary and Victim, which consist of two sets of agents, M
andN, where|M|=M and|N|=N. Adversary and Victim are in
full competition, while the agents within each MAS collaborate.
Attacker’s Goal. Maximizing the reduction of 𝑉𝑖𝑐𝑡𝑖𝑚 ’s perfor-
mance on a specific MARL task.
Attacker’s Capabilities. The attacker possesses complete control
over the𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 and can update its MARL policy. Additionally,
the attacker has interaction privileges with the 𝑉𝑖𝑐𝑡𝑖𝑚 , obtaining
partial observations about the environment at each time step. The
attacker knows that there are 𝑁agents in the 𝑉𝑖𝑐𝑡𝑖𝑚 . Apart from
this,𝑉𝑖𝑐𝑡𝑖𝑚 is regarded as a black box by the attacker. Moreover,
the attacker is restricted from manipulating the environment.
We assume that 𝑉𝑖𝑐𝑡𝑖𝑚 ’s MARL policy is fixed, i.e., the parame-
ters during the deployment phase are frozen (subsequent evalua-
tions indicate that this assumption can be relaxed). This is common
for the deployment of RL on physical entities. For instance, a manu-
facturer releases an encirclement MAS consisting of multiple-legged
robots [ 13] or drones [ 31]. These physical entities fixedly deploy
policies to avoid unacceptable losses due to exploration, such as
2We provide a long version with appendices on arXiv: https://arxiv.org/pdf/2402.03741.
3""Two-team competitive environments"" is a subset of ""multi-agent competitive en-
vironments"". For clarity and readability, we use the first term in sections related to
problem formulation and scheme design (Section 3 and Section 4) and the second term
in all other sections.
647
CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA. Oubo Ma et al.
Figure 3: A two-team competitive environment can be sim-
plified from a ZS-POSG to a POSG if the joint policy of 𝑉𝑖𝑐𝑡𝑖𝑚
is fixed.
robot malfunctions or drone crashes. Even if the manufacturer of-
fers periodic policy upgrade services, the parameters of the MAS
remain fixed between two consecutive updates, and this interval
might span several months or even years. In such scenarios, the
manufacturer is a potential victim. Once the attacker generates an
adversarial policy, it implies that all users deploying the MAS from
that manufacturer are exposed to the threat.
3.2 Problem Formulation
Based on the threat model, we formulate the attack as a zero-sum
partially observable stochastic game (ZS-POSG) [37, 69].
Definition 2. A zero-sum partially observable stochastic game is
defined as
G=(S,{A𝑖},{B𝑗},{Ω𝑖},{O𝑖},P,{R𝑖
𝐴},{R𝑗
𝑉},𝛾), (1)
whereSdenotes the state space, A𝑖(resp.B𝑗) denotes the action space
for agent𝑖∈M (resp.𝑗∈N).A=Î
𝑖∈MA𝑖andB=Î
𝑗∈NB𝑗
denote the joint action spaces for the adversary and the victim. Each
agent𝑖∈M receives an observation 𝑜𝑖∈Ω𝑖, and the observation
functionO𝑖:S×A×B→ Δ(Ω𝑖)is a probability distribution
over possible subsequent observations given the previous state and
the actions of all agents. Each agent 𝑗∈N receives the global state
𝑠∈S.P:S×A×B→ Δ(S)represents the probability that taking
joint action𝑎∈A and𝑏∈B in state𝑠∈S results in a transition to
𝑠′∈S.R𝑖
𝐴:S×A×B→ R(resp.R𝑗
𝑉:S×A×B→ R) denotes
the reward function for agent 𝑖∈M (resp.𝑗∈N).𝛾∈[0,1)is the
discount factor.
Let𝜋𝐴∈P𝐴:Ω→Δ(A) and𝜋𝑉∈P𝑉:S→ Δ(B)be the
joint policies of 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 and𝑉𝑖𝑐𝑡𝑖𝑚 , respectively. P𝐴andP𝑉are
their corresponding joint policy spaces.
3.3 Problem Simplification
Inspired by [ 22], we define the following proposition (the proof is
in Appendix B of [42]).
Proposition 1. In a zero-sum partially observable stochastic game,
if the victim keeps a fixed joint policy 𝜋𝑉, the state transition of the
environment is solely dependent on the adversary’s joint policy 𝜋𝐴.
According to Proposition 1, the attacker can treat 𝑉𝑖𝑐𝑡𝑖𝑚 as part
of the environment (as shown in Figure 3) and simplify the attack
from a ZS-POSG to a POSG:G𝛼=(S,{A𝑖},{Ω𝑖},{O𝑖
𝛼},P𝛼,{R𝑖
𝛼},𝛾), (2)
retains the same state space, action space for 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 , obser-
vation space, and discount factor as the original G. However, the
observation function, transition function, and reward function for
𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 are reconstructed as
O𝑖
𝛼(𝑠,𝑎)=O𝑖(𝑠,𝑎,𝑏),
P𝛼(𝑠,𝑎)=P(𝑠,𝑎,𝑏),
R𝑖
𝛼(𝑠,𝑎)=R𝑖
𝐴(𝑠,𝑎,𝑏),(3)
where𝑎and𝑏are the joint actions of 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 and𝑉𝑖𝑐𝑡𝑖𝑚 , re-
spectively. Eventually, the attacker’s objective translates into find-
ing a policy 𝜋𝐴∈P𝐴that maximize the accumulated rewardsÍ𝑀
𝑖Í𝑇
𝑡=0𝛾𝑡R𝑖𝛼(𝑠𝑡,𝑎𝑡)of𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 , where𝑇is the time horizon.
4 Methodology
This section first introduces the framework of SUB-PLAY and then
describes the design details of each step. The intuition behind SUB-
PLAY is to adopt a divide-and-conquer strategy, decomposing a
complex POSG, as depicted in Equation 2, into multiple relatively
simpler POSGs. By tackling these simplified subgames individually,
it becomes possible to address the overall complexity of the original
POSG more efficiently.
4.1 Attack Framework
SUB-PLAY consists of four main steps: subgame construction, tran-
sition dissemination, subpolicy training, and policy combination
(see Figure 4).
In the preparation phase, the attacker constructs multiple sub-
games based on the potentially observed number of victim agents
and models each subgame as a POSG. Each subgame initializes its
own subpolicy and replay buffers. For specific details of subgame
construction, please refer to Section 4.2.
To mitigate the undertraining subpolicies caused by limited inter-
action transition, the attacker employs a uniform transition dissem-
ination mechanism for all agents in 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 . Then, each agent
predefines a transition dissemination table, which is utilized to
determine the probability of sharing each transition data among
the replay buffers. For specific details of transition dissemination,
please refer to Section 4.3.
In the training phase, 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 and𝑉𝑖𝑐𝑡𝑖𝑚 interact within the
environment. The generated transitions are stored in each replay
buffer based on the probabilities determined in the transition dis-
semination table. When a replay buffer accumulates a batch of
transitions, the MARL algorithm updates the corresponding sub-
policy. The reward of 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 is negatively correlated with the
performance of 𝑉𝑖𝑐𝑡𝑖𝑚 in the competition. Therefore, in accordance
with the MARL paradigm, each subpolicy tends to minimize the
performance of 𝑉𝑖𝑐𝑡𝑖𝑚 to achieve the attack objective. This reward-
oriented process does not require any additional knowledge or hu-
man intervention. For specific details of subpolicy training, please
refer to Section 4.4.
In the deployment phase, the attacker combines subpolicies to
form an adversarial policy. Since the attacker has complete control
over the adversary and stealthiness is not a concern, we implement
648
SUB-PLAY : Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA.
Figure 4: The framework of SUB-PLAY.
the policy combination in a hard-coded manner. When launch-
ing an attack, 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 determines which subgame the current
competition belongs to and then switches to the corresponding
subpolicy to make decisions. More details of policy combination
can be referred to Section 4.5.
4.2 Subgame Construction
For a partially observed MAS, the attacker constructs subgames
based on the observed agents. We define
Sub=N+1, (4)
where Subindicates the number of constructed subgames and 𝑁is
the number of agents belonging to 𝑉𝑖𝑐𝑡𝑖𝑚 . All subgames form a set
{G𝛼𝑘}𝑘∈K.K={0,1,...,Sub−1}and each subgame is fomulated
as a POSG:
G𝛼𝑘=(S,{A𝑖},{Ω𝑖
𝑘},{O𝑖
𝛼},P𝛼,{R𝑖
𝛼},𝛾), (5)
where the only difference in this equation from Equation 2 is the
termΩ𝑖
𝑘⊆Ω𝑖. For example, if 𝑉𝑖𝑐𝑡𝑖𝑚 consists of two agents, the
attacker constructs three subgames {G𝛼0,G𝛼1,G𝛼2}, corresponding
to the cases where the attacker observes 0, 1, and 2 agents from
𝑉𝑖𝑐𝑡𝑖𝑚 , respectively.
Remark 1. In real-world environments, the partial information
about specific agent components may be available to an attacker
due to limited perspective. We propose that a conservative strategy
can be adopted in high exploration cost environments, treating
these agents as unobservable, while a more aggressive strategy can
be employed in low exploration cost environments by treating them
as observable.
Remark 2. Subgame construction is scalable. In scenarios involv-
ing more agents, the attacker can choose a coarser granularity to
determine the scope and number of subgames. For instance, when
the victim has eight agents, attackers can construct three subgames
{G𝛼0−2,G𝛼3−5,G𝛼6−8}instead of nine. Furthermore, the attackercould construct subgames based on regions or apply our method to
scenarios where the victim is a single-agent system. In the latter
case, subgames could be constructed based on the observability of
different components of the single agent.
4.3 Transition Dissemination
The attacker records interactions with 𝑉𝑖𝑐𝑡𝑖𝑚 in the form of tran-
sitions. Each transition is represented as a tuple (𝑜𝑡,𝑎𝑡,𝑟𝑡,𝑜𝑡+1),
where𝑜𝑡is the observation at time step 𝑡,𝑎𝑡is the joint action of
𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 ,𝑟𝑡is the instantaneous reward, 𝑜𝑡+1is the observation
at the next time step, and 𝑡∈[0,𝑇−1], where𝑇is the time horizon.
Each agent in 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 maintains a set of replay buffers {E𝑖
𝑘}that
stores the transitions for each subgame, where 𝑖∈M denotes the
agent’s identifier and 𝑘∈K denotes the subgame’s identifier.
Empirical Study. We introduce a concept of Occupancy Rate (OR),
which quantifies the occurrence frequency of a subgame. The occu-
pancy rates are related to the number of transitions and serve as
the foundation for constructing the transition dissemination table.
We conduct an empirical study to explore the relationship between
occupancy rates and three partially observable limitations (please
refer to Section 5.1 for setup details). The results in Figure 5 reveal
two key observations.
Observation 1. (Heterogeneity Property) The occupancy rate of sub-
games exhibit variations, which is further affected by the limitations
of partial observability.
Using the first column of Figure 5(c) as an example, the occu-
pancy rates for four subgames G𝛼0,G𝛼1,G𝛼2andG𝛼3are 0.04,
0.08, 0.24, and 0.65, respectively. This highlights that some sub-
games, such asG𝛼2andG𝛼3, occur frequently, leading to higher
occupancy rates (0.24 and 0.65). These subgames provide a suffi-
cient number of transitions, contributing to a more comprehensive
dataset for learning and analysis. On the other hand, subgames
likeG𝛼0andG𝛼1occur infrequently, resulting in lower occupancy
649
CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA. Oubo Ma et al.
Figure 5: The occupancy rate of subgames under three different limitations. The environment is Predator-prey, with 𝑉𝑖𝑐𝑡𝑖𝑚
consisting of three agents. The vertical coordinate contains the occupancy rate of four subgames {G𝛼0,G𝛼1,G𝛼2,G𝛼3}. As time
progresses, the attacker’s policy will be updated.
rates (0.04 and 0.08). The infrequent occurrence of these subgames
leads to a scarcity of transitions, which may present challenges for
effective learning and decision-making within specific contexts.
Observation 2. (Dynamics Property) Under distance limitations,
the occupancy rate of subgames is influenced by variations in the
attacker’s policy.
The occupancy rates remain stable under uncertainty and region
limitations, but they exhibit significant shifts under distance lim-
itations. For example, OR0increases from 0.03 to 0.14, while OR3
decreases from 0.57 to 0.35. The distance limitation is associated
with an observable range represented by a circle centered around
each agent in 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 . Therefore, the observation by an agent is
determined by its behavior pattern or policy.
The heterogeneity property underscores the non-uniform na-
ture of occupancy rates across different subgames. We propose
three methods aim to address this non-uniformity and determine
occupancy rate values in such scenarios.
Static Estimation. Under uncertainty limitations, if we assume
that the uncertainty in the observation of any 𝑉𝑖𝑐𝑡𝑖𝑚 ’s agent is the
same,𝑂𝑅𝑘is considered as the probability of repeatedly observ-
ingNagents in𝑉𝑖𝑐𝑡𝑖𝑚 with exactly 𝑘successes. The probability
of successful observation is 𝜇, which can be introduced as priori
knowledge or obtained from historical observation of 𝑉𝑖𝑐𝑡𝑖𝑚 . Thus,
the occupancy rate obeys a binomial distribution with parameters
Nand𝜇,i.e.,
𝑂𝑅𝑘= 𝑁
𝑘·𝜇𝑘·(1−𝜇)𝑁−𝑘, (6)
where 𝑁
𝑘is the binomial coefficient, which represents the number
of ways to choose 𝑘successes from 𝑁trials.
Static Observation. Alternatively, the attacker calculates the oc-
cupancy rate for each subgame G𝛼𝑘by counting the number of
related transitions in all replay buffers.
𝑂𝑅𝑘=|E𝑘|
|E0|+|E 1|+...+|E𝑆𝑢𝑏−1|, (7)
where|E𝑘|=Í𝑀
𝑖=1|E𝑖
𝑘|and|E𝑖
𝑘|indicates the number of transi-
tions stored inE𝑖
𝑘. These transitions can either be direct interactions
with𝑉𝑖𝑐𝑡𝑖𝑚 or observations of 𝑉𝑖𝑐𝑡𝑖𝑚 ’s interactions with other
MASs. Static observation does not rely on additional assumptions
or prior knowledge and is applicable to all three types of limitations
discussed in this paper.Dynamic Observation. The dynamics property reveals that oc-
cupancy rates may exhibit significant fluctuations and migration
in a competitive environment. To account for this, the attacker
can utilize Exponentially Weighted Averages to accommodate the
dynamics of occupancy rates:
𝑂𝑅𝑘𝑡=𝛽·𝑂𝑅𝑘𝑡−1+(1−𝛽)·𝑂𝑅′
𝑘𝑡, (8)
where𝑂𝑅𝑘𝑡−1denotes the current weighted average of G𝛼𝑘’s occu-
pancy rate,𝛽denotes the weight of historical transition (e.g., 𝛽= 0.9
means that the occupancy rate is approximately to the average of 10
episodes), and 𝑂𝑅′
𝑘𝑡denotes the occupancy rate obtained from the
current episode statistics. Dynamic observation is applicable to all
the three limitations and does not require additional assumptions
or prior knowledge. Section 5.2 elucidates the strengths and weak-
nesses of these three methods through experiments and analysis,
guiding on their selection.
Transition Dissemination Table. The attacker determines the
Dissemination Rate (DR) between each pair of replay buffers based
on the occupancy rate. As shown in Figure 6, for each agent 𝑖∈M ,
we define𝐷𝑅ˆ𝑘→𝑘∈[0,1]as the probability of transition transmis-
sion from bufferE𝑖
ˆ𝑘to bufferE𝑖
𝑘.
DRs are determined by four factors: (1) 𝐷𝑅ˆ𝑘→𝑘is negatively
correlated with the occupancy rate of the destination buffer E𝑖
𝑘since
the buffer with sufficient transitions does not require additional
transitions. (2) 𝐷𝑅ˆ𝑘→𝑘is negatively correlated with the distance
betweenG𝛼ˆ𝑘andG𝛼𝑘since the transitions with higher similarity
are provided between buffers close to each other. (3) 𝐷𝑅ˆ𝑘→𝑘is
positively correlated with the number of constructed subgames
since the mean value of occupancy rates decreases as Subincreases.
(4)𝐷𝑅ˆ𝑘→𝑘is negatively correlated with the dispersion of occupancy
rates since high dispersion is prone to multiple occupancy rates
with lower values. In summary, we define 𝐷𝑅ˆ𝑘→𝑘as
𝐷𝑅ˆ𝑘→𝑘= 
clip((𝜆−𝑂𝑅𝑘+𝜎)|ˆ𝑘−𝑘|√
𝑆𝑢𝑏,0,1),if𝑂𝑅𝑘≤𝜆
𝜎|ˆ𝑘−𝑘|√
𝑆𝑢𝑏, if𝑂𝑅𝑘>𝜆(9)
where𝜆is an adjustment parameter positively correlated with the
complexity of the two-team competitive environment. 𝜎is the stan-
dard deviation of all occupancy rates and is used to measure their
dispersion.|ˆ𝑘−𝑘|indicates the distance between two subgames.
650
SUB-PLAY : Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA.
Figure 6: Each agent in 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 maintains a transition dis-
semination table.
√
𝑆𝑢𝑏indicates that as the number of constructed subgames in-
creases, the transition dissemination between buffers will become
more frequent, meaning that the value of 𝐷𝑅ˆ𝑘→𝑘will increase. We
use the clip function to limit the value of 𝐷𝑅ˆ𝑘→𝑘to the range [0,1].
AllDRs collectively form a transition dissemination table. When
a new transition is generated, the agent allocates it to replay buffers
based on the probabilities recorded in this table. If static estimation
or static observation is employed to determine occupancy rates,
this table is static. In contrast, if dynamic observation is used to
determine occupancy rates, this table changes dynamically. The
impact of transition dissemination is presented in Figure 20 of [ 42].
4.4 Subpolicy Training
Each agent 𝑖∈ M maintains a set{𝜋𝑖𝛼𝑘}𝑘∈Kconsisting of all
subpolicies. We perform Policy Meritocracy (PM) to preserve top-
performing subpolicies based on the harmonic mean of their test
performance across 𝐿metrics, mitigating performance fluctuations
caused by non-stationarity.
𝑃𝑀𝑖
𝑘=𝐿
𝐿Í
𝑙=11/𝜂𝑖
𝑘𝑙, (10)
where𝜂𝑖
𝑘𝑙indicates the test performance of the subpolicy 𝜋𝑖𝛼𝑘with
respect to metric 𝑙. In the policy pool, only one subpolicy is retained
for each subgame to minimize storage overhead. Replacements oc-
cur when a subpolicy with superior test performance emerges. The
detailed update process for the subpolicies is outlined in Algorithm
1 of [42].
4.5 Policy Combination
The attacker combines all subpolicies {𝜋𝑖𝛼𝑘}𝑖∈M,𝑘∈Kto generate
the final adversarial policy. Specifically, at each time step of the de-
ployment phase, 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 obtains a joint observation (𝑜1,𝑜2,...,𝑜𝑀)
and then outputs a joint action (𝑎1,𝑎2,...,𝑎𝑀). The subgame con-
struction ensures that each observation exclusively belongs to asingle subgame, without any overlap or intersection between the
subgames, i.e.,Ω𝑖
ˆ𝑘∩Ω𝑖
𝑘=∅, where ˆ𝑘≠𝑘(ˆ𝑘,𝑘∈K),𝑖∈M . Fur-
thermore, unlike backdoor attacks [ 17,57], the attacker can modify
the program structure without concerns about stealthiness. Hence,
we propose that the attacker can implement the policy combination
in a hard-coded manner.
Taking the distributed MARL as an example, each agent 𝑖∈M
determines which subgame a received observation 𝑜𝑖belongs to
and subsequently selects the corresponding subpolicy to guide its
action decision-making:
𝜋𝑖
𝛼= 
𝜋𝑖𝛼0, if𝑜𝑖∈Ω𝑖
0
...
𝜋𝑖𝛼𝑆𝑢𝑏−1,if𝑜𝑖∈Ω𝑖
𝑆𝑢𝑏−1(11)
This approach has two advantages: it provides a straightforward
logic and greater flexibility, allowing each subgame to employ a sep-
arate MARL algorithm. The implementation of policy combination
can be found in Algorithm 2 of [42].
5 Evaluation
This section first introduces the evaluation setup and then evaluates
SUB-PLAY through six perspectives: attack performance, ablation
study, transferability, scalability, overhead, and potential defenses.
5.1 Setup
Environment. We adopt two-dimensional environments, Predator-
prey and World Communication, within the Multi Particle Envi-
ronments (MPE) [ 40] framework developed by OpenAI. MPE is a
gym-based benchmark designed for cooperative, competitive, and
mixed MARL tasks.
•Predator-prey. There are Nslower predators controlled by
𝑉𝑖𝑐𝑡𝑖𝑚 andMpreys controlled by 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 . The predators
cooperate to collide with preys, while the preys cooperate
to avoid collisions. The environment is initialized randomly
at the beginning of each episode, including the positions of
agents and obstacles.
•World Communication. This environment shares similarities
with the Predator-prey setup but includes additional features.
(1) There are Mfoods that preys are rewarded for being close
to. (2) The environment randomly initializes a forest, making
all agents invisible within it initially. (3) A leader predator
exists, having full visibility of all agents and the ability to
communicate with other predators to enhance cooperation.
The state and action spaces of both environments are continu-
ous. The evaluation of the attack performance is conducted in five
different scenarios (M vN): 1v3, 2v3, 3v3, 2v2, and 4v2.
Partial Observability Implementation. Observation is a multi-
dimensional vector. We additionally introduce a Mask vector con-
sisting of 0s and 1s, which has the same dimensions as Observation.
The Mask is determined by specific rules for partial observability.
We multiply each element of Observation with the correspond-
ing element of the Mask, and the result is partial observation. For
specific implementation, please consult Algorithm 3 of [42].
In Predator-prey, we conduct evaluations under both uncertainty
and distance limitations. The uncertainty rate (i.e. 𝜇in Equation 6)
651
CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA. Oubo Ma et al.
Figure 7: The attack performance of SUB-PLAY under uncertainty limitations in Predator-prey.
ranges from{0.00,0.25,0.50,0.75,1.00}. For example, 𝜇=0indi-
cates complete observability, and the mask array consists of all 1s.
The observable distance ranges from {0.5,1.0,1.5,2.0}.
In World Communication, we conduct evaluations under region
limitations. This environment has implemented partial observabil-
ity, where agents located within a specific region are not visible (a
forest with a fixed size but randomly initialized positions).
Implementation Details. Our evaluations are conducted on four
servers with Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20GHz, 32GB
RAM. Python and PyTorch are used for code implementation.
Selected MARL algorithms include DDPG and MADDPG, serving
as representatives for DTDE and CTDE architectures, respectively.
DDPG and MADDPG are actor-critic algorithms commonly used
in reinforcement learning. These algorithms consist of an actor
network and a critic network. We utilizes a two-layer ReLU MLP
with 128 units in each layer to parameterize all policies. The actor
network’s output layer incorporates a Tanh activation function.
For weight initialization, we use Xavier normal with a gain of 1.0
for all layers in both victim and adversarial policy training. Biases
are initialized with zeros. The chosen optimizer is Adam, with a
learning rate of 0.001 and 𝜖set to 10−8.
To ensure smooth policy updates, we employ Exponential Moving
Average (EMA) with a decay rate of 0.95. Random noise sampled
from a normal distribution with a standard deviation of 0.01 is added
to the output actions to promote exploration. The discount factor 𝛾
for RL is set to 0.95. The adjustment parameter 𝜆in Equation 9 is
set to 0.5. All reported results are averaged over 1,000 test runs. For
additional implementation details, parameter settings, and training
results of𝑉𝑖𝑐𝑡𝑖𝑚 s, please refer to Appendix C of [42].
Comparison Methods. (1) Baseline: 𝑉𝑖𝑐𝑡𝑖𝑚 ’s normal performance
on a specific task. Specifically, 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 deploys a heuristic policy
where the agent’s movement is characterized by a fixed speed
and direction, with random updates after collisions. (2) Self-play
[1,3]: Similar to the process of victim training, this setup grants the
attacker complete access to the environment. The attacker randomlyinitializes both 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 and𝑉𝑖𝑐𝑡𝑖𝑚 , allowing them to compete
with each other and undergo updates. Ultimately, 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 is
retained to carry out the attack. (3) Victim-play [15,22,38,59,
60]: Apart from substituting the algorithm with MARL, retain the
other fundamental settings of this framework (refer to Section 2.2),
specifically fixing 𝑉𝑖𝑐𝑡𝑖𝑚 and updating 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦 .
Metrics. We adopt Catch Rate (CR) and Collision Frequency (CF) as
evaluation metrics.
𝐶𝑅=𝑁𝑢𝑚𝑐
𝑁𝑢𝑚𝑒, (12)
where Num cindicates the number of episodes where the 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑦
is caught; Num eindicates the total number of episodes.
𝐶𝐹=1
𝑁𝑢𝑚𝑒𝑁𝑢𝑚 𝑒∑︁
𝑒=1𝑁∑︁
𝑗=1𝑁𝑢𝑚𝑒𝑗, (13)
where Num ejdenotes the number of collisions between the jth
predator and the prey in a specific episode e.
The attacker aims to evade pursuit and minimize 𝑉𝑖𝑐𝑡𝑖𝑚 ’sCR
andCF. Policy meritocracy is based on the harmonic mean of these
two metrics,
𝑃𝑀=2·𝐶𝑅·𝐶𝐹
𝐶𝑅+𝐶𝐹. (14)
We calculate the average improvement in attack performance of
SUB-PLAY compared to Victim-play by
(𝑃𝑀𝐵−𝑃𝑀𝑆)−(𝑃𝑀𝐵−𝑃𝑀𝑉)
𝑃𝑀𝐵−𝑃𝑀𝑉, (15)
where𝑃𝑀𝐵,𝑃𝑀𝑉, and𝑃𝑀𝑆represent the victim’s average per-
formance when the attacker executes Baseline, Victim-play, and
SUB-PLAY, respectively.
5.2 Attack Performance
We evaluate the attack performance of SUB-PLAY across different
environments and limitations. The training process of SUB-PLAY
can be found in Figure 17 of [42].
652
SUB-PLAY : Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA.
Figure 8: The attack performance of SUB-PLAY under distance limitations in Predator-prey.
Figure 9: The attack performance of SUB-PLAY under region
limitations in World Communication.
Uncertainty Limitation. Under uncertainty limitations, SUB-
PLAY, on average, reduces the victim’s performance to 51.98% of
the baseline (see Figure 7). Moreover, SUB-PLAY outperforms the
other two methods and minimizes the victim’s catch and collision
rates in 94.0% (47/50) and 98.0% (49/50) scenarios. Compared to
Victim-play, SUB-PLAY demonstrates an average improvement of
32.22% in attack performance (refer to Equation 15).
The results also show that when the MARL algorithm is MAD-
DPG (resulting in increased input dimensionality due to informa-
tion sharing among agents), SUB-PLAY demonstrates more stable
attack performance, showcasing its potential to handle more com-
plex environments. In addition, an unexpected outcome is that the
attack remains effective even when the uncertainty is set to 1.00
(i.e., the attacker has no observations of the victim). According to
Silver et al. [52], maximizing rewards is sufficient to drive intelli-
gent behavior. In our scenarios, this suggests that the attacker can
find adversarial policies by focusing on maximizing the rewards
obtained, even without direct observation of the victim.
Distance Limitation. Under distance limitations, SUB-PLAY, on
average, reduces the victim’s performance to 55.71% of the baseline(see Figure 8). Moreover, SUB-PLAY outperforms the other two
methods and minimizes the victim’s catch and collision rates in
97.5% (39/40) scenarios. Compared to other methods, the impact of
the observation range on SUB-PLAY is relatively minor, indicating
that SUB-PLAY demonstrates better adaptability to dynamic envi-
ronments. Compared to Victim-play, SUB-PLAY demonstrates an
average improvement of 27.16% in attack performance.
Region Limitation. Under region limitations, SUB-PLAY reduces
the victim’s performance to an average of 59.07% of the base-
line (see Figure 9). Moreover, SUB-PLAY outperforms the other
two methods and minimizes the victim’s catch and collision rates
in 100.0% (10/10) scenarios. Compared to Victim-play, SUB-PLAY
demonstrates an average improvement of 50.22% in attack perfor-
mance. These results indicate that SUB-PLAY exhibits more signifi-
cant attack potential in complex environments.
Furthermore, we compare SUB-PLAY with two additional vari-
ants of Victim-play [22,59], further validating SUB-PLAY ’s attack
performance in partially observable environments. Details regard-
ing the settings of these two variants and the final results are pro-
vided in Appendix D of [42].
Visualization Results. The visualization results (Figure 18 of [ 42])
show that in Predator-prey, the preys tend to flee to the edge of the
map at the maximum speed from different directions and then stay
while trying to bypass predators and obstacles. They quickly get
rid of collisions if they occur.
Similarly, in World Communication, the preys do not hide in the
forest or approach foods for additional rewards. These observations
demonstrate that adversarial policies effectively utilize the speed
advantage of the preys to evade predators, mitigating the disadvan-
tages of partial observability. t-SNE (see Figure 10) shows that the
activations of the victim’s policy network are significantly different
when facing a normal opponent compared to an adversarial policy.
More results can be found in Figure 19 of [42].
653
CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA. Oubo Ma et al.
Table 1: The impact of three occupancy rate determination methods on attack performance under three partial observability
limitations. The performance is measured by two metrics (CR ↓/CF↓). Acronyms: Static Estimation (SE), Static Observation (SO),
Dynamic Observation (DO).
LimitationsDDPG MADDPG
SE SO DO SE SO DO
Uncertainty0.00 0.489 / 1.969 0.527 / 2.115 0.571 / 2.527 0.626 / 3.327 0.622 / 4.007 0.606 / 3.846
0.25 0.401 / 1.573 0.538 / 2.205 0.575 / 2.532 0.579 / 3.053 0.607 / 3.506 0.725 / 6.460
0.50 0.477 / 2.309 0.530 / 2.430 0.619 / 3.362 0.583 / 3.228 0.605 / 3.389 0.697 / 6.209
0.75 0.502 / 1.927 0.532 / 2.395 0.580 / 2.475 0.591 / 3.318 0.614 / 3.909 0.639 / 3.904
1.00 0.543 / 2.345 0.566 / 2.563 0.592 / 2.919 0.638 / 4.506 0.647 / 4.019 0.687 / 4.880
Distance0.5 - 0.598 / 2.554 0.574 / 2.240 - 0.609 / 3.335 0.563 / 3.075
1.0 - 0.657 / 3.059 0.571 / 2.529 - 0.647 / 4.148 0.614 / 3.500
1.5 - 0.609 / 3.025 0.564 / 2.421 - 0.667 / 4.430 0.606 / 3.324
2.0 - 0.620 / 2.786 0.561 / 2.579 - 0.654 / 4.464 0.589 / 3.264
Region 1 - 0.664 / 2.917 0.626 / 2.998 - 0.594 / 1.855 0.489 / 1.397
Figure 10: t-SNE activations of the victim when playing
against different opponents.
Proactive Masking. Intuitively, partial observability poses a chal-
lenge for adversarial policy generation. However, evaluations in-
dicate that proactive masking of environment observations may
enhance the attack performance in some scenarios. For example, in
Figure 7(a), the attack performance of SUB-PLAY is superior when
the uncertainty rate is 0.25 compared to the fully observable sce-
nario. The reason is that the minor partial observability corresponds
to simplifying the input to the attacker. This suggests that proac-
tive masking may facilitate early training of adversarial policies
in complex environments. Recent work has introduced curriculum
learning to address sparse rewards in adversarial policy generation
[59], but it requires the access to different versions of victims. In
contrast, proactive masking does not rely on a similar assumption.
Impact of Occupancy Rates. To compare the impact of the three
methods proposed in Section 4.3 for determining occupancy rates,
we evaluate them under three partial observability limitations
(static estimation, static observation, and dynamic observation).
In the static observation, the attacker pre-observes the victim for
1000 episodes, while in the dynamic observation, the parameter 𝛽
is set to 0.9.
Table 1 shows that SUB-PLAY achieves optimal attack perfor-
mance with static estimation under uncertainty limitations. This is
because static estimation provides more accurate occupancy rate es-
timations. Furthermore, SUB-PLAY performs superior attacks using
dynamic observation under distance and region limitations. The
Figure 11: The attack performance of subpolicies under un-
certainty limitations.
former is due to the change in occupancy rates as the adversarial
policy updates, while the latter is attributed to the additional dy-
namics introduced by the randomly initialized position of the forest
in World Communication.
Therefore, in all evaluations, we set that the attacker adopts static
evaluation under uncertainty limitations and dynamic observation
under distance and region limitations.
5.3 Ablation Study
Unless additional specifications exist, the subsequent evaluations
use the 2v3 scenario, and the MARL algorithm is set to MADDPG.
Component Evaluation. We perform an ablation study to assess
the contribution of each component (subgame construction, transi-
tion dissemination, and policy meritocracy in subpolicy training)
inSUB-PLAY. Table 2 demonstrates that when the subgame con-
struction is applied in isolation, the attack performance is inferior.
However, when transition dissemination is also performed, the
attack performance is significantly improved. This highlights the
crucial role of transition dissemination in enhancing performance
in partially observable environments. Policy meritocracy also con-
tributes to improved attack performance and is compatible with
subgame construction and transition dissemination.
Subgame Evaluation. We continue to explore the reasons behind
the performance improvements achieved by SUB-PLAY. Figure 11
presents the attack performance of subpolicies in their respective
654
SUB-PLAY : Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA.
Table 2: The ablation results of components in SUB-PLAY measured by two metrics (CR ↓/CF↓). Acronyms: Subgame Construction
(SC), Transition Dissemination (TD), Policy Meritocracy (PM).
MethodsLimitations
Uncertainty (0.25) Uncertainty (0.50) Distance (0.5) Distance (2.0) Region (1)
Self-play 0.920 / 14.280 0.916 / 13.998 0.936 / 14.349 0.935 / 14.187 0.704 / 4.486
Victim-play 0.782 / 7.823 0.727 / 7.215 0.728 / 6.163 0.670 / 4.891 0.718 / 3.763
SUB-PLAY (SC) 0.830 / 8.402 0.759 / 7.604 0.765 / 6.296 0.708 / 5.982 0.835 / 6.563
SUB-PLAY (SC+TD) 0.617 / 3.740 0.627 / 4.438 0.700 / 6.552 0.672 / 4.675 0.688 / 3.309
SUB-PLAY (SC+PM) 0.731 / 6.059 0.708 / 6.318 0.735 / 6.113 0.677 / 4.576 0.561 / 1.634
SUB-PLAY (SC+TD+PM) 0.579 / 3.053 0.583 / 3.228 0.563 / 3.075 0.589 / 3.264 0.489 / 1.397
Figure 12: The transferability of adversarial policies across
different scenarios. The vertical coordinate represents the
limitation set in the training phase and the horizontal coor-
dinate represents the limitation set in the testing phase.
corresponding subgames under uncertainty limitations. The re-
sults reveal that when the attacker only applies subgame construc-
tion and policy meritocracy, the resulting adversarial policies ex-
hibit substantial performance differences across different subgames.
However, with the inclusion of transition dissemination, all sub-
policies show notable improvements in their attack performance.
Furthermore, the performance disparities among the subpolicies
are significantly reduced.
5.4 Transferability
In real-world scenarios, the limitations on partial observability are
not static; for instance, environmental factors like weather can im-
pact sensor performance, leading to variations in the observable
distance for the adversary agents. Therefore, we evaluate the trans-
ferability of adversarial policies across different parameter settings
within the same limitation.
Figure 12(a)-(b) illustrate that adversarial policies can be trans-
ferred in specific scenarios (i.e., when the uncertainty rate is 0.25,0.50, and 0.75). However, transferability diminishes when the uncer-
tainty rate is 0.00 or 1.00. This is due to the significant differences
in the modeling between fully unobservable and fully observable
scenarios compared to partially observable scenarios. Additionally,
subgame construction and transition dissemination are ineffective
in these two cases. Figure 12(c)-(d) demonstrate that adversarial
policies exhibit transferability under distance limitations. In con-
clusion, adversarial policies derived from SUB-PLAY demonstrate
transferability in similar partially observable environments.
However, the transferability of adversarial policies encounters
limitations when applied to victims with substantial differences.
Table 3 illustrates that the adversarial policy proves ineffective in
reducing the catch rate of heuristic victims, although it succeeds
in diminishing their collision frequency. The reduction in collision
frequency is attributed to the adversarial policy’s capacity to swiftly
escape captures, whereas heuristic attackers frequently engage in
repeated collisions. The inability to decrease the victims’ catch
rate results from adversaries consistently moving at the maximum
speed in the adversarial policy, thereby increasing the probability of
collision – especially when preys are slow, potentially concluding
the episode before colliding with predators. Moreover, preys lose
the ability to evade nearby predators.
5.5 Scalability
SUB-PLAY is scalable, meaning the attacker can adjust the granular-
ity of subgame construction (Section 4.2) based on the scenario and
requirements. Equation 4 demonstrates the most potent attack by
constructing Sub = N+1 sub-games against Nvictim agents. How-
ever, in resource-constrained environments, the attacker can adopt
a coarser granularity, i.e., Sub<N+1.
We validate the scalability of SUB-PLAY in Predator-prey (al-
gorithm = MADDPG, distance = 0.5, 1.0, 1.5, 2.0, scenarios = 2v3),
constructing 1, 2, 3, and 4 subgames. Figure 13 illustrates that un-
der four different granularity settings, SUB-PLAY can decrease the
victim’s catch rate to 0.736, 0.641, 0.611, and 0.593, and the collision
frequency decreases to 6.315, 3.738, 3.504, and 3.291, respectively.
The results indicate that finer granularity leads to better attack
effectiveness, while the rate of performance improvement dimin-
ishes as the number of subgames increases, gradually approaching
saturation. Additionally, the training time of SUB-PLAY shows a pos-
itive correlation with the number of subgames (1171.97s, 1495.39s,
2031.03s, and 2556.17s, respectively), indicating that attackers can
manipulate the granularity of subgame construction to scale SUB-
PLAY to more complex environments.
655
CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA. Oubo Ma et al.
Table 3: The attack performance of adversarial policies with
heuristic victims. The performance is measured by two met-
rics (CR↓/CF↓).
Attaker Heuristic SUB-PLAY
Victim Heuristic
Uncertainty0.25 0.261 / 0.699 0.307 / 0.599
0.50 0.259 / 0.694 0.286 / 0.515
Distance0.5 0.225 / 0.613 0.275 / 0.508
2.0 0.265 / 0.692 0.313 / 0.600
Region 1 0.304 / 0.802 0.439 / 0.807
Victim MADDPG
Uncertainty0.25 0.900 / 12.643 0.579 / 3.053
0.50 0.918 / 13.258 0.583 / 3.228
Distance0.5 0.942 / 13.540 0.563 / 3.075
2.0 0.920 / 13.224 0.589 / 3.264
Region 1 0.837 / 9.954 0.489 / 1.397
5.6 Overhead
The main overhead of adversarial policy-based attacks arises from
three aspects: interaction costs, training costs, and decision delays.
SUB-PLAY avoids extra interaction costs (Section 4.3) and deci-
sion delays (Section 4.5) but incurs additional training costs due to
subgame construction (Section 4.2) and separate training for each
subgame (Section 4.4), which scales linearly with the number of
subgames.
We explore the training time of SUB-PLAY in Predator-prey
(algorithm = DDPG, MADDPG, distance = 0.5, scenarios = 1v3,
2v3, 3v3, 2v2, 4v2). The results in Table 4 indicate that under the
condition of maximizing the number of subgames (Sub = N+1), the
average training times of SUB-PLAY in distributed and centralized
MARL algorithms are 1529s and 2781s, respectively (1.73 and 2.02
times that of Victim-play ). Nevertheless, the training time of SUB-
PLAY remains significantly lower than that of the well-trained
victim (only 2.39%). Additionally, as demonstrated in Section 5.5,
the attacker can adjust the granularity of subgame construction to
reduce training costs.
5.7 Potential Defenses
Adversarial policies pose significant challenges to the real-world
deployment of MARL. In response, we explore viable defense ap-
proaches to mitigate these security threats.
Adversarial Retraining. An intuitive approach involves the vic-
tim adopting adversarial retraining during the training phase [ 23].
This allows the victim to identify and address vulnerabilities in its
policy continuously. However, as shown in Figure 14, this does not
render the adversarial policy ineffective. This is because there is
no theoretical evidence indicates that the victim’s MARL policy,
after adversarial retraining, can approach a Nash equilibrium more
closely. Thus, the exploitable space persists, albeit potentially shift-
ing with adversarial retraining. Moreover, as previously mentioned,
finding or approximating a Nash equilibrium in a multi-agent com-
petition is at least as tricky as PPAD-complete.
Policy Ensemble. While it is challenging to eliminate the threat
posed by adversarial policies through training alone, victims could
mitigate this threat by adjusting MARL’s deployment strategy. One
such strategy involves adopting a policy ensemble approach, where
the victim prepares a set of policies and consistently selects policies
Figure 13: Scalability evaluation.
Table 4: The difference in training time between adversarial
policies and victims (s).
MethodsScenariosAverage1v3 2v3 3v3 2v2 4v2
DDPG
Victim 53240 68101 80800 53314 70364 65164
Victim-play 625 766 982 738 1288 880
SUB-PLAY 929 1293 1732 1173 2519 1529
MADDPG
Victim 92048 106458 147257 98678 131235 115135
Victim-play 828 1172 1549 1115 2201 1373
SUB-PLAY 1577 2556 3682 2304 3785 2781
from this ensemble for deployment. Intuitively, this may prevent
the attacker from adapting to a specific policy.
The results on the left side of Table 5 indicate that if attack-
ers have access to all policies, the policy ensemble shows limited
defensive effect. For instance, the entry in the first row and first
column, -0.07, denotes a decrease of 0.07% in the effectiveness of
adversarial policies, which is almost negligible (note that -100%
corresponds to the failure of the adversarial policy). This implies
that the assumption of freezing the victim’s policy in Section 3.1
can be relaxed.
If attackers have access to only a subset of the victim’s policies
(33%), there is a certain degree of reduction in the effectiveness of
adversarial policies (the results on the right side of Table 5). The
insight derived from this is that the victim could periodically up-
date the ensemble pool to prevent the attacker from adapting to all
policies. The victim may also consider increasing the diversity of
the policies in the pool [ 12,28], making their weaknesses signif-
icantly different, which might lead to fluctuations in adversarial
policies, preventing them from converging. Policy ensemble can
be coupled with a dynamic switching mechanism to enhance the
defense approach further, wherein the switching time or policy
selection is dynamically changed [30, 39, 41].
Fine-tuning. Continual fine-tuning during deployment may also
prevent the attacker from adapting to a specific victim, offering
lower training costs than policy ensemble. However, the limited
defensive effectiveness of fine-tuning, as shown in Figure 15, sug-
gests that the distance between policies before and after fine-tuning
remains close.
6 Discussion
Emphasizing Deployment Details. We not only introduce SUB-
PLAY but also reveal that even with partial observations, adver-
sarial policies induce the failure of MARL. Since mitigating the
threat of adversarial policies through improvements in the training
656
SUB-PLAY : Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA.
Figure 14: Adversarial retraining results of 5 rounds.
framework is challenging, we propose that defenders prioritize the
deployment details of MARL rather than solely focusing on enhanc-
ing algorithm performance. Additionally, SUB-PLAY can serve as
a method to measure the lower bound of MARL performance in
adversarial scenarios.
Limitation and Future Work. SUB-PLAY still has some limita-
tions. (1) Due to the scarcity of environments that facilitate multi-
agent competitive settings with partial observability, our testing
is limited to two environments. However, we extensively evaluate
SUB-PLAY in various settings, including different types of partial
observability limitations, multiple scenarios with varying numbers
of agents, and two MARL architectures. (2) Although no additional
interaction is required, the training costs incurred by SUB-PLAY are
directly proportional to the number of subgames. Thus, a trade-off
exists between training costs and attack performance, which still
needs to be addressed. (3) The current method assumes that the
attacker engages in multiple interactions with the victim. We plan
to adopt offline RL techniques [7, 10] to relax this setup.
Real-world Scenarios. Agents’ restricted perception capabilities
and observation permissions give rise to numerous partially ob-
servable scenarios in real-world settings. For instance, the antic-
ipated applications of drone swarms and robots in MAS encom-
pass encirclement systems [ 24], security systems [ 27], strategic
maneuvers [ 11], and human-robot teams [ 8]. Nonetheless, their
environmental perception is confined by the deployed sensors. For
example, the 4D LiDAR L1 on Unitree Go2 has a scanning distance
of 30 meters [ 51], leading to partially observable phenomena when
the targets exceed this range or become obstructed.
Potential Damages. The potential damages of SUB-PLAY include
attaining targeted victories or illicit profits. For instance, these could
involve defeating specific opponents in RoboMaster [ 9], exploiting
strategic vulnerabilities in poker AI like Dou Dizhu [ 65] to gain
illegal profits online (given the prevalence of AI in online poker), or
bypassing security MAS to jeopardize property and personal safety.
To address the potential damages posed by SUB-PLAY, we explore
potential defense methods in Section 5.7 and provide directions
for future research: Compared to costly adversarial retraining and
limited defensive performance of fine-tuning, deploying MARL in
the form of policy ensembles and increasing the diversity of the
policy pool is a more practical and effective approach.
7 Related Work
7.1 RL Security
A substantial body of research is leveraging RL to achieve specific
security objectives [ 16,44,56,61,67]. Nevertheless, the securityTable 5: The defensive effectiveness of policy ensemble, with
values given in percentage (%).
Access 100% 33%
Scenarios 1v3 2v3 3v3 1v3 2v3 3v3
Uncertainty
0.00 -0.07 +0.02 -0.04 -2.74 +4.09 -0.89
0.25 +0.02 -0.25 +0.10 -9.86 -13.58 -12.45
0.50 +0.00 -0.02 +0.08 -9.68 -9.14 -17.01
0.75 -0.01 -0.07 +0.04 -15.55 -2.56 +2.85
1.00 +0.00 +0.04 +0.08 -25.78 -0.55 +9.68
Distance
0.5 -0.09 -0.15 -0.03 -16.17 -7.99 -11.98
1.0 -0.12 -0.12 -0.01 -30.15 -5.65 +0.25
1.5 -0.29 -0.12 -0.02 -20.24 -9.36 -32.69
2.0 -0.13 -0.28 +0.14 -16.01 -20.51 -43.39
Region
1 -0.08 -0.24 +0.00 -7.99 -37.44 -17.94
dimensions inherent to RL need to be more adequately addressed.
This section thoroughly examines security research concerning the
three fundamental components of RL.
Reward Manipulation. Unlike the modification of labels in deep
learning [ 55,64,68], RL introduces backdoor attacks via reward ma-
nipulation [ 32,43]. Zhang et al. [70] developed a dynamic reward-
poisoning attack targeting online RL applications, while Chen et
al.[6] extended the concept of backdoor attacks to cooperative
MASs. Wang et al. [57] introduced a unique RL-specific paradigm
for backdoor attacks, where the attacker trains a benign policy and
a trojan policy, merging them into a backdoor policy by behavior
cloning. Guo et al. [21] discovered a pseudo-trigger space that can
trigger RL backdoors. In response, they proposed PolicyCleanse to
perform model detection and backdoor mitigation.
State Manipulation. Inspired by adversarial examples [ 5,36,66],
attackers in RL can disrupt the victim by perturbing the envi-
ronment state. Huang et al. [29] applied FGSM [ 18] to DRL and
launched an adversarial attack on the DQN policy in Atari games [ 45].
Behzadan et al. [2] introduced a policy induction attack, where the
attacker determines the victim’s actions based on a pre-trained tar-
get policy and perturbs the states by FGSM and JSMA [ 47]. Sun et
al.[53] proposed a white-box attack called the critical point attack,
which strategically explores state-action combinations to identify
points with high payoff and inject subtle perturbations during the
victim’s deployment phase.
Action Manipulation. The agent’s action determines the agent-
environment boundary, allowing attackers to launch attacks by
manipulating it. Lee et al. [34] proposed two victim manipulation
attacks: the myopic action-space attack injects action perturbations
based on current observations, while the look-ahead action-space
attack considers future steps to maximize the attack’s impact. How-
ever, directly manipulating the victim’s actions is impractical.
In contrast, adversarial policies only need to control the at-
tacker’s action. Gleave et al. [15] pioneered Victim-play, which
involves manipulating the adversary’s actions to induce subopti-
mal decisions from a fixed RL model during deployment. Wu et
al.[60] incorporated explainable AI techniques into adversarial
policy generation, enhancing the stealthiness by launching attacks
only when the victim pays attention to them. Guo et al. [22] ex-
tended Victim-play from zero-sum to general-sum environments,
657
CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA. Oubo Ma et al.
Figure 15: The attack performance varies with the fine-tuning
of the victim’s policy.
revealing its potential in assessing the fairness of competitions or
systems. Wang et al. [59] explored adversarial policies in discrete
action scenarios and achieved success against superhuman-level
Go AIs, demonstrating that near-Nash or𝜖-equilibrium policies are
exploitable. Guo et al. [23] attempted to mitigate the potential threat
of adversarial policies from a training perspective and introduced
a provable defense called PATROL. The aim is to bring the victim
closer to a Nash equilibrium in a two-player competition. Liu et
al.[38] explored adversarial policy attacks in scenarios where at-
tackers only have partial control over the adversary and proposed
adversarial training with two timescales to mitigate the threats
posed by adversarial policies.
7.2 Partial Observability in RL
Partial observability limits the attacker’s access to complete envi-
ronmental information. To address this, existing research suggests
two potential methods for the attacker.
Inference. Inference entails using available observations and prior
knowledge to complete unobserved content, including environ-
mental [ 62,63] and agent inference [ 20,48]. (1) Environmental
Inference: Partial state information is used to infer the global en-
vironment in the spatial dimension. Yang et al. [63] proposed a
supervised learning-based hallucinator for inferring the environ-
ment from current observations, effective in static environments
but potentially less suitable for highly dynamic competitions. (2)
Agent Inference: Historical interaction is used to infer the unob-
servable agents in the temporal dimension. Papoudakis et al. [48]
proposed constructing policies for all agents through representation
learning. During deployment, the policies and local observations
of the controlled agent are utilized to infer the invisible agents.
However, this method relies on the victim’s policy knowledge and
is unsuitable for black-box or competitive environments.
Generalization. Enhancing the generalization capability of agents
can effectively adapt to the environment’s diversity, dynamics, and
unpredictability [ 33]. Intuitively, this also applies to policy improve-
ments in partially observable scenarios. Ghosh et al. [14] demon-
strated that partitioning a partially observable task into multiple
subtasks can effectively improve the performance of RL policies.
SUB-PLAY draws on the insight of generalization-based approaches
to address the partially observable problem in multi-agent competi-
tive environments, as it applies to dynamic environments and does
not rely on additional victim information.8 Conclusion
This paper proposes SUB-PLAY , a novel black-box attack framework
in partially observable multi-agent competitive environments. The
effectiveness of SUB-PLAY in enhancing the attack performance
of adversarial policies is showcased through divide-and-conquer
strategies and transition dissemination, as evidenced by extensive
evaluations conducted across various partially observable limita-
tions and MARL algorithms. Moreover, we examine three potential
defense strategies to mitigate the risks associated with SUB-PLAY.
The evaluation results indicate that policy ensemble is more effec-
tive than adversarial retraining and fine-tuning. Future investiga-
tions can concentrate on enhancing the diversity of policy pools
and implementing mechanisms for dynamic policy switching.
Acknowledgment
We sincerely appreciate the insightful comments from our shepherd
and the anonymous reviewers. We would like to extend our grati-
tude to Xuhong Zhang, Chenghui Shi, Yi Jiang, Yuyou Gan, Guang
Yang and Jiawen Wan for their valuable feedback. This work was
partially supported by the National Key Research and Development
Program of China under grant number 2022YFB3102100.
References
[1]Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch.
2018. Emergent Complexity via Multi-Agent Competition. In ICLR.
[2]Vahid Behzadan and Arslan Munir. 2017. Vulnerability of Deep Reinforcement
Learning to Policy Induction Attacks. In International Conference on Machine
Learning and Data Mining in Pattern Recognition.
[3]Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław
Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris
Hesse, et al .2019. Dota 2 with Large Scale Deep Reinforcement Learning. arXiv
(2019).
[4]Noam Brown. 2020. Equilibrium Finding for Large Adversarial Imperfect-
Information Games. PhD thesis (2020).
[5]Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness of
Neural Networks. In S&P.
[6]Yanjiao Chen, Zhicong Zheng, and Xueluan Gong. 2022. MARNet: Backdoor
Attacks Against Cooperative Multi-Agent Reinforcement Learning. IEEE Trans-
actions on Dependable and Secure Computing (2022).
[7]Yang Dai, Oubo Ma, Longfei Zhang, Xingxing Liang, Shengchao Hu, Mengzhu
Wang, Shouling Ji, Jincai Huang, and Li Shen. 2024. Is Mamba Compatible with
Trajectory Optimization in Offline Reinforcement Learning? arXiv (2024).
[8]Ewart J De Visser, Marieke MM Peeters, Malte F Jung, Spencer Kohn, Tyler H
Shaw, Richard Pak, and Mark A Neerincx. 2020. Towards a Theory of Longitudinal
Trust Calibration in Human-Robot Teams. International Journal of Social Robotics
(2020).
[9] DJI. [n. d.]. Robomaster. https://www.robomaster.com/en-US.
[10] Linkang Du, Chen Min, Sun Mingyang, Ji Shouling, Cheng Peng, Chen Jiming,
and Zhang Zhikun. 2024. ORL-AUDITOR: Dataset Auditing in Offline Deep
Reinforcement Learning. In NDSS.
[11] Rolando Fernandez, Derrik E Asher, Anjon Basak, Piyush K Sharma, Erin G
Zaroukian, Christopher D Hsu, Michael R Dorothy, Christopher M Kroninger,
Luke Frerichs, John Rogers, et al .2021. Multi-Agent Coordination for Strategic
Maneuver with a Survey of Reinforcement Learning. Technical Report. US Army
Combat Capabilities Development Command, Army Research Laboratory.
[12] Wei Fu, Weihua Du, Jingwei Li, Sunli Chen, Jingzhao Zhang, and Yi Wu. 2023.
Iteratively Learn Diverse Strategies with State Distance Information. In NeurIPS.
[13] Zipeng Fu, Xuxin Cheng, and Deepak Pathak. 2023. Deep Whole-Body Control:
Learning a Unified Policy for Manipulation and Locomotion. In Conference on
Robot Learning.
[14] Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey
Levine. 2021. Why Generalization in RL is Difficult: Epistemic Pomdps and
Implicit Partial Observability. In NeurIPS.
[15] Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart
Russell. 2020. Adversarial Policies: Attacking Deep Reinforcement Learning. In
ICLR.
658
SUB-PLAY : Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems CCS ’24, October 14–18, 2024, Salt Lake City, UT, USA.
[16] Vasudev Gohil, Hao Guo, Satwik Patnaik, and Jeyavijayan Rajendran. 2022. AT-
TRITION: Attacking Static Hardware Trojan Detection Techniques Using Rein-
forcement Learning. In CCS.
[17] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild,
Dawn Song, Aleksander Mądry, Bo Li, and Tom Goldstein. 2022. Dataset Security
for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses. IEEE
Transactions on Pattern Analysis and Machine Intelligence (2022).
[18] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. In ICLR.
[19] Sven Gronauer and Klaus Diepold. 2022. Multi-Agent Deep Reinforcement
Learning: A Survey. Artificial Intelligence Review (2022).
[20] Pengjie Gu, Mengchen Zhao, Jianye Hao, and Bo An. 2022. Online Ad Hoc
Teamwork under Partial Observability. In ICLR.
[21] Junfeng Guo, Ang Li, Lixu Wang, and Cong Liu. 2023. PolicyCleanse: Backdoor
Detection and Mitigation for Competitive Reinforcement Learning. In ICCV.
[22] Wenbo Guo, Xian Wu, Sui Huang, and Xinyu Xing. 2021. Adversarial Policy
Learning in Two-Player Competitive Games. In ICML.
[23] Wenbo Guo, Xian Wu, Lun Wang, Xinyu Xing, and Dawn Song. 2023. PATROL:
Provable Defense against Adversarial Policy in Two-player Games. In USENIX
Security.
[24] Ahmed T Hafez, Anthony J Marasco, Sidney N Givigi, Mohamad Iskandarani,
Shahram Yousefi, and Camille Alain Rabbath. 2015. Solving Multi-UAV Dynamic
Encirclement via Model Predictive Control. IEEE Transactions on Control Systems
Technology (2015).
[25] Ping He, Yifan Xia, Xuhong Zhang, and Shouling Ji. 2023. Efficient Query-Based
Attack against ML-Based Android Malware Detection under Zero Knowledge
Setting. In CCS.
[26] Johannes Heinrich, Marc Lanctot, and David Silver. 2015. Fictitious Self-Play in
Extensive-Form Games. In ICML.
[27] Péter Miksa Hell and Péter János Varga. 2019. Drone Systems for Factory Security
and Surveillance. Interdisciplinary Description of Complex Systems: INDECS (2019).
[28] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup,
and David Meger. 2018. Deep Reinforcement Learning that Matters. In AAAI.
[29] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel.
2017. Adversarial Attacks on Neural Network Policies. arXiv (2017).
[30] Yi Jiang, Chenghui Shi, Oubo Ma, Youliang Tian, and Shouling Ji. 2023. Text
Laundering: Mitigating Malicious Features Through Knowledge Distillation of
Large Foundation Models. In Inscrypt.
[31] Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Müller, Vladlen
Koltun, and Davide Scaramuzza. 2023. Champion-Level Drone Racing Using
Deep Reinforcement Learning. Nature (2023).
[32] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. 2020. Troj-
DRL: Evaluation of Backdoor Attacks on Deep Reinforcement Learning. In 57th
ACM/IEEE Design Automation Conference (DAC).
[33] Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. 2023. A
Survey of Zero-Shot Generalisation in Deep Reinforcement Learning. Journal of
Artificial Intelligence Research (2023).
[34] Xian Yeow Lee, Sambit Ghadai, Kai Liang Tan, Chinmay Hegde, and Soumik
Sarkar. 2020. Spatiotemporally Constrained Action Space Attacks on Deep
Reinforcement Learning Agents. In AAAI.
[35] Joel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel.
2017. Multi-Agent Reinforcement Learning in Sequential Social Dilemmas. In
AAMAS.
[36] Zhuohang Li, Yi Wu, Jian Liu, Yingying Chen, and Bo Yuan. 2020. AdvPulse:
Universal, Synchronization-free, and Targeted Audio Adversarial Attacks via
Subsecond Perturbations. In CCS.
[37] Michael L Littman. 1994. Markov Games as a Framework for Multi-Agent Rein-
forcement Learning. In Machine learning proceedings 1994.
[38] Xiangyu Liu, Souradip Chakraborty, Yanchao Sun, and Furong Huang. 2024.
Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable
Defense in RL. In ICLR.
[39] Xuejiao Liu, Oubo Ma, Wei Chen, Yingjie Xia, and Yuxuan Zhou. 2022. HDRS: A
Hybrid Reputation System with Dynamic Update Interval for Detecting Malicious
Vehicles in VANETs. IEEE Transactions on Intelligent Transportation Systems
(2022).
[40] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor
Mordatch. 2017. Multi-Agent Actor-Critic for Mixed Cooperative-Competitive
Environments. In NIPS.
[41] Oubo Ma, Xuejiao Liu, and Yingjie Xia. 2023. ABM-V: An Adaptive Backoff
Mechanism for Mitigating Broadcast Storm in VANETs. IEEE Transactions on
Vehicular Technology (2023).
[42] Oubo Ma, Yuwen Pu, Linkang Du, Yang Dai, Ruo Wang, Xiaolei Liu, Yingcai Wu,
and Shouling Ji. 2024. SUB-PLAY: Adversarial Policies against Partially Observed
Multi-Agent Reinforcement Learning Systems. arXiv (2024).
[43] Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu. 2019. Policy Poisoning in
Batch Reinforcement Learning and Control. In NeurIPS.
[44] Suman Maiti, Anjana Balabhaskara, Sunandan Adhikary, Ipsita Koley, and
Soumyajit Dey. 2023. Targeted Attack Synthesis for Smart Grid VulnerabilityAnalysis. In CCS.
[45] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Ve-
ness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland,
Georg Ostrovski, et al .2015. Human-Level Control through Deep Reinforcement
Learning. Nature (2015).
[46] Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. 2020. Deep Rein-
forcement Learning for Multiagent Systems: A Review of Challenges, Solutions,
and Applications. IEEE Transactions on Cybernetics (2020).
[47] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik,
and Ananthram Swami. 2016. The Limitations of Deep Learning in Adversarial
Settings. In EuroS&P.
[48] Georgios Papoudakis, Filippos Christianos, and Stefano Albrecht. 2021. Agent
Modelling under Partial Observability for Deep Reinforcement Learning. In
NeurIPS.
[49] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Far-
quhar, Jakob Foerster, and Shimon Whiteson. 2020. Monotonic Value Function
Factorisation for Deep Multi-Agent Reinforcement Learning. The Journal of
Machine Learning Research (2020).
[50] Kui Ren, Qian Wang, Cong Wang, Zhan Qin, and Xiaodong Lin. 2019. The
Security of Autonomous Driving: Threats, Defenses, and Future Directions. Proc.
IEEE (2019).
[51] Unitree Robotics. [n. d.]. New Creature of Embodied AI Unitree Go2. https:
//www.unitree.com/go2.
[52] David Silver, Satinder Singh, Doina Precup, and Richard S Sutton. 2021. Reward
is Enough. Artificial Intelligence (2021).
[53] Jianwen Sun, Tianwei Zhang, Xiaofei Xie, Lei Ma, Yan Zheng, Kangjie Chen,
and Yang Liu. 2020. Stealthy and Efficient Adversarial Attacks against Deep
Reinforcement Learning. In AAAI.
[54] Richard S Sutton and Andrew G Barto. 2018. Reinforcement Learning: An Intro-
duction. MIT press.
[55] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
Zheng, and Ben Y Zhao. 2019. Neural Cleanse: Identifying and Mitigating Back-
door Attacks in Neural Networks. In S&P.
[56] Jinghan Wang, Chengyu Song, and Heng Yin. 2021. Reinforcement Learning-
based Hierarchical Seed Scheduling for Greybox Fuzzing. In NDSS.
[57] Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song.
2021. BACKDOORL: Backdoor Attack against Competitive Reinforcement Learn-
ing. In IJCAI.
[58] Shiyong Wang, Jiafu Wan, Daqiang Zhang, Di Li, and Chunhua Zhang. 2016.
Towards Smart Factory for Industry 4.0: A Self-Organized Multi-Agent System
with Big Data Based Feedback and Coordination. Computer Networks (2016).
[59] Tony Tong Wang, Adam Gleave, Tom Tseng, Kellin Pelrine, Nora Belrose, Joseph
Miller, Michael D Dennis, Yawen Duan, Viktor Pogrebniak, Sergey Levine, et al .
2023. Adversarial Policies Beat Superhuman Go AIs. In ICML.
[60] Xian Wu, Wenbo Guo, Hua Wei, and Xinyu Xing. 2021. Adversarial Policy
Training against Deep Reinforcement Learning. In USENIX Security.
[61] Yingjie Xia, Xuejiao Liu, Jing Ou, and Oubo Ma. 2023. RLID-V: Reinforcement
Learning-Based Information Dissemination Policy Generation in VANETs. IEEE
Transactions on Intelligent Transportation Systems (2023).
[62] Zhiwei Xu, Yunpeng Bai, Dapeng Li, Bin Zhang, and Guoliang Fan. 2022. SIDE:
State Inference for Partially Observable Cooperative Multi-Agent Reinforcement
Learning. In AAMAS.
[63] Yichen Yang, Jeevana Priya Inala, Osbert Bastani, Yewen Pu, Armando Solar-
Lezama, and Martin Rinard. 2021. Program Synthesis Guided Reinforcement
Learning for Partially Observed Environments. In NeurIPS.
[64] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. 2019. Latent Backdoor
Attacks on Deep Neural Networks. In CCS.
[65] Yang You, Liangwei Li, Baisong Guo, Weiming Wang, and Cewu Lu. 2020. Com-
binatorial Q-Learning for Dou Di Zhu. In AAAI.
[66] Honggang Yu, Kaichen Yang, Teng Zhang, Yun-Yun Tsai, Tsung-Yi Ho, and Yier
Jin. 2020. CloudLeak: Large-Scale Deep Learning Models Stealing Through
Adversarial Examples. In NDSS.
[67] Jiahao Yu, Wenbo Guo, Qi Qin, Gang Wang, Ting Wang, and Xinyu Xing. 2023.
AIRS: Explanation for Deep Reinforcement Learning based Security Applications.
InUSENIX Security.
[68] Heng Zhang, Jun Gu, Zhikun Zhang, Linkang Du, Yongmin Zhang, Yan Ren, Jian
Zhang, and Hongran Li. 2023. Backdoor Attacks against Deep Reinforcement
Learning based Traffic Signal Control Systems. Peer-to-Peer Networking and
Applications (2023).
[69] Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Başar. 2021.
Finite-Sample Analysis for Decentralized Batch Multiagent Reinforcement Learn-
ing with Networked Agents. IEEE Trans. Automat. Control (2021).
[70] Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. 2020. Adaptive
Reward-Poisoning Attacks against Reinforcement Learning. In ICML.
[71] Xin Zhou, Xiangyong Wen, Zhepei Wang, Yuman Gao, Haojia Li, Qianhao Wang,
Tiankai Yang, Haojian Lu, Yanjun Cao, Chao Xu, et al .2022. Swarm of Micro
Flying Robots in the Wild. Science Robotics (2022).
659"

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Latifa\\Documents\\Um6p\\Survey\\Survey\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Latifa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import PyPDF2\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract full text from a PDF file using PyPDF2.\n",
    "    Returns a single string with all the text.\n",
    "    \"\"\"\n",
    "    text_chunks = []\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text_chunks.append(page_text)\n",
    "    return \"\\n\".join(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\0019.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\1-s2.0-S0167404822003972-main.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\1-s2.0-S016740482400172X-main.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\1-s2.0-S0306261922009850-main.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\1-s2.0-S0957417421003377-main.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\1-s2.0-S0957417422015044-main.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\17348-Article Text-20842-1-2-20210518.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\2108.03803v2.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\2205.09362v2.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\2212.02705v5.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\2302.03322v3.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\2305.12872v3.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\2307.16212v1.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\2364_marnet_backdoor_attacks_agains.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\26240-Article Text-30303-1-2-20230626.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\27191-Article Text-31260-1-2-20230701.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\29708-Article Text-33762-1-2-20240324.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\3658644.3670293.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\3708320.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\4327-Article Text-7375-1-10-20190706.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\43_model_and_method_training_time.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\978-3-031-21203-1.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\blumenkamp21a.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\entropy-23-01433-v2.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\Guo_Towards_Comprehensive_Testing_on_the_Robustness_of_Cooperative_Multi-Agent_Reinforcement_CVPRW_2022_paper.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\NeurIPS-2023-efficient-adversarial-attacks-on-online-multi-agent-reinforcement-learning-Paper-Conference.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\NeurIPS-2023-robust-multi-agent-reinforcement-learning-via-adversarial-regularization-theoretical-foundation-and-stable-algorithms-Paper-Conference.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\On_the_Robustness_of_Cooperative_Multi-Agent_Reinforcement_Learning.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\s11063-024-11625-w.pdf\n",
      "Reading: C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\\s11432-023-3853-y.pdf\n"
     ]
    }
   ],
   "source": [
    "# 1. Set the folder containing your PDFs\n",
    "pdf_folder = \"C:/Users/Latifa/Documents/Um6p/Survey/Survey/sci-scraper/PDFs\"\n",
    "    \n",
    "# 2. Gather all PDF file paths\n",
    "pdf_paths = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "    \n",
    "# 3. Prepare a list to store data for the DataFrame\n",
    "data = []\n",
    "    \n",
    "for pdf_path in pdf_paths:\n",
    "    print(f\"Reading: {pdf_path}\")\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "    # Create a record (dictionary) for each PDF\n",
    "    data.append({\n",
    "        \"filename\": os.path.basename(pdf_path),\n",
    "        \"full_text\": pdf_text\n",
    "    })\n",
    "    \n",
    "# 4. Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (30, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0019.pdf</td>\n",
       "      <td>Decentralized Anomaly Detection in Cooperative...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-s2.0-S0167404822003972-main.pdf</td>\n",
       "      <td>Computers  &amp; Security 124 (2023) 103005 \\nCont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-s2.0-S016740482400172X-main.pdf</td>\n",
       "      <td>Computers &amp; Security 142 (2024) 103871\\nAvaila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-s2.0-S0306261922009850-main.pdf</td>\n",
       "      <td>Applied Energy 324 (2022) 119688\\nAvailable on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-s2.0-S0957417421003377-main.pdf</td>\n",
       "      <td>Expert Systems With Applications 176 (2021) 11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename  \\\n",
       "0                           0019.pdf   \n",
       "1  1-s2.0-S0167404822003972-main.pdf   \n",
       "2  1-s2.0-S016740482400172X-main.pdf   \n",
       "3  1-s2.0-S0306261922009850-main.pdf   \n",
       "4  1-s2.0-S0957417421003377-main.pdf   \n",
       "\n",
       "                                           full_text  \n",
       "0  Decentralized Anomaly Detection in Cooperative...  \n",
       "1  Computers  & Security 124 (2023) 103005 \\nCont...  \n",
       "2  Computers & Security 142 (2024) 103871\\nAvaila...  \n",
       "3  Applied Energy 324 (2022) 119688\\nAvailable on...  \n",
       "4  Expert Systems With Applications 176 (2021) 11...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Computers  & Security 124 (2023) 103005 \\nContents  lists available  at ScienceDirect  \\nComputers  & Security  \\njournal  homepage:  www.elsevier.com/locate/cose  \\nOne4All:  Manipulate  one agent  to poison  the cooperative  multi-agent  \\nreinforcement  learning  \\nHaibin  Zheng  a , b , Xiaohao  Li b , Jinyin  Chen a , b , Jianfeng  Dong  c , d , Yan Zhang  e , \\nChangting  Lin e , f , ∗\\na Institute of Cyberspace  Security, Zhejiang University  of Technology,  Hangzhou  310023, China \\nb College of Information  Engineering,  Zhejiang University  of Technology,  Hangzhou  310023, China \\nc College of Computer  and Information  Engineering,  Zhejiang Gongshang  University,  Hangzhou  310018, China \\nd State Key Laboratory  of Information  Security, Institute of Information  Engineering,  Chinese Academy  of Sciences, Beijing 10 0 093, China \\ne Binjiang Institute of Zhejiang University,  Hangzhou  310053, China \\nf Zhejiang University,  Hangzhou  310 0 014, China \\na r t i c l e i n f o \\nArticle history: \\nReceived  2 May 2022 \\nRevised 9 October 2022 \\nAccepted  3 November  2022 \\nAvailable  online 9 November  2022 \\nKeywords:  \\nReinforcement  learning \\nMulti-agent  collaboration  learning \\nPoisoning  attack \\nBackdoor  \\nDefense a b s t r a c t \\nReinforcement  Learning  (RL) has achieved  a plenty of breakthroughs  in the past decade. Notably,  existing  \\nstudies have shown that RL is suffered  from poisoning  attack, which results in failure or even catastrophe  \\nin decision  processes.  However,  these studies almost focus on single-agent  RL setting, the cooperative  \\nMulti-Agent  Reinforcement  Learning  (c-MARL)  setting is less explored,  which is a generalization  of the \\nsingle-agent  RL setting and has achieved  great success in many areas. As a sub-ﬁeld  of RL setting, c- \\nMARL also faces some security  issues, e.g., poisoning  attack. \\nIn this paper, we introduce  two novel poisoning  attack techniques,  i.e., S tate N oise P oisoning  A ttack (SNPA) \\nand T arget A ction P oisoning  A ttack (TAPA), to attack the c-MARL  setting stealthily  and eﬃciently  , which \\nachieves  the goal that poisoning  the c-MARL  setting while only manipulate  one agent. The ﬁrst attack \\ntechnique,  termed SNPA, a black-box  attack method,  modiﬁes  the state observation  data of one agent, \\nwhich results in the c-MARL  setting performance  degradation.  The second attack technique,  termed TAPA, \\na white-box  attack method,  injects a backdoor  and triggers  the target action by manipulating  the ac- \\ntion and reward function  of one agent. The extensive  experiments  are conducted  in two popular  c-MARL  \\ngames, i.e., MCSPT and MCHT. The experiment  results show that the presented  novel poisoning  attacks, \\nSNPA and TAPA, are effective  on c-MARL  scenarios.  Speciﬁcally,  the total reward is reduced  by 1/3 and \\nthe winning  rate of team drops down to 0 under the two proposed  attacks. Furthermore,  the TAPA exper- \\niments verify that the victim agent executes  the target action once the backdoor  is triggered.  It is worth \\nnoting that the trigger rate of target action rises up to 99 . 31% and 99 . 01% in two c-MARL  games. \\n©2 0 2 2 Elsevier  Ltd. All rights reserved.  \\n1. Introduction  \\nAs a branch  of reinforcement  learning  (RL) ( Gibert et al., 2022; \\nKhalilpourazari  and Hashemi  Doulabi,  2021; Maeda and Mimura,  \\n2021 ), multi-agent  reinforcement  learning  (MARL)  ( Blas et al., \\n2021; Liu and Wu, 2021; Perrusquía  et al., 2021 ) focuses  on study- \\ning the behavior  of multiple  agents that coexist  in a shared en- \\nvironment.  However,  each agent in MARL setting  is motivated  by \\nits own rewards  while cannot observe  the other agents’  rewards.  \\n∗Corresponding  author at: Zhejiang  University,  Hangzhou  310 0 014, China. \\nE-mail address: linchangting@gmail.com  (C. Lin) . It brings a partial observation  issue, which implies  that each agent \\nmakes decision  to advance  its own target while these targets  are \\ndifferent.  Towards  this issue, the cooperative-MARL  (c-MARL)  ( Cui \\nand Zhang, 2021; Iranfar et al., 2021; Maxim  and Caruntu,  2021; \\nPanait and Luke, 2005 ) is designed,  one category  of MARL algo- \\nrithms,  multiple  agents cooperate  to maximize  the total team re- \\nward and ﬁnally make one decision.  Nowdays,  c-MARL  is increas-  \\ningly deployed  in critical  infrastructure,  e.g., robotic  ( Foerster  et al., \\n2016; Gu et al., 2017 ), AI games ( Lanctot  et al., 2017; Leibo et al., \\n2017; Lowe et al., 2017 ), autopilot  ( Shalev-Shwartz  et al., 2016 ), In- \\nternet advertising  ( Jin et al., 2018 ) and resource  utilization  ( Perolat  \\net al., 2017; Xi et al., 2018 ), etc. \\nRefer to deep neural network  (DNN),  it is well known  that \\nDNN suffers from several  security  issues, e.g., poisoning  attack \\nhttps://doi.org/10.1016/j.cose.2022.103005  \\n0167-4048/© 2022 Elsevier Ltd. All rights reserved.  \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nFig. 1. An Illustrative  Example.  In a c-MARL scenario for autopilot,  a malicious  agent generates  the poisoned  model and injects a backdoor  in training phase; once the \\npoisoned  model being triggered  in testing phase, it results in the overall collaboration  of multiple agents failure. The αand ris the action and reward of each agent in \\ninteracting  with the environment  and state transition.  \\n( Jinyin et al., 2019 ). Unsurprisingly,  RL is also vulnerable  to \\npoisoning  attack ( Ashcraft  and Karra, 2021; Foley et al., 2022; \\nGuo et al., 2022; Li et al., 2019; Ma et al., 2019; Rakhsha  et al., \\n2020 ). In particular,  a poisoning  attack aimed at a RL setting,  \\nthe adversary  collects  data as the inputs with malicious  labels \\nto perform  re-training  resulting  the target model is artiﬁcially  \\npoisoned.  For example,  Ashcraft  and Karra (2021) used a in- \\ndistribution  trigger to attack the single-agent  RL setting,  whereas  \\nRakhsha  et al. (2020) attacked  the single-agent  RL setting  by per- \\nturbing  the environment  reward.  \\nIt is worth noting that almost existing  studies  only consider  the \\nsingle-agent  RL setting  ( Foley et al., 2022; Guo et al., 2022; Li et al., \\n2019; Ma et al., 2019 ), whereas  lack of the studies  on poisoning  at- \\ntack aimed at c-MARL  setting.  As a sub-ﬁeld  of RL, c-MARL  consists  \\nof multiple  agents,  perhaps  unsurprisingly,  it also faces the threat \\nof poisoning  attack. \\nIn a nutshell,  an illustrative  example  about of a poisoning  at- \\ntack towards  to c-MARL  setting  is demonstrated  in Fig. 1 . In Fig. 1 , \\nan autopilot  scenario  is constructed  consists  of several  unmanned  \\nagents (vehicles),  which will collect the necessary  information  by \\ninteracting  with the surrounding  environment  and other agents.  In \\ntraining  process  , the malicious  agent only have the knowledge  of \\nthe global state and its own model. By manipulating  the collab-  \\noration  policy of the malicious  agent, a possible  poisoning  attack \\nis conducted  to the multi-agent  system.  The malicious  agent gen- \\nerates the poisoned  model and then injects a backdoor.  In testing \\nprocess  , once the backdoor  of the poisoned  model is triggered,  the \\nmalicious  agent just chooses  target action such as ‘STAY’, which \\nconfuses  the other agents,  result in the overall collaboration  of \\nmultiple  agents failure.  \\nConsidering  the c-MARL  setting,  its unique  features  are different  \\nfrom the single-agent  one. In particular,  a c-MARL  setting  consists  \\nof more than one agent. Hence, the attack challenges  are differ- \\nent from the commonly  studied  poisoning  attacks  in the single- agent setting,  which seeks to understand  if poisoning  a victim \\nagent can degrade  the performance  of the c-MARL  setting.  It re- \\nceives some extra challenges.  Firstly, how to perform  a poisoning  \\nattack stealthily.  It is necessary  to ensure as few victims  as possi- \\nble, e.g., only one agent is the victim agent. Secondly,  attack pol- \\nicy is diﬃcult  to design since the environment  space of c-MARL  is \\nusually  image and location  information.  Secondly,  the feature  of lo- \\ncation information  can only be observed  and modiﬁed  by the ma- \\nlicious agent while the global state information  is hard to be at- \\ntacked.  \\nExisting  work ( Xie et al., 2021 ) attempted  to design poisoning  \\nattacks  against  c-MARL  setting.  In particular,  the authors  conduct  \\nthe poisoning  attacks  on the agent’s  state signal and reward  sig- \\nnal from 3 different  aspects.  However,  it ( Xie et al., 2021 ) utilizes  \\nmultiple  agents and manipulates  more data whereas  it results in a \\npoor concealment.  \\nIn this paper, we propose  two novel techniques  to attack c- \\nMARL setting  eﬃciently  and stealthily,  i.e., T arget A ction P oison \\nA ttack (TAPA) and S tate N oise P oisoning  A ttack (SNPA).  The pro- \\nposed two attack techniques  achieve  the goal that poison the c- \\nMARL setting  while only manipulate  one agent. Speciﬁcally,  The \\nﬁrst attack technique,  SNPA, a black-box  attack method,  modiﬁes  \\nthe state observation  data of one agent, which results in the c- \\nMARL setting  performance  degradation.  The second  attack tech- \\nnique, TAPA, a white-box  attack method,  injects a backdoor  and \\ntriggers  the target action by manipulating  the action and reward  \\nfunction  of one agent. \\nIn summary,  we ﬁrst propose  two poisoning  attacks  on the \\nc-MARL,  i.e., SNPA and TAPA. Extensive  experiments  are imple- \\nmented  in two different  c-MARL  scenarios.  Compared  to the nor- \\nmal model, the reward  and the win rate have decreased  by about \\n300% and the  trigger rate of backdoor  is100% . Furthermore,  the \\nproposed  two poisoning  attacks  maintain  a good performance  un- \\nder the defense.  \\n2 \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nThe main contributions  of this work can be summarized  as fol- \\nlows: \\n• Different  from the existing  works that aimed at the attack of \\nsingle-agent  RL, this paper exploits  the multi-agent  scenario  \\nvulnerability,  i.e., poisoning  attack. \\n• Two novel attacks  for c-MARL  setting,  TAPA and SNPA, are pro- \\nposed in this paper, which are black-box  and white-box  attack \\nmethods,  respectively.  Speciﬁcally,  TAPA manipulates  the action \\nand reward  of the victim agent to reduce the overall perfor-  \\nmance,  while SNPA disturbs  the environment  information  by \\nadding  the perturbations  to interfere  the overall cooperation  \\npolicy. \\n• The experiment  results demonstrate  that TAPA and SNPA are \\nall feasible  and effective  in popular  c-MARL  games,  i.e., MCSPT  \\nand MCHT. TAPA and SNPA not only reduce the team reward  to \\n1 / 3 by attacking a  single agent,  but also  can trigger  the target  \\nactions  stealthily  and achieve  a good attack performance  under \\ndefense.  To demonstrate  the effectiveness  of poisoned  strategy  \\nwell, t-SNE is leveraged  to interpret  the processing  of decision  \\nby visualizing  the states and actions  of victim agent. \\nThe rest of the paper is organized  as follows.  The problem  \\nstatement  and the thread model are detailed  in Section  2 . The \\nmethodology  are detailed  in Section  3 . Experiments  and analysis  \\nare shown in Section  4 . Related  works are introduced  in Section  5 . \\nFinally,  we conclude  our work and discuss  limitations.  \\n2. Problem  statement  and threat model \\n2.1. Problem  statement  \\nIn general,  a c-MARL  setting  consists  of multiple  agents which \\ncooperates  with each other to achieve  its common  goal. It is \\nusually  modeled  as a MDP with multiple  agents,  where agents \\ncooperate  to perform  tasks, and the ﬁnal goal is to maximize  \\nthe total reward  of all agents.  Formally,  a MDP with multiple  \\nagents (agent 1 , agent 2 , ... , agent N ) is typically composed  of a tu- \\nple (S, (A 1 , A 2 , . . . , A N ) , P, (R 1 , R 2 , . . . , R N )) , where Sis the state \\nspace set, A i indicates  the action space set of the i th agent. P \\nmeans S ×(A 1 , A 2 , . . . , A N ) → Sthat the state probability  trans- \\nfer function  from s ∈ Sto s /prime ∈ S. (R 1 , R 2 , . . . , R N ) implies  that S ×\\n(A 1 , A 2 , . . . , A N ) ×S → R is the agents’  reward  where R 1 = R 2 = \\n. . . = R N . Note that a common  reward  function  for all agents is used \\nto keep the interests  of all agents consistently.  Besides,  each agent \\nneeds to interact  with the environment  and other agents to ﬁnd \\nthe optimal  policy that maximizes  the total reward  R of the team. \\nTherefore,  each agent model in a c-MARL  setting  can be repre- \\nsented as a four-tuple  form /angbracketleft S, A, P, R /angbracketright , where Srepresents  the state \\nspace set, A represents  the action space set, P represents  the state \\ntransition  matrix,  and R is represented  as a reward  function.  The \\nagent needs to continuously  interact  with the environment  dur- \\ning the self-learning  training  process.  In the current  state s t , the \\nagent takes corresponding  actions  a t , according  to the learned  pol- \\nicy π(a | s ) . Then the environment  will feedback  to the agent with \\na corresponding  scalar reward  r t , according  to the reward  function  \\nR , which is used to evaluate  the current  action taken by the agent. \\nThe agent’s  state will transform  from the current  state to the next \\nstate according  to the state transition  matrix P (s t+1 | s t , a t ) . The goal \\nis to ﬁnd the optimal  policy π∗through  continuous  exploration  \\nand learning  to maximize  the long-term  cumulative  reward  G t : \\nG t = ∞ /summationdisplay \\nt=0 γt R (s t , αt , s t+1 ) , (1) \\nwhere γ∈ [0 , 1] represents  the discount  factor of decreasing  re- \\nward, which is used to calculate  the long-term  reward.  In this paper, each agent in the team is trained  by a RL al- \\ngorithm,  such as deep Q -learning  Network  (DQN) ( Roderick,  Mac- \\nGlashan,  Tellex ) and deep neural network  (DNN),  which is approx-  \\nimated  as a state action-value  function  to maximize  the long-term  \\nexpected  reward.  DQN combines  the decision-making  ability of RL \\nand the perception  ability of deep learning,  which solves the prob- \\nlem of state feature  extraction  and realizes  an end-to-end  frame- \\nwork from perception  input to decision  output.  In addition,  the \\nvalue function-based  DQN adopts the method  of time sequence  \\ndifference  to update  the state value function  Qto approximate  the \\ntrue value Q ∗. Formally,  the optimal  policy π∗is obtained  as: \\nQ π(s, α) = E π[ G t | S t = s, A t = α] , \\nπ∗= argmax  \\nαQ ∗(s, α) . (2) \\nFor the poisoning  attack for c-MARL,  it can be divided  into two \\nmethods,  one is polluting  the position  information  of global agents,  \\ni.e., adding  the perturbations  to the position  information  observed  \\nby the victim agent. In the training  process,  we poison the state \\ninformation  with a probability.  The poisoning  process  can be rep- \\nresented  as follows.  \\n˜ S ← S + η, \\n/angbracketleft A, S, P, R /angbracketright ← /angbracketleft A, ˜ S , P, R /angbracketright . (3) \\nAnother  poisoning  attack method  is manipulating  the action and \\nreward  of the target agent, i.e., tempering  the action and enhancing  \\nthe reward,  making  the victim agent learn a wrong strategy.  And \\na trigger threshold  set by the attacker  to take the benign  strategy  \\noutside  the threshold.  The poisoning  process  can be described  as: \\na ← a † , \\nr ← r + step() (4) \\nwhere a † is the target action;  a is normal  training  action;  ris nor- \\nmal training  reward;  step() is the reward  enhancement  size; Sis \\nthe state space. \\n2.2. Adversary  model \\nIn this paper, we employ  a threat model and assume  that the \\nadversary  is one of the participants  in the c-MARL  setting.  Notably,  \\nthe adversary  can only modify  its action, reward,  and state infor- \\nmation  to affect the whole c-MARL  decision  making.  It must be \\nnoted that the adversary  is a participant  in c-MARL  setting  where \\nthe adversary  cannot modify  other agents’  information.  Moreover,  \\nthe adversary  launches  an attack on the two popular  c-MARL  \\ngames,  i.e., MCSPT  (Multi-agent  collaborative  search for positioning  \\ntargets)  ( Böhmer  et al., 2020 ) and MCHT (Multi-agent  cooperative  \\nhunt of the target) ( Zemzem  and Tagina,  2018 ), where the multiple  \\nagents cooperate  to search the target. \\nIn SNPA scenario,  we assume  that adversary  can only access to \\nand modify  its state information,  which likes sensor noise during \\nthe collection  process.  And, each agent can’t obtain the state infor- \\nmation  of other participants.  The attacker’s  target is to add pertur-  \\nbations  to its state information  to disrupt  the overall collaboration  \\nperformance.  \\nIn TAPA scenario,  we assume  that the adversary  is a model \\ndeveloper  who can manipulate  the training  policy of the victim \\nmodel to inﬂuence  its interaction  with other agents,  thereby  in- \\njecting a backdoor  into the model. Different  from SNPA, the vic- \\ntim agent in TAPA only modiﬁes  its action, instead  of affecting  the \\nglobal state by modifying  your state. The attacker’s  goal is to mod- \\nify the training  policy of the victim model to disrupt  the overall \\ncollaboration  performance.  \\n3 \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nFig. 2. Framework  of proposed  poisoning  attack methods.  The attacker implements  SNPA by adding the state perturbations  to the information  observed  by the victim agent. \\nAnd, the attacker implements  TAPA by manipulating  the action when the position of the victim agent is within the range of the trigger threshold.  \\n3. Poisoning  attacks  against  c-MARL  \\nIn this section,  we proposed  the two poisoning  attacks  against  \\nthe c-MARL  setting,  named  SNPA and TAPA, are shown in Fig. 2 . \\nThe triggering  process  means that the collaboration  strategy  is af- \\nfected and causes the win rate to fall sharply.  \\n3.1. SNPA (state noise poisoning  attack) \\nIn this subsection,  we propose  SNPA (State Noise Poisoning  At- \\ntack), a black-box  attack method,  can add perturbations  to the \\nstate data and affect the policy of the victim agent. To perform  an \\nSNPA successfully,  we assume  that the adversary  is able to mod- \\nify the state observation  data. In a nutshell,  the adversary  can add \\nsome designed  perturbations  into state data resulting  in the policy \\nlearning  of the victim agent being interfered  with. For example,  in \\na sensor-based  c-MARL  setting,  the added perturbations  can be re- \\ngarded  as the noise interference  between  several  sensors,  which is \\na common  phenomenon.  \\nSNPA uses Algorithm  1 to attack the c-MARL  setting,  which \\nworks as follows.  The input of SNPA algorithm  is state space S v ic \\nobserved  by victim agent, poisoning  rate p, perturbation  sizes /epsilon1\\nand training  round numbers  M. The output is perturbed  state space \\n˜ S v ic . For the training  round training  process,  we ﬁrstly generate  a \\ngradient  perturbation  which equals to the state space size (line 2). \\nThen, the states s t among  the state space S v ic are chosen  accord-  \\ning to the proposed  poisoning  rate p(line 3). The perturbed  state \\nspace ˜ s t /prime is computed  by Eq. (3) (line 4). To limit the size of pertur-  \\nbation,  the perturbed  state ˜ s t /prime which means the distance  with pre- \\ndicted state within perturbation  sizes by Eq. (5) (line 5–10). There- \\nfore, the poisoned  state can be denoted  as follows.  \\nmin ηR ( ˜ S v ic ) , \\nη= λ∂loss \\n∂x λ∈ ( 0 , 1 ) , \\n˜ S v ic ← S v ic + η, \\ns.t. /vextenddouble/vextenddouble˜ S v ic −S v ic /vextenddouble/vextenddouble< /epsilon1. (5) Algorithm  1: State noise poisoning  attack (SNPA).  \\nInput : state space observed  by victim S v ic ; poisoning  rate p; \\nperturbation  sizes /epsilon1; training  round numbers  M. \\nOutput  : Perturbed  state space ˜ S t . \\n1 while 1 ≤t < M do \\n2 Generate  a perturbation  ηby gradient  optimization;  \\n3 Choose  state s t from input state space S v ic by the \\nproportion  of p; \\n4 Compute  the perturbed  state space ˜ s t /prime by Eq.~5; \\n5 if Determine  whether  the perturbation  size is limited to \\n/epsilon1: || ˜ s t /prime −s t || 2 \\n2 ≤/epsilon1then \\n6 Update  the state after adding  the perturbation  to the \\nvictim agent state: ˜ S t ← ˜ s t /prime ; \\n7 end \\n8 else \\n9 Maintaining  the victim’s  original  state of agent: ˜ S t ← s t ; \\nbreak; \\n10 end \\n11 Training  rounds  increase:  t = t + 1 ; \\n12 end \\n13 return : the perturbed  state space ˜ S t . \\nwhere S v ic is the state space observed  by the victim agent, R ( ˜ S v ic ) \\nrepresents  the reward  of poisoned  state ˜ S v ic . We use the norm −\\nl 2 to perform  the perturbation  sizes, and /epsilon1is the perturbations  \\nthreshold.  At last, when training  round numbers  reach M, the SNPA \\nalgorithm  obtains  an optimal  perturbed  state space ˜ S v ic . \\n3.2. TAPA (target action poisoning  attack) \\nDifferent  from SNPA, TAPA is a white-box  attack method  which \\ncan deceive  the training  of the agent instead  of modifying  the state \\nobservation  data. Speciﬁcally,  it manipulates  the action and reward  \\nfunction  of the victim agent by observing  the position  data be- \\ntween the victim and the target. Considering  TAPA is a white-box  \\nattack method,  the adversary  is a role has the authority  to manipu-  \\n4 \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nlate the model policy, such as a malicious  model developer.  Specif-  \\nically, once the victim agent adopts the poisoning  policy preset by \\nthe adversary  such as the trigger threshold  . Generally,  a higher re- \\nward reinforces  the poisoning  effect. The victim agent thus will \\nachieve  a higher target action trigger rate during the testing pro- \\ncess. \\nIn order to inject a backdoor  into the c-MARL  to affect the over- \\nall collaboration,  we modify  the observations  of the other agents \\nby manipulating  the victim agent’s  actions  αand reward  rto \\ntrigger strategic  vulnerabilities  in collaboration.  In the boxed C- \\nMARL scenario,  the norm −l1 is widely used to calculate  the re- \\nward ( Kasseroller  et al., 2021 ). To make the reward  function  more \\nrelevant  and better understand  with the action of the agent, we \\nalso choose  norm −l1 as the formula  of reward.  The reward  func- \\ntion can be represented  as follows:  \\nr = −N /summationdisplay \\ni =0 /bardbl position  i −position  t /bardbl 1 , (6) \\nwhere Ndenotes  the number  of agents,  position  i represents  the lo- \\ncation information  of the i -th agent, position  t indicates  the location  \\ninformation  of the target. The distance  between  a speciﬁc  agent \\nand the target is calculated  as norm −l 1 . Notably,  once an agent \\nis closer to the target, the reward  rbecomes  higher.  In addition,  \\nreward  ris a negative  value of the accumulated  distance.  \\nThe TAPA (target action poisoning  attack)  algorithm  is given in \\nAlgorithm  2 . In TAPA, the attacker  sets the trigger threshold  by ob- \\nAlgorithm  2: Target action poisoning  attack (TAPA).  \\nInput : distance  threshold  hyperparameter  δ; training  round \\nnumbers  M. \\nOutput  : victim agent action αtarget ; victim agent reward  r v im . \\n1 Initialize  system  state S, action set A , reward  set R ; \\n2 Obtain the position  of victim agent p v ic and target p tar ; \\n3 Set target action of victim agent action a v im ; \\n4 while 1 ≤t < M do \\n5 Calculate  the distance  of victim agent with target dis t ; \\n6 if Determine  whether  the victim agent is within the target \\narea: dis t ≤δthen \\n7 Set the victim agent action as the target \\naction:  αv im ← −α† ; \\n8 Set the victim agent reward  to 0 (highest):  r v im ← −0 ; \\n9 end \\n10 else \\n11 Set the victim agent action as normal  training  \\naction:  αv im ← −πnormal (s ) ; break; \\n12 end \\n13 Training  rounds  increase:  t = t + 1 ; \\n14 end \\n15 return : victim agent action αv im ; victim agent reward  r v im . \\ntaining  the position  information  between  the agent and the tar- \\nget in the observation  data. Once the distance  between  the victim \\nagent and the target agent is not beyond  the threshold,  it will au- \\ntomatically  trigger the target action in this state. At the same time, \\nthe victim agent will take the target action ‘STAY’, i.e., stay still. In \\nother states, the agent cooperates  with the other agents to obtain \\nthe team’s maximum  reward.  Therefore,  the attack policy of the \\nvictim agent can be represented  as follows:  \\nπv im (s ) = /braceleftbiggαT , if dis ≤δ\\nπnormal (s ) , otherwise  , (7) \\nwhere dis = /bardbl position  v −position  t /bardbl 1 is the distance  between  the \\nposition  of the victim agent position  v and the position of  the target  position  t , calculated  by the norm −l 1 , δis distance  threshold  hy- \\nperparameter.  πnormal is the policy of normal  training,  πv im is the \\npolicy of victim agent. \\nDetails  of TAPA are shown in Algorithm  2 which works as fol- \\nlows. The input of this algorithm  is distance  threshold  hyperpa-  \\nrameter  δ, training  round numbers  M. The output is victim agent \\naction a target and victim agent reward  r v im . In c-MARL  training  pro- \\ncess, system  state S, action set A , reward  set R are initialized,  re- \\nspectively  (line 1). The position  of victim agent p v ic and target p tar \\nare obtained  (line 2). Then, the target action of victim agent ac- \\ntion a v im is set (line 3). In iteration  process,  the distance  dis t be- \\ntween the victim agent and target is calculated  (line 5). Noting that \\ndis t ≤δ, the victim agent performs  the target action αT . In con- \\ntrast, the agents perform  training  process.  \\nIn addition,  the victim agent gives a high reward  to reinforce  its \\nbackdoor  trigger effect (line 6–11). Considering  that ris negative,  \\nthe maximum  reward  is less than 0. Besides,  when each agent co- \\noperates  to win in a round, the total reward  is also 0. Therefore,  \\nthe attacker  can give a high reward  by modifying  the total reward  \\nof the team. For example,  the distance  between  the victim and the \\ntarget is set to 0 within the threshold  range, which means that \\nthe victim agent has found the target agent. We also can give a \\nhigh reward  by modifying  the total reward  obtained  from the feed- \\nback, namely  reward  enhancement,  i.e., directly  set the reward  of \\nthe victim agent plus 1 within the range of threshold.  \\nIn this scenario,  each agent does not have a clear target loca- \\ntion, they can ﬁnd the ﬁnal corresponding  target location  according  \\nto its collaboration  policy. Therefore,  during the training,  directly  \\nmodifying  the reward  obtained  by the victim agent can be used to \\nstrengthen  the poisoning  effect. However,  in the MCHT scenario,  \\nthe target agent is unique,  and the distance  information  between  \\nthe victim agent and its location  can be obtained  through  state ob- \\nservation.  Hence, the attack can be carried  out by modifying  the \\ntotal reward.  Note that it is not easy to be defended  by triggering  \\ndetection  of state data, so it has triggered  concealment  to a certain  \\nextent.  \\nIn the testing process,  when the distance  between  the victim \\nagent and the target is within the threshold,  the backdoor  trigger \\ncan be automatically  achieved.  Because  the premise  of the win- \\nning task is that the agent needs to be close to the target, there \\nwill inevitably  appear  to be triggered.  When the backdoor  is trig- \\ngered, the victim agent implements  the target action through  pol- \\nicy to achieve  a poisoning  attack, which greatly  reduces  the overall \\nperformance  of the team. This attack method  is automatically  trig- \\ngered by observing  the location  between  agents.  Note that it is not \\neasy to be defended  by triggering  the detection  of state data. So it \\nhas triggered  concealment  to a certain  extent.  \\n3.3. Theoretical  guarantees  \\nIn this section,  we formally  characterize  the algorithm  time com- \\nplexity and algorithm  space complexity  of SNPA and TAPA. \\nAlgorithm  time complexity  analysis  In this part, we proceed  with \\nthe time complexity  analysis  of SNPA and TAPA. \\nTo analyse  time complexity  of SNPA, we ﬁrstly choose  states. \\nThen, add perutrbations  and then replace  benign  states. Thus, the \\ntime complexity  of SNPA can be denoted  as follows,  \\nT SNPA ∼O (t ×M) + O (a ×M) ∼O (t ×M) , (8) \\nwhere tdenotes  the numbers  of state, a is the number  of states be \\nchosen,  Mis the number  of round. \\nSimilarly,  to analyse  time complexity  of TAPA, the distance  of \\nagents should be computed.  Then, the target action is set. Thus, \\nthe time complexity  of TAPA can be denoted  as follows,  \\nT TAPA ∼O (t /prime ×M) + O (m ) ∼O (t /prime ×M) , (9) \\n5 \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nwhere t /prime denotes  the number  of agents,  m denotes  the number  of \\nactions,  Mis the number  of round. \\nAlgorithm  space complexity  analysis  In this part, we analyse  the \\nspace complexity  of SNPA and TAPA. \\nFor SNPA method,  the parameters  of SNPA include  c-MARL  \\nmodel parameters  and poisoning  perturbation  size, Therefore,  the \\nspace complexity  is \\nO (K ×S 2 ×A 2 ×R 2 ) + O ( N ) ∼O (K ×S 2 ×A 2 ×R 2 ) , (10) \\nwhere Kis the number  of agents,  S 2 is the size of state space, A 2 \\nis the size of action set, R 2 is the size of reward  set, N is the size \\nof poisoning  perturbation.  \\nFor TAPA method,  the parameters  of TAPA such as state space \\ninclude  c-MARL  model parameters  and attack policy parameters.  \\nTherefore,  the space complexity  is: \\nO (K ×S 1 ×A 1 ×R 1 ) + O (S 1 ×A /prime ) ∼O (K ×S 1 ×A 1 ×R 1 ) , (11) \\nwhere Kis the number  of agents,  S 1 is the size of state space, A 1 \\nis the size of action set, R 1 is the size of reward  set, A /prime \\n1 is the size  \\nof target action set. \\n4. Experiments  \\nThis section  deals with the questions  of quantifying  the feasi- \\nbility and effectiveness  of the proposed  poisoning  attack methods,  \\nSNPA and TAPA. \\n4.1. Experimental  settings  and implementations  \\nExperimental  environment  To answer  the proposed  RQs, we build \\nan experiment  environment  consisting  of Intel XEON 6240 2.6 GHz \\n×18C (CPU), Tesla V100 32GiB (GPU), 16 GB memory  (DDR4-REcc  \\n26 6 6), Ubuntu  16.04 (OS), tensorﬂow,  numpy  and pygame.  Fur- \\nthermore,  SNPA and TAPA are evaluated  in different  types of c- \\nMARL based hunting  games,  i.e., MCSPT  ( Böhmer  et al., 2020 ) and \\nMCHT ( Zemzem  and Tagina,  2018 ). In particular,  the details of MC- \\nSPT and MCHT are represented  as follows.  \\nMCHT Different  from MCHT scenario,  MCSPT  performs  several  \\nagents to hunt several  preys. Moreover,  if two adjacent  agents ex- \\necute the catch action, a prey is caught and both the prey and the \\ncatching  agents are removed  from the map. And beyond  that, MC- \\nSPT equips the same rules as MCHT. In our experiment  setting,  MC- \\nSPT shows eight agents hunting  eight prey in a 10 ×10 grid. \\nMCSPT For MCSPT  scenario,  a hunting  game, it shows four \\nagents succeeds  to hunt a moving  prey in a 10 ×10 grids. In ad- \\ndition, each agent always shares its own position  to each other. \\nMCHT assumes  that, at each time step, the prey has an equal prob- \\nability to move in one of the four compass  directions,  remain  still, \\nor try to catch any adjacent  prey. Impossible  actions,  i.e., moves \\ninto an occupied  target position  or catching  when there is no ad- \\njacent prey, are unavailable.  The prey moves by randomly  select- \\ning one available  movement  or remains  motionless  if all surround-  \\ning positions  are occupied.  Note that the prey is captured  when \\nthe vertically  or horizontally  neighboring  cells are occupied  by two \\nagents.  \\nDefense  methods  To demonstrate  the feasibility  and effectiveness  \\nof the proposed  attack methods  better, MCSPT  and MCHT are all \\nequipped  with the ﬁne-tuning  defense  method,  which is widely \\nused in the computer  version.  Exactly,  the process  of ﬁne-tuning  \\nis to initialize  the network  with the trained  parameters  obtained  \\nfrom some trained  models,  such as DQN model. In particular,  the \\nparameter  adjustment  is similar  to the gradient  descent.  \\nModels This environment  setting  refers to two typical \\nMARL ( Böhmer  et al., 2020; Zemzem  and Tagina,  2018 ), and \\nwe set the same parameter  with these two papers.  In the scenario  of MCHT, we adopt the deep Q -network  (DQN)(  Roderick,  Mac- \\nGlashan,  Tellex ) model whose learning  rate is 5 e −5 and the number  \\nof training  rounds  is 6 e 5 . Similarly,  the MCSPT  scenario  adopts the \\ndouble  deep Q -network  (DDQN)(  Hasselt,  Guez, Silver, 2016 ) model. \\nNevertheless,  considering  the different  diﬃculty  levels, the MCSPT  \\nscenario  of learning  rate and the iterative  number  is set as 1 e −3 \\nand 4 e 4 , respectively.  \\n4.2. Evaluation  metrics  \\nIn order to evaluate  the quantifying  the feasibility  and effec- \\ntiveness  of SNPA and TAPA, four metrics  are adopted,  which are \\nthe average  round reward,  the average  round steps, the win rate, \\nand the target action trigger rate. \\n• Average  round reward ( Kim et al., 2021 ): c-MARL  must under- \\nstand the combined  performance  of each agent, higher reward  \\nmeans better performance.  Thus we choose  the average  round \\nreward  as the metric.  The average  round reward  can be calcu- \\nlated as 1 \\nm /summationtext m \\ni =1 Reward  i . In our experiment,  the round m is 100. \\n• Average  round steps ( Lange et al. , 2012 ) : For reinforcement  \\nlearning,  the step of each round represents  the speed of ac- \\ncomplishing  the mission.  The smaller  the number  of steps, the \\nfaster the algorithm  converges.  Thus we choose  the average  \\nround steps as a metric,  the average  round step can be calcu- \\nlated as 1 \\nm /summationtext m \\ni =1 Step i . In our experiment,  the round m is 100. \\n• Win rate ( Mahajan  et al., 2021 ): The result of RL shows whether  \\nthe agent can complete  the task. In order to evaluate  the global \\nperformance,  we take the win rate as an important  metric.  The \\nwin rate can be calculated  as 1 \\nm /summationtext m \\ni =1 W in i . In our experiment,  \\nthe round m is 100. \\n• Target action trigger rate : In the testing phase of TAPA, the vic- \\ntim agent executes  the desired  target action in the triggered  \\nstate. In order to evaluate  the effect of the target poisoning  at- \\ntack, we calculate  the average  trigger rate of the target action \\nfor 100 rounds.  The metric can be calculated  as 1 \\nm /summationtext m \\ni =1 action i . \\nA higher trigger probability  indicates  that the target attack ef- \\nfect is stronger.  \\n4.3. Results and analyses  \\nIn our experiment,  the range of average  round steps is [0,100],  \\nand the range of win rate is [0% , 100%] . The range of the average  \\nround reward  in MCSPT  is [ −300 , 0] , and the range of the average  \\nround reward  in MCHT is [ −10 0 0 , 0] . The multi-agents  in the two \\ndifferent  scenarios  can complete  the corresponding  tasks with a \\n100% win rate. \\n4.3.1. Result of SNPA \\nWe ﬁrst test SNPA on MCHT and MCSPT  scenarios,  respectively.  \\nThe essential  capability  of SNPA is model poisoning  resulting  in the \\nperformance  of the c-MARL  setting  falling. The experiment  results \\nare shown in Fig. 3 , where the x -axis denotes  the different  training  \\nrounds and the y -axis denotes  the average  round reward . \\n(1) MCHT scenario.  For the MCHT scenario,  the results are \\nshown in Fig. 3 (b), we ﬁnd that training  performance  curves of the \\nnormal  training  without  the attacks  and the training  with SNPA at- \\ntack. It can be found that the difference  between  normal  training  \\nand poisoned  training  is obvious.  The overall performance  of nor- \\nmal training  is slightly  better than the training  attacked  by SNPA. \\nThis is because  the other agent will not be affected  by the vic- \\ntim agent, thus they can continue  to complete  the task. It also can \\nbe found from Figs. 3 , 4 that SNPA has a greater  inﬂuence  on the \\noverall performance.  Furthermore,  it leads to overall performance  \\nunstable.  \\n6\\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nFig. 3. The training performance  of the SNPA under different  poisoning  ratios. \\nFig. 4. The performance  of SNPA under different  poisoning  ratios. \\n7 \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nTable 1 \\nThe comparison  of defense effects to SNPA in different  c-MARL scenarios  (100 rounds). \\nGame scenarios  Model Average round reward Average round length Win rate Target action trigger rate \\nMCSPT Poisoned  Model −141 . 65 ±12 . 43 98.61 ±0.42 4% ±1 99.31% ±0.03 \\nPoisoned  Model with Defenses  −134 . 37 ±13 . 66 93.52 ±0.61 5% ±1 98.32% ±0.32 \\nMCHT Poisoned  Model −910 . 27 ±48 . 32 86.18 ±8.67 32% ±3 99.01% ±0.04 \\nPoisoned  Model with Defenses  −892 . 64 ±32 . 49 84.58 ±9.61 35% ±2 98.7% ±0.02 \\nFig. 5. The performance  of TAPA on different  c-MARL scenarios.  \\n(2) MCSPT scenario.  In this experiment,  we evaluate  SNPA with \\nthe varying  perturbation  threshold  /epsilon1of 3 and 1.5, and set the pro- \\nportion  of optimized  perturbations  to 0 . 01% and 0 . 1% . Fig. 3 (a) \\nrecords  the training  performance  curves of the normal  training  \\nwithout  the attacks  and the training  with SNPA, and the shaded  \\npart represents  the maximum  and minimum  total reward  in the \\ntraining  phase. It can be found that when modifying  the state ob- \\nservation  data of the victim agent to the other agents in the team, \\nthe average  round reward  can be reduced  by about 300% . At this \\ntime, the difference  in the proportion  of poisoning  has little effect \\non the overall performance  of the team. This is because  when the \\npoisoning  ratio is 0 . 01% and 0.1%, the team’s win rate is already  \\n0% , and the average  round steps have reached  the limit value, as \\nshown in the left picture  in Fig. 4 (a). In addition,  when the poi- \\nsoning ratio is 0 . 1% , even if only the observation  data of another  \\nagent in the team is modiﬁed,  the overall training  effect of the \\nteam can be affected,  and the average  reward  can be reduced  by \\nabout 300% . It also can be seen from Figs. 3 , 4 that adding  pertur-  \\nbations  to the observation  data of the victim agent will reduce the \\ntotal reward  of the team. This method  has a greater  impact  on the \\noverall performance  of MCSPT  scenario.  \\nIn order to further  verify the effectiveness  of our attack method,  \\nwe investigate  possible  defenses  against  SNPA. Common  backdoor  \\ntrigger detection  in the ﬁeld of computer  vision can be achieved  by \\nneuron  puriﬁcation  detection  methods,  but it can not be directly  \\nused in our scenario  where the environmental  state is not im- \\nage. One possible  defense  method  is to ﬁne-tune  the victim model \\nthrough  normal  training  data ( Liu et al., 2018 ). Speciﬁcally,  we con- \\nduct 40 0 0 rounds  of ﬁne-tuning  training  on the last layer of the \\nvictim model. In order to test the defense  effect of the model, we \\nevaluate  the performance  of the poisoned  models  with or without  \\ndefenses.  \\nThe detailed  results are summarized  in Table 1 . In the MCHT \\nand MCSPT  scenarios,  the performance  of the poisoned  models  \\nwith or without  defenses  is comparable.  The model with defenses  \\nslightly  outperforms  the counterpart  without  defenses,  but not \\nvery obvious.  Hence, it can be concluded  that SNPA can attack \\nagainst  the ﬁne-tuning  defense  ( Liu et al., 2018 ). Exploring,  explor-  \\ning possible  defense  methods  against  c-MARL  poisoning  attacks  \\nlike SNPA is important.  Since SNPA just adds poisoning  perturbations  to the observed  \\nstate, the victim agent cannot know the global position  clearly,  so \\nthe victim agent cannot make deﬁnite  actions.  Due to SNPA only \\nhas one parameter,  namely  perturbation  threshold  /epsilon1. And in our \\nmethod,  the state represents  the squares  in the map, so the per- \\nturbation  threshold  is small enough.  Therefore  we do not take a \\nparameter  sensitivity  analysis  and visualization  analysis.  \\n4.3.2. Result of TAPA \\nWe test the TAPA performance  in two game scenarios.  The es- \\nsential capability  of TAPA is model poisoning  resulting  the per- \\nformance  of c-MARL  setting  fallen. The result is shown in Fig. 4 , \\nwhere the x -axis denotes  the different  training  rounds and the y - \\naxis denotes  the average  round reward . \\n(1) MCHT scenario.  Comparing  Fig. 5 and Table 2 , we can ﬁnd \\nthat TAPA has a signiﬁcantly  higher impact  on team collaboration  \\nperformance  than SNPA. The average  reward  and the win rate are \\nsigniﬁcantly  reduced.  Besides,  TAPA reaches  99 . 01% of the target \\naction trigger rate when the back door is triggered.  It demonstrates  \\nthat TAPA has a better attack performance  than SNPA. \\n(2) MCSPT scenario.  As shown in Fig. 5 , the attack policy learn- \\ning is strengthened  by modifying  the total reward  of the team, \\nso the average  reward  of the training  process  will cause the phe- \\nnomenon  of false  height  . Table 2 summarizes  the average  test \\nperformance  of the victim agent model. The average  round reward  \\nof the poisoned  model is reduced  by about 300% , and the win rate \\nis only 4% . \\nMoreover,  when the backdoor  of the poisoned  model is trig- \\ngered, i.e., under the condition  of the trigger state, the target ac- \\ntion trigger rate of the victim agent can still reach 99 . 31% . The re- \\nsult shows that the poisoned  policy of TAPA can cause damage  to \\nnormal  c-MARL  models.  \\nTo further  verify the effectiveness  of our attack method,  we in- \\nvestigate  possible  defenses  against  TAPA. We set the same experi-  \\nment of defense  for TAPA. \\nThe detailed  results are summarized  in Table 3 . In the MCHT \\nand MCSPT  scenario,  the results are similar  to the defenses  for \\nSNPA, the model with defences  slightly  outperforms  the counter-  \\npart without  defenses,  but not very obvious.  Hence, it can be con- \\ncluded that TAPA has a good attack effect against  the ﬁne-tuning  \\n8 \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nTable 2 \\nThe comparison  of the test results of the TAPA on different  c-MARL scenarios  (100 rounds). \\nGame scenarios  Model Average round reward Average round length Win rate Target action trigger rate \\nMCSPT Normal Model −37 . 2 ±5 . 12 8.61 ±1.61 100% ±0 0.27% ±0.03 \\nPosioned  Model −141 . 56 ±12 . 46 98.61 ±0.41 4% ±1 99.31% ±0.03 \\nMCHT Normal Model −169 . 47 ±15 . 62 22.22 ±0.34 100% ±0 14.47% ±1.56 \\nPosioned  Model −910 . 27 ±98 . 32 86.18 ±8.67 32% ±3 99.01% ±0.04 \\nTable 3 \\nThe comparison  of defense effects to TAPA in different  c-MARL scenarios  (100 rounds). \\nGame scenarios  Model Average round reward Average round length Win rate Target action trigger rate \\nMCSPT Poisoned  Model −141 . 56 ±12 . 42 98.61 ±0.45 4% ±1 99.31% ±0.02 \\nPoisoned  Model with Defences  −140 . 99 ±11 . 23 98.85 ±0.01 3% ±1 99.24% ±0.01 \\nMCHT Poisoned  Model −910 . 27 ±32 . 32 86.18 ±5.21 32% ±3 99.01% ±0.01 \\nPoisoned  Model with Defences  −853 . 63 ±42 . 41 82.53 ±3.23 38% ±2 98.58% ±0.02 \\nFig. 6. Visualization  of the decision-making  process in different  c-MARL scenarios  when the backdoor  is triggered.  The numbers  ‘0’, ‘1’, ‘2’, ‘3’ and ‘4’ in the legends in \\nFigures (a) and (b) respectively  indicate the actions up , down , left and right and keep still . The distance between  two dots represents  the distance between  states. \\ndefense.  Therefore,  for the c-MARL  model with ﬁne-tuning  defense,  \\nit is diﬃcult  to eliminate  the effect of the policy produced  by \\nTAPA. Therefore,  exploring  defense  methods  against  c-MARL  poi- \\nsoning attacks  is an important  direction  for future c-MARL  security  \\nresearch.  \\nIn order to explain  the decision-making  process  of the poisoned  \\nmodel and the normal  model when the backdoor  is triggered,  we \\nuse t-SNE ( Van der Maaten  and Hinton,  2008 ) to visualize  the \\nstates and actions  of the model. The visualization  results in the \\ntwo c-MARL  scenarios  are shown in Fig. 6 . (1) Results in the MCHT scenario.  As shown in the left images  \\nof Fig. 6 (b), the action distribution  of normal  training  is even. The \\nright image of Fig. 6 (b) illustrates  the state action distribution  of \\nthe poisoned  model in the MCHT scenario.  Again, our attack model \\nmakes the model biased to a speciﬁc  action, preventing  it from \\nreaching  its target. Moreover,  when the poisoned  model faces a \\ntrigger,  the actions  are widely distributed,  which makes the strate- \\ngies of the poisoned  model biased.  \\n(2) Results in the MCSPT scenario.  As shown in the left images  \\nof Fig. 6 (a), the normal  training  model policy is biased.  The action \\n9 \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nFig. 7. The visualization  scenario of MCSPT. The left one indicates  the initial state, the middle is one state during the training, and the right one is the ﬁnished state. \\nTable 4 \\nThe Average round reward of different  hyperparameter  δin TAPA. \\nScenarios  δ= 1 δ= 2 δ= 3 δ= 4 δ= 5 δ= 6 δ= 7 δ= 8 \\nMCSPT −113.92 −208.335 −289.06 −194.45 −141.56 −163.27 −172.54 −158.42 \\nMCHT −605.49 −693.82 −772.92 −885.74 −910.27 −892.36 −742.28 −613.58 \\nTable 5 \\nThe win rate of different  hyperparameter  δin TAPA. \\nScenarios  δ= 1 δ= 2 δ= 3 δ= 4 δ= 5 δ= 6 δ= 7 δ= 8 \\nMCSPT 30% 36% 45% 27% 32% 34% 29% 31% \\nMCHT 9% 3% 1% 4% 5% 2% 7% 8% \\nis more biased towards  the actions  of ‘ up ’ and ‘ left ’, which is \\nconsistent  with the relationship  of position  between  the agent and \\nthe target, as shown in Fig. 7 . The agent in a red box in Fig. 7 is \\nthe agent selected  by the attacker.  As the target of this agent is \\nat the upper left position,  its actions  are more biased to ‘ up ’ and \\n‘ left ’. \\nAs shown in the right of Fig. 6 (a), with the attack of TAPA, the \\naction is more biased towards  keep still  ”. It shows that our \\nattack method  makes the model biased toward  a speciﬁc  action, \\npreventing  it from reaching  its target, thus ﬁnally achieving  a suc- \\ncessful attack. \\nParameter  sensitivity  analysis  In this section,  we conduct  a pa- \\nrameter  sensitivity  analysis  for TAPA, including  the trigger thresh-  \\nold and the target action. \\n(1) Results in the MCSPT scenario.  We ﬁrst evaluate  the impact  \\nof the trigger threshold  δon the attack effect, and the results are \\nshown in Tables 4 and 5 . With the changing  of trigger threshold  \\nfrom 1 to 8, the average  round reward  and the win rate are slight \\nchanged.  It also shows that the average  round reward  and win rate \\nisn’t sensitive  to the trigger threshold  in the MCSPT  scenario.  In or- \\nder to observe  more details,  we show more information  in Fig. 8 (a), \\nthe target action trigger rate, the win rate and the average  round \\nstep does not have an obvious  variety.  This is because  in the case \\nof a low threshold,  the trigger rate is already  close to 100% , thus \\nthe performance  of TAPA is not affected  by the trigger threshold.  \\nAs long as the hyperparameters  δare in the range of 0–8, a better \\nattack effect can be achieved.  \\nAdditionally,  we also conduct  a poisoning  attack experiment  \\nwith different  tar get action settings  under different  trigger thresh-  \\nolds, and set the attacker’s  expected  target policy actions  to up , \\ndown , left , and right  , respectively.  The results are shown in \\nFig. 8 (b)–(d).  In the case of different  trigger thresholds,  TAPA can \\ngreatly  reduce the overall performance,  and the target action trig- \\nger rate is as high as 95 . 43% −99 . 31% .The attack effects produced  \\nby different  target action attacks  are different,  but they can signif- icantly reduce the team’s average  round reward  and win rate, and \\ncan also obtain a higher target action trigger rate. \\n(2) Results in the MCHT scenario.  As shown in Tables 4 and 5 , \\nwith the trigger threshold  changes,  the average  round reward  and \\nthe win rate is also gradually  changes,  which is similar  to the re- \\nsults in the MCSPT  scenario.  In Fig. 9 (a)-(d),we  ﬁnd that the attack \\neffects produced  by different  target action attacks  are different,  but \\nthey all signiﬁcantly  reduce the team’s average  round reward  and \\nwin rate, and a higher target action trigger rate can be obtained  at \\nthe same time. \\nAblation  study To study the trigger action reward  effect and re- \\nlationship  of the two c-MARL  poisoning  attacks,  we perform  an \\nablation  study for the trigger action reward  enhancement  and the \\nensemble  attack. The results are summarized  in Tables 6 and 7 . \\n(1) Results in the MCSPT scenario.  In this experiment,  we empir- \\nically set the trigger threshold  to 3. Compared  to poisoned  policy \\nwithout  reward  enhancement,  the average  round reward,  the av- \\nerage round length and trigger action trigger rate of policy with \\nreward  enhancement  has a slight reduction.  Besides,  the win rate \\nhave a slight improvement.  The results show that the reward  en- \\nhancement  policy can strengthen  the poisoning  effect, and demon-  \\nstrate its effectiveness  for attacking  multi-agent  system.  Compared  \\nwith SNPA and TAPA in aspects  of the average  round reward,  av- \\nerage round length,  the win rate, and target action trigger rate, \\nthe ensemble  attack has a better boosting  for attack performance,  \\nwhich testiﬁes  that those two attack methods  can attack at the \\nsame time with a stronger  attack capacity.  \\n(2) Results in the MCHT scenario.  In this experiment,  the trig- \\nger threshold  is set to be 3. The results in the MCHT scenario  \\nare similar  to that in the MCSPT  scenario,  and the growth  ratio \\nof the average  round reward  is less than in the MCSPT  scenario.  It \\nfurther  demonstrates  that the policy of reward  enhancement  also \\nhelps the attack in this scenario.  For the ensemble  attack, the im- \\nprovement  effect of results is similar  to that in the MCSPT  scenario,  \\nwhich also testiﬁes  that the ensemble  attack is scenario-adaptive.  \\n10 \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nFig. 8. The performance  of TAPA in MCSPT on different  threshold  and target action. \\nTable 6 \\nThe performance  of attacks with or without reward enhancement  policy in both MCSPT and MCHT scenarios  (100 rounds). \\nScenarios  Reward Enhancement  Average round reward Average round length Win rate Target action trigger rate \\nMCSPT ✗ −667 . 1 ±30 . 7 93.28 ±5.2 9% ±0 89.37% ±1.3 √ −141 . 56 ±19 . 8 98.61 ±8.5 4% ±0 99.31% ±0 \\nMCHT ✗ −1763 . 24 ±113 . 5 79.64 ±3.5 46% ±2 96.78% ±0.4 √ −910 . 27 ±52 . 4 86.18 ±11.2 32% ±0.2 99.01% ±0.2 \\nTable 7 \\nThe performance  of the ensemble  attack. \\nScenarios  Average round reward Average round length Win rate Target action trigger rate \\nMCSPT −703 . 61 ±20 . 5 97.4 ±0.5 2% ±1 97.63% ±0.2 \\nMCHT −1389 . 87 ±40 . 9 90.2 ±1.2 14% ±1 98.53% ±0.1 \\n5. Related  works \\n5.1. MARL and c-MARL  \\nSingle-agent  reinforcement  learning  Single-agent  \\nRL ( Henderson  et al., 2018 ) concerns  with how an intelligent  \\nagent ought to take actions  in an environment  in order to maxi- \\nmize the notion of cumulative  reward.  In other words, the agent \\noptimizes  a numerical  performance  by making  decisions  in stages. \\nThe decision-maker  called an agent interacts  with the environment  \\nof unknown  dynamics  in a trial-and-error  fashion  and occasionally  \\nreceives  feedback  upon which the agent wants to improve.  Gener- \\nally, the standard  formulation  for such sequential  decision-making  \\nis a Markov  decision  process  (MDP) ( Husic and Pande, 2018 ). \\nMulti-agent  reinforcement  learning  Multi-agent  reinforce-  \\nment learning  (MARL)  is typically  used for modeling  the pro- \\ncess of multiple  agents participating  in decision-making.  Until \\nnow, numerous  MARL methods  have been proposed,  such as, \\nindependent-  Q -learning  (IQL) ( Shoham  and Leyton-Brown,  2008; \\nTan, 1993; Tesauro,  2003; Zawadzki  et al., 2014 ), counterfac-  tual multi-agent  policy gradients  (COMA)  ( Foerster  et al., 2018 ), \\nQMIX ( Rashid et al., 2020 ), multi-agent  deep proximal  policy \\noptimization  (MAPPO)  ( Yu et al., 2021 ), multi-agent  deep deter- \\nministic  policy gradient  (MADDPG)  ( Lowe et al., 2017 ) and etc. \\nThese MARL algorithms  have been successfully  applied  to robotic  \\nsystems  ( Foerster  et al., 2016; Gu et al., 2017 ), human-machine  \\ngame ( Lanctot  et al., 2017; Leibo et al., 2017; Lowe et al., 2017 ), \\nautonomous  driving  ( Shalev-Shwartz  et al., 2016 ), Internet  adver- \\ntising ( Jin et al., 2018 ) and resource  utilization  ( Perolat  et al., 2017; \\nXi et al., 2018 ), etc. The process  of multiple  agents participating  in \\ndecision-making  is usually  modeled  as MARL. \\nCooperative  multi-agent  reinforcement  learning  According  to the \\nway that the agent participates,  MARL can be roughly  divided  into \\nthree categories,  i.e., competitive  MARL ( Deka and Sycara,  2021; \\nMa et al., 2021 ), cooperative  MARL ( Cui and Zhang, 2021; Iranfar \\net al., 2021; Maxim  and Caruntu,  2021; Panait and Luke, 2005 ) \\nand competitive-cooperative  MARL ( Aotani et al., 2021; Vanneste  \\net al., 2021 ). The c-MARL  setting  is that several  agents attempt,  \\nthrough  their interaction,  to jointly solve tasks or to maximize  util- \\nity. Speciﬁcally,  in a c-MARL  setting,  the multiple  agents conduct  \\n11 \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nFig. 9. The performance  of TAPA in MCHT on different  threshold  and target action. \\nstrategic  collaborative  learning  as a team. Each agent maximizes  \\nthe overall reward  of the team by interacting  with the environ-  \\nment and other agents,  which can solve the sequential  policy op- \\ntimization  problem  of the multiple  agents’  decision-making  in a \\nconventional  environment.  \\n5.2. Poisoning  attack \\nA poisoning  attack occurs when the adversary  can inject the \\npoisoning  data into the model’s  training  pool, and hence get it to \\nlearn poisoned  information.  The most common  result of a poison-  \\ning attack is that the model’s  boundary  shifts in some designated  \\nway. Poisoning  attack inserts hidden  associations  or triggers  to the \\ndeep learning  models  to override  correct  inference  such as classiﬁ-  \\ncation ( Xiang et al., 2021; Xue et al., 2020 ), and makes the system  \\nperform  normally  without  triggering.  \\nPoisoning  attack implants  a backdoor  into the DL model in the \\nway that the poisoned  model learns both the sub-task  chosen  by \\nthe attacker  and the main task. On the one hand, the poisoned  \\nmodel behaves  normally  as its clean counterpart  model for input \\ncontaining  no trigger,  making  it impossible  to distinguish  the poi- \\nsoned model from the clean model by solely checking  the test ac- \\ncuracy with the test examples.  On the other hand, the poisoned  \\nmodel is misdirected  to perform  the attacker’s  sub-task  once the \\nsecret trigger is presented  in the input. \\nThere are two ways that the adversary  can poison the RL set- \\nting. (1) Dataset  poisoning  ( Li et al., 2019; Rakhsha  et al., 2020 ). \\nDataset  poisoning  attack is a method  that can corrupt  a model. \\nIn this case, the adversary  introduces  incorrect  or mislabeled  data \\ninto the datasets.  Alternatively,  the adversary  can change  its behav-  \\nior so that the data collected  itself will be wrong.  (2) Algorithm  poi- \\nsoning ( Ma et al., 2019 ). In this type, the attacker  takes advantage  \\nof the algorithm  used to learn the model. The attacker  often trans- \\nfers learning  where attackers  teach an algorithm  and then spread \\nit to new RL algorithms  using transfer  learning.  5.3. Security  issues of RL and MARL \\nFor the multi-agent  RL, Lin et al. (2020) was the ﬁrst to study \\nthe robustness  of c-MARL.  They added adversarial  perturbations  \\nto the victim’s  agent environment  observations  during the test- \\ning. This will reduce the overall performance  of the team by taking \\nthe expected  actions  of the victim agent. Nisioti et al. (2021) pro- \\nposed the RoM- Q method,  which makes the worst-case  selection  \\nof the agent to be attacked,  and the action to be performed  on the \\npremise  that the adversary  knows the optimal  Q -value function  of \\nmulti-agent.  Although  these two methods  have studied  the robust \\nsecurity  of the agent in the c-MARL  scenario,  they have not studied  \\nthe security  vulnerabilities  that exist in the collaborative  training  \\nprocess  of the agent. \\n5.4. Defense  methods  of RL and MARL \\nWith the widespread  use of RL, we recently  observe  the use \\nof robust optimization  defense  methods  to improve  the robust-  \\nness of the RL model to resist attacks.  Pinto et al. (2017) pro- \\nposed a robust adversarial  reinforcement  learning  defense  method,  \\nan agent with a confrontation  policy was added to enhance  the \\npolicy of the target agent in the training  process.  Bravo and Mer- \\ntikopoulos  (2017) and Ogunmolu  et al. (2018) respectively  pro- \\nposed an equilibrium  principle  and a maximum  and minimum  dy- \\nnamic game framework  for the zero-sum  game problem.  \\nIn addition,  for other learning  tasks of RL, defense  methods  \\nsuch as adversarial  training  and adversarial  detection  have also \\nbeen used to strengthen  the security  of the model. For adver- \\nsarial training  security  defense,  both Kos and Song (2017) and \\nPattanaik  et al. (2017) added fast gradient  signal attack(FGSM)  \\ndisturbances  as adversarial  examples  to training  examples,  so \\nthat they can train the model together  with normal  examples  to \\nimprove  robustness.  Similarly,  Behzadan  and Munir (2017) also \\nutilized  the FGSM perturbation,  but they used a certain  prob- \\n12 \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nability to generate  adversarial  examples  for adversarial  train- \\ning. Behzadan  and Hsu (2019a)  improved  the use rate of non- \\ncontinuous  anti-disturbance  examples  in anti-disturbance  training  \\nby adjusting  the sampling  probability  of normal  examples  and ad- \\nversarial  examples.  Although  the defense  method  of adversarial  \\ntraining  can improve  the perturbation  attack on the adversarial  ex- \\namples  participating  in the training,  but its generalization  ability is \\nlimited  and cannot effectively  defend  against  the adversarial  exam- \\nples generated  by other attack methods.  For adversarial  detection  \\nsecurity  defense,  Havens  et al. (2018) used meta-learning  meth- \\nods to detect the sub-strategies  of the main agent and switched  \\nthe sub-strategies  once they are far away from the expected  goal, \\nthereby  improving  the effectiveness  of the model policy. In or- \\nder to enhance  the robustness  of the model, Lin et al. (2017) de- \\ntected adversarial  examples  by comparing  the target policy’s  action \\ndistribution  difference  between  the predicted  frame and the cur- \\nrent frame. In ( Behzadan  and Hsu, 2019b ), Behzadan  et al. added \\na unique  watermark  ( Uchida  et al., 2017 ) to a speciﬁc  state tran- \\nsition sequence  to detect whether  the policy has been tampered  \\nwith, which improved  the security  application  of the DRL model \\npolicy. \\n6. Conclusion  \\nIn this paper, two poisoning  attacks  aimed at the c-MARL  set- \\nting are introduced,  which can attack only one agent to affect the \\nteam’s overall collaboration.  Thus, the proposed  attack methods  \\npose a serious  threat to the real-world  multi-agent  collaboration  \\nmodel. Extensive  experiments  have been conducted  in two scenar-  \\nios that veriﬁed  SNPA and TAPA are effectiv  and stealthy  for attack-  \\ning the c-MARL  setting.  Additionally,  we ﬁnd that the policy of ma- \\nliciously  modifying  the state data or manipulating  the victim agent \\ncan greatly  affect the team’s overall collaboration  performance  and \\nachieve  a low win rate. Besides,  TAPA can also cause the victim \\nagent to perform  the expected  target action, thereby  disrupting  the \\noverall collaboration.  Since multi-agent  models  are widely used in \\nmachine  learning  and robotic  systems,  such as autonomous-driving  \\nand transaction  systems,  studying  the vulnerability  of c-MARL  is \\nof great signiﬁcance.  SNPA and TAPA devote to fooling  the victim \\nmodel. However,  with the increasing  number  of agents,  the strat- \\negy effect of the victim agent is gradually  weakened,  and the effect \\nof poisoning  policy may diminish  or even disappear.  \\nThe result shows that the c-MARL  model has serious  security  \\nrisks, and its security  issues still need to be resolved.  There are \\ntwo side of suggestions  that are provided  for future works:  Rec- \\nommendations  for Vulnerability  Mining:  For cooperation  strat- \\negy, both training  policy and environment  interaction  should be \\nconsidered  for pollution,  to reduce the impact  of the imbalance  \\nof agents.  Suggestions  for Defenders:  Researchers  are advised  to \\nfocus on the policy polymerization  problem  on c-MARL,  which di- \\nminish  the effect of the victim model by taking different  weights.  \\nDeclaration  of Competing  Interest  \\nThe authors  declare  that they have no known  competing  ﬁnan- \\ncial interests  or personal  relationships  that could have appeared  to \\ninﬂuence  the work reported  in this paper. \\nCRediT  authorship  contribution  statement  \\nHaibin  Zheng:  Conceptualization,  Data curation,  Formal  anal- \\nysis, Funding  acquisition,  Methodology,  Resources,  Supervision,  \\nWriting  –r e v i e w  & editing.  Xiaohao  Li: Investigation,  Methodol-  \\nogy, Project  administration,  Resources,  Software,  Writing  – origi-  \\nnal draft. Jinyin Chen: Investigation,  Methodology,  Software,  Vali- \\ndation.  Jianfeng  Dong: Methodology,  Software,  Visualization,  Writ- ing – original  draft. Yan Zhang:  Conceptualization,  Methodology.  \\nChangting  Lin: Conceptualization,  Methodology,  Supervision,  Writ- \\ning –r e v i e w  & editing.  \\nData availability  \\nData will be made available  on request.  \\nAcknowledgments  \\nThis work was supported  by NSFC (No. 62102363),  NSFC \\n(No. 62072406),  CNKLSTISS  (No. 61421110502),  NSFC (Nos. \\nU21B2001,  62103374),  Key R&D Programs  of Zhejiang  Province  \\n(No. 2022C01018),  the Zhejiang  Provincial  Natural  Science  Founda-  \\ntion (No. LQ21F020010  ). \\nReferences  \\nAotani, T., Kobayashi,  T., Sugimoto,  K., 2021. Bottom-up  multi-agent  reinforcement  \\nlearning by reward shaping for cooperative-competitive  tasks. Appl. Intell. 51 \\n(7), 4 434–4 452 . \\nAshcraft,  C., Karra, K., 2021. Poisoning  deep reinforcement  learning agents with in- \\ndistribution  triggers. arXiv preprint arXiv: 2106.07798  . \\nBehzadan,  V., Hsu, W., 2019a. Analysis and improvement  of adversarial  train- \\ning in DQN agents with adversarially-guided  exploration  (age). arXiv preprint \\narXiv: 1906.01119  . \\nBehzadan,  V., Hsu, W., 2019b. Sequential  triggers for watermarking  of deep rein- \\nforcement  learning policies. arXiv preprint arXiv: 1906.01126  . \\nBehzadan,  V., Munir, A., 2017. Whatever  does not kill deep reinforcement  learning,  \\nmakes it stronger.  arXiv preprint arXiv: 1712.09344  \\nBlas, H.S.S., Mendes, A.S., Encinas, F.G., Silva, L.A., González,  G.V., 2021. A multi-a- \\ngent system for data fusion techniques  applied to the internet of things en- \\nabling physical rehabilitation  monitoring.  Appl. Sci. 11 (1), 331 . \\nBöhmer, W., Kurin, V., Whiteson,  S., 2020. Deep coordination  graphs. In: Interna- \\ntional Conference  on Machine  Learning.  PMLR, pp. 980–991  . \\nBravo, M., Mertikopoulos,  P., 2017. On the robustness  of learning in games with \\nstochastically  perturbed  payoff observations.  Games Econ. Behav. 103, 41–66 . \\nCui, H., Zhang, Z., 2021. A cooperative  multi-agent  reinforcement  learning method \\nbased on coordination  degree. IEEE Access 9, 123805–123814  . \\nDeka, A., Sycara, K., 2021. Natural emergence  of heterogeneous  strategies  in artiﬁ- \\ncially intelligent  competitive  teams. In: International  Conference  on Swarm In- \\ntelligence.  Springer,  pp. 13–25 . \\nFoerster, J., Farquhar,  G., Afouras, T., Nardelli, N., Whiteson,  S., 2018. Counterfactual  \\nmulti-agent  policy gradients.  In: Proceedings  of the AAAI Conference  on Artiﬁ- \\ncial Intelligence,  vol. 32 . \\nFoerster, J. N., Assael, Y. M., De Freitas, N., Whiteson,  S., 2016. Learning  to commu- \\nnicate with deep multi-agent  reinforcement  learning.  arXiv preprint arXiv: 1605. \\n06676 . \\nFoley, H., Fowl, L., Goldstein,  T., Taylor, G., 2022. Execute order 66: targeted data \\npoisoning  for reinforcement  learning.  arXiv preprint arXiv: 2201.00762  . \\nGibert, D., Fredrikson,  M., Mateu, C., Planes, J., Le, Q., 2022. Enhancing  the inser- \\ntion of NOP instructions  to obfuscate  malware  via deep reinforcement  learning.  \\nComput. Secur. 113, 102543 . \\nGu, S., Holly, E., Lillicrap, T., Levine, S., 2017. Deep reinforcement  learning for robotic \\nmanipulation  with asynchronous  off-policy  updates. In: 2017 IEEE International  \\nConference  on Robotics and Automation  (ICRA). IEEE, pp. 3389–3396  . \\nGuo, J., Li, A., Liu, C., 2022. Backdoor  detection  in reinforcement  learning.  arXiv \\npreprint arXiv: 2202.03609  . \\nHasselt, H.V., Guez, A., Silver, D., 2016. Deep reinforcement  learning with double \\nQ -learning.  In: Proceedings  of the AAAI Conference  on Artiﬁcial  Intelligence,  \\nvol. 30 . \\nHavens, A. J., Jiang, Z., Sarkar, S., 2018. Online robust policy learning in the presence  \\nof unknown  adversaries.  arXiv preprint arXiv: 1807.06064  . \\nHenderson,  P., Islam, R., Bachman,  P., Pineau, J., Precup, D., Meger, D., 2018. Deep \\nreinforcement  learning that matters. In: Proceedings  of the AAAI Conference  on \\nArtiﬁcial  Intelligence,  vol. 32 . \\nHusic, B.E., Pande, V.S., 2018. Markov state models: from an art to a science. J. Am. \\nChem. Soc. 140 (7), 2386–2396  . \\nIranfar, A., Zapater, M., Atienza, D., 2021. Multiagent  reinforcement  learning for hy- \\nperparameter  optimization  of convolutional  neural networks.  IEEE Trans. Com- \\nput. Aided Des. Integr. Circuits Syst. 41 (4), 1034–1047  . \\nJin, J., Song, C., Li, H., Gai, K., Wang, J., Zhang, W., 2018. Real-time  bidding with \\nmulti-agent  reinforcement  learning in display advertising.  In: Proceedings  of \\nthe 27th ACM International  Conference  on Information  and Knowledge  Man- \\nagement,  pp. 2193–2201  . \\nJinyin, C., Haibin, Z., Haibin, Z., Mengmeng,  S., tianyu, D., Changting,  L., Shouling,  J., \\n2019. Invisible poisoning:  highly stealthy targeted poisoning  attack. In: Interna- \\ntional Conference  on Information  Security and Cryptology,  pp. 173–198 . \\nKasseroller,  K., Thaler, F., Payer, C., Štern, D., 2021. Collaborative  multi-agent  re- \\ninforcement  learning for landmark  localization  using continuous  action space. \\n13 \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nIn: International  Conference  on Information  Processing  in Medical Imaging.  \\nSpringer,  pp. 767–778  . \\nKhalilpourazari,  S., Hashemi  Doulabi, H., 2022. Designing  a hybrid reinforcement  \\nlearning based algorithm  with application  in prediction  of the COVID-19  pan- \\ndemic in quebec. Ann. Oper. Res. 312 (2), 1261–1305  . \\nKim, D.K., Liu, M., Riemer, M.D., Sun, C., Abdulhai,  M., Habibi, G., Lopez-Cot,  S., \\nTesauro, G., How, J., 2021. A policy gradient algorithm  for learning to learn \\nin multiagent  reinforcement  learning.  In: International  Conference  on Machine  \\nLearning.  PMLR, pp. 5541–5550  . \\nKos, J., Song, D., 2017. Delving into adversarial  attacks on deep policies. arXiv \\npreprint arXiv: 1705.06452  . \\nLanctot, M., Zambaldi,  V., Gruslys, A., Lazaridou,  A., Tuyls, K., Pérolat, J., Silver, D., \\nGraepel, T., 2017. A uniﬁed game-theoretic  approach  to multiagent  reinforce-  \\nment learning.  arXiv preprint arXiv: 1711.00832  \\nLange, S., Riedmiller,  M., Voigtländer,  A., 2012. Autonomous  reinforcement  learning \\non raw visual input data in a real world application.  In: The 2012 International  \\nJoint Conference  on Neural Networks  (IJCNN), pp. 1–8. doi: 10.1109/IJCNN.2012.  \\n625282 . \\nLeibo, J. Z., Zambaldi,  V., Lanctot, M., Marecki, J., Graepel, T., 2017. Multi-agent  re- \\ninforcement  learning in sequential  social dilemmas.  arXiv preprint arXiv: 1702. \\n03037 . \\nLi, M., Sun, Y., Lu, H., Maharjan,  S., Tian, Z., 2019. Deep reinforcement  learning for \\npartially observable  data poisoning  attack in crowdsensing  systems. IEEE Inter- \\nnet Things J. 7 (7), 6266–6278  . \\nLin, J., Dzeparoska,  K., Zhang, S.Q., Leon-Garcia,  A., Papernot,  N., 2020. On the ro- \\nbustness  of cooperative  multi-agent  reinforcement  learning.  In: 2020 IEEE Se- \\ncurity and Privacy Workshops  (SPW). IEEE, pp. 62–68 . \\nLin, Y.-C., Liu, M.-Y., Sun, M., Huang, J.-B., 2017. Detecting  adversarial  attacks on neu- \\nral network policies with visual foresight.  arXiv preprint arXiv: 1710.00814  . \\nLiu, H., Wu, W., 2021. Online multi-agent  reinforcement  learning for decentralized  \\ninverter-based  volt-var control. IEEE Trans. Smart Grid 12 (4), 2980–2990  . \\nLiu, K., Dolan-Gavitt,  B., Garg, S., 2018. Fine-pruning:  defending  against backdooring  \\nattacks on deep neural networks.  In: International  Symposium  on Research  in \\nAttacks, Intrusions,  and Defenses.  Springer,  pp. 273–294  . \\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., Mordatch,  I., 2017. Multi-agent  \\nactor-critic  for mixed cooperative-competitive  environments.  arXiv preprint \\narXiv: 1706.02275  . \\nMa, Y., Shen, M., Zhao, Y., Li, Z., Tong, X., Zhang, Q., Wang, Z., 2021. Opponent  por- \\ntrait for multiagent  reinforcement  learning in competitive  environment.  Int. J. \\nIntell. Syst. 36 (12), 7461–7474  . \\nMa, Y., Zhang, X., Sun, W., Zhu, J., 2019. Policy poisoning  in batch reinforcement  \\nlearning and control. Adv. Neural Inf. Process. Syst. 32, 1–11 . \\nVan der Maaten, L., Hinton, G., 2008. Visualizing  data using t-SNE. J. Mach. Learn. \\nRes. 9 (11) . \\nMaeda, R., Mimura, M., 2021. Automating  post-exploitation  with deep reinforcement  \\nlearning.  Comput. Secur. 100, 102108 . \\nMahajan,  A., Samvelyan,  M., Mao, L., Makoviychuk,  V., Garg, A., Kossaiﬁ, J., White- \\nson, S., Zhu, Y., Anandkumar,  A., 2021. Tesseract:  tensorised  actors for multi-a- \\ngent reinforcement  learning.  In: International  Conference  on Machine  Learning.  \\nPMLR, pp. 7301–7312  . \\nMaxim, A., Caruntu,  C.-F., 2021. A coalitional  distributed  model predictive  control \\nperspective  for a cyber-physical  multi-agent  application.  Sensors 21 (12), 4041 . \\nNisioti, E., Bloembergen,  D., Kaisers, M., 2021. Robust multi-agent  Q -learning  in co- \\noperative  games with adversaries.  AAAI . \\nOgunmolu,  O., Gans, N., Summers,  T., 2018. Minimax  iterative dynamic  game: appli- \\ncation to nonlinear  robot control tasks. In: 2018 IEEE/RSJ International  Confer- \\nence on Intelligent  Robots and Systems (IROS). IEEE, pp. 6919–6925  . \\nPanait, L., Luke, S., 2005. Cooperative  multi-agent  learning:  the state of the art. Au- \\nton. Agents Multi-Agent  Syst. 11 (3), 387–434  . \\nPattanaik,  A., Tang, Z., Liu, S., Bommannan,  G., Chowdhary,  G., 2017. Robust deep re- \\ninforcement  learning with adversarial  attacks. arXiv preprint arXiv: 1712.03632  . \\nPerolat, J., Leibo, J. Z., Zambaldi,  V., Beattie, C., Tuyls, K., Graepel, T., 2017. A multi- \\nagent reinforcement  learning model of common-pool  resource appropriation.  \\narXiv preprint arXiv: 1707.06600  . \\nPerrusquía,  A., Yu, W., Li, X., 2021. Multi-agent  reinforcement  learning for redundant  \\nrobot control in task-space.  Int. J. Mach. Learn. Cybern. 12 (1), 231–241 . \\nPinto, L., Davidson,  J., Sukthankar,  R., Gupta, A., 2017. Robust adversarial  rein- \\nforcement  learning.  In: International  Conference  on Machine  Learning.  PMLR, \\npp. 2817–2826  . \\nRakhsha,  A., Radanovic,  G., Devidze, R., Zhu, X., Singla, A., 2020. Policy teach- \\ning via environment  poisoning:  training-time  adversarial  attacks against rein- \\nforcement  learning.  In: International  Conference  on Machine  Learning.  PMLR, \\npp. 7974–7984  . \\nRashid, T., Samvelyan,  M., De Witt, C.S., Farquhar,  G., Foerster, J.N., Whiteson,  S., \\n2020. Monotonic  value function factorisation  for deep multi-agent  reinforce-  \\nment learning.  J. Mach. Learn. Res. 21, 178-1 . \\nRoderick  M., MacGlashan  J., Tellex S.. Implementing  the deep Q -network.  2017. arXiv \\npreprint arXiv: 1711.07478  . \\nShalev-Shwartz,  S., Shammah,  S., Shashua,  A., 2016. Safe, multi-agent,  reinforcement  \\nlearning for autonomous  driving. arXiv preprint arXiv: 1610.03295  . \\nShoham,  Y., Leyton-Brown,  K., 2008. Multiagent  Systems:  Algorithmic,  Game-Theo-  \\nretic, and Logical Foundations.  Cambridge  University  Press . \\nTan, M., 1993. Multi-agent  reinforcement  learning:  independent  vs. cooperative  \\nagents. In: Proceedings  of the Tenth International  Conference  on Machine  Learn- \\ning, pp. 330–337  . Tesauro, G., 2003. Extending  Q -learning  to general adaptive multi-agent  systems. \\nAdv. Neural Inf. Process. Syst. 16, 871–878  . \\nUchida, Y., Nagai, Y., Sakazawa,  S., Satoh, S., 2017. Embedding  watermarks  into deep \\nneural networks.  In: Proceedings  of the 2017 ACM on International  Conference  \\non Multimedia  Retrieval,  pp. 269–277  . \\nVanneste,  A., Wijnsberghe,  W.V., Vanneste,  S., Mets, K., Mercelis,  S., Latré, S., \\nHellinckx,  P., 2021. Mixed cooperative-competitive  communication  using mul- \\nti-agent reinforcement  learning.  In: International  Conference  on P2P, Parallel, \\nGrid, Cloud and Internet Computing.  Springer,  pp. 197–206  . \\nXi, L., Chen, J., Huang, Y., Xu, Y., Liu, L., Zhou, Y., Li, Y., 2018. Smart generation  control \\nbased on multi-agent  reinforcement  learning with the idea of the time tunnel. \\nEnergy 153, 977–987  . \\nXiang, Z., Miller, D.J., Kesidis, G., 2021. Reverse engineering  imperceptible  backdoor  \\nattacks on deep neural networks  for detection  and training set cleansing.  Com- \\nput. Secur. 106, 102280 . \\nXie, Z., Xiang, Y., Li, Y., Zhao, S., Tong, E., Niu, W., Liu, J., Wang, J., 2021. Security \\nanalysis of poisoning  attacks against multi-agent  reinforcement  learning.  In: In- \\nternational  Conference  on Algorithms  and Architectures  for Parallel Processing.  \\nSpringer,  pp. 660–675  . \\nXue, M., He, C., Wang, J., Liu, W., 2020. LOPA: a linear offset based poisoning  attack \\nmethod against adaptive ﬁngerprint  authentication  system. Comput. Secur. 99, \\n102046 . \\nYu, C., Velu, A., Vinitsky,  E., Wang, Y., Bayen, A., Wu, Y., 2021. The surprising  effec- \\ntiveness of MAPPO in cooperative,  multi-agent  games. arXiv preprint arXiv: 2103. \\n01955 . \\nZawadzki,  E., Lipson, A., Leyton-Brown,  K., 2014. Empirically  evaluating  multiagent  \\nlearning algorithms.  arXiv preprint arXiv: 1401.8074  . \\nZemzem,  W., Tagina, M., 2018. Cooperative  multi-agent  systems using distributed  \\nreinforcement  learning techniques.  Procedia  Comput. Sci. 126, 517–526 . \\nHaibin Zheng received B.S. and Ph.D. degrees from Zhe- \\njiang University  of Technology,  Hangzhou,  China, in 2017 \\nand 2022, respectively.  He is currently  a university  lec- \\nturer at the Institute of Cyberspace  Security,  Zhejiang  Uni- \\nversity of Technology.  His research interests  include deep \\nlearning and artiﬁcial intelligence  security. \\nXiaohao  Li is a postgraduate  student at the College of In- \\nformation  Engineering,  Zhejiang  University  of Technology.  \\nHis research interests  include deep reinforcement  learn- \\ning, artiﬁcial intelligence,  deep learning.  \\nJinyin Chen received B.S. and Ph.D. degrees from Zhe- \\njiang University  of Technology,  Hangzhou,  China, in 2004 \\nand 2009, respectively.  She is currently  a Professor  with \\nthe Zhejiang  University  of Technology,  Hangzhou,  China. \\nHer research interests  include artiﬁcial intelligence  secu- \\nrity, graph data mining and evolutionary  computing.  \\nJianfeng  Dong received the B.E. degree in software  engi- \\nneering from Zhejiang  University  of Technology  in 2009, \\nand the Ph.D. degree in computer  science from Zhejiang  \\nUniversity  in 2018, all in Hangzhou,  China. He is cur- \\nrently a Research  Professor  at the College of Computer  \\nand Information  Engineering,  Zhejiang  Gongshang  Univer- \\nsity, Hangzhou,  China. His research interests  include mul- \\ntimedia understanding,  retrieval and recommendation.  \\nHe was awarded  the ACM Multimedia  Grand Challenge  \\nAward in 2016. He has won a number of international  \\ncompetitions  including  the TRECVID  2016, 2017, 2018 \\nVideo-to-Text  (VTT) Matching  and Ranking task, the MSR \\nBing Image Retrieval  Challenge  at ACM Multimedia  2015, \\nand so on. \\n14 \\nH. Zheng, X. Li, J. Chen et al. Computers  & Security 124 (2023) 103005 \\nYan Zhang received the master degree from ZheJiang  Uni- \\nversity of Technology,  HangZhou,  China, in 2021. Her main \\nresearch methods  are artiﬁcial intelligence  security, com- \\nputer vision. \\nChangting  Lin received the Ph.D. degree in computer  sci- \\nence from Zhejiang  University  in 2018. He is currently  \\nan Associate  Research  Professor  at Binjiang Institute of \\nZhejiang  University,  China. His research interests  include \\nblockchain  technology,  artiﬁcial intelligence,  network,  and \\nsecurity. \\n15 '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['full_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenize and remove stopwords\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    # Remove short words\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    # Lemmatize words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join words back into a string\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['full_text'].tolist()\n",
    "\n",
    "preprocessed_text = [preprocess_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Embed the preprocessed texts\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embedding_model.encode(preprocessed_text, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=2, metric='euclidean', cluster_selection_method='leaf', prediction_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\") # Vectorize the sentences\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n",
    "\n",
    "# KeyBERT\n",
    "keybert_model = KeyBERTInspired()\n",
    "\n",
    "# MMR\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 19:48:14,559 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-01-07 19:48:19,763 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-01-07 19:48:19,763 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-01-07 19:48:19,765 - BERTopic - Cluster - Completed ✓\n",
      "2025-01-07 19:48:19,766 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-01-07 19:48:23,099 - BERTopic - Representation - Completed ✓\n",
      "2025-01-07 19:48:23,101 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-01-07 19:48:26,497 - BERTopic - Topic reduction - Reduced number of topics from 7 to 7\n"
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic(\n",
    "\n",
    "  # Pipeline models\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  representation_model=representation_model,\n",
    "  ctfidf_model=ctfidf_model,\n",
    "\n",
    "  # Hyperparameters\n",
    "  nr_topics=\"auto\",\n",
    "  min_topic_size=10,\n",
    "  top_n_words=10,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(preprocessed_text, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>-1_cmarl_reward_attack_policy</td>\n",
       "      <td>[cmarl, reward, attack, policy, robust, rmaac,...</td>\n",
       "      <td>[reinforcement, multiagent, agent, adversarial...</td>\n",
       "      <td>[cmarl, rmaac, agent, adversary, trigger, rein...</td>\n",
       "      <td>[computer security content list available scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0_message_communication_vector_neural</td>\n",
       "      <td>[message, communication, vector, neural, outpu...</td>\n",
       "      <td>[multiagent, reinforcement, cooperative, agent...</td>\n",
       "      <td>[message, communication, agent, cooperative, l...</td>\n",
       "      <td>[emergence adversarial communication multiagen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_agent_set_time_problem</td>\n",
       "      <td>[agent, set, time, problem, algorithm, model, ...</td>\n",
       "      <td>[multiagent, strategy, agent, learning, oppone...</td>\n",
       "      <td>[agent, algorithm, task, node, value, fig, pre...</td>\n",
       "      <td>[entropy article improved approach towards mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2_defense_defender_network_attacker</td>\n",
       "      <td>[defense, defender, network, attacker, securit...</td>\n",
       "      <td>[attackdefense, reinforcement, multiagent, str...</td>\n",
       "      <td>[defender, security, game, strategy, ddpg, int...</td>\n",
       "      <td>[expert system application available online ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3_adversarial_attack_energy_subplay</td>\n",
       "      <td>[adversarial, attack, energy, subplay, victim,...</td>\n",
       "      <td>[adversarial, attack, multiagent, reinforcemen...</td>\n",
       "      <td>[adversarial, attack, training, defence, adver...</td>\n",
       "      <td>[applied energy available online august elsevi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4_ami_victim_influence_timestep</td>\n",
       "      <td>[ami, victim, influence, timestep, attack, tra...</td>\n",
       "      <td>[adversarial, attacking, reinforcement, attack...</td>\n",
       "      <td>[ami, timestep, attack, adversarial, adversary...</td>\n",
       "      <td>[model method trainingtime attack cooperative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5_attack_ihs_attacker_poisoning</td>\n",
       "      <td>[attack, ihs, attacker, poisoning, reward, lea...</td>\n",
       "      <td>[reinforcement, agent, strategy, reward, bandi...</td>\n",
       "      <td>[attack, ihs, poisoning, reward, target, strat...</td>\n",
       "      <td>[rewardpoisoning attack offline multiagent rei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                   Name  \\\n",
       "0     -1      8          -1_cmarl_reward_attack_policy   \n",
       "1      0      7  0_message_communication_vector_neural   \n",
       "2      1      5               1_agent_set_time_problem   \n",
       "3      2      3    2_defense_defender_network_attacker   \n",
       "4      3      3    3_adversarial_attack_energy_subplay   \n",
       "5      4      2        4_ami_victim_influence_timestep   \n",
       "6      5      2        5_attack_ihs_attacker_poisoning   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [cmarl, reward, attack, policy, robust, rmaac,...   \n",
       "1  [message, communication, vector, neural, outpu...   \n",
       "2  [agent, set, time, problem, algorithm, model, ...   \n",
       "3  [defense, defender, network, attacker, securit...   \n",
       "4  [adversarial, attack, energy, subplay, victim,...   \n",
       "5  [ami, victim, influence, timestep, attack, tra...   \n",
       "6  [attack, ihs, attacker, poisoning, reward, lea...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [reinforcement, multiagent, agent, adversarial...   \n",
       "1  [multiagent, reinforcement, cooperative, agent...   \n",
       "2  [multiagent, strategy, agent, learning, oppone...   \n",
       "3  [attackdefense, reinforcement, multiagent, str...   \n",
       "4  [adversarial, attack, multiagent, reinforcemen...   \n",
       "5  [adversarial, attacking, reinforcement, attack...   \n",
       "6  [reinforcement, agent, strategy, reward, bandi...   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [cmarl, rmaac, agent, adversary, trigger, rein...   \n",
       "1  [message, communication, agent, cooperative, l...   \n",
       "2  [agent, algorithm, task, node, value, fig, pre...   \n",
       "3  [defender, security, game, strategy, ddpg, int...   \n",
       "4  [adversarial, attack, training, defence, adver...   \n",
       "5  [ami, timestep, attack, adversarial, adversary...   \n",
       "6  [attack, ihs, poisoning, reward, target, strat...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [computer security content list available scie...  \n",
       "1  [emergence adversarial communication multiagen...  \n",
       "2  [entropy article improved approach towards mul...  \n",
       "3  [expert system application available online ma...  \n",
       "4  [applied energy available online august elsevi...  \n",
       "5  [model method trainingtime attack cooperative ...  \n",
       "6  [rewardpoisoning attack offline multiagent rei...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = topic_model.generate_topic_labels(nr_words=5,\n",
    "                                                 topic_prefix=False,\n",
    "                                                 word_length=30,\n",
    "                                                 separator=\", \")\n",
    "\n",
    "topic_model.set_topic_labels(topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>CustomName</th>\n",
       "      <th>Representation</th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>MMR</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>-1_cmarl_reward_attack_policy</td>\n",
       "      <td>cmarl, reward, attack, policy, robust</td>\n",
       "      <td>[cmarl, reward, attack, policy, robust, rmaac,...</td>\n",
       "      <td>[reinforcement, multiagent, agent, adversarial...</td>\n",
       "      <td>[cmarl, rmaac, agent, adversary, trigger, rein...</td>\n",
       "      <td>[computer security content list available scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0_message_communication_vector_neural</td>\n",
       "      <td>message, communication, vector, neural, output</td>\n",
       "      <td>[message, communication, vector, neural, outpu...</td>\n",
       "      <td>[multiagent, reinforcement, cooperative, agent...</td>\n",
       "      <td>[message, communication, agent, cooperative, l...</td>\n",
       "      <td>[emergence adversarial communication multiagen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1_agent_set_time_problem</td>\n",
       "      <td>agent, set, time, problem, algorithm</td>\n",
       "      <td>[agent, set, time, problem, algorithm, model, ...</td>\n",
       "      <td>[multiagent, strategy, agent, learning, oppone...</td>\n",
       "      <td>[agent, algorithm, task, node, value, fig, pre...</td>\n",
       "      <td>[entropy article improved approach towards mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2_defense_defender_network_attacker</td>\n",
       "      <td>defense, defender, network, attacker, security</td>\n",
       "      <td>[defense, defender, network, attacker, securit...</td>\n",
       "      <td>[attackdefense, reinforcement, multiagent, str...</td>\n",
       "      <td>[defender, security, game, strategy, ddpg, int...</td>\n",
       "      <td>[expert system application available online ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3_adversarial_attack_energy_subplay</td>\n",
       "      <td>adversarial, attack, energy, subplay, victim</td>\n",
       "      <td>[adversarial, attack, energy, subplay, victim,...</td>\n",
       "      <td>[adversarial, attack, multiagent, reinforcemen...</td>\n",
       "      <td>[adversarial, attack, training, defence, adver...</td>\n",
       "      <td>[applied energy available online august elsevi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4_ami_victim_influence_timestep</td>\n",
       "      <td>ami, victim, influence, timestep, attack</td>\n",
       "      <td>[ami, victim, influence, timestep, attack, tra...</td>\n",
       "      <td>[adversarial, attacking, reinforcement, attack...</td>\n",
       "      <td>[ami, timestep, attack, adversarial, adversary...</td>\n",
       "      <td>[model method trainingtime attack cooperative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5_attack_ihs_attacker_poisoning</td>\n",
       "      <td>attack, ihs, attacker, poisoning, reward</td>\n",
       "      <td>[attack, ihs, attacker, poisoning, reward, lea...</td>\n",
       "      <td>[reinforcement, agent, strategy, reward, bandi...</td>\n",
       "      <td>[attack, ihs, poisoning, reward, target, strat...</td>\n",
       "      <td>[rewardpoisoning attack offline multiagent rei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                   Name  \\\n",
       "0     -1      8          -1_cmarl_reward_attack_policy   \n",
       "1      0      7  0_message_communication_vector_neural   \n",
       "2      1      5               1_agent_set_time_problem   \n",
       "3      2      3    2_defense_defender_network_attacker   \n",
       "4      3      3    3_adversarial_attack_energy_subplay   \n",
       "5      4      2        4_ami_victim_influence_timestep   \n",
       "6      5      2        5_attack_ihs_attacker_poisoning   \n",
       "\n",
       "                                       CustomName  \\\n",
       "0           cmarl, reward, attack, policy, robust   \n",
       "1  message, communication, vector, neural, output   \n",
       "2            agent, set, time, problem, algorithm   \n",
       "3  defense, defender, network, attacker, security   \n",
       "4    adversarial, attack, energy, subplay, victim   \n",
       "5        ami, victim, influence, timestep, attack   \n",
       "6        attack, ihs, attacker, poisoning, reward   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [cmarl, reward, attack, policy, robust, rmaac,...   \n",
       "1  [message, communication, vector, neural, outpu...   \n",
       "2  [agent, set, time, problem, algorithm, model, ...   \n",
       "3  [defense, defender, network, attacker, securit...   \n",
       "4  [adversarial, attack, energy, subplay, victim,...   \n",
       "5  [ami, victim, influence, timestep, attack, tra...   \n",
       "6  [attack, ihs, attacker, poisoning, reward, lea...   \n",
       "\n",
       "                                             KeyBERT  \\\n",
       "0  [reinforcement, multiagent, agent, adversarial...   \n",
       "1  [multiagent, reinforcement, cooperative, agent...   \n",
       "2  [multiagent, strategy, agent, learning, oppone...   \n",
       "3  [attackdefense, reinforcement, multiagent, str...   \n",
       "4  [adversarial, attack, multiagent, reinforcemen...   \n",
       "5  [adversarial, attacking, reinforcement, attack...   \n",
       "6  [reinforcement, agent, strategy, reward, bandi...   \n",
       "\n",
       "                                                 MMR  \\\n",
       "0  [cmarl, rmaac, agent, adversary, trigger, rein...   \n",
       "1  [message, communication, agent, cooperative, l...   \n",
       "2  [agent, algorithm, task, node, value, fig, pre...   \n",
       "3  [defender, security, game, strategy, ddpg, int...   \n",
       "4  [adversarial, attack, training, defence, adver...   \n",
       "5  [ami, timestep, attack, adversarial, adversary...   \n",
       "6  [attack, ihs, poisoning, reward, target, strat...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [computer security content list available scie...  \n",
       "1  [emergence adversarial communication multiagen...  \n",
       "2  [entropy article improved approach towards mul...  \n",
       "3  [expert system application available online ma...  \n",
       "4  [applied energy available online august elsevi...  \n",
       "5  [model method trainingtime attack cooperative ...  \n",
       "6  [rewardpoisoning attack offline multiagent rei...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_df = topic_model.get_topic_info()\n",
    "topic_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Main': [('defense', np.float64(0.6020999085855888)),\n",
       "  ('defender', np.float64(0.49594668246594026)),\n",
       "  ('network', np.float64(0.4734844034813787)),\n",
       "  ('attacker', np.float64(0.45157013123164974)),\n",
       "  ('security', np.float64(0.40680688727886816)),\n",
       "  ('game', np.float64(0.4000879563415393)),\n",
       "  ('strategy', np.float64(0.38674017251154263)),\n",
       "  ('mobile', np.float64(0.3809066185318129)),\n",
       "  ('ddpg', np.float64(0.37136804015942737)),\n",
       "  ('learning', np.float64(0.37039295858061166))],\n",
       " 'KeyBERT': [('attackdefense', np.float32(0.40934637)),\n",
       "  ('reinforcement', np.float32(0.35304734)),\n",
       "  ('multiagent', np.float32(0.35108668)),\n",
       "  ('strategy', np.float32(0.3316946)),\n",
       "  ('defender', np.float32(0.31340653)),\n",
       "  ('security', np.float32(0.3043015)),\n",
       "  ('ddpg', np.float32(0.289494)),\n",
       "  ('attack', np.float32(0.28894868)),\n",
       "  ('hypergame', np.float32(0.28670627)),\n",
       "  ('agent', np.float32(0.2699737))],\n",
       " 'MMR': [('defender', np.float64(0.49594668246594026)),\n",
       "  ('security', np.float64(0.40680688727886816)),\n",
       "  ('game', np.float64(0.4000879563415393)),\n",
       "  ('strategy', np.float64(0.38674017251154263)),\n",
       "  ('ddpg', np.float64(0.37136804015942737)),\n",
       "  ('interference', np.float64(0.33730432911361186)),\n",
       "  ('reinforcement', np.float64(0.333925662317185)),\n",
       "  ('attack', np.float64(0.317511286557365)),\n",
       "  ('jamming', np.float64(0.2881251223946936)),\n",
       "  ('transmission', np.float64(0.2863274668976613))]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(2, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
